{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"AstroLab","text":"<p>AstroLab is a modern framework for astronomical machine learning workflows, built on PyTorch Lightning, Torch Geometric, and Pydantic.</p>"},{"location":"#features","title":"Features","text":"<ul> <li>State-of-the-art ML for astronomy</li> <li>PyTorch Lightning + Torch Geometric integration</li> <li>Pydantic-based configuration</li> <li>MLflow and Optuna integration</li> <li>Automatic API reference (API Reference)</li> <li>Modular, extensible, and production-ready</li> </ul>"},{"location":"#documentation","title":"Documentation","text":"<ul> <li>API Reference: See API Reference for all classes, functions, and configuration models.</li> <li>Configuration Examples: See the <code>configs/</code> directory.</li> <li>Example Workflows: See the <code>examples/</code> directory.</li> </ul> <p>For more information, visit the GitHub repository.</p>"},{"location":"api/","title":"API Reference","text":"<p>The API is organized by module. Each main module has its own page:</p> <ul> <li>astro_lab</li> <li>astro_lab.data</li> <li>astro_lab.models</li> <li>astro_lab.tensors</li> <li>astro_lab.training</li> <li>astro_lab.ui</li> <li>astro_lab.utils</li> <li>astro_lab.widgets</li> <li>astro_lab.cli</li> </ul> <p>Each page documents the respective module and all contained classes, functions, and submodules. </p>"},{"location":"api/astro_lab.cli/","title":"astro_lab.cli","text":""},{"location":"api/astro_lab.cli/#astro_lab.cli","title":"cli","text":""},{"location":"api/astro_lab.cli/#astro_lab.cli--astrolab-cli-command-line-interface","title":"AstroLab CLI - Command Line Interface","text":"<p>Provides command-line tools for training, evaluation, and visualization.</p> <p>Modules:</p> Name Description <code>download</code> <p>Download CLI for AstroLab - Thin wrapper around data download functions.</p> <code>optimize</code> <p>Optimization CLI module for AstroLab - Thin wrapper around training module.</p> <code>preprocess</code> <p>Preprocessing CLI for AstroLab - Thin wrapper around data preprocessing.</p> <code>train</code> <p>Training CLI module for AstroLab - Thin wrapper around training module.</p> <p>Functions:</p> Name Description <code>main</code> <p>Main entry point for the AstroLab CLI.</p>"},{"location":"api/astro_lab.cli/#astro_lab.cli.main","title":"main","text":"<pre><code>main(argv: Optional[List[str]] = None) -&gt; int\n</code></pre> <p>Main entry point for the AstroLab CLI.</p> <p>Parameters:</p> Name Type Description Default <code>argv</code> <code>Optional[List[str]]</code> <p>Command line arguments (defaults to sys.argv)</p> <code>None</code> <p>Returns:</p> Type Description <code>int</code> <p>Exit code (0 for success, non-zero for error)</p> Source code in <code>src\\astro_lab\\cli\\__init__.py</code> <pre><code>def main(argv: Optional[List[str]] = None) -&gt; int:\n    \"\"\"\n    Main entry point for the AstroLab CLI.\n\n    Args:\n        argv: Command line arguments (defaults to sys.argv)\n\n    Returns:\n        Exit code (0 for success, non-zero for error)\n    \"\"\"\n    parser = argparse.ArgumentParser(\n        prog=\"astro-lab\",\n        description=\"AstroLab: Modern Astronomical Machine Learning\",\n        formatter_class=argparse.RawDescriptionHelpFormatter,\n        epilog=\"\"\"\nAvailable commands:\n  train       Train models on astronomical data\n  download    Download survey data\n  optimize    Optimize model hyperparameters\n  preprocess  Preprocess survey data\n        \"\"\",\n    )\n\n    parser.add_argument(\n        \"command\",\n        choices=COMMANDS.keys(),\n        help=\"Command to run\",\n    )\n\n    parser.add_argument(\n        \"args\",\n        nargs=argparse.REMAINDER,\n        help=\"Arguments for the command\",\n    )\n\n    # Parse only the command\n    args = parser.parse_args(argv or sys.argv[1:2])\n\n    # Import and run the command module lazily\n    try:\n        import importlib\n        module = importlib.import_module(COMMANDS[args.command])\n\n        # Set argv for the subcommand\n        sys.argv = [f\"astro-lab {args.command}\"] + (argv or sys.argv)[2:]\n\n        # Run the command's main function\n        return module.main()\n\n    except KeyboardInterrupt:\n        print(\"\\nInterrupted by user\", file=sys.stderr)\n        return 130\n    except Exception as e:\n        print(f\"Error: {e}\", file=sys.stderr)\n        return 1\n</code></pre>"},{"location":"api/astro_lab.data/","title":"astro_lab.data","text":""},{"location":"api/astro_lab.data/#astro_lab.data","title":"data","text":"<p>AstroLab Data Module - High-Performance Astronomical Data Processing</p> <p>Modern data loading and processing for astronomical surveys using Polars, PyTorch, and specialized astronomical tensors. Perfect for prototyping and modern astronomical ML pipelines.</p> Quick Start <p>from astro_lab.data import load_survey_data dataset = load_survey_data(\"gaia\", max_samples=5000)  # Done!</p> <p>Modules:</p> Name Description <code>config</code> <p>AstroLab Data Configuration</p> <code>core</code> <p>AstroLab Data Core Module</p> <code>datamodule</code> <p>AstroLab DataModule - Lightning DataModule for Astronomical Data</p> <code>manager</code> <p>AstroLab Data Manager - Data Management</p> <code>preprocessing</code> <p>Data preprocessing utilities for astronomical surveys.</p> <code>processing</code> <p>AstroLab Data Processing - Advanced data processing with memory management</p> <code>transforms</code> <p>Astronomical Transform Classes for PyTorch Geometric</p> <code>utils</code> <p>Data utilities for astronomical surveys.</p> <p>Classes:</p> Name Description <code>AstroDataManager</code> <p>data manager with comprehensive memory management.</p> <code>AstroDataModule</code> <p>Clean Lightning DataModule for astronomical datasets.</p> <code>AstroDataset</code> <p>Minimal PyTorch Geometric dataset f\u00fcr astronomische Daten.</p> <code>DataConfig</code> <p>Centralized data configuration for AstroLab.</p> <p>Functions:</p> Name Description <code>check_astroquery_available</code> <p>Check if astroquery is available.</p> <code>create_astro_datamodule</code> <p>Create AstroDataModule for given survey.</p> <code>create_graph_from_dataframe</code> <p>Create PyTorch Geometric graph from DataFrame using configuration. \ud83d\udd78\ufe0f</p> <code>create_training_splits</code> <p>Create train/validation/test splits from DataFrame.</p> <code>detect_survey_type</code> <p>Detect survey type from dataset name or data structure.</p> <code>download_2mass</code> <p>Download 2MASS catalog.</p> <code>download_pan_starrs</code> <p>Download Pan-STARRS catalog.</p> <code>download_sdss</code> <p>Download SDSS catalog.</p> <code>download_survey</code> <p>Download any survey catalog.</p> <code>download_wise</code> <p>Download WISE catalog.</p> <code>get_data_statistics</code> <p>Get comprehensive statistics for a DataFrame.</p> <code>get_fits_info</code> <p>Get information about FITS file.</p> <code>get_survey_paths</code> <p>Get all standard paths for a survey.</p> <code>import_fits</code> <p>Import FITS catalog.</p> <code>import_tng50</code> <p>Convenience function to import a TNG50 HDF5 file.</p> <code>list_catalogs</code> <p>Convenience function to list available catalogs.</p> <code>load_catalog</code> <p>Convenience function to load a catalog.</p> <code>load_fits_optimized</code> <p>Load FITS file with memory optimization.</p> <code>load_fits_table_optimized</code> <p>Load FITS table with optimization.</p> <code>load_splits_from_parquet</code> <p>Load train/val/test splits from parquet files.</p> <code>load_survey_data</code> <p>Load survey data as an AstroDataset or SurveyTensor.</p> <code>load_tng50_data</code> <p>Load TNG50 simulation data as a survey-like dataset.</p> <code>load_tng50_temporal_data</code> <p>Load TNG50 temporal simulation data.</p> <code>preprocess_catalog</code> <p>Preprocess astronomical catalog data. \ud83d\udcca</p> <code>process_for_ml</code> <p>Convenience function to process a raw file for ML.</p> <code>process_survey</code> <p>Process a survey with automatic source file detection. \ud83d\udcca</p> <code>save_splits_to_parquet</code> <p>Save train/val/test splits to parquet files.</p>"},{"location":"api/astro_lab.data/#astro_lab.data.AstroDataManager","title":"AstroDataManager","text":"<p>data manager with comprehensive memory management.</p> <p>Handles loading, preprocessing, and managing astronomical survey data with automatic memory cleanup and optimization.</p> <p>Methods:</p> Name Description <code>cleanup_temp_files</code> <p>Clean up temporary files with memory management.</p> <code>convert_to_physical_units</code> <p>Convert Gaia catalog to physical units using astropy.</p> <code>create_dataset</code> <p>Create AstroDataset with memory management.</p> <code>download_gaia_catalog</code> <p>Download Gaia DR3 catalog to raw storage.</p> <code>download_survey_catalog</code> <p>Download survey catalog to raw storage. Generic function for all surveys.</p> <code>get_memory_stats</code> <p>Get current memory statistics.</p> <code>import_fits_catalog</code> <p>Import FITS catalog to raw storage.</p> <code>import_tng50_hdf5</code> <p>Import TNG50 simulation data from HDF5.</p> <code>list_catalogs</code> <p>List available catalogs, only survey-based for processed.</p> <code>load_catalog</code> <p>Load catalog with memory management.</p> <code>process_file</code> <p>Process a single file with memory management.</p> <code>process_for_ml</code> <p>Process raw catalog for ML training and save as {survey}.parquet in processed/{survey}/</p> <code>process_surveys</code> <p>Process multiple surveys with batch memory management.</p> <code>setup_directories</code> <p>Create standardized data directory structure using new config.</p> <p>Attributes:</p> Name Type Description <code>base_dir</code> <code>Path</code> <p>Get base data directory.</p> Source code in <code>src\\astro_lab\\data\\manager.py</code> <pre><code>class AstroDataManager:\n    \"\"\"\n    data manager with comprehensive memory management.\n\n    Handles loading, preprocessing, and managing astronomical survey data\n    with automatic memory cleanup and optimization.\n    \"\"\"\n\n    def __init__(self, config: Optional[Union[str, DataConfig]] = None, **kwargs):\n        \"\"\"Initialize data manager with memory management.\"\"\"\n        # Load configuration\n        if isinstance(config, str):\n            # Simple config loading without from_yaml\n            self.config = DataConfig(config)\n        elif isinstance(config, DataConfig):\n            self.config = config\n        else:\n            self.config = DataConfig(**kwargs)\n\n        # Set up paths\n        self.data_dir = Path(self.config.base_dir)\n        self._processed_dir = self.data_dir / \"processed\"\n        self._processed_dir.mkdir(parents=True, exist_ok=True)\n\n        logger.info(f\"\ud83d\udcc2 Data manager initialized: {self.data_dir}\")\n\n    def setup_directories(self):\n        \"\"\"Create standardized data directory structure using new config.\"\"\"\n        # Use new clean structure from config\n        self.config.setup_directories()\n\n    @property\n    def base_dir(self) -&gt; Path:\n        \"\"\"Get base data directory.\"\"\"\n        return self.data_dir\n\n    @property\n    def raw_dir(self) -&gt; Path:\n        return self.config.raw_dir\n\n    @property\n    def processed_dir(self) -&gt; Path:\n        return self.config.processed_dir\n\n    @property\n    def cache_dir(self) -&gt; Path:\n        return self.config.cache_dir\n\n    def download_gaia_catalog(\n        self,\n        magnitude_limit: float = 15.0,\n        region: str = \"lmc\",\n        max_sources: int = 1000000,\n    ) -&gt; Path:\n        \"\"\"Download Gaia DR3 catalog to raw storage.\"\"\"\n\n        # astroquery is now a required dependency\n\n        # WARNING: All-sky catalog is MASSIVE\n        if region == \"all_sky\":\n            estimated_size_tb = 10.0  # ~10 TB for complete catalog\n            print(f\"\u26a0\ufe0f  WARNING: All-sky Gaia DR3 catalog is ~{estimated_size_tb} TB!\")\n            print(\"   This would take days to download and require massive storage.\")\n            print(\"   Consider using regional catalogs instead.\")\n\n            response = input(\"Continue with all-sky download? (yes/no): \")\n            if response.lower() != \"yes\":\n                raise ValueError(\"All-sky download cancelled by user\")\n\n        # Ensure gaia directories exist\n        self.config.ensure_survey_directories(\"gaia\")\n\n        output_file = (\n            self.config.get_survey_raw_dir(\"gaia\")\n            / f\"gaia_dr3_{region}_mag{magnitude_limit:.1f}.parquet\"\n        )\n\n        if output_file.exists():\n            print(f\"\ud83d\udcc2 Gaia catalog exists: {output_file.name}\")\n            return output_file\n\n        print(f\"\ud83c\udf1f Downloading Gaia DR3: {region}, G &lt; {magnitude_limit}\")\n\n        # Region-specific queries\n        region_queries = {\n            \"lmc\": \"l BETWEEN 270 AND 290 AND b BETWEEN -35 AND -15\",\n            \"smc\": \"l BETWEEN 295 AND 315 AND b BETWEEN -50 AND -35\",\n            \"galactic_plane\": \"ABS(b) &lt; 10\",\n            \"galactic_poles\": \"ABS(b) &gt; 60\",\n            \"all_sky\": \"\",  # No spatial filter = entire sky\n            \"bright_all_sky\": \"\",  # Bright stars from entire sky (G &lt; 12)\n        }\n\n        # Special handling for bright all-sky catalog\n        if region == \"bright_all_sky\":\n            if magnitude_limit &gt; 12.0:\n                print(\n                    f\"\u26a0\ufe0f  Adjusting magnitude limit from {magnitude_limit} to 12.0 for bright all-sky catalog\"\n                )\n                magnitude_limit = 12.0\n            print(\n                f\"\ud83c\udf1f Downloading bright stars from entire sky (G &lt; {magnitude_limit})\"\n            )\n            print(\"   Expected: ~10-15 million stars, ~1 GB download\")\n\n        region_clause = (\n            f\"AND {region_queries.get(region, '')}\"\n            if region_queries.get(region)\n            else \"\"\n        )\n\n        query = f\"\"\"\n        SELECT \n            source_id, ra, dec, l, b,\n            parallax, parallax_error,\n            pmra, pmdec,\n            phot_g_mean_mag, phot_bp_mean_mag, phot_rp_mean_mag,\n            bp_rp, g_rp,\n            teff_gspphot, logg_gspphot\n        FROM gaiadr3.gaia_source \n        WHERE phot_g_mean_mag &lt; {magnitude_limit}\n        AND parallax IS NOT NULL \n        AND parallax &gt; 0\n        {region_clause}\n        \"\"\"\n\n        try:\n            job = Gaia.launch_job_async(query)\n            results = job.get_results()\n\n            if results is None or len(results) == 0:\n                raise ValueError(\"No Gaia data returned\")\n\n            # Convert to Polars with proper types\n            df = pl.DataFrame({col: np.array(results[col]) for col in results.colnames})\n\n            # Add derived columns with proper units (step by step to avoid dependency issues)\n            # First: Distance in parsecs (1000/parallax)\n            df = df.with_columns((1000.0 / pl.col(\"parallax\")).alias(\"distance_pc\"))\n\n            # Second: Add other derived columns that depend on distance_pc\n            df = df.with_columns(\n                [\n                    # Absolute magnitude: M = m + 5*log10(parallax) - 10\n                    (\n                        pl.col(\"phot_g_mean_mag\") + 5 * pl.col(\"parallax\").log10() - 10\n                    ).alias(\"abs_g_mag\"),\n                    # Proper motion in mas/yr -&gt; km/s (4.74 * mu * d)\n                    (4.74 * pl.col(\"pmra\") * pl.col(\"distance_pc\") / 1000.0).alias(\n                        \"vra_km_s\"\n                    ),\n                    (4.74 * pl.col(\"pmdec\") * pl.col(\"distance_pc\") / 1000.0).alias(\n                        \"vdec_km_s\"\n                    ),\n                    # Total proper motion\n                    (pl.col(\"pmra\").pow(2) + pl.col(\"pmdec\").pow(2))\n                    .sqrt()\n                    .alias(\"pm_total\"),\n                ]\n            )\n\n            # Save as compressed Parquet\n            df.write_parquet(output_file, compression=\"zstd\")\n\n            # Save metadata\n            metadata = {\n                \"source\": \"Gaia DR3\",\n                \"region\": region,\n                \"magnitude_limit\": magnitude_limit,\n                \"n_sources\": len(df),\n                \"columns\": df.columns,\n                \"file_size_mb\": output_file.stat().st_size / 1024**2,\n            }\n\n            metadata_file = output_file.with_suffix(\".json\")\n            with open(metadata_file, \"w\") as f:\n                json.dump(metadata, f, indent=2)\n\n            print(\n                f\"\u2705 Downloaded {len(df):,} Gaia sources ({metadata['file_size_mb']:.1f} MB)\"\n            )\n            return output_file\n\n        except Exception as e:\n            print(f\"\u274c Gaia download failed: {e}\")\n            raise\n\n    def import_fits_catalog(\n        self,\n        fits_file: Union[str, Path],\n        catalog_name: str,\n        hdu_index: int = 1,\n    ) -&gt; Path:\n        \"\"\"Import FITS catalog to raw storage.\"\"\"\n\n        fits_file = Path(fits_file)\n        if not fits_file.exists():\n            raise FileNotFoundError(f\"FITS file not found: {fits_file}\")\n\n        # Ensure sdss directories exist (fits -&gt; sdss)\n        self.config.ensure_survey_directories(\"sdss\")\n\n        output_file = self.config.get_survey_raw_dir(\"sdss\") / f\"{catalog_name}.parquet\"\n\n        if output_file.exists():\n            print(f\"\ud83d\udcc2 FITS catalog exists: {output_file.name}\")\n            return output_file\n\n        print(f\"\ud83d\udce5 Importing FITS: {fits_file.name} -&gt; {catalog_name}\")\n\n        try:\n            # Read FITS table\n            with fits.open(fits_file) as hdul:\n                table = Table(hdul[hdu_index].data)  # type: ignore\n\n            # Convert to Polars\n            df = pl.from_pandas(table.to_pandas())\n\n            # Save as compressed Parquet\n            df.write_parquet(output_file, compression=\"zstd\")\n\n            # Save metadata\n            metadata = {\n                \"source\": f\"FITS: {fits_file.name}\",\n                \"catalog_name\": catalog_name,\n                \"hdu_index\": hdu_index,\n                \"n_sources\": len(df),\n                \"columns\": df.columns,\n                \"file_size_mb\": output_file.stat().st_size / 1024**2,\n                \"original_size_mb\": fits_file.stat().st_size / 1024**2,\n            }\n\n            metadata_file = output_file.with_suffix(\".json\")\n            with open(metadata_file, \"w\") as f:\n                json.dump(metadata, f, indent=2)\n\n            print(\n                f\"\u2705 Imported {len(df):,} objects ({metadata['file_size_mb']:.1f} MB)\"\n            )\n            return output_file\n\n        except Exception as e:\n            print(f\"\u274c FITS import failed: {e}\")\n            raise\n\n    def import_tng50_hdf5(\n        self,\n        hdf5_file: Union[str, Path],\n        dataset_name: str = \"PartType0\",\n        max_particles: int = 1000000,\n    ) -&gt; Path:\n        \"\"\"Import TNG50 simulation data from HDF5.\"\"\"\n\n        hdf5_file = Path(hdf5_file)\n        if not hdf5_file.exists():\n            raise FileNotFoundError(f\"HDF5 file not found: {hdf5_file}\")\n\n        output_file = self.raw_dir / \"tng50\" / f\"tng50_{dataset_name.lower()}.parquet\"\n        output_file.parent.mkdir(exist_ok=True)\n\n        if output_file.exists():\n            print(f\"\ud83d\udcc2 TNG50 catalog exists: {output_file.name}\")\n            return output_file\n\n        print(f\"\ud83c\udf0c Importing TNG50: {hdf5_file.name} -&gt; {dataset_name}\")\n\n        try:\n            with h5py.File(hdf5_file, \"r\") as f:  # type: ignore\n                # Get particle data\n                if dataset_name not in f:  # type: ignore\n                    available = list(f.keys())  # type: ignore\n                    raise ValueError(\n                        f\"Dataset {dataset_name} not found. Available: {available}\"\n                    )\n\n                group = f[dataset_name]  # type: ignore\n\n                # Common TNG50 fields\n                data_dict = {}\n\n                # Coordinates (essential)\n                if \"Coordinates\" in group:  # type: ignore\n                    coords = np.array(group[\"Coordinates\"][:])  # type: ignore\n                    if len(coords) &gt; max_particles:  # type: ignore\n                        indices = np.random.choice(  # type: ignore\n                            len(coords),  # type: ignore\n                            max_particles,\n                            replace=False,\n                        )\n                        coords = coords[indices]  # type: ignore\n                    else:\n                        indices = slice(None)\n\n                    data_dict[\"x\"] = coords[:, 0]  # type: ignore\n                    data_dict[\"y\"] = coords[:, 1]  # type: ignore\n                    data_dict[\"z\"] = coords[:, 2]  # type: ignore\n\n                # Other common fields\n                for field in [\n                    \"Masses\",\n                    \"Velocities\",\n                    \"Density\",\n                    \"Temperature\",\n                    \"Metallicity\",\n                ]:\n                    if field in group:  # type: ignore\n                        data = np.array(group[field][:])  # type: ignore\n                        if indices is not slice(None):\n                            data = data[indices]  # type: ignore\n\n                        # Sanitize field name for column\n                        col_name = field.lower()\n                        if col_name.endswith(\"es\"):\n                            col_name = col_name[:-2]\n                        elif col_name.endswith(\"s\"):\n                            col_name = col_name[:-1]\n\n                        if data.ndim &gt; 1:  # type: ignore\n                            # Vector quantities\n                            for i in range(data.shape[1]):  # type: ignore\n                                data_dict[f\"{col_name}_{i}\"] = data[:, i]  # type: ignore\n                        else:\n                            data_dict[col_name] = data  # type: ignore\n\n                # Convert to Polars\n                df = pl.DataFrame(data_dict)\n\n                # Save as compressed Parquet\n                df.write_parquet(output_file, compression=\"zstd\")\n\n                # Save metadata\n                metadata = {\n                    \"source\": f\"TNG50: {hdf5_file.name}\",\n                    \"dataset_name\": dataset_name,\n                    \"n_particles\": len(df),\n                    \"max_particles\": max_particles,\n                    \"columns\": df.columns,\n                    \"file_size_mb\": output_file.stat().st_size / 1024**2,\n                    \"original_size_mb\": hdf5_file.stat().st_size / 1024**2,\n                }\n\n                metadata_file = output_file.with_suffix(\".json\")\n                with open(metadata_file, \"w\") as f:\n                    json.dump(metadata, f, indent=2)\n\n                print(\n                    f\"\u2705 Imported {len(df):,} particles ({metadata['file_size_mb']:.1f} MB)\"\n                )\n                return output_file\n\n        except Exception as e:\n            print(f\"\u274c TNG50 import failed: {e}\")\n            raise\n\n    def process_for_ml(\n        self,\n        raw_file: Union[str, Path],\n        survey: Optional[str] = None,\n        filters: Optional[Dict] = None,\n    ) -&gt; Path:\n        \"\"\"Process raw catalog for ML training and save as {survey}.parquet in processed/{survey}/\"\"\"\n        raw_file = Path(raw_file)\n        if survey is None:\n            # Survey must be specified explicitly!\n            raise ValueError(\"survey must be specified for ML-Processing\")\n        processed_dir = self.processed_dir / survey\n        processed_dir.mkdir(parents=True, exist_ok=True)\n        output_path = processed_dir / f\"{survey}.parquet\"\n        print(f\"\\U0001f504 Processing {raw_file.name} for ML as {output_path} ...\")\n        df = pl.read_parquet(raw_file)\n        if filters:\n            for col, (min_val, max_val) in filters.items():\n                if col in df.columns:\n                    df = df.filter(pl.col(col).is_between(min_val, max_val))\n        critical_cols = [\"ra\", \"dec\"]\n        if \"phot_g_mean_mag\" in df.columns:\n            critical_cols.append(\"phot_g_mean_mag\")\n        elif \"psfMag_r\" in df.columns:\n            critical_cols.append(\"psfMag_r\")\n        df = df.drop_nulls(subset=critical_cols)\n        df = df.with_columns(\n            [\n                (pl.col(\"ra\") / 360.0).alias(\"ra_norm\"),\n                ((pl.col(\"dec\") + 90) / 180.0).alias(\"dec_norm\"),\n            ]\n        )\n        df.write_parquet(output_path)\n        metadata = {\n            \"source_file\": str(raw_file),\n            \"processing_date\": datetime.datetime.now().isoformat(),\n            \"filters_applied\": filters or {},\n            \"n_sources_input\": len(pl.read_parquet(raw_file)),\n            \"n_sources_output\": len(df),\n            \"columns\": df.columns,\n        }\n        metadata_file = output_path.with_suffix(\".json\")\n        with open(metadata_file, \"w\") as f:\n            json.dump(metadata, f, indent=2)\n        print(f\"\\U00002705 Processed {len(df):,} sources for ML: {output_path}\")\n        return output_path\n\n    def list_catalogs(self, data_type: str = \"all\") -&gt; pl.DataFrame:\n        \"\"\"List available catalogs, only survey-based for processed.\"\"\"\n        catalogs = []\n        if data_type in [\"all\", \"raw\"]:\n            for parquet_file in self.raw_dir.rglob(\"*.parquet\"):\n                metadata_file = parquet_file.with_suffix(\".json\")\n                if metadata_file.exists():\n                    with open(metadata_file) as f:\n                        metadata = json.load(f)\n                else:\n                    metadata = {\"source\": \"Unknown\", \"n_sources\": 0}\n                catalogs.append(\n                    {\n                        \"name\": parquet_file.name,\n                        \"type\": \"raw\",\n                        \"source\": metadata.get(\"source\", \"Unknown\"),\n                        \"n_sources\": metadata.get(\"n_sources\", 0),\n                        \"size_mb\": parquet_file.stat().st_size / 1024**2,\n                        \"path\": str(parquet_file),\n                    }\n                )\n        if data_type in [\"all\", \"processed\"]:\n            # Only show survey-based\n            for survey_dir in (self.processed_dir).iterdir():\n                if survey_dir.is_dir():\n                    parquet_file = survey_dir / f\"{survey_dir.name}.parquet\"\n                    if parquet_file.exists():\n                        metadata_file = parquet_file.with_suffix(\".json\")\n                        if metadata_file.exists():\n                            with open(metadata_file) as f:\n                                metadata = json.load(f)\n                        else:\n                            metadata = {\"source\": \"Unknown\", \"n_sources\": 0}\n                        catalogs.append(\n                            {\n                                \"name\": parquet_file.name,\n                                \"type\": \"processed\",\n                                \"source\": metadata.get(\"source_file\", \"Unknown\"),\n                                \"n_sources\": metadata.get(\"n_sources_output\", 0),\n                                \"size_mb\": parquet_file.stat().st_size / 1024**2,\n                                \"path\": str(parquet_file),\n                            }\n                        )\n        if not catalogs:\n            return pl.DataFrame(\n                {\n                    \"name\": [],\n                    \"type\": [],\n                    \"source\": [],\n                    \"n_sources\": [],\n                    \"size_mb\": [],\n                    \"path\": [],\n                }\n            )\n        return pl.DataFrame(catalogs).sort(\"size_mb\", descending=True)\n\n    def load_catalog(self, catalog_path: Union[str, Path]) -&gt; pl.DataFrame:\n        \"\"\"Load catalog with memory management.\"\"\"\n        catalog_path = Path(catalog_path)\n\n        logger.info(f\"\ud83d\udcc2 Loading catalog: {catalog_path}\")\n\n        if catalog_path.suffix.lower() in [\".fits\", \".fit\"]:\n            data = load_fits_optimized(catalog_path)\n            # Ensure we return a Polars DataFrame\n            if data is None:\n                raise ValueError(f\"Could not load FITS file: {catalog_path}\")\n            if isinstance(data, pl.DataFrame):\n                return data\n            else:\n                # Convert to Polars DataFrame\n                import pandas as pd\n\n                # Handle different data types\n                if hasattr(data, \"__array__\"):\n                    # numpy arrays and similar\n                    df_pandas = pd.DataFrame(data)\n                elif hasattr(data, \"to_pandas\"):\n                    # astropy tables and similar\n                    df_pandas = data.to_pandas()\n                else:\n                    # fallback\n                    df_pandas = pd.DataFrame([data])\n                return pl.from_pandas(df_pandas)\n        elif catalog_path.suffix.lower() in [\".parquet\", \".pq\"]:\n            data = pl.read_parquet(catalog_path)\n        elif catalog_path.suffix.lower() == \".csv\":\n            data = pl.read_csv(catalog_path)\n        else:\n            raise ValueError(f\"Unsupported format: {catalog_path.suffix}\")\n\n        logger.info(f\"\u2705 Catalog loaded: {len(data)} rows, {len(data.columns)} columns\")\n        return data\n\n    def convert_to_physical_units(self, df: pl.DataFrame) -&gt; pl.DataFrame:\n        \"\"\"\n        Convert Gaia catalog to physical units using astropy.\n\n        Parameters\n        ----------\n        df : pl.DataFrame\n            Gaia catalog with parallax, proper motions, etc.\n\n        Returns\n        -------\n        pl.DataFrame\n            Catalog with additional physical unit columns\n        \"\"\"\n        # astropy is now a required dependency\n\n        # Add physical unit conversions using astropy constants\n        df_with_units = df.with_columns(\n            [\n                # Distance: parallax (mas) -&gt; distance (pc)\n                # Using astropy: 1 / (parallax * u.mas).to(u.arcsec) * u.pc\n                (1000.0 / pl.col(\"parallax\")).alias(\"distance_pc\"),\n                # Absolute magnitude: M = m + 5*log10(\u03c0) - 10\n                (pl.col(\"phot_g_mean_mag\") + 5 * pl.col(\"parallax\").log10() - 10).alias(\n                    \"abs_g_mag\"\n                ),\n                # Tangential velocity using astropy conversion factor\n                # v_tan = 4.74047 * \u03bc * d (exact astropy constant)\n                (\n                    4.74047 * pl.col(\"pmra\") * (1000.0 / pl.col(\"parallax\")) / 1000.0\n                ).alias(\"vra_km_s\"),\n                (\n                    4.74047 * pl.col(\"pmdec\") * (1000.0 / pl.col(\"parallax\")) / 1000.0\n                ).alias(\"vdec_km_s\"),\n                # Total proper motion (mas/yr) and tangential velocity (km/s)\n                (pl.col(\"pmra\").pow(2) + pl.col(\"pmdec\").pow(2))\n                .sqrt()\n                .alias(\"pm_total_mas_yr\"),\n                (\n                    4.74047\n                    * (pl.col(\"pmra\").pow(2) + pl.col(\"pmdec\").pow(2)).sqrt()\n                    * (1000.0 / pl.col(\"parallax\"))\n                    / 1000.0\n                ).alias(\"v_tan_km_s\"),\n                # Galactic coordinates with explicit units\n                pl.col(\"l\").alias(\"gal_lon_deg\"),\n                pl.col(\"b\").alias(\"gal_lat_deg\"),\n            ]\n        )\n\n        print(\"\u2705 Converted to physical units using astropy constants\")\n\n        # Add physical unit conversions\n        df_with_units = df.with_columns(\n            [\n                # Distance: parallax (mas) -&gt; distance (pc)\n                (1000.0 / pl.col(\"parallax\")).alias(\"distance_pc\"),\n                # Absolute magnitude: apparent + distance modulus\n                (pl.col(\"phot_g_mean_mag\") + 5 * pl.col(\"parallax\").log10() - 10).alias(\n                    \"abs_g_mag\"\n                ),\n                # Tangential velocity: proper motion (mas/yr) -&gt; velocity (km/s)\n                # v_tan = 4.74 * \u03bc * d, where \u03bc in mas/yr, d in pc\n                (4.74 * pl.col(\"pmra\") * (1000.0 / pl.col(\"parallax\")) / 1000.0).alias(\n                    \"vra_km_s\"\n                ),\n                (4.74 * pl.col(\"pmdec\") * (1000.0 / pl.col(\"parallax\")) / 1000.0).alias(\n                    \"vdec_km_s\"\n                ),\n                # Total proper motion and tangential velocity\n                (pl.col(\"pmra\").pow(2) + pl.col(\"pmdec\").pow(2))\n                .sqrt()\n                .alias(\"pm_total_mas_yr\"),\n                # Galactic coordinates already in degrees\n                pl.col(\"l\").alias(\"gal_lon_deg\"),\n                pl.col(\"b\").alias(\"gal_lat_deg\"),\n            ]\n        )\n\n        return df_with_units\n\n    def process_file(self, file_path: Union[str, Path]) -&gt; Dict[str, Any]:\n        \"\"\"\n        Process a single file with memory management.\n\n        Args:\n            file_path: Path to the file to process\n\n        Returns:\n            Processing results\n        \"\"\"\n        file_path = Path(file_path)\n\n        logger.info(f\"\ud83d\udcc2 Processing file: {file_path}\")\n\n        # Load data with memory optimization\n        if file_path.suffix.lower() in [\".fits\", \".fit\"]:\n            data = load_fits_optimized(file_path)\n        elif file_path.suffix.lower() in [\".parquet\", \".pq\"]:\n            data = pl.read_parquet(file_path)\n        elif file_path.suffix.lower() == \".csv\":\n            data = pl.read_csv(file_path)\n        else:\n            raise ValueError(f\"Unsupported file format: {file_path.suffix}\")\n\n        # Preprocess data\n        # Detect survey type from filename or use generic\n        survey_type = \"generic\"\n        if \"gaia\" in file_path.name.lower():\n            survey_type = \"gaia\"\n        elif \"sdss\" in file_path.name.lower():\n            survey_type = \"sdss\"\n        elif \"nsa\" in file_path.name.lower():\n            survey_type = \"nsa\"\n        elif \"linear\" in file_path.name.lower():\n            survey_type = \"linear\"\n        elif \"tng\" in file_path.name.lower():\n            survey_type = \"tng50\"\n\n        lf_processed = preprocess_catalog_lazy(\n            data, survey_type=survey_type, use_streaming=True\n        )\n        processed_data = lf_processed.collect()\n\n        # Save processed data\n        output_path = self.processed_dir / f\"{file_path.stem}.parquet\"\n        processed_data.write_parquet(output_path)\n\n        results = {\n            \"input_file\": str(file_path),\n            \"output_file\": str(output_path),\n            \"num_rows\": len(processed_data),\n            \"num_columns\": len(processed_data.columns),\n        }\n\n        logger.info(f\"\u2705 File processed: {len(processed_data)} rows\")\n        return results\n\n    def process_surveys(self, surveys: Optional[List[str]] = None) -&gt; Dict[str, Any]:\n        \"\"\"\n        Process multiple surveys with batch memory management.\n\n        Args:\n            surveys: List of survey names to process\n\n        Returns:\n            Batch processing results\n        \"\"\"\n        # Default surveys if none specified\n        if surveys is None:\n            surveys = [\"gaia\", \"sdss\", \"nsa\", \"linear\", \"tng50\"]\n\n        logger.info(f\"\ud83d\udcca Processing {len(surveys)} surveys\")\n\n        results = {\n            \"surveys_processed\": [],\n            \"total_rows\": 0,\n        }\n\n        for survey_name in surveys:\n            try:\n                survey_result = self._process_single_survey(survey_name)\n                results[\"surveys_processed\"].append(survey_result)\n                results[\"total_rows\"] += survey_result.get(\"num_rows\", 0)\n\n                logger.info(f\"\u2705 Survey {survey_name} processed successfully\")\n\n            except Exception as e:\n                logger.error(f\"\u274c Failed to process survey {survey_name}: {e}\")\n                results[\"surveys_processed\"].append(\n                    {\"survey\": survey_name, \"error\": str(e), \"status\": \"failed\"}\n                )\n\n        logger.info(\n            f\"\ud83d\udcca Batch processing completed: {results['total_rows']} total rows\"\n        )\n        return results\n\n    def _process_single_survey(self, survey_name: str) -&gt; Dict[str, Any]:\n        \"\"\"Process a single survey with memory management.\"\"\"\n        # Find survey data files\n        survey_files = list(self.data_dir.glob(f\"*{survey_name}*\"))\n        if not survey_files:\n            raise FileNotFoundError(f\"No files found for survey: {survey_name}\")\n\n        # Process each file\n        survey_results = []\n        total_rows = 0\n\n        for file_path in survey_files:\n            if file_path.suffix.lower() in [\".fits\", \".parquet\", \".csv\"]:\n                file_result = self.process_file(file_path)\n                survey_results.append(file_result)\n                total_rows += file_result.get(\"num_rows\", 0)\n\n        return {\n            \"survey\": survey_name,\n            \"files_processed\": len(survey_results),\n            \"num_rows\": total_rows,\n            \"files\": survey_results,\n            \"status\": \"completed\",\n        }\n\n    def create_dataset(\n        self, survey_name: str, force_reload: bool = False\n    ) -&gt; AstroDataset:\n        \"\"\"\n        Create AstroDataset with memory management.\n\n        Args:\n            survey_name: Name of the survey\n            force_reload: Whether to force reload data\n\n        Returns:\n            AstroDataset instance\n        \"\"\"\n        logger.info(f\"\ud83d\udd04 Creating dataset for survey: {survey_name}\")\n\n        dataset = AstroDataset(\n            root=str(self.processed_dir),\n            survey=survey_name,\n        )\n\n        logger.info(f\"\u2705 Dataset created: {len(dataset)} samples\")\n        return dataset\n\n    def get_memory_stats(self) -&gt; Dict[str, Any]:\n        \"\"\"Get current memory statistics.\"\"\"\n        current_memory = psutil.Process().memory_info().rss / 1024 / 1024\n\n        stats = {\n            \"system_memory_mb\": current_memory,\n            \"data_dir\": str(self.data_dir),\n            \"processed_dir\": str(self.processed_dir),\n        }\n\n        return stats\n\n    def cleanup_temp_files(self):\n        \"\"\"Clean up temporary files with memory management.\"\"\"\n        temp_patterns = [\"*.tmp\", \"*.temp\", \"*_temp_*\"]\n        cleaned_files = 0\n\n        for pattern in temp_patterns:\n            for temp_file in self.data_dir.rglob(pattern):\n                try:\n                    temp_file.unlink()\n                    cleaned_files += 1\n                except Exception as e:\n                    logger.warning(f\"Failed to delete {temp_file}: {e}\")\n\n        logger.info(f\"\ud83e\uddf9 Cleaned up {cleaned_files} temporary files\")\n\n    def download_survey_catalog(\n        self,\n        survey: str,\n        magnitude_limit: float = 15.0,\n        region: str = \"all_sky\",\n        max_sources: int = 1000000,\n    ) -&gt; Path:\n        \"\"\"Download survey catalog to raw storage. Generic function for all surveys.\"\"\"\n\n        # Get survey configuration\n        from astro_lab.utils.config.surveys import get_survey_config\n\n        survey_config = get_survey_config(survey)\n\n        # Ensure survey directories exist\n        self.config.ensure_survey_directories(survey)\n\n        output_file = (\n            self.config.get_survey_raw_dir(survey)\n            / f\"{survey}_{region}_mag{magnitude_limit:.1f}.parquet\"\n        )\n\n        if output_file.exists():\n            print(f\"\ud83d\udcc2 {survey} catalog exists: {output_file.name}\")\n            return output_file\n\n        print(f\"\ud83c\udf1f Downloading {survey}: {region}, magnitude limit &lt; {magnitude_limit}\")\n\n        # Survey-specific download logic\n        if survey.lower() == \"gaia\":\n            return self.download_gaia_catalog(magnitude_limit, region, max_sources)\n        elif survey.lower() == \"sdss\":\n            return self._download_sdss_catalog(magnitude_limit, region, max_sources)\n        elif survey.lower() == \"2mass\":\n            return self._download_2mass_catalog(magnitude_limit, region, max_sources)\n        elif survey.lower() == \"wise\":\n            return self._download_wise_catalog(magnitude_limit, region, max_sources)\n        elif survey.lower() == \"pan_starrs\":\n            return self._download_pan_starrs_catalog(\n                magnitude_limit, region, max_sources\n            )\n        else:\n            raise ValueError(f\"Download not implemented for survey: {survey}\")\n\n    def _download_sdss_catalog(\n        self,\n        magnitude_limit: float = 15.0,\n        region: str = \"all_sky\",\n        max_sources: int = 1000000,\n    ) -&gt; Path:\n        \"\"\"Download SDSS catalog.\"\"\"\n        # Placeholder for SDSS download\n        print(\"\u26a0\ufe0f SDSS download not yet implemented\")\n        raise NotImplementedError(\"SDSS download not yet implemented\")\n\n    def _download_2mass_catalog(\n        self,\n        magnitude_limit: float = 15.0,\n        region: str = \"all_sky\",\n        max_sources: int = 1000000,\n    ) -&gt; Path:\n        \"\"\"Download 2MASS catalog.\"\"\"\n        # Placeholder for 2MASS download\n        print(\"\u26a0\ufe0f 2MASS download not yet implemented\")\n        raise NotImplementedError(\"2MASS download not yet implemented\")\n\n    def _download_wise_catalog(\n        self,\n        magnitude_limit: float = 15.0,\n        region: str = \"all_sky\",\n        max_sources: int = 1000000,\n    ) -&gt; Path:\n        \"\"\"Download WISE catalog.\"\"\"\n        # Placeholder for WISE download\n        print(\"\u26a0\ufe0f WISE download not yet implemented\")\n        raise NotImplementedError(\"WISE download not yet implemented\")\n\n    def _download_pan_starrs_catalog(\n        self,\n        magnitude_limit: float = 15.0,\n        region: str = \"all_sky\",\n        max_sources: int = 1000000,\n    ) -&gt; Path:\n        \"\"\"Download Pan-STARRS catalog.\"\"\"\n        # Placeholder for Pan-STARRS download\n        print(\"\u26a0\ufe0f Pan-STARRS download not yet implemented\")\n        raise NotImplementedError(\"Pan-STARRS download not yet implemented\")\n</code></pre>"},{"location":"api/astro_lab.data/#astro_lab.data.AstroDataManager.base_dir","title":"base_dir  <code>property</code>","text":"<pre><code>base_dir: Path\n</code></pre> <p>Get base data directory.</p>"},{"location":"api/astro_lab.data/#astro_lab.data.AstroDataManager.cleanup_temp_files","title":"cleanup_temp_files","text":"<pre><code>cleanup_temp_files()\n</code></pre> <p>Clean up temporary files with memory management.</p> Source code in <code>src\\astro_lab\\data\\manager.py</code> <pre><code>def cleanup_temp_files(self):\n    \"\"\"Clean up temporary files with memory management.\"\"\"\n    temp_patterns = [\"*.tmp\", \"*.temp\", \"*_temp_*\"]\n    cleaned_files = 0\n\n    for pattern in temp_patterns:\n        for temp_file in self.data_dir.rglob(pattern):\n            try:\n                temp_file.unlink()\n                cleaned_files += 1\n            except Exception as e:\n                logger.warning(f\"Failed to delete {temp_file}: {e}\")\n\n    logger.info(f\"\ud83e\uddf9 Cleaned up {cleaned_files} temporary files\")\n</code></pre>"},{"location":"api/astro_lab.data/#astro_lab.data.AstroDataManager.convert_to_physical_units","title":"convert_to_physical_units","text":"<pre><code>convert_to_physical_units(df: DataFrame) -&gt; DataFrame\n</code></pre> <p>Convert Gaia catalog to physical units using astropy.</p>"},{"location":"api/astro_lab.data/#astro_lab.data.AstroDataManager.convert_to_physical_units--parameters","title":"Parameters","text":"<p>df : pl.DataFrame     Gaia catalog with parallax, proper motions, etc.</p>"},{"location":"api/astro_lab.data/#astro_lab.data.AstroDataManager.convert_to_physical_units--returns","title":"Returns","text":"<p>pl.DataFrame     Catalog with additional physical unit columns</p> Source code in <code>src\\astro_lab\\data\\manager.py</code> <pre><code>def convert_to_physical_units(self, df: pl.DataFrame) -&gt; pl.DataFrame:\n    \"\"\"\n    Convert Gaia catalog to physical units using astropy.\n\n    Parameters\n    ----------\n    df : pl.DataFrame\n        Gaia catalog with parallax, proper motions, etc.\n\n    Returns\n    -------\n    pl.DataFrame\n        Catalog with additional physical unit columns\n    \"\"\"\n    # astropy is now a required dependency\n\n    # Add physical unit conversions using astropy constants\n    df_with_units = df.with_columns(\n        [\n            # Distance: parallax (mas) -&gt; distance (pc)\n            # Using astropy: 1 / (parallax * u.mas).to(u.arcsec) * u.pc\n            (1000.0 / pl.col(\"parallax\")).alias(\"distance_pc\"),\n            # Absolute magnitude: M = m + 5*log10(\u03c0) - 10\n            (pl.col(\"phot_g_mean_mag\") + 5 * pl.col(\"parallax\").log10() - 10).alias(\n                \"abs_g_mag\"\n            ),\n            # Tangential velocity using astropy conversion factor\n            # v_tan = 4.74047 * \u03bc * d (exact astropy constant)\n            (\n                4.74047 * pl.col(\"pmra\") * (1000.0 / pl.col(\"parallax\")) / 1000.0\n            ).alias(\"vra_km_s\"),\n            (\n                4.74047 * pl.col(\"pmdec\") * (1000.0 / pl.col(\"parallax\")) / 1000.0\n            ).alias(\"vdec_km_s\"),\n            # Total proper motion (mas/yr) and tangential velocity (km/s)\n            (pl.col(\"pmra\").pow(2) + pl.col(\"pmdec\").pow(2))\n            .sqrt()\n            .alias(\"pm_total_mas_yr\"),\n            (\n                4.74047\n                * (pl.col(\"pmra\").pow(2) + pl.col(\"pmdec\").pow(2)).sqrt()\n                * (1000.0 / pl.col(\"parallax\"))\n                / 1000.0\n            ).alias(\"v_tan_km_s\"),\n            # Galactic coordinates with explicit units\n            pl.col(\"l\").alias(\"gal_lon_deg\"),\n            pl.col(\"b\").alias(\"gal_lat_deg\"),\n        ]\n    )\n\n    print(\"\u2705 Converted to physical units using astropy constants\")\n\n    # Add physical unit conversions\n    df_with_units = df.with_columns(\n        [\n            # Distance: parallax (mas) -&gt; distance (pc)\n            (1000.0 / pl.col(\"parallax\")).alias(\"distance_pc\"),\n            # Absolute magnitude: apparent + distance modulus\n            (pl.col(\"phot_g_mean_mag\") + 5 * pl.col(\"parallax\").log10() - 10).alias(\n                \"abs_g_mag\"\n            ),\n            # Tangential velocity: proper motion (mas/yr) -&gt; velocity (km/s)\n            # v_tan = 4.74 * \u03bc * d, where \u03bc in mas/yr, d in pc\n            (4.74 * pl.col(\"pmra\") * (1000.0 / pl.col(\"parallax\")) / 1000.0).alias(\n                \"vra_km_s\"\n            ),\n            (4.74 * pl.col(\"pmdec\") * (1000.0 / pl.col(\"parallax\")) / 1000.0).alias(\n                \"vdec_km_s\"\n            ),\n            # Total proper motion and tangential velocity\n            (pl.col(\"pmra\").pow(2) + pl.col(\"pmdec\").pow(2))\n            .sqrt()\n            .alias(\"pm_total_mas_yr\"),\n            # Galactic coordinates already in degrees\n            pl.col(\"l\").alias(\"gal_lon_deg\"),\n            pl.col(\"b\").alias(\"gal_lat_deg\"),\n        ]\n    )\n\n    return df_with_units\n</code></pre>"},{"location":"api/astro_lab.data/#astro_lab.data.AstroDataManager.create_dataset","title":"create_dataset","text":"<pre><code>create_dataset(survey_name: str, force_reload: bool = False) -&gt; AstroDataset\n</code></pre> <p>Create AstroDataset with memory management.</p> <p>Parameters:</p> Name Type Description Default <code>survey_name</code> <code>str</code> <p>Name of the survey</p> required <code>force_reload</code> <code>bool</code> <p>Whether to force reload data</p> <code>False</code> <p>Returns:</p> Type Description <code>AstroDataset</code> <p>AstroDataset instance</p> Source code in <code>src\\astro_lab\\data\\manager.py</code> <pre><code>def create_dataset(\n    self, survey_name: str, force_reload: bool = False\n) -&gt; AstroDataset:\n    \"\"\"\n    Create AstroDataset with memory management.\n\n    Args:\n        survey_name: Name of the survey\n        force_reload: Whether to force reload data\n\n    Returns:\n        AstroDataset instance\n    \"\"\"\n    logger.info(f\"\ud83d\udd04 Creating dataset for survey: {survey_name}\")\n\n    dataset = AstroDataset(\n        root=str(self.processed_dir),\n        survey=survey_name,\n    )\n\n    logger.info(f\"\u2705 Dataset created: {len(dataset)} samples\")\n    return dataset\n</code></pre>"},{"location":"api/astro_lab.data/#astro_lab.data.AstroDataManager.download_gaia_catalog","title":"download_gaia_catalog","text":"<pre><code>download_gaia_catalog(\n    magnitude_limit: float = 15.0, region: str = \"lmc\", max_sources: int = 1000000\n) -&gt; Path\n</code></pre> <p>Download Gaia DR3 catalog to raw storage.</p> Source code in <code>src\\astro_lab\\data\\manager.py</code> <pre><code>def download_gaia_catalog(\n    self,\n    magnitude_limit: float = 15.0,\n    region: str = \"lmc\",\n    max_sources: int = 1000000,\n) -&gt; Path:\n    \"\"\"Download Gaia DR3 catalog to raw storage.\"\"\"\n\n    # astroquery is now a required dependency\n\n    # WARNING: All-sky catalog is MASSIVE\n    if region == \"all_sky\":\n        estimated_size_tb = 10.0  # ~10 TB for complete catalog\n        print(f\"\u26a0\ufe0f  WARNING: All-sky Gaia DR3 catalog is ~{estimated_size_tb} TB!\")\n        print(\"   This would take days to download and require massive storage.\")\n        print(\"   Consider using regional catalogs instead.\")\n\n        response = input(\"Continue with all-sky download? (yes/no): \")\n        if response.lower() != \"yes\":\n            raise ValueError(\"All-sky download cancelled by user\")\n\n    # Ensure gaia directories exist\n    self.config.ensure_survey_directories(\"gaia\")\n\n    output_file = (\n        self.config.get_survey_raw_dir(\"gaia\")\n        / f\"gaia_dr3_{region}_mag{magnitude_limit:.1f}.parquet\"\n    )\n\n    if output_file.exists():\n        print(f\"\ud83d\udcc2 Gaia catalog exists: {output_file.name}\")\n        return output_file\n\n    print(f\"\ud83c\udf1f Downloading Gaia DR3: {region}, G &lt; {magnitude_limit}\")\n\n    # Region-specific queries\n    region_queries = {\n        \"lmc\": \"l BETWEEN 270 AND 290 AND b BETWEEN -35 AND -15\",\n        \"smc\": \"l BETWEEN 295 AND 315 AND b BETWEEN -50 AND -35\",\n        \"galactic_plane\": \"ABS(b) &lt; 10\",\n        \"galactic_poles\": \"ABS(b) &gt; 60\",\n        \"all_sky\": \"\",  # No spatial filter = entire sky\n        \"bright_all_sky\": \"\",  # Bright stars from entire sky (G &lt; 12)\n    }\n\n    # Special handling for bright all-sky catalog\n    if region == \"bright_all_sky\":\n        if magnitude_limit &gt; 12.0:\n            print(\n                f\"\u26a0\ufe0f  Adjusting magnitude limit from {magnitude_limit} to 12.0 for bright all-sky catalog\"\n            )\n            magnitude_limit = 12.0\n        print(\n            f\"\ud83c\udf1f Downloading bright stars from entire sky (G &lt; {magnitude_limit})\"\n        )\n        print(\"   Expected: ~10-15 million stars, ~1 GB download\")\n\n    region_clause = (\n        f\"AND {region_queries.get(region, '')}\"\n        if region_queries.get(region)\n        else \"\"\n    )\n\n    query = f\"\"\"\n    SELECT \n        source_id, ra, dec, l, b,\n        parallax, parallax_error,\n        pmra, pmdec,\n        phot_g_mean_mag, phot_bp_mean_mag, phot_rp_mean_mag,\n        bp_rp, g_rp,\n        teff_gspphot, logg_gspphot\n    FROM gaiadr3.gaia_source \n    WHERE phot_g_mean_mag &lt; {magnitude_limit}\n    AND parallax IS NOT NULL \n    AND parallax &gt; 0\n    {region_clause}\n    \"\"\"\n\n    try:\n        job = Gaia.launch_job_async(query)\n        results = job.get_results()\n\n        if results is None or len(results) == 0:\n            raise ValueError(\"No Gaia data returned\")\n\n        # Convert to Polars with proper types\n        df = pl.DataFrame({col: np.array(results[col]) for col in results.colnames})\n\n        # Add derived columns with proper units (step by step to avoid dependency issues)\n        # First: Distance in parsecs (1000/parallax)\n        df = df.with_columns((1000.0 / pl.col(\"parallax\")).alias(\"distance_pc\"))\n\n        # Second: Add other derived columns that depend on distance_pc\n        df = df.with_columns(\n            [\n                # Absolute magnitude: M = m + 5*log10(parallax) - 10\n                (\n                    pl.col(\"phot_g_mean_mag\") + 5 * pl.col(\"parallax\").log10() - 10\n                ).alias(\"abs_g_mag\"),\n                # Proper motion in mas/yr -&gt; km/s (4.74 * mu * d)\n                (4.74 * pl.col(\"pmra\") * pl.col(\"distance_pc\") / 1000.0).alias(\n                    \"vra_km_s\"\n                ),\n                (4.74 * pl.col(\"pmdec\") * pl.col(\"distance_pc\") / 1000.0).alias(\n                    \"vdec_km_s\"\n                ),\n                # Total proper motion\n                (pl.col(\"pmra\").pow(2) + pl.col(\"pmdec\").pow(2))\n                .sqrt()\n                .alias(\"pm_total\"),\n            ]\n        )\n\n        # Save as compressed Parquet\n        df.write_parquet(output_file, compression=\"zstd\")\n\n        # Save metadata\n        metadata = {\n            \"source\": \"Gaia DR3\",\n            \"region\": region,\n            \"magnitude_limit\": magnitude_limit,\n            \"n_sources\": len(df),\n            \"columns\": df.columns,\n            \"file_size_mb\": output_file.stat().st_size / 1024**2,\n        }\n\n        metadata_file = output_file.with_suffix(\".json\")\n        with open(metadata_file, \"w\") as f:\n            json.dump(metadata, f, indent=2)\n\n        print(\n            f\"\u2705 Downloaded {len(df):,} Gaia sources ({metadata['file_size_mb']:.1f} MB)\"\n        )\n        return output_file\n\n    except Exception as e:\n        print(f\"\u274c Gaia download failed: {e}\")\n        raise\n</code></pre>"},{"location":"api/astro_lab.data/#astro_lab.data.AstroDataManager.download_survey_catalog","title":"download_survey_catalog","text":"<pre><code>download_survey_catalog(\n    survey: str,\n    magnitude_limit: float = 15.0,\n    region: str = \"all_sky\",\n    max_sources: int = 1000000,\n) -&gt; Path\n</code></pre> <p>Download survey catalog to raw storage. Generic function for all surveys.</p> Source code in <code>src\\astro_lab\\data\\manager.py</code> <pre><code>def download_survey_catalog(\n    self,\n    survey: str,\n    magnitude_limit: float = 15.0,\n    region: str = \"all_sky\",\n    max_sources: int = 1000000,\n) -&gt; Path:\n    \"\"\"Download survey catalog to raw storage. Generic function for all surveys.\"\"\"\n\n    # Get survey configuration\n    from astro_lab.utils.config.surveys import get_survey_config\n\n    survey_config = get_survey_config(survey)\n\n    # Ensure survey directories exist\n    self.config.ensure_survey_directories(survey)\n\n    output_file = (\n        self.config.get_survey_raw_dir(survey)\n        / f\"{survey}_{region}_mag{magnitude_limit:.1f}.parquet\"\n    )\n\n    if output_file.exists():\n        print(f\"\ud83d\udcc2 {survey} catalog exists: {output_file.name}\")\n        return output_file\n\n    print(f\"\ud83c\udf1f Downloading {survey}: {region}, magnitude limit &lt; {magnitude_limit}\")\n\n    # Survey-specific download logic\n    if survey.lower() == \"gaia\":\n        return self.download_gaia_catalog(magnitude_limit, region, max_sources)\n    elif survey.lower() == \"sdss\":\n        return self._download_sdss_catalog(magnitude_limit, region, max_sources)\n    elif survey.lower() == \"2mass\":\n        return self._download_2mass_catalog(magnitude_limit, region, max_sources)\n    elif survey.lower() == \"wise\":\n        return self._download_wise_catalog(magnitude_limit, region, max_sources)\n    elif survey.lower() == \"pan_starrs\":\n        return self._download_pan_starrs_catalog(\n            magnitude_limit, region, max_sources\n        )\n    else:\n        raise ValueError(f\"Download not implemented for survey: {survey}\")\n</code></pre>"},{"location":"api/astro_lab.data/#astro_lab.data.AstroDataManager.get_memory_stats","title":"get_memory_stats","text":"<pre><code>get_memory_stats() -&gt; Dict[str, Any]\n</code></pre> <p>Get current memory statistics.</p> Source code in <code>src\\astro_lab\\data\\manager.py</code> <pre><code>def get_memory_stats(self) -&gt; Dict[str, Any]:\n    \"\"\"Get current memory statistics.\"\"\"\n    current_memory = psutil.Process().memory_info().rss / 1024 / 1024\n\n    stats = {\n        \"system_memory_mb\": current_memory,\n        \"data_dir\": str(self.data_dir),\n        \"processed_dir\": str(self.processed_dir),\n    }\n\n    return stats\n</code></pre>"},{"location":"api/astro_lab.data/#astro_lab.data.AstroDataManager.import_fits_catalog","title":"import_fits_catalog","text":"<pre><code>import_fits_catalog(\n    fits_file: Union[str, Path], catalog_name: str, hdu_index: int = 1\n) -&gt; Path\n</code></pre> <p>Import FITS catalog to raw storage.</p> Source code in <code>src\\astro_lab\\data\\manager.py</code> <pre><code>def import_fits_catalog(\n    self,\n    fits_file: Union[str, Path],\n    catalog_name: str,\n    hdu_index: int = 1,\n) -&gt; Path:\n    \"\"\"Import FITS catalog to raw storage.\"\"\"\n\n    fits_file = Path(fits_file)\n    if not fits_file.exists():\n        raise FileNotFoundError(f\"FITS file not found: {fits_file}\")\n\n    # Ensure sdss directories exist (fits -&gt; sdss)\n    self.config.ensure_survey_directories(\"sdss\")\n\n    output_file = self.config.get_survey_raw_dir(\"sdss\") / f\"{catalog_name}.parquet\"\n\n    if output_file.exists():\n        print(f\"\ud83d\udcc2 FITS catalog exists: {output_file.name}\")\n        return output_file\n\n    print(f\"\ud83d\udce5 Importing FITS: {fits_file.name} -&gt; {catalog_name}\")\n\n    try:\n        # Read FITS table\n        with fits.open(fits_file) as hdul:\n            table = Table(hdul[hdu_index].data)  # type: ignore\n\n        # Convert to Polars\n        df = pl.from_pandas(table.to_pandas())\n\n        # Save as compressed Parquet\n        df.write_parquet(output_file, compression=\"zstd\")\n\n        # Save metadata\n        metadata = {\n            \"source\": f\"FITS: {fits_file.name}\",\n            \"catalog_name\": catalog_name,\n            \"hdu_index\": hdu_index,\n            \"n_sources\": len(df),\n            \"columns\": df.columns,\n            \"file_size_mb\": output_file.stat().st_size / 1024**2,\n            \"original_size_mb\": fits_file.stat().st_size / 1024**2,\n        }\n\n        metadata_file = output_file.with_suffix(\".json\")\n        with open(metadata_file, \"w\") as f:\n            json.dump(metadata, f, indent=2)\n\n        print(\n            f\"\u2705 Imported {len(df):,} objects ({metadata['file_size_mb']:.1f} MB)\"\n        )\n        return output_file\n\n    except Exception as e:\n        print(f\"\u274c FITS import failed: {e}\")\n        raise\n</code></pre>"},{"location":"api/astro_lab.data/#astro_lab.data.AstroDataManager.import_tng50_hdf5","title":"import_tng50_hdf5","text":"<pre><code>import_tng50_hdf5(\n    hdf5_file: Union[str, Path],\n    dataset_name: str = \"PartType0\",\n    max_particles: int = 1000000,\n) -&gt; Path\n</code></pre> <p>Import TNG50 simulation data from HDF5.</p> Source code in <code>src\\astro_lab\\data\\manager.py</code> <pre><code>def import_tng50_hdf5(\n    self,\n    hdf5_file: Union[str, Path],\n    dataset_name: str = \"PartType0\",\n    max_particles: int = 1000000,\n) -&gt; Path:\n    \"\"\"Import TNG50 simulation data from HDF5.\"\"\"\n\n    hdf5_file = Path(hdf5_file)\n    if not hdf5_file.exists():\n        raise FileNotFoundError(f\"HDF5 file not found: {hdf5_file}\")\n\n    output_file = self.raw_dir / \"tng50\" / f\"tng50_{dataset_name.lower()}.parquet\"\n    output_file.parent.mkdir(exist_ok=True)\n\n    if output_file.exists():\n        print(f\"\ud83d\udcc2 TNG50 catalog exists: {output_file.name}\")\n        return output_file\n\n    print(f\"\ud83c\udf0c Importing TNG50: {hdf5_file.name} -&gt; {dataset_name}\")\n\n    try:\n        with h5py.File(hdf5_file, \"r\") as f:  # type: ignore\n            # Get particle data\n            if dataset_name not in f:  # type: ignore\n                available = list(f.keys())  # type: ignore\n                raise ValueError(\n                    f\"Dataset {dataset_name} not found. Available: {available}\"\n                )\n\n            group = f[dataset_name]  # type: ignore\n\n            # Common TNG50 fields\n            data_dict = {}\n\n            # Coordinates (essential)\n            if \"Coordinates\" in group:  # type: ignore\n                coords = np.array(group[\"Coordinates\"][:])  # type: ignore\n                if len(coords) &gt; max_particles:  # type: ignore\n                    indices = np.random.choice(  # type: ignore\n                        len(coords),  # type: ignore\n                        max_particles,\n                        replace=False,\n                    )\n                    coords = coords[indices]  # type: ignore\n                else:\n                    indices = slice(None)\n\n                data_dict[\"x\"] = coords[:, 0]  # type: ignore\n                data_dict[\"y\"] = coords[:, 1]  # type: ignore\n                data_dict[\"z\"] = coords[:, 2]  # type: ignore\n\n            # Other common fields\n            for field in [\n                \"Masses\",\n                \"Velocities\",\n                \"Density\",\n                \"Temperature\",\n                \"Metallicity\",\n            ]:\n                if field in group:  # type: ignore\n                    data = np.array(group[field][:])  # type: ignore\n                    if indices is not slice(None):\n                        data = data[indices]  # type: ignore\n\n                    # Sanitize field name for column\n                    col_name = field.lower()\n                    if col_name.endswith(\"es\"):\n                        col_name = col_name[:-2]\n                    elif col_name.endswith(\"s\"):\n                        col_name = col_name[:-1]\n\n                    if data.ndim &gt; 1:  # type: ignore\n                        # Vector quantities\n                        for i in range(data.shape[1]):  # type: ignore\n                            data_dict[f\"{col_name}_{i}\"] = data[:, i]  # type: ignore\n                    else:\n                        data_dict[col_name] = data  # type: ignore\n\n            # Convert to Polars\n            df = pl.DataFrame(data_dict)\n\n            # Save as compressed Parquet\n            df.write_parquet(output_file, compression=\"zstd\")\n\n            # Save metadata\n            metadata = {\n                \"source\": f\"TNG50: {hdf5_file.name}\",\n                \"dataset_name\": dataset_name,\n                \"n_particles\": len(df),\n                \"max_particles\": max_particles,\n                \"columns\": df.columns,\n                \"file_size_mb\": output_file.stat().st_size / 1024**2,\n                \"original_size_mb\": hdf5_file.stat().st_size / 1024**2,\n            }\n\n            metadata_file = output_file.with_suffix(\".json\")\n            with open(metadata_file, \"w\") as f:\n                json.dump(metadata, f, indent=2)\n\n            print(\n                f\"\u2705 Imported {len(df):,} particles ({metadata['file_size_mb']:.1f} MB)\"\n            )\n            return output_file\n\n    except Exception as e:\n        print(f\"\u274c TNG50 import failed: {e}\")\n        raise\n</code></pre>"},{"location":"api/astro_lab.data/#astro_lab.data.AstroDataManager.list_catalogs","title":"list_catalogs","text":"<pre><code>list_catalogs(data_type: str = 'all') -&gt; DataFrame\n</code></pre> <p>List available catalogs, only survey-based for processed.</p> Source code in <code>src\\astro_lab\\data\\manager.py</code> <pre><code>def list_catalogs(self, data_type: str = \"all\") -&gt; pl.DataFrame:\n    \"\"\"List available catalogs, only survey-based for processed.\"\"\"\n    catalogs = []\n    if data_type in [\"all\", \"raw\"]:\n        for parquet_file in self.raw_dir.rglob(\"*.parquet\"):\n            metadata_file = parquet_file.with_suffix(\".json\")\n            if metadata_file.exists():\n                with open(metadata_file) as f:\n                    metadata = json.load(f)\n            else:\n                metadata = {\"source\": \"Unknown\", \"n_sources\": 0}\n            catalogs.append(\n                {\n                    \"name\": parquet_file.name,\n                    \"type\": \"raw\",\n                    \"source\": metadata.get(\"source\", \"Unknown\"),\n                    \"n_sources\": metadata.get(\"n_sources\", 0),\n                    \"size_mb\": parquet_file.stat().st_size / 1024**2,\n                    \"path\": str(parquet_file),\n                }\n            )\n    if data_type in [\"all\", \"processed\"]:\n        # Only show survey-based\n        for survey_dir in (self.processed_dir).iterdir():\n            if survey_dir.is_dir():\n                parquet_file = survey_dir / f\"{survey_dir.name}.parquet\"\n                if parquet_file.exists():\n                    metadata_file = parquet_file.with_suffix(\".json\")\n                    if metadata_file.exists():\n                        with open(metadata_file) as f:\n                            metadata = json.load(f)\n                    else:\n                        metadata = {\"source\": \"Unknown\", \"n_sources\": 0}\n                    catalogs.append(\n                        {\n                            \"name\": parquet_file.name,\n                            \"type\": \"processed\",\n                            \"source\": metadata.get(\"source_file\", \"Unknown\"),\n                            \"n_sources\": metadata.get(\"n_sources_output\", 0),\n                            \"size_mb\": parquet_file.stat().st_size / 1024**2,\n                            \"path\": str(parquet_file),\n                        }\n                    )\n    if not catalogs:\n        return pl.DataFrame(\n            {\n                \"name\": [],\n                \"type\": [],\n                \"source\": [],\n                \"n_sources\": [],\n                \"size_mb\": [],\n                \"path\": [],\n            }\n        )\n    return pl.DataFrame(catalogs).sort(\"size_mb\", descending=True)\n</code></pre>"},{"location":"api/astro_lab.data/#astro_lab.data.AstroDataManager.load_catalog","title":"load_catalog","text":"<pre><code>load_catalog(catalog_path: Union[str, Path]) -&gt; DataFrame\n</code></pre> <p>Load catalog with memory management.</p> Source code in <code>src\\astro_lab\\data\\manager.py</code> <pre><code>def load_catalog(self, catalog_path: Union[str, Path]) -&gt; pl.DataFrame:\n    \"\"\"Load catalog with memory management.\"\"\"\n    catalog_path = Path(catalog_path)\n\n    logger.info(f\"\ud83d\udcc2 Loading catalog: {catalog_path}\")\n\n    if catalog_path.suffix.lower() in [\".fits\", \".fit\"]:\n        data = load_fits_optimized(catalog_path)\n        # Ensure we return a Polars DataFrame\n        if data is None:\n            raise ValueError(f\"Could not load FITS file: {catalog_path}\")\n        if isinstance(data, pl.DataFrame):\n            return data\n        else:\n            # Convert to Polars DataFrame\n            import pandas as pd\n\n            # Handle different data types\n            if hasattr(data, \"__array__\"):\n                # numpy arrays and similar\n                df_pandas = pd.DataFrame(data)\n            elif hasattr(data, \"to_pandas\"):\n                # astropy tables and similar\n                df_pandas = data.to_pandas()\n            else:\n                # fallback\n                df_pandas = pd.DataFrame([data])\n            return pl.from_pandas(df_pandas)\n    elif catalog_path.suffix.lower() in [\".parquet\", \".pq\"]:\n        data = pl.read_parquet(catalog_path)\n    elif catalog_path.suffix.lower() == \".csv\":\n        data = pl.read_csv(catalog_path)\n    else:\n        raise ValueError(f\"Unsupported format: {catalog_path.suffix}\")\n\n    logger.info(f\"\u2705 Catalog loaded: {len(data)} rows, {len(data.columns)} columns\")\n    return data\n</code></pre>"},{"location":"api/astro_lab.data/#astro_lab.data.AstroDataManager.process_file","title":"process_file","text":"<pre><code>process_file(file_path: Union[str, Path]) -&gt; Dict[str, Any]\n</code></pre> <p>Process a single file with memory management.</p> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>Union[str, Path]</code> <p>Path to the file to process</p> required <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Processing results</p> Source code in <code>src\\astro_lab\\data\\manager.py</code> <pre><code>def process_file(self, file_path: Union[str, Path]) -&gt; Dict[str, Any]:\n    \"\"\"\n    Process a single file with memory management.\n\n    Args:\n        file_path: Path to the file to process\n\n    Returns:\n        Processing results\n    \"\"\"\n    file_path = Path(file_path)\n\n    logger.info(f\"\ud83d\udcc2 Processing file: {file_path}\")\n\n    # Load data with memory optimization\n    if file_path.suffix.lower() in [\".fits\", \".fit\"]:\n        data = load_fits_optimized(file_path)\n    elif file_path.suffix.lower() in [\".parquet\", \".pq\"]:\n        data = pl.read_parquet(file_path)\n    elif file_path.suffix.lower() == \".csv\":\n        data = pl.read_csv(file_path)\n    else:\n        raise ValueError(f\"Unsupported file format: {file_path.suffix}\")\n\n    # Preprocess data\n    # Detect survey type from filename or use generic\n    survey_type = \"generic\"\n    if \"gaia\" in file_path.name.lower():\n        survey_type = \"gaia\"\n    elif \"sdss\" in file_path.name.lower():\n        survey_type = \"sdss\"\n    elif \"nsa\" in file_path.name.lower():\n        survey_type = \"nsa\"\n    elif \"linear\" in file_path.name.lower():\n        survey_type = \"linear\"\n    elif \"tng\" in file_path.name.lower():\n        survey_type = \"tng50\"\n\n    lf_processed = preprocess_catalog_lazy(\n        data, survey_type=survey_type, use_streaming=True\n    )\n    processed_data = lf_processed.collect()\n\n    # Save processed data\n    output_path = self.processed_dir / f\"{file_path.stem}.parquet\"\n    processed_data.write_parquet(output_path)\n\n    results = {\n        \"input_file\": str(file_path),\n        \"output_file\": str(output_path),\n        \"num_rows\": len(processed_data),\n        \"num_columns\": len(processed_data.columns),\n    }\n\n    logger.info(f\"\u2705 File processed: {len(processed_data)} rows\")\n    return results\n</code></pre>"},{"location":"api/astro_lab.data/#astro_lab.data.AstroDataManager.process_for_ml","title":"process_for_ml","text":"<pre><code>process_for_ml(\n    raw_file: Union[str, Path],\n    survey: Optional[str] = None,\n    filters: Optional[Dict] = None,\n) -&gt; Path\n</code></pre> <p>Process raw catalog for ML training and save as {survey}.parquet in processed/{survey}/</p> Source code in <code>src\\astro_lab\\data\\manager.py</code> <pre><code>def process_for_ml(\n    self,\n    raw_file: Union[str, Path],\n    survey: Optional[str] = None,\n    filters: Optional[Dict] = None,\n) -&gt; Path:\n    \"\"\"Process raw catalog for ML training and save as {survey}.parquet in processed/{survey}/\"\"\"\n    raw_file = Path(raw_file)\n    if survey is None:\n        # Survey must be specified explicitly!\n        raise ValueError(\"survey must be specified for ML-Processing\")\n    processed_dir = self.processed_dir / survey\n    processed_dir.mkdir(parents=True, exist_ok=True)\n    output_path = processed_dir / f\"{survey}.parquet\"\n    print(f\"\\U0001f504 Processing {raw_file.name} for ML as {output_path} ...\")\n    df = pl.read_parquet(raw_file)\n    if filters:\n        for col, (min_val, max_val) in filters.items():\n            if col in df.columns:\n                df = df.filter(pl.col(col).is_between(min_val, max_val))\n    critical_cols = [\"ra\", \"dec\"]\n    if \"phot_g_mean_mag\" in df.columns:\n        critical_cols.append(\"phot_g_mean_mag\")\n    elif \"psfMag_r\" in df.columns:\n        critical_cols.append(\"psfMag_r\")\n    df = df.drop_nulls(subset=critical_cols)\n    df = df.with_columns(\n        [\n            (pl.col(\"ra\") / 360.0).alias(\"ra_norm\"),\n            ((pl.col(\"dec\") + 90) / 180.0).alias(\"dec_norm\"),\n        ]\n    )\n    df.write_parquet(output_path)\n    metadata = {\n        \"source_file\": str(raw_file),\n        \"processing_date\": datetime.datetime.now().isoformat(),\n        \"filters_applied\": filters or {},\n        \"n_sources_input\": len(pl.read_parquet(raw_file)),\n        \"n_sources_output\": len(df),\n        \"columns\": df.columns,\n    }\n    metadata_file = output_path.with_suffix(\".json\")\n    with open(metadata_file, \"w\") as f:\n        json.dump(metadata, f, indent=2)\n    print(f\"\\U00002705 Processed {len(df):,} sources for ML: {output_path}\")\n    return output_path\n</code></pre>"},{"location":"api/astro_lab.data/#astro_lab.data.AstroDataManager.process_surveys","title":"process_surveys","text":"<pre><code>process_surveys(surveys: Optional[List[str]] = None) -&gt; Dict[str, Any]\n</code></pre> <p>Process multiple surveys with batch memory management.</p> <p>Parameters:</p> Name Type Description Default <code>surveys</code> <code>Optional[List[str]]</code> <p>List of survey names to process</p> <code>None</code> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Batch processing results</p> Source code in <code>src\\astro_lab\\data\\manager.py</code> <pre><code>def process_surveys(self, surveys: Optional[List[str]] = None) -&gt; Dict[str, Any]:\n    \"\"\"\n    Process multiple surveys with batch memory management.\n\n    Args:\n        surveys: List of survey names to process\n\n    Returns:\n        Batch processing results\n    \"\"\"\n    # Default surveys if none specified\n    if surveys is None:\n        surveys = [\"gaia\", \"sdss\", \"nsa\", \"linear\", \"tng50\"]\n\n    logger.info(f\"\ud83d\udcca Processing {len(surveys)} surveys\")\n\n    results = {\n        \"surveys_processed\": [],\n        \"total_rows\": 0,\n    }\n\n    for survey_name in surveys:\n        try:\n            survey_result = self._process_single_survey(survey_name)\n            results[\"surveys_processed\"].append(survey_result)\n            results[\"total_rows\"] += survey_result.get(\"num_rows\", 0)\n\n            logger.info(f\"\u2705 Survey {survey_name} processed successfully\")\n\n        except Exception as e:\n            logger.error(f\"\u274c Failed to process survey {survey_name}: {e}\")\n            results[\"surveys_processed\"].append(\n                {\"survey\": survey_name, \"error\": str(e), \"status\": \"failed\"}\n            )\n\n    logger.info(\n        f\"\ud83d\udcca Batch processing completed: {results['total_rows']} total rows\"\n    )\n    return results\n</code></pre>"},{"location":"api/astro_lab.data/#astro_lab.data.AstroDataManager.setup_directories","title":"setup_directories","text":"<pre><code>setup_directories()\n</code></pre> <p>Create standardized data directory structure using new config.</p> Source code in <code>src\\astro_lab\\data\\manager.py</code> <pre><code>def setup_directories(self):\n    \"\"\"Create standardized data directory structure using new config.\"\"\"\n    # Use new clean structure from config\n    self.config.setup_directories()\n</code></pre>"},{"location":"api/astro_lab.data/#astro_lab.data.AstroDataModule","title":"AstroDataModule","text":"<p>               Bases: <code>LightningDataModule</code></p> <p>Clean Lightning DataModule for astronomical datasets.</p> <p>Handles train/val/test splits and data loading. Uses unified AstroDataset from core.py.</p> <p>2025 Optimizations: - PIN memory for faster GPU transfer - Persistent workers to avoid recreation overhead - Prefetch factor tuning - Drop last for consistent batch sizes - Better distributed sampling - Fixed PyTorch Geometric batch handling</p> <p>Methods:</p> Name Description <code>get_info</code> <p>Get dataset information.</p> <code>load_state_dict</code> <p>Load datamodule state.</p> <code>prepare_data</code> <p>Download or prepare data. Called only on rank 0 in distributed training.</p> <code>setup</code> <p>Setup datasets for training/validation/testing.</p> <code>state_dict</code> <p>Save datamodule state.</p> <code>teardown</code> <p>Clean up after training/testing.</p> <code>test_dataloader</code> <p>Create test dataloader.</p> <code>train_dataloader</code> <p>Create training dataloader with optimizations.</p> <code>val_dataloader</code> <p>Create validation dataloader.</p> Source code in <code>src\\astro_lab\\data\\datamodule.py</code> <pre><code>class AstroDataModule(L.LightningDataModule):\n    \"\"\"\n    Clean Lightning DataModule for astronomical datasets.\n\n    Handles train/val/test splits and data loading.\n    Uses unified AstroDataset from core.py.\n\n    2025 Optimizations:\n    - PIN memory for faster GPU transfer\n    - Persistent workers to avoid recreation overhead\n    - Prefetch factor tuning\n    - Drop last for consistent batch sizes\n    - Better distributed sampling\n    - Fixed PyTorch Geometric batch handling\n    \"\"\"\n\n    def __init__(\n        self,\n        survey: str,\n        data_root: Optional[str] = None,\n        k_neighbors: int = 8,\n        max_samples: Optional[int] = None,\n        batch_size: int = 1,  # Graph datasets typically use batch_size=1\n        train_ratio: float = 0.7,\n        val_ratio: float = 0.15,\n        num_workers: Optional[int] = None,  # Auto-detect optimal workers\n        pin_memory: bool = True,\n        persistent_workers: bool = True,\n        prefetch_factor: int = 2,\n        drop_last: bool = True,\n        use_distributed_sampler: bool = True,\n        # New parameters for laptop optimization\n        max_nodes_per_graph: int = 1000,  # Limit graph size for laptop GPUs\n        use_subgraph_sampling: bool = True,  # Use subgraph sampling for large graphs\n        **kwargs,\n    ):\n        super().__init__()\n        self.save_hyperparameters()\n\n        self.survey = survey\n        self.dataset_name = survey  # For results organization\n        self.data_root = data_root or str(data_config.base_dir)\n        self.k_neighbors = k_neighbors\n        self.max_samples = max_samples\n        self.batch_size = batch_size\n        self.train_ratio = train_ratio\n        self.val_ratio = val_ratio\n        self.drop_last = drop_last\n        self.use_distributed_sampler = use_distributed_sampler\n\n        # Laptop optimization parameters\n        self.max_nodes_per_graph = max_nodes_per_graph\n        self.use_subgraph_sampling = use_subgraph_sampling\n\n        # Optimize num_workers for laptop\n        if num_workers is None:\n            # Conservative settings for laptop\n            try:\n                cpu_count = os.cpu_count()\n                # Use fewer workers on laptop to avoid memory pressure\n                self.num_workers = max(0, min(cpu_count // 2, 4))\n            except (OSError, AttributeError):\n                self.num_workers = 2\n        else:\n            self.num_workers = num_workers\n\n        # Conservative settings for laptop GPUs\n        if batch_size == 1:\n            self.pin_memory = False\n            self.persistent_workers = False\n            self.prefetch_factor = None\n            self.num_workers = 0\n        else:\n            # Conservative memory settings for laptop\n            self.pin_memory = pin_memory and torch.cuda.is_available()\n            self.persistent_workers = persistent_workers and self.num_workers &gt; 0\n            self.prefetch_factor = prefetch_factor if self.num_workers &gt; 0 else None\n\n        # Dataset will be created in setup()\n        self.dataset = None\n\n        # Store the main data object\n        self._main_data = None\n\n        # Class information for Lightning module\n        self.num_classes = None\n        self.num_features = None\n\n    def prepare_data(self):\n        \"\"\"\n        Download or prepare data. Called only on rank 0 in distributed training.\n        \"\"\"\n        # This method is called before setup() and only on rank 0\n        # Use it for downloading or one-time data preparation\n        pass\n\n    def setup(self, stage: Optional[str] = None):\n        \"\"\"Setup datasets for training/validation/testing.\"\"\"\n        if self.dataset is None:\n            self.dataset = AstroDataset(\n                root=self.data_root,\n                survey=self.survey,\n                k_neighbors=self.k_neighbors,\n                max_samples=self.max_samples,\n            )\n\n        # Get the main data object\n        full_data = self.dataset[0]\n\n        # Apply subgraph sampling if the graph is too large\n        if (\n            self.use_subgraph_sampling\n            and full_data.num_nodes &gt; self.max_nodes_per_graph\n        ):\n            logger.info(\n                f\"Graph too large ({full_data.num_nodes} nodes). Creating subgraph with {self.max_nodes_per_graph} nodes.\"\n            )\n            self._main_data = self._create_subgraph_samples(\n                full_data, self.max_nodes_per_graph\n            )\n        else:\n            self._main_data = full_data\n\n        # Extract dataset information for Lightning module\n        self.num_features = (\n            self._main_data.x.size(1) if hasattr(self._main_data, \"x\") else None\n        )\n        if hasattr(self._main_data, \"y\"):\n            unique_labels = torch.unique(self._main_data.y)\n            self.num_classes = max(len(unique_labels), 2)  # Ensure at least 2 classes\n\n            # If we only have one class, create synthetic binary labels for demonstration\n            if len(unique_labels) == 1:\n                logger.warning(\n                    f\"Dataset has only 1 unique label ({unique_labels[0].item()}). Creating synthetic binary labels for training.\"\n                )\n                # Create binary labels based on node features or random split\n                # Use feature-based split: nodes with feature sum &gt; median get label 1\n                feature_sums = self._main_data.x.sum(dim=1)\n                median_val = feature_sums.median()\n                self._main_data.y = (feature_sums &gt; median_val).long()\n                self.num_classes = 2\n\n        # Create train/val/test splits using masks\n        self._create_data_splits()\n\n    def _create_data_splits(self):\n        \"\"\"Create train/val/test masks for the graph data.\"\"\"\n        data = self._main_data\n        num_nodes = data.num_nodes\n\n        # Create train/val/test masks\n        data.train_mask = torch.zeros(num_nodes, dtype=torch.bool)\n        data.val_mask = torch.zeros(num_nodes, dtype=torch.bool)\n        data.test_mask = torch.zeros(num_nodes, dtype=torch.bool)\n\n        # Random split\n        indices = torch.randperm(num_nodes)\n        train_size = int(num_nodes * self.train_ratio)\n        val_size = int(num_nodes * self.val_ratio)\n\n        data.train_mask[indices[:train_size]] = True\n        data.val_mask[indices[train_size : train_size + val_size]] = True\n        data.test_mask[indices[train_size + val_size :]] = True\n\n        # Log split information\n        logger.info(\n            f\"Data splits - Train: {data.train_mask.sum()}, \"\n            f\"Val: {data.val_mask.sum()}, Test: {data.test_mask.sum()}\"\n        )\n\n    def _create_subgraph_samples(self, data, max_nodes: int):\n        \"\"\"Create smaller subgraph samples for laptop training.\"\"\"\n        from torch_geometric.utils import subgraph\n\n        # Randomly sample nodes\n        num_nodes = data.num_nodes\n        if num_nodes &lt;= max_nodes:\n            return data\n\n        # Sample nodes randomly\n        indices = torch.randperm(num_nodes)[:max_nodes]\n\n        # Create subgraph\n        edge_index, edge_attr = subgraph(\n            indices,\n            data.edge_index,\n            edge_attr=data.edge_attr if hasattr(data, \"edge_attr\") else None,\n            relabel_nodes=True,\n            num_nodes=num_nodes,\n        )\n\n        # Create new data object with subgraph\n        sub_data = data.__class__()\n        sub_data.num_nodes = len(indices)\n        sub_data.x = data.x[indices]\n        sub_data.edge_index = edge_index\n        if hasattr(data, \"pos\"):\n            sub_data.pos = data.pos[indices]\n        if hasattr(data, \"y\"):\n            sub_data.y = data.y[indices]\n        if edge_attr is not None:\n            sub_data.edge_attr = edge_attr\n\n        return sub_data\n\n    def _estimate_memory_usage(self, data) -&gt; float:\n        \"\"\"Estimate memory usage of graph data in MB.\"\"\"\n        total_memory = 0\n\n        # Node features\n        if hasattr(data, \"x\"):\n            total_memory += data.x.numel() * data.x.element_size()\n\n        # Edge indices\n        if hasattr(data, \"edge_index\"):\n            total_memory += data.edge_index.numel() * data.edge_index.element_size()\n\n        # Positions\n        if hasattr(data, \"pos\"):\n            total_memory += data.pos.numel() * data.pos.element_size()\n\n        # Labels\n        if hasattr(data, \"y\"):\n            total_memory += data.y.numel() * data.y.element_size()\n\n        # Masks\n        if hasattr(data, \"train_mask\"):\n            total_memory += data.train_mask.numel() * data.train_mask.element_size()\n\n        return total_memory / (1024 * 1024)  # Convert to MB\n\n    def _get_dataloader_kwargs(self) -&gt; Dict[str, Any]:\n        \"\"\"Get optimized dataloader kwargs based on settings.\"\"\"\n        kwargs = {\n            \"batch_size\": self.batch_size,\n            \"num_workers\": self.num_workers,\n            \"persistent_workers\": self.persistent_workers,\n            \"pin_memory\": self.pin_memory,\n            \"drop_last\": self.drop_last,\n        }\n\n        # Add prefetch_factor only if using workers\n        if self.num_workers &gt; 0 and self.prefetch_factor is not None:\n            kwargs[\"prefetch_factor\"] = self.prefetch_factor\n\n        return kwargs\n\n    def train_dataloader(self):\n        \"\"\"Create training dataloader with optimizations.\"\"\"\n        if self._main_data is None:\n            self.setup()\n\n        # Create a simple dataloader that yields the data object directly\n        class SingleGraphDataLoader:\n            def __init__(self, data):\n                self.data = data\n\n            def __iter__(self):\n                yield self.data\n\n            def __len__(self):\n                return 1\n\n        return SingleGraphDataLoader(self._main_data)\n\n    def val_dataloader(self):\n        \"\"\"Create validation dataloader.\"\"\"\n        if self._main_data is None:\n            self.setup()\n\n        # Create a simple dataloader that yields the data object directly\n        class SingleGraphDataLoader:\n            def __init__(self, data):\n                self.data = data\n\n            def __iter__(self):\n                yield self.data\n\n            def __len__(self):\n                return 1\n\n        return SingleGraphDataLoader(self._main_data)\n\n    def test_dataloader(self):\n        \"\"\"Create test dataloader.\"\"\"\n        if self._main_data is None:\n            self.setup()\n\n        # Create a simple dataloader that yields the data object directly\n        class SingleGraphDataLoader:\n            def __init__(self, data):\n                self.data = data\n\n            def __iter__(self):\n                yield self.data\n\n            def __len__(self):\n                return 1\n\n        return SingleGraphDataLoader(self._main_data)\n\n    def teardown(self, stage: Optional[str] = None):\n        \"\"\"Clean up after training/testing.\"\"\"\n        # Clean up cached data\n        self._main_data = None\n\n        # Force garbage collection\n        import gc\n\n        gc.collect()\n\n        # Clear CUDA cache if available\n        if torch.cuda.is_available():\n            torch.cuda.empty_cache()\n\n    def get_info(self) -&gt; Dict[str, Any]:\n        \"\"\"Get dataset information.\"\"\"\n        if self.dataset is None:\n            return {\"error\": \"Dataset not initialized\"}\n\n        info = self.dataset.get_info()\n\n        # Add datamodule-specific info\n        info.update(\n            {\n                \"batch_size\": self.batch_size,\n                \"num_workers\": self.num_workers,\n                \"pin_memory\": self.pin_memory,\n                \"persistent_workers\": self.persistent_workers,\n                \"train_ratio\": self.train_ratio,\n                \"val_ratio\": self.val_ratio,\n                \"num_classes\": self.num_classes,\n                \"num_features\": self.num_features,\n            }\n        )\n\n        return info\n\n    def state_dict(self) -&gt; Dict[str, Any]:\n        \"\"\"Save datamodule state.\"\"\"\n        return {\n            \"survey\": self.survey,\n            \"k_neighbors\": self.k_neighbors,\n            \"max_samples\": self.max_samples,\n            \"batch_size\": self.batch_size,\n            \"train_ratio\": self.train_ratio,\n            \"val_ratio\": self.val_ratio,\n        }\n\n    def load_state_dict(self, state_dict: Dict[str, Any]):\n        \"\"\"Load datamodule state.\"\"\"\n        for key, value in state_dict.items():\n            if hasattr(self, key):\n                setattr(self, key, value)\n</code></pre>"},{"location":"api/astro_lab.data/#astro_lab.data.AstroDataModule.get_info","title":"get_info","text":"<pre><code>get_info() -&gt; Dict[str, Any]\n</code></pre> <p>Get dataset information.</p> Source code in <code>src\\astro_lab\\data\\datamodule.py</code> <pre><code>def get_info(self) -&gt; Dict[str, Any]:\n    \"\"\"Get dataset information.\"\"\"\n    if self.dataset is None:\n        return {\"error\": \"Dataset not initialized\"}\n\n    info = self.dataset.get_info()\n\n    # Add datamodule-specific info\n    info.update(\n        {\n            \"batch_size\": self.batch_size,\n            \"num_workers\": self.num_workers,\n            \"pin_memory\": self.pin_memory,\n            \"persistent_workers\": self.persistent_workers,\n            \"train_ratio\": self.train_ratio,\n            \"val_ratio\": self.val_ratio,\n            \"num_classes\": self.num_classes,\n            \"num_features\": self.num_features,\n        }\n    )\n\n    return info\n</code></pre>"},{"location":"api/astro_lab.data/#astro_lab.data.AstroDataModule.load_state_dict","title":"load_state_dict","text":"<pre><code>load_state_dict(state_dict: Dict[str, Any])\n</code></pre> <p>Load datamodule state.</p> Source code in <code>src\\astro_lab\\data\\datamodule.py</code> <pre><code>def load_state_dict(self, state_dict: Dict[str, Any]):\n    \"\"\"Load datamodule state.\"\"\"\n    for key, value in state_dict.items():\n        if hasattr(self, key):\n            setattr(self, key, value)\n</code></pre>"},{"location":"api/astro_lab.data/#astro_lab.data.AstroDataModule.prepare_data","title":"prepare_data","text":"<pre><code>prepare_data()\n</code></pre> <p>Download or prepare data. Called only on rank 0 in distributed training.</p> Source code in <code>src\\astro_lab\\data\\datamodule.py</code> <pre><code>def prepare_data(self):\n    \"\"\"\n    Download or prepare data. Called only on rank 0 in distributed training.\n    \"\"\"\n    # This method is called before setup() and only on rank 0\n    # Use it for downloading or one-time data preparation\n    pass\n</code></pre>"},{"location":"api/astro_lab.data/#astro_lab.data.AstroDataModule.setup","title":"setup","text":"<pre><code>setup(stage: Optional[str] = None)\n</code></pre> <p>Setup datasets for training/validation/testing.</p> Source code in <code>src\\astro_lab\\data\\datamodule.py</code> <pre><code>def setup(self, stage: Optional[str] = None):\n    \"\"\"Setup datasets for training/validation/testing.\"\"\"\n    if self.dataset is None:\n        self.dataset = AstroDataset(\n            root=self.data_root,\n            survey=self.survey,\n            k_neighbors=self.k_neighbors,\n            max_samples=self.max_samples,\n        )\n\n    # Get the main data object\n    full_data = self.dataset[0]\n\n    # Apply subgraph sampling if the graph is too large\n    if (\n        self.use_subgraph_sampling\n        and full_data.num_nodes &gt; self.max_nodes_per_graph\n    ):\n        logger.info(\n            f\"Graph too large ({full_data.num_nodes} nodes). Creating subgraph with {self.max_nodes_per_graph} nodes.\"\n        )\n        self._main_data = self._create_subgraph_samples(\n            full_data, self.max_nodes_per_graph\n        )\n    else:\n        self._main_data = full_data\n\n    # Extract dataset information for Lightning module\n    self.num_features = (\n        self._main_data.x.size(1) if hasattr(self._main_data, \"x\") else None\n    )\n    if hasattr(self._main_data, \"y\"):\n        unique_labels = torch.unique(self._main_data.y)\n        self.num_classes = max(len(unique_labels), 2)  # Ensure at least 2 classes\n\n        # If we only have one class, create synthetic binary labels for demonstration\n        if len(unique_labels) == 1:\n            logger.warning(\n                f\"Dataset has only 1 unique label ({unique_labels[0].item()}). Creating synthetic binary labels for training.\"\n            )\n            # Create binary labels based on node features or random split\n            # Use feature-based split: nodes with feature sum &gt; median get label 1\n            feature_sums = self._main_data.x.sum(dim=1)\n            median_val = feature_sums.median()\n            self._main_data.y = (feature_sums &gt; median_val).long()\n            self.num_classes = 2\n\n    # Create train/val/test splits using masks\n    self._create_data_splits()\n</code></pre>"},{"location":"api/astro_lab.data/#astro_lab.data.AstroDataModule.state_dict","title":"state_dict","text":"<pre><code>state_dict() -&gt; Dict[str, Any]\n</code></pre> <p>Save datamodule state.</p> Source code in <code>src\\astro_lab\\data\\datamodule.py</code> <pre><code>def state_dict(self) -&gt; Dict[str, Any]:\n    \"\"\"Save datamodule state.\"\"\"\n    return {\n        \"survey\": self.survey,\n        \"k_neighbors\": self.k_neighbors,\n        \"max_samples\": self.max_samples,\n        \"batch_size\": self.batch_size,\n        \"train_ratio\": self.train_ratio,\n        \"val_ratio\": self.val_ratio,\n    }\n</code></pre>"},{"location":"api/astro_lab.data/#astro_lab.data.AstroDataModule.teardown","title":"teardown","text":"<pre><code>teardown(stage: Optional[str] = None)\n</code></pre> <p>Clean up after training/testing.</p> Source code in <code>src\\astro_lab\\data\\datamodule.py</code> <pre><code>def teardown(self, stage: Optional[str] = None):\n    \"\"\"Clean up after training/testing.\"\"\"\n    # Clean up cached data\n    self._main_data = None\n\n    # Force garbage collection\n    import gc\n\n    gc.collect()\n\n    # Clear CUDA cache if available\n    if torch.cuda.is_available():\n        torch.cuda.empty_cache()\n</code></pre>"},{"location":"api/astro_lab.data/#astro_lab.data.AstroDataModule.test_dataloader","title":"test_dataloader","text":"<pre><code>test_dataloader()\n</code></pre> <p>Create test dataloader.</p> Source code in <code>src\\astro_lab\\data\\datamodule.py</code> <pre><code>def test_dataloader(self):\n    \"\"\"Create test dataloader.\"\"\"\n    if self._main_data is None:\n        self.setup()\n\n    # Create a simple dataloader that yields the data object directly\n    class SingleGraphDataLoader:\n        def __init__(self, data):\n            self.data = data\n\n        def __iter__(self):\n            yield self.data\n\n        def __len__(self):\n            return 1\n\n    return SingleGraphDataLoader(self._main_data)\n</code></pre>"},{"location":"api/astro_lab.data/#astro_lab.data.AstroDataModule.train_dataloader","title":"train_dataloader","text":"<pre><code>train_dataloader()\n</code></pre> <p>Create training dataloader with optimizations.</p> Source code in <code>src\\astro_lab\\data\\datamodule.py</code> <pre><code>def train_dataloader(self):\n    \"\"\"Create training dataloader with optimizations.\"\"\"\n    if self._main_data is None:\n        self.setup()\n\n    # Create a simple dataloader that yields the data object directly\n    class SingleGraphDataLoader:\n        def __init__(self, data):\n            self.data = data\n\n        def __iter__(self):\n            yield self.data\n\n        def __len__(self):\n            return 1\n\n    return SingleGraphDataLoader(self._main_data)\n</code></pre>"},{"location":"api/astro_lab.data/#astro_lab.data.AstroDataModule.val_dataloader","title":"val_dataloader","text":"<pre><code>val_dataloader()\n</code></pre> <p>Create validation dataloader.</p> Source code in <code>src\\astro_lab\\data\\datamodule.py</code> <pre><code>def val_dataloader(self):\n    \"\"\"Create validation dataloader.\"\"\"\n    if self._main_data is None:\n        self.setup()\n\n    # Create a simple dataloader that yields the data object directly\n    class SingleGraphDataLoader:\n        def __init__(self, data):\n            self.data = data\n\n        def __iter__(self):\n            yield self.data\n\n        def __len__(self):\n            return 1\n\n    return SingleGraphDataLoader(self._main_data)\n</code></pre>"},{"location":"api/astro_lab.data/#astro_lab.data.AstroDataset","title":"AstroDataset","text":"<p>               Bases: <code>InMemoryDataset</code></p> <p>Minimal PyTorch Geometric dataset f\u00fcr astronomische Daten. L\u00e4dt Daten einmal beim Init und h\u00e4lt sie im Speicher.</p> Source code in <code>src\\astro_lab\\data\\core.py</code> <pre><code>class AstroDataset(InMemoryDataset):\n    \"\"\"\n    Minimal PyTorch Geometric dataset f\u00fcr astronomische Daten.\n    L\u00e4dt Daten einmal beim Init und h\u00e4lt sie im Speicher.\n    \"\"\"\n\n    def __init__(\n        self,\n        root: str,\n        survey: str,\n        k_neighbors: int = 8,\n        max_samples: Optional[int] = None,\n        transform: Optional[Any] = None,\n        pre_transform: Optional[Any] = None,\n        pre_filter: Optional[Any] = None,\n        use_streaming: bool = True,\n    ):\n        self.survey = survey\n        self.k_neighbors = k_neighbors\n        self.max_samples = int(max_samples) if max_samples is not None else None\n        self.use_streaming = use_streaming\n        self.survey_config = get_survey_config(survey)\n        super().__init__(root, transform, pre_transform, pre_filter)\n        self._processed_dir = Path(self.root) / \"processed\" / self.survey\n        self._processed_dir.mkdir(parents=True, exist_ok=True)\n        self._pt_path = self._processed_dir / f\"{self.survey}.pt\"\n        self._parquet_path = self._processed_dir / f\"{self.survey}.parquet\"\n        self._load_data()\n\n    def _load_data(self):\n        \"\"\"L\u00e4dt Daten einmal beim Initialisieren.\"\"\"\n        if self._pt_path.exists():\n            print(f\"\ud83d\udd04 Lade Graph aus: {self._pt_path}\")\n            try:\n                graph_data = torch.load(self._pt_path, weights_only=False)\n                self.data, self.slices = self.collate([graph_data])\n                return\n            except Exception:\n                print(\"\u26a0\ufe0f Fehler beim Laden von .pt, nutze Parquet...\")\n        if not self._parquet_path.exists():\n            raise FileNotFoundError(f\"Parquet nicht gefunden: {self._parquet_path}\")\n        print(f\"\ud83d\udd04 Lade Parquet: {self._parquet_path}\")\n        df = pl.read_parquet(self._parquet_path)\n        graph_data = self._create_graph_from_dataframe(df)\n        self.data, self.slices = self.collate([graph_data])\n        torch.save(graph_data, self._pt_path)\n        print(f\"\ud83d\udcbe Graph gespeichert: {self._pt_path}\")\n\n    def _create_graph_from_dataframe(self, df: pl.DataFrame) -&gt; Data:\n        coord_cols = self.survey_config.get(\"coordinates\", [\"ra\", \"dec\"])\n        feature_cols = self.survey_config.get(\"features\", coord_cols)\n        coords = df.select([c for c in coord_cols if c in df.columns]).to_numpy()\n        features = (\n            df.select([c for c in feature_cols if c in df.columns]).to_numpy()\n            if feature_cols\n            else coords\n        )\n        if features.shape[1] == 0:\n            features = coords\n        pos = torch.tensor(coords, dtype=torch.float32)\n        x = torch.tensor(features, dtype=torch.float32)\n        y = torch.zeros(len(df), dtype=torch.long)\n        device = get_optimal_device()\n        pos_device = pos.to(device)\n        edge_index = torch_cluster.knn_graph(pos_device, k=self.k_neighbors, batch=None)\n        edge_index = edge_index.cpu()\n        data = Data(x=x, pos=pos, edge_index=edge_index, y=y, num_nodes=len(pos))\n        data.survey_name = self.survey\n        data.survey = self.survey\n        data.k_neighbors = self.k_neighbors\n        return data\n\n    def len(self) -&gt; int:\n        return 1 if hasattr(self.data, \"num_nodes\") else 0\n\n    def get(self, idx: int) -&gt; Data:\n        if idx == 0:\n            return self.data\n        else:\n            raise IndexError(f\"Index {idx} out of range for single graph dataset\")\n\n    def get_info(self) -&gt; Dict[str, Any]:\n        if len(self) == 0:\n            return {\"error\": \"Dataset empty\", \"columns\": []}\n\n        sample = self[0]\n\n        # Get standard survey columns based on survey type\n        standard_columns = self._get_standard_columns_for_survey()\n\n        return {\n            \"survey\": self.survey,\n            \"num_samples\": len(self),\n            \"num_nodes\": sample.num_nodes if hasattr(sample, \"num_nodes\") else 0,\n            \"num_edges\": sample.edge_index.shape[1]\n            if hasattr(sample, \"edge_index\")\n            else 0,\n            \"num_features\": sample.x.shape[1] if hasattr(sample, \"x\") else 0,\n            \"k_neighbors\": self.k_neighbors,\n            \"columns\": standard_columns,  # Add columns info\n        }\n\n    def _get_standard_columns_for_survey(self) -&gt; List[str]:\n        \"\"\"Get standard column names for the survey type.\"\"\"\n        try:\n            from astro_lab.utils.config.surveys import get_survey_config\n\n            config = get_survey_config(self.survey)\n            # Combine coordinate, magnitude, and extra columns\n            columns = []\n            columns.extend(config.get(\"coord_cols\", [\"ra\", \"dec\"]))\n            columns.extend(config.get(\"mag_cols\", []))\n            columns.extend(config.get(\"extra_cols\", []))\n            return columns\n        except Exception:\n            # Fallback for unknown surveys\n            return (\n                [\"ra\", \"dec\", \"mag\"]\n                if self.survey in [\"gaia\", \"nsa\", \"exoplanet\"]\n                else []\n            )\n</code></pre>"},{"location":"api/astro_lab.data/#astro_lab.data.DataConfig","title":"DataConfig","text":"<p>Centralized data configuration for AstroLab.</p> <p>Manages all data paths, directory structures, and configuration for astronomical data processing and analysis.</p> <p>Methods:</p> Name Description <code>ensure_experiment_directories</code> <p>Create experiment directories only when needed.</p> <code>ensure_results_directories</code> <p>Create organized results directory structure.</p> <code>ensure_survey_directories</code> <p>Create directories for a specific survey only when needed.</p> <code>get_catalog_path</code> <p>Get standard catalog path for survey.</p> <code>get_experiment_paths</code> <p>Get all paths for an experiment.</p> <code>get_graph_path</code> <p>Get graph data path for survey.</p> <code>get_results_structure</code> <p>Get organized results directory structure for survey/experiment.</p> <code>get_survey_processed_dir</code> <p>Get processed directory for specific survey.</p> <code>get_survey_raw_dir</code> <p>Get raw directory for specific survey.</p> <code>get_tensor_path</code> <p>Get tensor data path for survey.</p> <code>migrate_old_structure</code> <p>Migrate from old chaotic structure to new clean structure.</p> <code>setup_directories</code> <p>Create only core data directory structure (no survey templates).</p> <p>Attributes:</p> Name Type Description <code>artifacts_dir</code> <code>Path</code> <p>MLflow artifacts directory.</p> <code>cache_dir</code> <code>Path</code> <p>Cache directory.</p> <code>checkpoints_dir</code> <code>Path</code> <p>Lightning checkpoints directory.</p> <code>configs_dir</code> <code>Path</code> <p>Configuration files directory.</p> <code>experiments_dir</code> <code>Path</code> <p>Experiments directory for MLflow and checkpoints.</p> <code>logs_dir</code> <code>Path</code> <p>Logs directory for training logs.</p> <code>mlruns_dir</code> <code>Path</code> <p>MLflow tracking directory.</p> <code>processed_dir</code> <code>Path</code> <p>Processed data directory.</p> <code>raw_dir</code> <code>Path</code> <p>Raw data directory.</p> <code>results_dir</code> <code>Path</code> <p>Results directory for organized model outputs.</p> Source code in <code>src\\astro_lab\\data\\config.py</code> <pre><code>class DataConfig:\n    \"\"\"\n    Centralized data configuration for AstroLab.\n\n    Manages all data paths, directory structures, and configuration\n    for astronomical data processing and analysis.\n    \"\"\"\n\n    def __init__(self, base_dir: Union[str, Path] = \"data\"):\n        self.base_dir = Path(base_dir)\n\n    @property\n    def raw_dir(self) -&gt; Path:\n        \"\"\"Raw data directory.\"\"\"\n        return self.base_dir / \"raw\"\n\n    @property\n    def processed_dir(self) -&gt; Path:\n        \"\"\"Processed data directory.\"\"\"\n        return self.base_dir / \"processed\"\n\n    @property\n    def cache_dir(self) -&gt; Path:\n        \"\"\"Cache directory.\"\"\"\n        return self.base_dir / \"cache\"\n\n    @property\n    def experiments_dir(self) -&gt; Path:\n        \"\"\"Experiments directory for MLflow and checkpoints.\"\"\"\n        return self.base_dir / \"experiments\"\n\n    @property\n    def mlruns_dir(self) -&gt; Path:\n        \"\"\"MLflow tracking directory.\"\"\"\n        return self.experiments_dir / \"mlruns\"\n\n    @property\n    def checkpoints_dir(self) -&gt; Path:\n        \"\"\"Lightning checkpoints directory.\"\"\"\n        return self.experiments_dir / \"checkpoints\"\n\n    @property\n    def results_dir(self) -&gt; Path:\n        \"\"\"Results directory for organized model outputs.\"\"\"\n        return self.base_dir / \"results\"\n\n    @property\n    def logs_dir(self) -&gt; Path:\n        \"\"\"Logs directory for training logs.\"\"\"\n        return self.experiments_dir / \"logs\"\n\n    @property\n    def configs_dir(self) -&gt; Path:\n        \"\"\"Configuration files directory.\"\"\"\n        return self.base_dir / \"configs\"\n\n    @property\n    def artifacts_dir(self) -&gt; Path:\n        \"\"\"MLflow artifacts directory.\"\"\"\n        return self.experiments_dir / \"artifacts\"\n\n    def get_survey_raw_dir(self, survey: str) -&gt; Path:\n        \"\"\"Get raw directory for specific survey.\"\"\"\n        return self.raw_dir / survey\n\n    def get_survey_processed_dir(self, survey: str) -&gt; Path:\n        \"\"\"Get processed directory for specific survey.\"\"\"\n        return self.processed_dir / survey\n\n    def get_catalog_path(self, survey: str, processed: bool = True) -&gt; Path:\n        \"\"\"Get standard catalog path for survey.\"\"\"\n        if processed:\n            return self.get_survey_processed_dir(survey) / \"catalog.parquet\"\n        else:\n            # Raw catalog naming depends on survey\n            raw_dir = self.get_survey_raw_dir(survey)\n            return raw_dir / f\"{survey}_catalog.parquet\"\n\n    def get_graph_path(self, survey: str, k_neighbors: int = 8) -&gt; Path:\n        \"\"\"Get graph data path for survey.\"\"\"\n        return self.get_survey_processed_dir(survey) / f\"graphs_k{k_neighbors}.pt\"\n\n    def get_tensor_path(self, survey: str) -&gt; Path:\n        \"\"\"Get tensor data path for survey.\"\"\"\n        return self.get_survey_processed_dir(survey) / \"tensors.pt\"\n\n    def setup_directories(self):\n        \"\"\"Create only core data directory structure (no survey templates).\"\"\"\n        # Only create core directories\n        core_dirs = [\n            self.raw_dir,\n            self.processed_dir,\n            self.cache_dir,\n            self.experiments_dir,\n            self.results_dir,\n            self.configs_dir,\n        ]\n\n        # Create only core directories\n        for dir_path in core_dirs:\n            dir_path.mkdir(parents=True, exist_ok=True)\n\n        logger.info(f\"\ud83d\udcc1 Core data structure created in: {self.base_dir}\")\n\n    def ensure_survey_directories(self, survey: str):\n        \"\"\"Create directories for a specific survey only when needed.\"\"\"\n        raw_dir = self.get_survey_raw_dir(survey)\n        processed_dir = self.get_survey_processed_dir(survey)\n\n        # Create only if they don't exist\n        raw_dir.mkdir(parents=True, exist_ok=True)\n        processed_dir.mkdir(parents=True, exist_ok=True)\n\n        logger.info(f\"\ud83d\udcc1 Created directories for {survey} survey\")\n\n    def ensure_experiment_directories(self, experiment_name: str):\n        \"\"\"Create experiment directories only when needed.\"\"\"\n        mlruns_dir = self.mlruns_dir\n        checkpoint_dir = self.checkpoints_dir / experiment_name\n        logs_dir = self.logs_dir / experiment_name\n        artifacts_dir = self.artifacts_dir\n\n        # Create only if they don't exist\n        mlruns_dir.mkdir(parents=True, exist_ok=True)\n        checkpoint_dir.mkdir(parents=True, exist_ok=True)\n        logs_dir.mkdir(parents=True, exist_ok=True)\n        artifacts_dir.mkdir(parents=True, exist_ok=True)\n\n        logger.info(f\"\ud83e\uddea Created experiment directories for {experiment_name}\")\n\n    def get_results_structure(\n        self, survey: str, experiment_name: str\n    ) -&gt; Dict[str, Path]:\n        \"\"\"Get organized results directory structure for survey/experiment.\"\"\"\n        base_results = self.results_dir / survey / experiment_name\n\n        return {\n            \"base\": base_results,\n            \"models\": base_results / \"models\",\n            \"plots\": base_results / \"plots\",\n            \"optuna_plots\": base_results / \"plots\" / \"optuna\",\n        }\n\n    def ensure_results_directories(self, survey: str, experiment_name: str):\n        \"\"\"Create organized results directory structure.\"\"\"\n        structure = self.get_results_structure(survey, experiment_name)\n\n        # Create all directories\n        for dir_name, dir_path in structure.items():\n            dir_path.mkdir(parents=True, exist_ok=True)\n\n        logger.info(f\"\ud83d\udcca Created results structure for {survey}/{experiment_name}\")\n        return structure\n\n    def get_experiment_paths(self, experiment_name: str) -&gt; Dict[str, Path]:\n        \"\"\"Get all paths for an experiment.\"\"\"\n        return {\n            \"mlruns\": self.mlruns_dir,\n            \"checkpoints\": self.checkpoints_dir / experiment_name,\n            \"artifacts\": self.artifacts_dir,\n            \"logs\": self.logs_dir / experiment_name,\n        }\n\n    def migrate_old_structure(self):\n        \"\"\"Migrate from old chaotic structure to new clean structure.\"\"\"\n        logger.info(\"\ud83d\udd04 Migrating old data structure...\")\n\n        # Map old paths to new paths\n        migrations = [\n            # Raw data migrations\n            (self.raw_dir / \"fits\", self.get_survey_raw_dir(\"sdss\")),\n            (self.raw_dir / \"hdf5\", self.get_survey_raw_dir(\"tng50\") / \"hdf5\"),\n            # Processed data migrations\n            (self.processed_dir / \"catalogs\", self.processed_dir / \"temp_catalogs\"),\n            (self.processed_dir / \"ml_ready\", self.processed_dir / \"temp_ml_ready\"),\n            (self.processed_dir / \"features\", self.processed_dir / \"temp_features\"),\n        ]\n\n        for old_path, new_path in migrations:\n            if old_path.exists() and old_path.is_dir():\n                new_path.parent.mkdir(parents=True, exist_ok=True)\n                logger.info(f\"  \ud83d\udce6 {old_path} -&gt; {new_path}\")\n                # Note: Actual file moving would be done manually or with additional logic\n\n        logger.info(\"\u2705 Migration plan created. Manual file moving required.\")\n</code></pre>"},{"location":"api/astro_lab.data/#astro_lab.data.DataConfig.artifacts_dir","title":"artifacts_dir  <code>property</code>","text":"<pre><code>artifacts_dir: Path\n</code></pre> <p>MLflow artifacts directory.</p>"},{"location":"api/astro_lab.data/#astro_lab.data.DataConfig.cache_dir","title":"cache_dir  <code>property</code>","text":"<pre><code>cache_dir: Path\n</code></pre> <p>Cache directory.</p>"},{"location":"api/astro_lab.data/#astro_lab.data.DataConfig.checkpoints_dir","title":"checkpoints_dir  <code>property</code>","text":"<pre><code>checkpoints_dir: Path\n</code></pre> <p>Lightning checkpoints directory.</p>"},{"location":"api/astro_lab.data/#astro_lab.data.DataConfig.configs_dir","title":"configs_dir  <code>property</code>","text":"<pre><code>configs_dir: Path\n</code></pre> <p>Configuration files directory.</p>"},{"location":"api/astro_lab.data/#astro_lab.data.DataConfig.experiments_dir","title":"experiments_dir  <code>property</code>","text":"<pre><code>experiments_dir: Path\n</code></pre> <p>Experiments directory for MLflow and checkpoints.</p>"},{"location":"api/astro_lab.data/#astro_lab.data.DataConfig.logs_dir","title":"logs_dir  <code>property</code>","text":"<pre><code>logs_dir: Path\n</code></pre> <p>Logs directory for training logs.</p>"},{"location":"api/astro_lab.data/#astro_lab.data.DataConfig.mlruns_dir","title":"mlruns_dir  <code>property</code>","text":"<pre><code>mlruns_dir: Path\n</code></pre> <p>MLflow tracking directory.</p>"},{"location":"api/astro_lab.data/#astro_lab.data.DataConfig.processed_dir","title":"processed_dir  <code>property</code>","text":"<pre><code>processed_dir: Path\n</code></pre> <p>Processed data directory.</p>"},{"location":"api/astro_lab.data/#astro_lab.data.DataConfig.raw_dir","title":"raw_dir  <code>property</code>","text":"<pre><code>raw_dir: Path\n</code></pre> <p>Raw data directory.</p>"},{"location":"api/astro_lab.data/#astro_lab.data.DataConfig.results_dir","title":"results_dir  <code>property</code>","text":"<pre><code>results_dir: Path\n</code></pre> <p>Results directory for organized model outputs.</p>"},{"location":"api/astro_lab.data/#astro_lab.data.DataConfig.ensure_experiment_directories","title":"ensure_experiment_directories","text":"<pre><code>ensure_experiment_directories(experiment_name: str)\n</code></pre> <p>Create experiment directories only when needed.</p> Source code in <code>src\\astro_lab\\data\\config.py</code> <pre><code>def ensure_experiment_directories(self, experiment_name: str):\n    \"\"\"Create experiment directories only when needed.\"\"\"\n    mlruns_dir = self.mlruns_dir\n    checkpoint_dir = self.checkpoints_dir / experiment_name\n    logs_dir = self.logs_dir / experiment_name\n    artifacts_dir = self.artifacts_dir\n\n    # Create only if they don't exist\n    mlruns_dir.mkdir(parents=True, exist_ok=True)\n    checkpoint_dir.mkdir(parents=True, exist_ok=True)\n    logs_dir.mkdir(parents=True, exist_ok=True)\n    artifacts_dir.mkdir(parents=True, exist_ok=True)\n\n    logger.info(f\"\ud83e\uddea Created experiment directories for {experiment_name}\")\n</code></pre>"},{"location":"api/astro_lab.data/#astro_lab.data.DataConfig.ensure_results_directories","title":"ensure_results_directories","text":"<pre><code>ensure_results_directories(survey: str, experiment_name: str)\n</code></pre> <p>Create organized results directory structure.</p> Source code in <code>src\\astro_lab\\data\\config.py</code> <pre><code>def ensure_results_directories(self, survey: str, experiment_name: str):\n    \"\"\"Create organized results directory structure.\"\"\"\n    structure = self.get_results_structure(survey, experiment_name)\n\n    # Create all directories\n    for dir_name, dir_path in structure.items():\n        dir_path.mkdir(parents=True, exist_ok=True)\n\n    logger.info(f\"\ud83d\udcca Created results structure for {survey}/{experiment_name}\")\n    return structure\n</code></pre>"},{"location":"api/astro_lab.data/#astro_lab.data.DataConfig.ensure_survey_directories","title":"ensure_survey_directories","text":"<pre><code>ensure_survey_directories(survey: str)\n</code></pre> <p>Create directories for a specific survey only when needed.</p> Source code in <code>src\\astro_lab\\data\\config.py</code> <pre><code>def ensure_survey_directories(self, survey: str):\n    \"\"\"Create directories for a specific survey only when needed.\"\"\"\n    raw_dir = self.get_survey_raw_dir(survey)\n    processed_dir = self.get_survey_processed_dir(survey)\n\n    # Create only if they don't exist\n    raw_dir.mkdir(parents=True, exist_ok=True)\n    processed_dir.mkdir(parents=True, exist_ok=True)\n\n    logger.info(f\"\ud83d\udcc1 Created directories for {survey} survey\")\n</code></pre>"},{"location":"api/astro_lab.data/#astro_lab.data.DataConfig.get_catalog_path","title":"get_catalog_path","text":"<pre><code>get_catalog_path(survey: str, processed: bool = True) -&gt; Path\n</code></pre> <p>Get standard catalog path for survey.</p> Source code in <code>src\\astro_lab\\data\\config.py</code> <pre><code>def get_catalog_path(self, survey: str, processed: bool = True) -&gt; Path:\n    \"\"\"Get standard catalog path for survey.\"\"\"\n    if processed:\n        return self.get_survey_processed_dir(survey) / \"catalog.parquet\"\n    else:\n        # Raw catalog naming depends on survey\n        raw_dir = self.get_survey_raw_dir(survey)\n        return raw_dir / f\"{survey}_catalog.parquet\"\n</code></pre>"},{"location":"api/astro_lab.data/#astro_lab.data.DataConfig.get_experiment_paths","title":"get_experiment_paths","text":"<pre><code>get_experiment_paths(experiment_name: str) -&gt; Dict[str, Path]\n</code></pre> <p>Get all paths for an experiment.</p> Source code in <code>src\\astro_lab\\data\\config.py</code> <pre><code>def get_experiment_paths(self, experiment_name: str) -&gt; Dict[str, Path]:\n    \"\"\"Get all paths for an experiment.\"\"\"\n    return {\n        \"mlruns\": self.mlruns_dir,\n        \"checkpoints\": self.checkpoints_dir / experiment_name,\n        \"artifacts\": self.artifacts_dir,\n        \"logs\": self.logs_dir / experiment_name,\n    }\n</code></pre>"},{"location":"api/astro_lab.data/#astro_lab.data.DataConfig.get_graph_path","title":"get_graph_path","text":"<pre><code>get_graph_path(survey: str, k_neighbors: int = 8) -&gt; Path\n</code></pre> <p>Get graph data path for survey.</p> Source code in <code>src\\astro_lab\\data\\config.py</code> <pre><code>def get_graph_path(self, survey: str, k_neighbors: int = 8) -&gt; Path:\n    \"\"\"Get graph data path for survey.\"\"\"\n    return self.get_survey_processed_dir(survey) / f\"graphs_k{k_neighbors}.pt\"\n</code></pre>"},{"location":"api/astro_lab.data/#astro_lab.data.DataConfig.get_results_structure","title":"get_results_structure","text":"<pre><code>get_results_structure(survey: str, experiment_name: str) -&gt; Dict[str, Path]\n</code></pre> <p>Get organized results directory structure for survey/experiment.</p> Source code in <code>src\\astro_lab\\data\\config.py</code> <pre><code>def get_results_structure(\n    self, survey: str, experiment_name: str\n) -&gt; Dict[str, Path]:\n    \"\"\"Get organized results directory structure for survey/experiment.\"\"\"\n    base_results = self.results_dir / survey / experiment_name\n\n    return {\n        \"base\": base_results,\n        \"models\": base_results / \"models\",\n        \"plots\": base_results / \"plots\",\n        \"optuna_plots\": base_results / \"plots\" / \"optuna\",\n    }\n</code></pre>"},{"location":"api/astro_lab.data/#astro_lab.data.DataConfig.get_survey_processed_dir","title":"get_survey_processed_dir","text":"<pre><code>get_survey_processed_dir(survey: str) -&gt; Path\n</code></pre> <p>Get processed directory for specific survey.</p> Source code in <code>src\\astro_lab\\data\\config.py</code> <pre><code>def get_survey_processed_dir(self, survey: str) -&gt; Path:\n    \"\"\"Get processed directory for specific survey.\"\"\"\n    return self.processed_dir / survey\n</code></pre>"},{"location":"api/astro_lab.data/#astro_lab.data.DataConfig.get_survey_raw_dir","title":"get_survey_raw_dir","text":"<pre><code>get_survey_raw_dir(survey: str) -&gt; Path\n</code></pre> <p>Get raw directory for specific survey.</p> Source code in <code>src\\astro_lab\\data\\config.py</code> <pre><code>def get_survey_raw_dir(self, survey: str) -&gt; Path:\n    \"\"\"Get raw directory for specific survey.\"\"\"\n    return self.raw_dir / survey\n</code></pre>"},{"location":"api/astro_lab.data/#astro_lab.data.DataConfig.get_tensor_path","title":"get_tensor_path","text":"<pre><code>get_tensor_path(survey: str) -&gt; Path\n</code></pre> <p>Get tensor data path for survey.</p> Source code in <code>src\\astro_lab\\data\\config.py</code> <pre><code>def get_tensor_path(self, survey: str) -&gt; Path:\n    \"\"\"Get tensor data path for survey.\"\"\"\n    return self.get_survey_processed_dir(survey) / \"tensors.pt\"\n</code></pre>"},{"location":"api/astro_lab.data/#astro_lab.data.DataConfig.migrate_old_structure","title":"migrate_old_structure","text":"<pre><code>migrate_old_structure()\n</code></pre> <p>Migrate from old chaotic structure to new clean structure.</p> Source code in <code>src\\astro_lab\\data\\config.py</code> <pre><code>def migrate_old_structure(self):\n    \"\"\"Migrate from old chaotic structure to new clean structure.\"\"\"\n    logger.info(\"\ud83d\udd04 Migrating old data structure...\")\n\n    # Map old paths to new paths\n    migrations = [\n        # Raw data migrations\n        (self.raw_dir / \"fits\", self.get_survey_raw_dir(\"sdss\")),\n        (self.raw_dir / \"hdf5\", self.get_survey_raw_dir(\"tng50\") / \"hdf5\"),\n        # Processed data migrations\n        (self.processed_dir / \"catalogs\", self.processed_dir / \"temp_catalogs\"),\n        (self.processed_dir / \"ml_ready\", self.processed_dir / \"temp_ml_ready\"),\n        (self.processed_dir / \"features\", self.processed_dir / \"temp_features\"),\n    ]\n\n    for old_path, new_path in migrations:\n        if old_path.exists() and old_path.is_dir():\n            new_path.parent.mkdir(parents=True, exist_ok=True)\n            logger.info(f\"  \ud83d\udce6 {old_path} -&gt; {new_path}\")\n            # Note: Actual file moving would be done manually or with additional logic\n\n    logger.info(\"\u2705 Migration plan created. Manual file moving required.\")\n</code></pre>"},{"location":"api/astro_lab.data/#astro_lab.data.DataConfig.setup_directories","title":"setup_directories","text":"<pre><code>setup_directories()\n</code></pre> <p>Create only core data directory structure (no survey templates).</p> Source code in <code>src\\astro_lab\\data\\config.py</code> <pre><code>def setup_directories(self):\n    \"\"\"Create only core data directory structure (no survey templates).\"\"\"\n    # Only create core directories\n    core_dirs = [\n        self.raw_dir,\n        self.processed_dir,\n        self.cache_dir,\n        self.experiments_dir,\n        self.results_dir,\n        self.configs_dir,\n    ]\n\n    # Create only core directories\n    for dir_path in core_dirs:\n        dir_path.mkdir(parents=True, exist_ok=True)\n\n    logger.info(f\"\ud83d\udcc1 Core data structure created in: {self.base_dir}\")\n</code></pre>"},{"location":"api/astro_lab.data/#astro_lab.data.check_astroquery_available","title":"check_astroquery_available","text":"<pre><code>check_astroquery_available() -&gt; bool\n</code></pre> <p>Check if astroquery is available.</p> Source code in <code>src\\astro_lab\\data\\utils.py</code> <pre><code>def check_astroquery_available() -&gt; bool:\n    \"\"\"Check if astroquery is available.\"\"\"\n    try:\n        import astroquery\n\n        return True\n    except ImportError:\n        return False\n</code></pre>"},{"location":"api/astro_lab.data/#astro_lab.data.create_astro_datamodule","title":"create_astro_datamodule","text":"<pre><code>create_astro_datamodule(survey: str, **kwargs) -&gt; AstroDataModule\n</code></pre> <p>Create AstroDataModule for given survey.</p> Source code in <code>src\\astro_lab\\data\\__init__.py</code> <pre><code>def create_astro_datamodule(survey: str, **kwargs) -&gt; AstroDataModule:\n    \"\"\"Create AstroDataModule for given survey.\"\"\"\n    return AstroDataModule(survey=survey, **kwargs)\n</code></pre>"},{"location":"api/astro_lab.data/#astro_lab.data.create_graph_from_dataframe","title":"create_graph_from_dataframe","text":"<pre><code>create_graph_from_dataframe(\n    df: DataFrame,\n    survey_type: str,\n    k_neighbors: int = 8,\n    distance_threshold: float = 50.0,\n    output_path: Optional[Path] = None,\n    **kwargs: Any\n) -&gt; Optional[Data]\n</code></pre> <p>Create PyTorch Geometric graph from DataFrame using configuration. \ud83d\udd78\ufe0f</p> Source code in <code>src\\astro_lab\\data\\preprocessing.py</code> <pre><code>def create_graph_from_dataframe(\n    df: pl.DataFrame,\n    survey_type: str,\n    k_neighbors: int = 8,\n    distance_threshold: float = 50.0,\n    output_path: Optional[Path] = None,\n    **kwargs: Any,\n) -&gt; Optional[Data]:\n    \"\"\"\n    Create PyTorch Geometric graph from DataFrame using configuration. \ud83d\udd78\ufe0f\n    \"\"\"\n    logger.info(f\"\ud83d\udd04 Creating graph for {survey_type} with k={k_neighbors}\")\n\n    # Get survey configuration or use generic\n    config = SURVEY_GRAPH_CONFIG.get(survey_type, {})\n\n    # Extract coordinates\n    coord_cols = config.get(\"coord_cols\", [])\n    if not coord_cols:\n        # Try to find coordinate columns\n        coord_patterns = [\n            [\"ra\", \"dec\"],\n            [\"raLIN\", \"decLIN\"],\n            [\"x\", \"y\", \"z\"],\n        ]\n        for pattern in coord_patterns:\n            if all(col in df.columns for col in pattern):\n                coord_cols = pattern\n                break\n\n    if not coord_cols:\n        raise ValueError(f\"No coordinate columns found for {survey_type}\")\n\n    coords = df.select(coord_cols).to_numpy()\n\n    # Create k-NN graph\n    edge_index = _create_knn_graph_gpu(coords, k_neighbors)\n\n    # Prepare features\n    feature_cols = config.get(\"feature_cols\", [])\n    if not feature_cols:\n        # Use all numeric columns as features\n        feature_cols = [\n            col for col in df.columns\n            if col not in coord_cols\n            and df[col].dtype in [pl.Float32, pl.Float64, pl.Int32, pl.Int64]\n        ]\n\n    # Filter to available columns\n    available_features = [col for col in feature_cols if col in df.columns]\n    if not available_features:\n        # Fallback to first numeric column or dummy\n        numeric_cols = [\n            col for col in df.columns\n            if df[col].dtype in [pl.Float32, pl.Float64, pl.Int32, pl.Int64]\n        ]\n        available_features = numeric_cols[:1] if numeric_cols else []\n\n    if available_features:\n        features = df.select(available_features).to_torch(dtype=pl.Float32)\n        features = torch.nan_to_num(features, nan=0.0)\n    else:\n        features = torch.ones((len(df), 1), dtype=torch.float32)\n        available_features = [\"dummy\"]\n\n    # Create labels\n    num_classes = config.get(\"num_classes\", 2)\n    label_source = config.get(\"label_source\")\n\n    if label_source and label_source in df.columns:\n        label_bins = config.get(\"label_bins\")\n        if label_bins:\n            values = df[label_source].to_numpy()\n            values = np.nan_to_num(values, nan=0.0)\n            labels = np.digitize(values, bins=np.array(label_bins)) - 1\n            labels = np.clip(labels, 0, num_classes - 1)\n            y = torch.tensor(labels, dtype=torch.long)\n        else:\n            y = torch.randint(0, num_classes, (len(df),), dtype=torch.long)\n    else:\n        y = torch.randint(0, num_classes, (len(df),), dtype=torch.long)\n\n    # Create graph\n    data = Data(\n        x=features,\n        edge_index=edge_index,\n        pos=torch.tensor(coords, dtype=torch.float32),\n        y=y,\n        num_nodes=len(df),\n    )\n\n    # Add metadata\n    data.survey_name = survey_type.capitalize()\n    data.feature_names = available_features\n    data.coord_names = coord_cols\n    data.k_neighbors = k_neighbors\n\n    # Save graph if output path provided\n    if output_path is None:\n        output_dir = Path(\"data/processed\") / survey_type\n        output_dir.mkdir(parents=True, exist_ok=True)\n        output_path = output_dir / f\"{survey_type}.pt\"\n    else:\n        output_path.parent.mkdir(parents=True, exist_ok=True)\n\n    torch.save(data, output_path)\n    logger.info(f\"\ud83d\udcbe Saved graph to {output_path}\")\n\n    return data\n</code></pre>"},{"location":"api/astro_lab.data/#astro_lab.data.create_training_splits","title":"create_training_splits","text":"<pre><code>create_training_splits(\n    df: DataFrame,\n    test_size: float = 0.2,\n    val_size: float = 0.1,\n    stratify_column: Optional[str] = None,\n    random_state: Optional[int] = 42,\n    shuffle: bool = True,\n) -&gt; Tuple[DataFrame, DataFrame, DataFrame]\n</code></pre> <p>Create train/validation/test splits from DataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>Input DataFrame</p> required <code>test_size</code> <code>float</code> <p>Fraction for test set</p> <code>0.2</code> <code>val_size</code> <code>float</code> <p>Fraction for validation set</p> <code>0.1</code> <code>stratify_column</code> <code>Optional[str]</code> <p>Column to stratify on</p> <code>None</code> <code>random_state</code> <code>Optional[int]</code> <p>Random seed</p> <code>42</code> <code>shuffle</code> <code>bool</code> <p>Whether to shuffle data</p> <code>True</code> <p>Returns:</p> Type Description <code>Tuple[DataFrame, DataFrame, DataFrame]</code> <p>Tuple of (train_df, val_df, test_df)</p> Source code in <code>src\\astro_lab\\data\\utils.py</code> <pre><code>def create_training_splits(\n    df: pl.DataFrame,\n    test_size: float = 0.2,\n    val_size: float = 0.1,\n    stratify_column: Optional[str] = None,\n    random_state: Optional[int] = 42,\n    shuffle: bool = True,\n) -&gt; Tuple[pl.DataFrame, pl.DataFrame, pl.DataFrame]:\n    \"\"\"\n    Create train/validation/test splits from DataFrame.\n\n    Args:\n        df: Input DataFrame\n        test_size: Fraction for test set\n        val_size: Fraction for validation set\n        stratify_column: Column to stratify on\n        random_state: Random seed\n        shuffle: Whether to shuffle data\n\n    Returns:\n        Tuple of (train_df, val_df, test_df)\n    \"\"\"\n    if random_state is not None:\n        np.random.seed(random_state)\n\n    n_samples = len(df)\n\n    if stratify_column and stratify_column in df.columns:\n        # Stratified split\n        unique_values = df[stratify_column].unique()\n        train_dfs = []\n        val_dfs = []\n        test_dfs = []\n\n        for value in unique_values:\n            subset = df.filter(pl.col(stratify_column) == value)\n            n_subset = len(subset)\n\n            if n_subset &lt; 3:\n                # Too few samples for stratification, add to train\n                train_dfs.append(subset)\n                continue\n\n            # Calculate split sizes\n            n_test = max(1, int(n_subset * test_size))\n            n_val = max(1, int(n_subset * val_size))\n            n_train = n_subset - n_test - n_val\n\n            # Create indices\n            indices = np.arange(n_subset)\n            if shuffle:\n                np.random.shuffle(indices)\n\n            # Split indices\n            train_indices = indices[:n_train]\n            val_indices = indices[n_train : n_train + n_val]\n            test_indices = indices[n_train + n_val :]\n\n            # Create splits\n            train_dfs.append(subset.take(train_indices))\n            val_dfs.append(subset.take(val_indices))\n            test_dfs.append(subset.take(test_indices))\n\n        # Combine splits\n        train_df = pl.concat(train_dfs) if train_dfs else pl.DataFrame()\n        val_df = pl.concat(val_dfs) if val_dfs else pl.DataFrame()\n        test_df = pl.concat(test_dfs) if test_dfs else pl.DataFrame()\n\n    else:\n        # Simple random split\n        indices = np.arange(n_samples)\n        if shuffle:\n            np.random.shuffle(indices)\n\n        n_test = int(n_samples * test_size)\n        n_val = int(n_samples * val_size)\n        n_train = n_samples - n_test - n_val\n\n        train_indices = indices[:n_train]\n        val_indices = indices[n_train : n_train + n_val]\n        test_indices = indices[n_train + n_val :]\n\n        train_df = df.take(train_indices)\n        val_df = df.take(val_indices)\n        test_df = df.take(test_indices)\n\n    return train_df, val_df, test_df\n</code></pre>"},{"location":"api/astro_lab.data/#astro_lab.data.detect_survey_type","title":"detect_survey_type","text":"<pre><code>detect_survey_type(dataset_name: str, df: Optional[DataFrame]) -&gt; str\n</code></pre> <p>Detect survey type from dataset name or data structure.</p> Source code in <code>src\\astro_lab\\data\\core.py</code> <pre><code>def detect_survey_type(dataset_name: str, df: Optional[pl.DataFrame]) -&gt; str:\n    \"\"\"Detect survey type from dataset name or data structure.\"\"\"\n    name_lower = dataset_name.lower()\n    if \"gaia\" in name_lower:\n        return \"gaia\"\n    elif \"sdss\" in name_lower:\n        return \"sdss\"\n    elif \"nsa\" in name_lower:\n        return \"nsa\"\n    elif \"linear\" in name_lower:\n        return \"linear\"\n    elif \"exoplanet\" in name_lower:\n        return \"exoplanet\"\n    elif \"tng\" in name_lower:  # More general for TNG50, TNG100 etc.\n        return \"tng\"\n    elif \"rrlyrae\" in name_lower:\n        return \"rrlyrae\"\n    # Fallback based on columns if name is not indicative\n    if df is not None:\n        cols = {c.lower() for c in df.columns}\n        if \"phot_g_mean_mag\" in cols and \"parallax\" in cols:\n            return \"gaia\"\n        if \"specobjid\" in cols and \"z\" in cols:\n            return \"sdss\"\n        if \"iauname\" in cols and \"nsa_version\" in cols:\n            return \"nsa\"\n\n    return \"generic\"\n</code></pre>"},{"location":"api/astro_lab.data/#astro_lab.data.download_2mass","title":"download_2mass","text":"<pre><code>download_2mass(region: str = 'all_sky', magnitude_limit: float = 15.0) -&gt; Path\n</code></pre> <p>Download 2MASS catalog.</p> Source code in <code>src\\astro_lab\\data\\manager.py</code> <pre><code>def download_2mass(region: str = \"all_sky\", magnitude_limit: float = 15.0) -&gt; Path:\n    \"\"\"Download 2MASS catalog.\"\"\"\n    return data_manager.download_survey_catalog(\"2mass\", magnitude_limit, region)\n</code></pre>"},{"location":"api/astro_lab.data/#astro_lab.data.download_pan_starrs","title":"download_pan_starrs","text":"<pre><code>download_pan_starrs(region: str = 'all_sky', magnitude_limit: float = 15.0) -&gt; Path\n</code></pre> <p>Download Pan-STARRS catalog.</p> Source code in <code>src\\astro_lab\\data\\manager.py</code> <pre><code>def download_pan_starrs(region: str = \"all_sky\", magnitude_limit: float = 15.0) -&gt; Path:\n    \"\"\"Download Pan-STARRS catalog.\"\"\"\n    return data_manager.download_survey_catalog(\"pan_starrs\", magnitude_limit, region)\n</code></pre>"},{"location":"api/astro_lab.data/#astro_lab.data.download_sdss","title":"download_sdss","text":"<pre><code>download_sdss(region: str = 'all_sky', magnitude_limit: float = 15.0) -&gt; Path\n</code></pre> <p>Download SDSS catalog.</p> Source code in <code>src\\astro_lab\\data\\manager.py</code> <pre><code>def download_sdss(region: str = \"all_sky\", magnitude_limit: float = 15.0) -&gt; Path:\n    \"\"\"Download SDSS catalog.\"\"\"\n    return data_manager.download_survey_catalog(\"sdss\", magnitude_limit, region)\n</code></pre>"},{"location":"api/astro_lab.data/#astro_lab.data.download_survey","title":"download_survey","text":"<pre><code>download_survey(\n    survey: str, region: str = \"all_sky\", magnitude_limit: float = 15.0\n) -&gt; Path\n</code></pre> <p>Download any survey catalog.</p> Source code in <code>src\\astro_lab\\data\\manager.py</code> <pre><code>def download_survey(\n    survey: str, region: str = \"all_sky\", magnitude_limit: float = 15.0\n) -&gt; Path:\n    \"\"\"Download any survey catalog.\"\"\"\n    return data_manager.download_survey_catalog(survey, magnitude_limit, region)\n</code></pre>"},{"location":"api/astro_lab.data/#astro_lab.data.download_wise","title":"download_wise","text":"<pre><code>download_wise(region: str = 'all_sky', magnitude_limit: float = 15.0) -&gt; Path\n</code></pre> <p>Download WISE catalog.</p> Source code in <code>src\\astro_lab\\data\\manager.py</code> <pre><code>def download_wise(region: str = \"all_sky\", magnitude_limit: float = 15.0) -&gt; Path:\n    \"\"\"Download WISE catalog.\"\"\"\n    return data_manager.download_survey_catalog(\"wise\", magnitude_limit, region)\n</code></pre>"},{"location":"api/astro_lab.data/#astro_lab.data.get_data_statistics","title":"get_data_statistics","text":"<pre><code>get_data_statistics(df: DataFrame) -&gt; Dict[str, Any]\n</code></pre> <p>Get comprehensive statistics for a DataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>Polars DataFrame</p> required <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dictionary with statistics</p> Source code in <code>src\\astro_lab\\data\\utils.py</code> <pre><code>def get_data_statistics(df: pl.DataFrame) -&gt; Dict[str, Any]:\n    \"\"\"\n    Get comprehensive statistics for a DataFrame.\n\n    Args:\n        df: Polars DataFrame\n\n    Returns:\n        Dictionary with statistics\n    \"\"\"\n    stats = {\n        \"n_rows\": len(df),\n        \"n_columns\": len(df.columns),\n        \"memory_usage_mb\": df.estimated_size() / (1024 * 1024),\n        \"columns\": df.columns,\n        \"dtypes\": {col: str(dtype) for col, dtype in df.schema.items()},\n    }\n\n    # Detect magnitude columns\n    mag_cols = _detect_magnitude_columns(df)\n    if mag_cols:\n        stats[\"magnitude_columns\"] = mag_cols\n        # Calculate magnitude statistics\n        for col in mag_cols:\n            if col in df.columns:\n                col_stats = df[col].describe()\n                stats[f\"{col}_stats\"] = {\n                    \"mean\": col_stats[\"mean\"],\n                    \"std\": col_stats[\"std\"],\n                    \"min\": col_stats[\"min\"],\n                    \"max\": col_stats[\"max\"],\n                    \"null_count\": df[col].null_count(),\n                }\n\n    # Detect coordinate columns\n    coord_cols = [\n        col for col in df.columns if col.lower() in [\"ra\", \"dec\", \"x\", \"y\", \"z\"]\n    ]\n    if coord_cols:\n        stats[\"coordinate_columns\"] = coord_cols\n\n    return stats\n</code></pre>"},{"location":"api/astro_lab.data/#astro_lab.data.get_fits_info","title":"get_fits_info","text":"<pre><code>get_fits_info(fits_path: Union[str, Path]) -&gt; Dict[str, Any]\n</code></pre> <p>Get information about FITS file.</p> <p>Parameters:</p> Name Type Description Default <code>fits_path</code> <code>Union[str, Path]</code> <p>Path to FITS file</p> required <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dictionary with FITS information</p> Source code in <code>src\\astro_lab\\data\\utils.py</code> <pre><code>def get_fits_info(fits_path: Union[str, Path]) -&gt; Dict[str, Any]:\n    \"\"\"\n    Get information about FITS file.\n\n    Args:\n        fits_path: Path to FITS file\n\n    Returns:\n        Dictionary with FITS information\n    \"\"\"\n    try:\n        fits_path = Path(fits_path)\n        if not fits_path.exists():\n            raise FileNotFoundError(f\"FITS file not found: {fits_path}\")\n\n        info = {\n            \"filename\": fits_path.name,\n            \"file_size_mb\": fits_path.stat().st_size / (1024 * 1024),\n            \"hdus\": [],\n        }\n\n        with fits.open(fits_path) as hdul:\n            for i, hdu in enumerate(hdul):\n                hdu_info = {\n                    \"index\": i,\n                    \"name\": hdu.name,\n                    \"type\": hdu.header.get(\"XTENSION\", \"IMAGE\"),\n                }\n\n                if hdu.is_image:\n                    hdu_info.update(\n                        {\n                            \"shape\": hdu.data.shape if hdu.data is not None else None,\n                            \"dtype\": str(hdu.data.dtype)\n                            if hdu.data is not None\n                            else None,\n                        }\n                    )\n                elif hdu.is_table:\n                    hdu_info.update(\n                        {\n                            \"n_rows\": len(hdu.data),\n                            \"n_cols\": len(hdu.data.dtype.names)\n                            if hdu.data.dtype.names\n                            else 0,\n                            \"columns\": list(hdu.data.dtype.names)\n                            if hdu.data.dtype.names\n                            else [],\n                        }\n                    )\n\n                info[\"hdus\"].append(hdu_info)\n\n        return info\n\n    except Exception as e:\n        return {\"error\": str(e)}\n</code></pre>"},{"location":"api/astro_lab.data/#astro_lab.data.get_survey_paths","title":"get_survey_paths","text":"<pre><code>get_survey_paths(survey: str) -&gt; Dict[str, Path]\n</code></pre> <p>Get all standard paths for a survey.</p> Source code in <code>src\\astro_lab\\data\\config.py</code> <pre><code>def get_survey_paths(survey: str) -&gt; Dict[str, Path]:\n    \"\"\"Get all standard paths for a survey.\"\"\"\n    return {\n        \"raw_dir\": data_config.get_survey_raw_dir(survey),\n        \"processed_dir\": data_config.get_survey_processed_dir(survey),\n        \"catalog\": data_config.get_catalog_path(survey),\n        \"graphs\": data_config.get_graph_path(survey),\n        \"tensors\": data_config.get_tensor_path(survey),\n    }\n</code></pre>"},{"location":"api/astro_lab.data/#astro_lab.data.import_fits","title":"import_fits","text":"<pre><code>import_fits(fits_file: Union[str, Path], catalog_name: str) -&gt; Path\n</code></pre> <p>Import FITS catalog.</p> Source code in <code>src\\astro_lab\\data\\manager.py</code> <pre><code>def import_fits(fits_file: Union[str, Path], catalog_name: str) -&gt; Path:\n    \"\"\"Import FITS catalog.\"\"\"\n    return data_manager.import_fits_catalog(fits_file, catalog_name)\n</code></pre>"},{"location":"api/astro_lab.data/#astro_lab.data.import_tng50","title":"import_tng50","text":"<pre><code>import_tng50(hdf5_file: Union[str, Path], dataset_name: str = 'PartType0') -&gt; Path\n</code></pre> <p>Convenience function to import a TNG50 HDF5 file.</p> Source code in <code>src\\astro_lab\\data\\manager.py</code> <pre><code>def import_tng50(hdf5_file: Union[str, Path], dataset_name: str = \"PartType0\") -&gt; Path:\n    \"\"\"Convenience function to import a TNG50 HDF5 file.\"\"\"\n    return data_manager.import_tng50_hdf5(hdf5_file, dataset_name)\n</code></pre>"},{"location":"api/astro_lab.data/#astro_lab.data.list_catalogs","title":"list_catalogs","text":"<pre><code>list_catalogs() -&gt; DataFrame\n</code></pre> <p>Convenience function to list available catalogs.</p> Source code in <code>src\\astro_lab\\data\\manager.py</code> <pre><code>def list_catalogs() -&gt; pl.DataFrame:\n    \"\"\"Convenience function to list available catalogs.\"\"\"\n    return data_manager.list_catalogs()\n</code></pre>"},{"location":"api/astro_lab.data/#astro_lab.data.load_catalog","title":"load_catalog","text":"<pre><code>load_catalog(catalog_path: Union[str, Path]) -&gt; DataFrame\n</code></pre> <p>Convenience function to load a catalog.</p> Source code in <code>src\\astro_lab\\data\\manager.py</code> <pre><code>def load_catalog(catalog_path: Union[str, Path]) -&gt; pl.DataFrame:\n    \"\"\"Convenience function to load a catalog.\"\"\"\n    return data_manager.load_catalog(catalog_path)\n</code></pre>"},{"location":"api/astro_lab.data/#astro_lab.data.load_fits_optimized","title":"load_fits_optimized","text":"<pre><code>load_fits_optimized(\n    fits_path: Union[str, Path],\n    hdu_index: int = 0,\n    memmap: bool = True,\n    do_not_scale: bool = False,\n    section: Optional[Tuple[slice, ...]] = None,\n    max_memory_mb: float = 1000.0,\n) -&gt; Optional[Union[ndarray, Any]]\n</code></pre> <p>Load FITS file with memory optimization.</p> <p>Parameters:</p> Name Type Description Default <code>fits_path</code> <code>Union[str, Path]</code> <p>Path to FITS file</p> required <code>hdu_index</code> <code>int</code> <p>HDU index to load</p> <code>0</code> <code>memmap</code> <code>bool</code> <p>Use memory mapping for large files</p> <code>True</code> <code>do_not_scale</code> <code>bool</code> <p>Don't scale data</p> <code>False</code> <code>section</code> <code>Optional[Tuple[slice, ...]]</code> <p>Section to load (for partial loading)</p> <code>None</code> <code>max_memory_mb</code> <code>float</code> <p>Maximum memory usage in MB</p> <code>1000.0</code> <p>Returns:</p> Type Description <code>Optional[Union[ndarray, Any]]</code> <p>FITS data as numpy array or astropy Table</p> Source code in <code>src\\astro_lab\\data\\utils.py</code> <pre><code>def load_fits_optimized(\n    fits_path: Union[str, Path],\n    hdu_index: int = 0,\n    memmap: bool = True,\n    do_not_scale: bool = False,\n    section: Optional[Tuple[slice, ...]] = None,\n    max_memory_mb: float = 1000.0,\n) -&gt; Optional[Union[np.ndarray, Any]]:\n    \"\"\"\n    Load FITS file with memory optimization.\n\n    Args:\n        fits_path: Path to FITS file\n        hdu_index: HDU index to load\n        memmap: Use memory mapping for large files\n        do_not_scale: Don't scale data\n        section: Section to load (for partial loading)\n        max_memory_mb: Maximum memory usage in MB\n\n    Returns:\n        FITS data as numpy array or astropy Table\n    \"\"\"\n    try:\n        fits_path = Path(fits_path)\n        if not fits_path.exists():\n            raise FileNotFoundError(f\"FITS file not found: {fits_path}\")\n\n        # Check file size\n        file_size_mb = fits_path.stat().st_size / (1024 * 1024)\n        if file_size_mb &gt; max_memory_mb:\n            print(f\"\u26a0\ufe0f Large FITS file: {file_size_mb:.1f} MB\")\n            if not memmap:\n                print(\"   Consider using memmap=True for large files\")\n\n        # Load FITS file\n        with fits.open(fits_path, memmap=memmap, do_not_scale=do_not_scale) as hdul:\n            hdu = hdul[hdu_index]\n\n            # Check if it's a table or image\n            is_image = hasattr(hdu, \"is_image\") and hdu.is_image\n            if is_image:\n                # Image data\n                if section is not None:\n                    data = hdu.data[section] if hasattr(hdu, \"data\") else None\n                else:\n                    data = hdu.data if hasattr(hdu, \"data\") else None\n                return data\n            else:\n                # Table data\n                try:\n                    table = Table(hdu.data)\n                except Exception:\n                    return None\n                try:\n                    names = [\n                        name\n                        for name in table.colnames\n                        if hasattr(table[name], \"shape\") and len(table[name].shape) &lt;= 1\n                    ]\n                    if len(names) &lt; len(table.colnames):\n                        filtered_cols = set(table.colnames) - set(names)\n                        print(\n                            f\"\ud83d\udccb Filtered out {len(filtered_cols)} multidimensional columns: {list(filtered_cols)[:5]}{'...' if len(filtered_cols) &gt; 5 else ''}\"\n                        )\n                    filtered_table = table[names]\n                    df_pandas = filtered_table.to_pandas()  # type: ignore\n                    return pl.from_pandas(df_pandas)\n                except Exception as e:\n                    print(f\"\u26a0\ufe0f Error converting table: {e}\")\n                    return table\n\n    except Exception as e:\n        print(f\"Error loading FITS file: {e}\")\n        return None\n</code></pre>"},{"location":"api/astro_lab.data/#astro_lab.data.load_fits_table_optimized","title":"load_fits_table_optimized","text":"<pre><code>load_fits_table_optimized(\n    fits_path: Union[str, Path],\n    hdu_index: int = 1,\n    columns: Optional[List[str]] = None,\n    max_rows: Optional[int] = None,\n    as_polars: bool = True,\n) -&gt; Optional[Union[DataFrame, Any]]\n</code></pre> <p>Load FITS table with optimization.</p> <p>Parameters:</p> Name Type Description Default <code>fits_path</code> <code>Union[str, Path]</code> <p>Path to FITS file</p> required <code>hdu_index</code> <code>int</code> <p>HDU index (usually 1 for tables)</p> <code>1</code> <code>columns</code> <code>Optional[List[str]]</code> <p>Specific columns to load</p> <code>None</code> <code>max_rows</code> <code>Optional[int]</code> <p>Maximum number of rows to load</p> <code>None</code> <code>as_polars</code> <code>bool</code> <p>Return as Polars DataFrame</p> <code>True</code> <p>Returns:</p> Type Description <code>Optional[Union[DataFrame, Any]]</code> <p>FITS table as Polars DataFrame or astropy Table</p> Source code in <code>src\\astro_lab\\data\\utils.py</code> <pre><code>def load_fits_table_optimized(\n    fits_path: Union[str, Path],\n    hdu_index: int = 1,\n    columns: Optional[List[str]] = None,\n    max_rows: Optional[int] = None,\n    as_polars: bool = True,\n) -&gt; Optional[Union[pl.DataFrame, Any]]:\n    \"\"\"\n    Load FITS table with optimization.\n\n    Args:\n        fits_path: Path to FITS file\n        hdu_index: HDU index (usually 1 for tables)\n        columns: Specific columns to load\n        max_rows: Maximum number of rows to load\n        as_polars: Return as Polars DataFrame\n\n    Returns:\n        FITS table as Polars DataFrame or astropy Table\n    \"\"\"\n    try:\n        fits_path = Path(fits_path)\n        if not fits_path.exists():\n            raise FileNotFoundError(f\"FITS file not found: {fits_path}\")\n\n        # Load FITS table\n        with fits.open(fits_path) as hdul:\n            hdu = hdul[hdu_index]\n\n            is_table = hasattr(hdu, \"is_table\") and hdu.is_table\n            if not is_table:\n                raise ValueError(f\"HDU {hdu_index} is not a table\")\n\n            # Load table\n            try:\n                table = Table(hdu.data)\n            except Exception:\n                return None\n\n            # Filter columns if specified\n            if columns:\n                available_cols = [\n                    col for col in columns if col in getattr(table, \"colnames\", [])\n                ]\n                if available_cols:\n                    table = table[available_cols]\n                else:\n                    raise ValueError(f\"No specified columns found: {columns}\")\n\n            # Filter rows if specified\n            if max_rows and len(table) &gt; max_rows:\n                table = table[:max_rows]\n\n            # Convert to Polars if requested\n            if as_polars:\n                try:\n                    # Handle multidimensional columns\n                    names = [\n                        name\n                        for name in getattr(table, \"colnames\", [])\n                        if hasattr(table[name], \"shape\") and len(table[name].shape) &lt;= 1\n                    ]\n                    if len(names) &lt; len(getattr(table, \"colnames\", [])):\n                        filtered_cols = set(table.colnames) - set(names)\n                        print(\n                            f\"\ud83d\udccb Filtered out {len(filtered_cols)} multidimensional columns: {list(filtered_cols)[:5]}{'...' if len(filtered_cols) &gt; 5 else ''}\"\n                        )\n\n                    # Use only 1D columns for Polars conversion\n                    filtered_table = table[names]\n                    df_pandas = filtered_table.to_pandas()  # type: ignore\n                    return pl.from_pandas(df_pandas)\n                except Exception as e:\n                    print(f\"\u26a0\ufe0f Error converting to Polars: {e}\")\n                    return table\n            else:\n                return table\n\n    except Exception as e:\n        print(f\"Error loading FITS table: {e}\")\n        return None\n</code></pre>"},{"location":"api/astro_lab.data/#astro_lab.data.load_splits_from_parquet","title":"load_splits_from_parquet","text":"<pre><code>load_splits_from_parquet(\n    base_path: Union[str, Path], dataset_name: str\n) -&gt; Tuple[DataFrame, DataFrame, DataFrame]\n</code></pre> <p>Load train/val/test splits from parquet files.</p> <p>Parameters:</p> Name Type Description Default <code>base_path</code> <code>Union[str, Path]</code> <p>Base directory containing splits</p> required <code>dataset_name</code> <code>str</code> <p>Name of dataset</p> required <p>Returns:</p> Type Description <code>Tuple[DataFrame, DataFrame, DataFrame]</code> <p>Tuple of (train_df, val_df, test_df)</p> Source code in <code>src\\astro_lab\\data\\utils.py</code> <pre><code>def load_splits_from_parquet(\n    base_path: Union[str, Path], dataset_name: str\n) -&gt; Tuple[pl.DataFrame, pl.DataFrame, pl.DataFrame]:\n    \"\"\"\n    Load train/val/test splits from parquet files.\n\n    Args:\n        base_path: Base directory containing splits\n        dataset_name: Name of dataset\n\n    Returns:\n        Tuple of (train_df, val_df, test_df)\n    \"\"\"\n    base_path = Path(base_path)\n\n    train_path = base_path / f\"{dataset_name}_train.parquet\"\n    val_path = base_path / f\"{dataset_name}_val.parquet\"\n    test_path = base_path / f\"{dataset_name}_test.parquet\"\n\n    if not all(p.exists() for p in [train_path, val_path, test_path]):\n        raise FileNotFoundError(f\"Split files not found in {base_path}\")\n\n    df_train = pl.read_parquet(train_path)\n    df_val = pl.read_parquet(val_path)\n    df_test = pl.read_parquet(test_path)\n\n    return df_train, df_val, df_test\n</code></pre>"},{"location":"api/astro_lab.data/#astro_lab.data.load_survey_data","title":"load_survey_data","text":"<pre><code>load_survey_data(\n    survey: str,\n    max_samples: Optional[int] = None,\n    return_tensor: bool = True,\n    **kwargs: Any\n) -&gt; Union[AstroDataset, SurveyTensorDict]\n</code></pre> <p>Load survey data as an AstroDataset or SurveyTensor. Generic function for all surveys.</p> Source code in <code>src\\astro_lab\\data\\core.py</code> <pre><code>def load_survey_data(\n    survey: str,\n    max_samples: Optional[int] = None,\n    return_tensor: bool = True,\n    **kwargs: Any,\n) -&gt; Union[AstroDataset, \"SurveyTensorDict\"]:\n    \"\"\"\n    Load survey data as an AstroDataset or SurveyTensor.\n    Generic function for all surveys.\n    \"\"\"\n    from .manager import data_manager\n\n    # Ensure root is passed to AstroDataset\n    if \"root\" not in kwargs:\n        kwargs[\"root\"] = str(data_config.base_dir)\n\n    dataset = AstroDataset(survey=survey, max_samples=max_samples, **kwargs)\n\n    if return_tensor:\n        # Assumes process() creates a tensor accessible via dataset\n        # This part needs to be aligned with AstroDataset implementation\n        if hasattr(dataset, \"to_tensor\"):\n            return dataset.to_tensor()\n        else:\n            # Fallback or error\n            raise NotImplementedError(\n                \"AstroDataset must have a method to convert to SurveyTensor\"\n            )\n    return dataset\n</code></pre>"},{"location":"api/astro_lab.data/#astro_lab.data.load_tng50_data","title":"load_tng50_data","text":"<pre><code>load_tng50_data(\n    max_samples: Optional[int] = None,\n    particle_type: str = \"PartType0\",\n    return_tensor: bool = False,\n) -&gt; Union[AstroDataset, Any]\n</code></pre> <p>Load TNG50 simulation data as a survey-like dataset.</p> <p>Parameters:</p> Name Type Description Default <code>max_samples</code> <code>Optional[int]</code> <p>Maximum number of particles to load (None = all)</p> <code>None</code> <code>particle_type</code> <code>str</code> <p>Particle type to load (PartType0, PartType1, PartType4, PartType5)</p> <code>'PartType0'</code> <code>return_tensor</code> <code>bool</code> <p>Whether to return as tensor instead of AstroDataset</p> <code>False</code> <p>Returns:</p> Type Description <code>Union[AstroDataset, Any]</code> <p>AstroDataset or tensor with TNG50 simulation data</p> Source code in <code>src\\astro_lab\\data\\core.py</code> <pre><code>def load_tng50_data(\n    max_samples: Optional[int] = None,\n    particle_type: str = \"PartType0\",\n    return_tensor: bool = False,\n) -&gt; Union[AstroDataset, Any]:\n    \"\"\"Load TNG50 simulation data as a survey-like dataset.\n\n    Args:\n        max_samples: Maximum number of particles to load (None = all)\n        particle_type: Particle type to load (PartType0, PartType1, PartType4, PartType5)\n        return_tensor: Whether to return as tensor instead of AstroDataset\n\n    Returns:\n        AstroDataset or tensor with TNG50 simulation data\n    \"\"\"\n    config = get_survey_config(\"tng50\")\n\n    # Load TNG50 data from processed files\n    data_path = Path(\"data/processed/tng50_combined.parquet\")\n    if not data_path.exists():\n        print(f\"\u26a0\ufe0f TNG50 data not found at {data_path}. Generating demo data...\")\n        # Create AstroDataset with survey parameter and generate demo data\n        dataset = AstroDataset(\n            survey=\"tng50\", max_samples=max_samples, return_tensor=False\n        )\n        dataset.download()  # Generate demo data\n        # Don't set dataset.data directly - let PyG handle it properly\n        return dataset\n\n    # Load with Polars\n    df = pl.read_parquet(data_path)\n\n    # Filter by particle type if specified\n    if particle_type and \"particle_type\" in df.columns:\n        df = df.filter(pl.col(\"particle_type\") == particle_type)\n\n    # Apply sampling if requested\n    if max_samples and len(df) &gt; max_samples:\n        df = df.sample(n=max_samples, seed=42)\n\n    # Create AstroDataset\n    dataset = AstroDataset(\n        name=config[\"name\"],\n        data=df,\n        coord_cols=config[\"coord_cols\"],\n        mag_cols=config[\"mag_cols\"],\n        extra_cols=config[\"extra_cols\"],\n        color_pairs=config[\"color_pairs\"],\n        tensor_metadata=config,\n    )\n\n    if return_tensor:\n        # Return as tensor if requested\n        return dataset.get_spatial_tensor()\n\n    return dataset\n</code></pre>"},{"location":"api/astro_lab.data/#astro_lab.data.load_tng50_temporal_data","title":"load_tng50_temporal_data","text":"<pre><code>load_tng50_temporal_data(\n    max_samples: Optional[int] = None, snapshot_id: Optional[int] = None\n) -&gt; AstroDataset\n</code></pre> <p>Load TNG50 temporal simulation data.</p> <p>Returns:</p> Type Description <code>AstroDataset</code> <p>AstroDataset with TNG50 temporal simulation data</p> Source code in <code>src\\astro_lab\\data\\core.py</code> <pre><code>def load_tng50_temporal_data(\n    max_samples: Optional[int] = None, snapshot_id: Optional[int] = None\n) -&gt; AstroDataset:\n    \"\"\"\n    Load TNG50 temporal simulation data.\n\n    Returns:\n        AstroDataset with TNG50 temporal simulation data\n    \"\"\"\n\n    # Load TNG50 temporal data from processed files\n    data_path = Path(\n        \"data/processed/tng50_temporal_100mb/processed/tng50_temporal_graphs_r1.0.pt\"\n    )\n    if not data_path.exists():\n        raise FileNotFoundError(\n            f\"TNG50 temporal data not found at {data_path}. Run preprocessing first.\"\n        )\n\n    # Load PyTorch tensors\n    temporal_data = torch.load(data_path, weights_only=False)\n\n    # Extract data from temporal structure\n    if snapshot_id is not None:\n        # Load specific snapshot\n        if snapshot_id &gt;= len(temporal_data):\n            raise ValueError(\n                f\"Snapshot {snapshot_id} not available. Max: {len(temporal_data) - 1}\"\n            )\n\n        graph_data = temporal_data[snapshot_id]\n\n        # Handle dict format (actual format of the data)\n        if isinstance(graph_data, dict):\n            positions = graph_data[\"x\"][:, :3].numpy()  # x, y, z\n            masses = graph_data[\"x\"][:, 3].numpy()  # mass\n            velocities = graph_data[\"x\"][:, 4:7].numpy()  # vx, vy, vz\n            particle_types = graph_data[\"x\"][:, 7].numpy()  # particle_type\n\n            # Get cosmological parameters\n            redshifts = graph_data.get(\"redshift\", 0.0)\n            if hasattr(redshifts, \"item\"):\n                redshifts = redshifts.item()\n            elif hasattr(redshifts, \"__len__\") and len(redshifts) &gt; 0:\n                redshifts = (\n                    redshifts[0].item()\n                    if hasattr(redshifts[0], \"item\")\n                    else float(redshifts[0])\n                )\n            else:\n                redshifts = 0.0\n\n            time_gyr = graph_data.get(\"time_gyr\", 0.0)\n            if hasattr(time_gyr, \"item\"):\n                time_gyr = time_gyr.item()\n            elif hasattr(time_gyr, \"__len__\") and len(time_gyr) &gt; 0:\n                time_gyr = (\n                    time_gyr[0].item()\n                    if hasattr(time_gyr[0], \"item\")\n                    else float(time_gyr[0])\n                )\n            else:\n                time_gyr = 0.0\n\n            scale_factor = graph_data.get(\"scale_factor\", 1.0)\n            if hasattr(scale_factor, \"item\"):\n                scale_factor = scale_factor.item()\n            elif hasattr(scale_factor, \"__len__\") and len(scale_factor) &gt; 0:\n                scale_factor = (\n                    scale_factor[0].item()\n                    if hasattr(scale_factor[0], \"item\")\n                    else float(scale_factor[0])\n                )\n            else:\n                scale_factor = 1.0\n\n        elif hasattr(graph_data, \"x\"):\n            # Standard PyTorch Geometric format\n            positions = graph_data.x[:, :3].numpy()  # x, y, z\n            masses = graph_data.x[:, 3].numpy()  # mass\n            velocities = graph_data.x[:, 4:7].numpy()  # vx, vy, vz\n            particle_types = graph_data.x[:, 7].numpy()  # particle_type\n\n            # Get cosmological parameters\n            redshifts = getattr(graph_data, \"redshift\", 0.0)\n            if hasattr(redshifts, \"item\"):\n                redshifts = redshifts.item()\n            elif hasattr(redshifts, \"__len__\") and len(redshifts) &gt; 0:\n                redshifts = (\n                    redshifts[0].item()\n                    if hasattr(redshifts[0], \"item\")\n                    else float(redshifts[0])\n                )\n            else:\n                redshifts = 0.0\n\n            time_gyr = getattr(graph_data, \"time_gyr\", 0.0)\n            if hasattr(time_gyr, \"item\"):\n                time_gyr = time_gyr.item()\n            elif hasattr(time_gyr, \"__len__\") and len(time_gyr) &gt; 0:\n                time_gyr = (\n                    time_gyr[0].item()\n                    if hasattr(time_gyr[0], \"item\")\n                    else float(time_gyr[0])\n                )\n            else:\n                time_gyr = 0.0\n\n            scale_factor = getattr(graph_data, \"scale_factor\", 1.0)\n            if hasattr(scale_factor, \"item\"):\n                scale_factor = scale_factor.item()\n            elif hasattr(scale_factor, \"__len__\") and len(scale_factor) &gt; 0:\n                scale_factor = (\n                    scale_factor[0].item()\n                    if hasattr(scale_factor[0], \"item\")\n                    else float(scale_factor[0])\n                )\n            else:\n                scale_factor = 1.0\n\n        elif hasattr(graph_data, \"pos\"):\n            # Alternative format with pos attribute\n            positions = graph_data.pos[:, :3].numpy()\n            masses = (\n                graph_data.mass.numpy()\n                if hasattr(graph_data, \"mass\")\n                else np.ones(len(positions))\n            )\n            velocities = (\n                graph_data.vel.numpy()\n                if hasattr(graph_data, \"vel\")\n                else np.zeros((len(positions), 3))\n            )\n            particle_types = (\n                graph_data.particle_type.numpy()\n                if hasattr(graph_data, \"particle_type\")\n                else np.zeros(len(positions))\n            )\n\n            # Get cosmological parameters\n            redshifts = getattr(graph_data, \"redshift\", 0.0)\n            if hasattr(redshifts, \"item\"):\n                redshifts = redshifts.item()\n            elif hasattr(redshifts, \"__len__\") and len(redshifts) &gt; 0:\n                redshifts = (\n                    redshifts[0].item()\n                    if hasattr(redshifts[0], \"item\")\n                    else float(redshifts[0])\n                )\n            else:\n                redshifts = 0.0\n\n            time_gyr = getattr(graph_data, \"time_gyr\", 0.0)\n            if hasattr(time_gyr, \"item\"):\n                time_gyr = time_gyr.item()\n            elif hasattr(time_gyr, \"__len__\") and len(time_gyr) &gt; 0:\n                time_gyr = (\n                    time_gyr[0].item()\n                    if hasattr(time_gyr[0], \"item\")\n                    else float(time_gyr[0])\n                )\n            else:\n                time_gyr = 0.0\n\n            scale_factor = getattr(graph_data, \"scale_factor\", 1.0)\n            if hasattr(scale_factor, \"item\"):\n                scale_factor = scale_factor.item()\n            elif hasattr(scale_factor, \"__len__\") and len(scale_factor) &gt; 0:\n                scale_factor = (\n                    scale_factor[0].item()\n                    if hasattr(scale_factor[0], \"item\")\n                    else float(scale_factor[0])\n                )\n            else:\n                scale_factor = 1.0\n\n        else:\n            # Fallback: assume it's a dictionary or other format\n            raise ValueError(\n                f\"Unknown TNG50 temporal data format for snapshot {snapshot_id}\"\n            )\n\n        # Create DataFrame for single snapshot\n        df_data = {\n            \"x\": positions[:, 0],\n            \"y\": positions[:, 1],\n            \"z\": positions[:, 2],\n            \"mass\": masses,\n            \"velocity_0\": velocities[:, 0],\n            \"velocity_1\": velocities[:, 1],\n            \"velocity_2\": velocities[:, 2],\n            \"particle_type\": particle_types,\n            \"snapshot_id\": snapshot_id,\n            \"redshift\": redshifts,\n            \"time_gyr\": time_gyr,\n            \"scale_factor\": scale_factor,\n        }\n        df = pl.DataFrame(df_data)\n\n    else:\n        # Load all snapshots combined\n        all_data = []\n        for i, graph_data in enumerate(temporal_data):\n            # Handle dict format (actual format of the data)\n            if isinstance(graph_data, dict):\n                positions = graph_data[\"x\"][:, :3].numpy()\n                masses = graph_data[\"x\"][:, 3].numpy()\n                velocities = graph_data[\"x\"][:, 4:7].numpy()\n                particle_types = graph_data[\"x\"][:, 7].numpy()\n\n                # Get cosmological parameters\n                redshifts = graph_data.get(\"redshift\", 0.0)\n                if hasattr(redshifts, \"item\"):\n                    redshifts = redshifts.item()\n                elif hasattr(redshifts, \"__len__\") and len(redshifts) &gt; 0:\n                    redshifts = (\n                        redshifts[0].item()\n                        if hasattr(redshifts[0], \"item\")\n                        else float(redshifts[0])\n                    )\n                else:\n                    redshifts = 0.0\n\n                time_gyr = graph_data.get(\"time_gyr\", 0.0)\n                if hasattr(time_gyr, \"item\"):\n                    time_gyr = time_gyr.item()\n                elif hasattr(time_gyr, \"__len__\") and len(time_gyr) &gt; 0:\n                    time_gyr = (\n                        time_gyr[0].item()\n                        if hasattr(time_gyr[0], \"item\")\n                        else float(time_gyr[0])\n                    )\n                else:\n                    time_gyr = 0.0\n\n                scale_factor = graph_data.get(\"scale_factor\", 1.0)\n                if hasattr(scale_factor, \"item\"):\n                    scale_factor = scale_factor.item()\n                elif hasattr(scale_factor, \"__len__\") and len(scale_factor) &gt; 0:\n                    scale_factor = (\n                        scale_factor[0].item()\n                        if hasattr(scale_factor[0], \"item\")\n                        else float(scale_factor[0])\n                    )\n                else:\n                    scale_factor = 1.0\n\n            elif hasattr(graph_data, \"x\"):\n                positions = graph_data.x[:, :3].numpy()\n                masses = graph_data.x[:, 3].numpy()\n                velocities = graph_data.x[:, 4:7].numpy()\n                particle_types = graph_data.x[:, 7].numpy()\n\n                # Get cosmological parameters\n                redshifts = getattr(graph_data, \"redshift\", 0.0)\n                if hasattr(redshifts, \"item\"):\n                    redshifts = redshifts.item()\n                elif hasattr(redshifts, \"__len__\") and len(redshifts) &gt; 0:\n                    redshifts = (\n                        redshifts[0].item()\n                        if hasattr(redshifts[0], \"item\")\n                        else float(redshifts[0])\n                    )\n                else:\n                    redshifts = 0.0\n\n                time_gyr = getattr(graph_data, \"time_gyr\", 0.0)\n                if hasattr(time_gyr, \"item\"):\n                    time_gyr = time_gyr.item()\n                elif hasattr(time_gyr, \"__len__\") and len(time_gyr) &gt; 0:\n                    time_gyr = (\n                        time_gyr[0].item()\n                        if hasattr(time_gyr[0], \"item\")\n                        else float(time_gyr[0])\n                    )\n                else:\n                    time_gyr = 0.0\n\n                scale_factor = getattr(graph_data, \"scale_factor\", 1.0)\n                if hasattr(scale_factor, \"item\"):\n                    scale_factor = scale_factor.item()\n                elif hasattr(scale_factor, \"__len__\") and len(scale_factor) &gt; 0:\n                    scale_factor = (\n                        scale_factor[0].item()\n                        if hasattr(scale_factor[0], \"item\")\n                        else float(scale_factor[0])\n                    )\n                else:\n                    scale_factor = 1.0\n\n            elif hasattr(graph_data, \"pos\"):\n                positions = graph_data.pos[:, :3].numpy()\n                masses = (\n                    graph_data.mass.numpy()\n                    if hasattr(graph_data, \"mass\")\n                    else np.ones(len(positions))\n                )\n                velocities = (\n                    graph_data.vel.numpy()\n                    if hasattr(graph_data, \"vel\")\n                    else np.zeros((len(positions), 3))\n                )\n                particle_types = (\n                    graph_data.particle_type.numpy()\n                    if hasattr(graph_data, \"particle_type\")\n                    else np.zeros(len(positions))\n                )\n\n                # Get cosmological parameters\n                redshifts = getattr(graph_data, \"redshift\", 0.0)\n                if hasattr(redshifts, \"item\"):\n                    redshifts = redshifts.item()\n                elif hasattr(redshifts, \"__len__\") and len(redshifts) &gt; 0:\n                    redshifts = (\n                        redshifts[0].item()\n                        if hasattr(redshifts[0], \"item\")\n                        else float(redshifts[0])\n                    )\n                else:\n                    redshifts = 0.0\n\n                time_gyr = getattr(graph_data, \"time_gyr\", 0.0)\n                if hasattr(time_gyr, \"item\"):\n                    time_gyr = time_gyr.item()\n                elif hasattr(time_gyr, \"__len__\") and len(time_gyr) &gt; 0:\n                    time_gyr = (\n                        time_gyr[0].item()\n                        if hasattr(time_gyr[0], \"item\")\n                        else float(time_gyr[0])\n                    )\n                else:\n                    time_gyr = 0.0\n\n                scale_factor = getattr(graph_data, \"scale_factor\", 1.0)\n                if hasattr(scale_factor, \"item\"):\n                    scale_factor = scale_factor.item()\n                elif hasattr(scale_factor, \"__len__\") and len(scale_factor) &gt; 0:\n                    scale_factor = (\n                        scale_factor[0].item()\n                        if hasattr(scale_factor[0], \"item\")\n                        else float(scale_factor[0])\n                    )\n                else:\n                    scale_factor = 1.0\n\n            else:\n                continue  # Skip unknown format\n\n            snapshot_data = {\n                \"x\": positions[:, 0],\n                \"y\": positions[:, 1],\n                \"z\": positions[:, 2],\n                \"mass\": masses,\n                \"velocity_0\": velocities[:, 0],\n                \"velocity_1\": velocities[:, 1],\n                \"velocity_2\": velocities[:, 2],\n                \"particle_type\": particle_types,\n                \"snapshot_id\": i,\n                \"redshift\": redshifts,\n                \"time_gyr\": time_gyr,\n                \"scale_factor\": scale_factor,\n            }\n            all_data.append(pl.DataFrame(snapshot_data))\n\n        df = pl.concat(all_data)\n\n    # Apply sampling if requested\n    if max_samples and len(df) &gt; max_samples:\n        df = df.sample(n=max_samples, seed=42)\n\n    # Create AstroDataset\n    dataset = AstroDataset(\n        survey=\"tng50_temporal\",\n        data_path=None,  # We already have the data\n        k_neighbors=8,\n        max_samples=max_samples,\n        force_reload=False,\n        return_tensor=True,  # Return as SurveyTensor\n    )\n\n    # Set the processed data directly\n    dataset._df_cache = df\n    dataset._processed_data = [df]  # Ensure it's treated as processed\n\n    return dataset\n</code></pre>"},{"location":"api/astro_lab.data/#astro_lab.data.preprocess_catalog","title":"preprocess_catalog","text":"<pre><code>preprocess_catalog(\n    input_path: Union[str, Path],\n    survey_type: str,\n    max_samples: Optional[int] = None,\n    output_dir: Optional[Union[str, Path]] = None,\n    write_graph: bool = False,\n    k_neighbors: int = 8,\n    distance_threshold: float = 50.0,\n    **kwargs: Any\n) -&gt; DataFrame\n</code></pre> <p>Preprocess astronomical catalog data. \ud83d\udcca</p> <p>Parameters:</p> Name Type Description Default <code>input_path</code> <code>Union[str, Path]</code> <p>Path to input catalog file</p> required <code>survey_type</code> <code>str</code> <p>Type of survey ('gaia', 'sdss', 'nsa', 'linear')</p> required <code>max_samples</code> <code>Optional[int]</code> <p>Maximum number of samples to process</p> <code>None</code> <code>output_dir</code> <code>Optional[Union[str, Path]]</code> <p>Output directory for processed data</p> <code>None</code> <code>write_graph</code> <code>bool</code> <p>Whether to write the graph data</p> <code>False</code> <code>k_neighbors</code> <code>int</code> <p>Number of neighbors for graph</p> <code>8</code> <code>distance_threshold</code> <code>float</code> <p>Distance threshold for edges</p> <code>50.0</code> <code>**kwargs</code> <code>Any</code> <p>Additional preprocessing parameters</p> <code>{}</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>Preprocessed DataFrame</p> Source code in <code>src\\astro_lab\\data\\preprocessing.py</code> <pre><code>def preprocess_catalog(\n    input_path: Union[str, Path],\n    survey_type: str,\n    max_samples: Optional[int] = None,\n    output_dir: Optional[Union[str, Path]] = None,\n    write_graph: bool = False,\n    k_neighbors: int = 8,\n    distance_threshold: float = 50.0,\n    **kwargs: Any,\n) -&gt; pl.DataFrame:\n    \"\"\"\n    Preprocess astronomical catalog data. \ud83d\udcca\n\n    Args:\n        input_path: Path to input catalog file\n        survey_type: Type of survey ('gaia', 'sdss', 'nsa', 'linear')\n        max_samples: Maximum number of samples to process\n        output_dir: Output directory for processed data\n        write_graph: Whether to write the graph data\n        k_neighbors: Number of neighbors for graph\n        distance_threshold: Distance threshold for edges\n        **kwargs: Additional preprocessing parameters\n\n    Returns:\n        Preprocessed DataFrame\n    \"\"\"\n    logger.info(f\"\ud83d\udd04 Preprocessing {survey_type} catalog: {input_path}\")\n\n    # Load data\n    input_path = Path(input_path)\n    if input_path.suffix == \".parquet\":\n        df = pl.read_parquet(input_path)\n    elif input_path.suffix == \".csv\":\n        df = pl.read_csv(input_path)\n    else:\n        raise ValueError(f\"Unsupported file format: {input_path.suffix}\")\n\n    logger.info(f\"\ud83d\udcca Loaded {len(df)} objects, {len(df.columns)} columns\")\n\n    # Apply survey-specific preprocessing\n    df_clean = _apply_survey_preprocessing(df, survey_type)\n\n    # Sample if requested\n    if max_samples and len(df_clean) &gt; max_samples:\n        df_clean = df_clean.sample(max_samples, seed=42)\n        logger.info(f\"\ud83d\udcca Sampled {max_samples} objects\")\n\n    # Save processed data\n    pt_path = None\n    if output_dir:\n        output_path = Path(output_dir) / f\"{survey_type}.parquet\"\n        output_path.parent.mkdir(parents=True, exist_ok=True)\n        df_clean.write_parquet(output_path)\n        logger.info(f\"\ud83d\udcbe Saved processed data to {output_path}\")\n        # Optional: Schreibe PT-File\n        if write_graph:\n            pt_path = Path(output_dir) / f\"{survey_type}.pt\"\n            graph_data = create_graph_from_dataframe(\n                df_clean,\n                survey_type,\n                k_neighbors=k_neighbors,\n                distance_threshold=distance_threshold,\n            )\n            if graph_data is not None:\n                torch.save(graph_data, pt_path)\n                logger.info(f\"\ud83d\udcbe Saved graph to {pt_path}\")\n    return df_clean\n</code></pre>"},{"location":"api/astro_lab.data/#astro_lab.data.process_for_ml","title":"process_for_ml","text":"<pre><code>process_for_ml(raw_file: Union[str, Path], **kwargs) -&gt; Path\n</code></pre> <p>Convenience function to process a raw file for ML.</p> Source code in <code>src\\astro_lab\\data\\manager.py</code> <pre><code>def process_for_ml(raw_file: Union[str, Path], **kwargs) -&gt; Path:\n    \"\"\"Convenience function to process a raw file for ML.\"\"\"\n    return data_manager.process_for_ml(raw_file, **kwargs)\n</code></pre>"},{"location":"api/astro_lab.data/#astro_lab.data.process_survey","title":"process_survey","text":"<pre><code>process_survey(\n    survey: str,\n    source_file: Optional[str] = None,\n    k_neighbors: int = 8,\n    max_samples: Optional[int] = None,\n    force: bool = False,\n) -&gt; Dict[str, Path]\n</code></pre> <p>Process a survey with automatic source file detection. \ud83d\udcca For TNG50(-4), process all HDF5 snapshots as a time series and save outputs under data/processed/tng50/ with simple names (tng50.parquet, tng50.pt, tng50_metadata.json). If max_samples is set, sample the combined DataFrame before saving and graph creation.</p> Source code in <code>src\\astro_lab\\data\\preprocessing.py</code> <pre><code>def process_survey(\n    survey: str,\n    source_file: Optional[str] = None,\n    k_neighbors: int = 8,\n    max_samples: Optional[int] = None,\n    force: bool = False,\n) -&gt; Dict[str, Path]:\n    \"\"\"\n    Process a survey with automatic source file detection. \ud83d\udcca\n    For TNG50(-4), process all HDF5 snapshots as a time series and save outputs under data/processed/tng50/ with simple names (tng50.parquet, tng50.pt, tng50_metadata.json).\n    If max_samples is set, sample the combined DataFrame before saving and graph creation.\n    \"\"\"\n    logger = logging.getLogger(__name__)\n\n    # Special case: TNG50(-4) \u2192 time series graph processing\n    if survey.lower() in [\"tng50\", \"tng50-4\"]:\n        # Ensure max_samples is int or None\n        if isinstance(max_samples, str):\n            if max_samples.lower() == \"all\":\n                max_samples = None\n            else:\n                try:\n                    max_samples = int(max_samples)\n                except Exception:\n                    logger.warning(\n                        f\"Invalid max_samples value: {max_samples}, using all data.\"\n                    )\n                    max_samples = None\n        # Default: limit to 2 million particles if not set\n        if max_samples is None:\n            max_samples = 2_000_000\n        # Find all HDF5 snapshots\n        project_root = Path(__file__).parent.parent.parent.parent\n        snapdir = project_root / \"data\" / \"raw\" / \"TNG50-4\" / \"output\" / \"snapdir_099\"\n        hdf5_files = sorted(\n            snapdir.glob(\"snap_099.*.hdf5\"), key=lambda x: int(x.stem.split(\".\")[-1])\n        )\n        if not hdf5_files:\n            raise FileNotFoundError(f\"No TNG50 HDF5 snapshots found in {snapdir}\")\n        logger.info(f\"\ud83d\udd0d Found {len(hdf5_files)} TNG50 snapshots in {snapdir}\")\n        # Process all snapshots into a single DataFrame (time series)\n        all_snapshots = []\n        for i, hdf5_file in enumerate(hdf5_files):\n            logger.info(\n                f\"\ud83d\udcca Processing snapshot {i + 1}/{len(hdf5_files)}: {hdf5_file.name}\"\n            )\n            try:\n                with h5py.File(hdf5_file, \"r\") as f:\n                    snapshot_id = int(hdf5_file.stem.split(\".\")[-1])\n                    data_dict = {}\n                    if \"PartType0\" in f:\n                        group = f[\"PartType0\"]\n                        if \"Coordinates\" in group:\n                            coords = np.array(group[\"Coordinates\"][:])\n                            data_dict[\"x\"] = coords[:, 0]\n                            data_dict[\"y\"] = coords[:, 1]\n                            data_dict[\"z\"] = coords[:, 2]\n                        for field in [\n                            \"Masses\",\n                            \"Velocities\",\n                            \"Density\",\n                            \"Temperature\",\n                            \"Metallicity\",\n                        ]:\n                            if field in group:\n                                data = np.array(group[field][:])\n                                col_name = field.lower()\n                                if col_name.endswith(\"es\"):\n                                    col_name = col_name[:-2]\n                                elif col_name.endswith(\"s\"):\n                                    col_name = col_name[:-1]\n                                if data.ndim &gt; 1:\n                                    for j in range(data.shape[1]):\n                                        data_dict[f\"{col_name}_{j}\"] = data[:, j]\n                                else:\n                                    data_dict[col_name] = data\n                    data_dict[\"snapshot_id\"] = snapshot_id\n                    data_dict[\"time_step\"] = i\n                    if \"Header\" in f:\n                        header = f[\"Header\"]\n                        if \"Redshift\" in header:\n                            data_dict[\"redshift\"] = header[\"Redshift\"][0]\n                        if \"Time\" in header:\n                            data_dict[\"time_gyr\"] = header[\"Time\"][0]\n                        if \"ScaleFactor\" in header:\n                            data_dict[\"scale_factor\"] = header[\"ScaleFactor\"][0]\n                    df_snapshot = pl.DataFrame(data_dict)\n                    all_snapshots.append(df_snapshot)\n                    logger.info(\n                        f\"\u2705 Snapshot {snapshot_id}: {len(df_snapshot)} particles, {len(df_snapshot.columns)} columns\"\n                    )\n            except Exception as e:\n                logger.warning(f\"\u26a0\ufe0f Error loading {hdf5_file.name}: {e}\")\n                continue\n        if not all_snapshots:\n            raise ValueError(\"No valid TNG50 snapshots found\")\n        combined_df = pl.concat(all_snapshots)\n        # Apply sampling if requested\n        if max_samples is not None and len(combined_df) &gt; max_samples:\n            combined_df = combined_df.sample(max_samples, seed=42)\n            logger.info(f\"\ud83d\udcc9 Sampled to {max_samples} particles for processing\")\n        # Save as Parquet and Graph\n        output_dir = project_root / \"data\" / \"processed\" / \"tng50\"\n        output_dir.mkdir(parents=True, exist_ok=True)\n        parquet_path = output_dir / \"tng50.parquet\"\n        pt_path = output_dir / \"tng50.pt\"\n        metadata_path = output_dir / \"tng50_metadata.json\"\n        combined_df.write_parquet(str(parquet_path))\n        logger.info(f\"\u2705 TNG50 time series Parquet saved: {parquet_path}\")\n        # Create graph (uses GPU for k-NN if available)\n        graph_data = _create_tng50_graph(combined_df, k_neighbors, 50.0)\n        torch.save(graph_data, pt_path)\n        logger.info(f\"\u2705 TNG50 graph saved: {pt_path}\")\n        # Metadata\n        metadata = {\n            \"survey_name\": \"tng50\",\n            \"num_snapshots\": len(hdf5_files),\n            \"num_particles\": len(combined_df),\n            \"columns\": combined_df.columns,\n            \"k_neighbors\": k_neighbors,\n            \"max_samples\": max_samples,\n        }\n        with open(metadata_path, \"w\") as f:\n            json.dump(metadata, f, indent=2)\n        logger.info(f\"\u2705 TNG50 metadata saved: {metadata_path}\")\n        return {\"parquet\": parquet_path, \"graph\": pt_path, \"metadata\": metadata_path}\n    # Special case: NSA \u2192 FITS\u2192Parquet, dann Standard-Workflow\n    if survey.lower() == \"nsa\":\n        # Ensure max_samples is int or None\n        if isinstance(max_samples, str):\n            if max_samples.lower() == \"all\":\n                max_samples = None\n            else:\n                try:\n                    max_samples = int(max_samples)\n                except Exception:\n                    logger.warning(\n                        f\"Invalid max_samples value: {max_samples}, using all data.\"\n                    )\n                    max_samples = None\n        project_root = Path(__file__).parent.parent.parent.parent\n        nsa_dir = project_root / \"data\" / \"raw\" / \"nsa\"\n        # Immer FITS\u2192Parquet pr\u00fcfen/ausf\u00fchren\n        parquet_path = find_or_create_catalog_file(\"nsa\", nsa_dir)\n        # Standardisierte Ausgabe\n        output_dir = project_root / \"data\" / \"processed\" / \"nsa\"\n        files = create_standardized_files(\n            survey=\"nsa\",\n            input_parquet=parquet_path,\n            output_dir=output_dir,\n            k_neighbors=k_neighbors,\n            max_samples=max_samples,\n            force=force,\n        )\n        logger.info(f\"\u2705 NSA processing complete: {files}\")\n        return files\n    # Standard: all other surveys\n    # ... existing code ...\n    # Fallback: If no return occurred, raise error\n    raise RuntimeError(\"Survey processing did not complete and no files were returned.\")\n</code></pre>"},{"location":"api/astro_lab.data/#astro_lab.data.save_splits_to_parquet","title":"save_splits_to_parquet","text":"<pre><code>save_splits_to_parquet(\n    df_train: DataFrame,\n    df_val: DataFrame,\n    df_test: DataFrame,\n    base_path: Union[str, Path],\n    dataset_name: str,\n) -&gt; Dict[str, Path]\n</code></pre> <p>Save train/val/test splits to parquet files.</p> <p>Parameters:</p> Name Type Description Default <code>df_train</code> <code>DataFrame</code> <p>Training DataFrame</p> required <code>df_val</code> <code>DataFrame</code> <p>Validation DataFrame</p> required <code>df_test</code> <code>DataFrame</code> <p>Test DataFrame</p> required <code>base_path</code> <code>Union[str, Path]</code> <p>Base directory for saving</p> required <code>dataset_name</code> <code>str</code> <p>Name of dataset</p> required <p>Returns:</p> Type Description <code>Dict[str, Path]</code> <p>Dictionary with paths to saved files</p> Source code in <code>src\\astro_lab\\data\\utils.py</code> <pre><code>def save_splits_to_parquet(\n    df_train: pl.DataFrame,\n    df_val: pl.DataFrame,\n    df_test: pl.DataFrame,\n    base_path: Union[str, Path],\n    dataset_name: str,\n) -&gt; Dict[str, Path]:\n    \"\"\"\n    Save train/val/test splits to parquet files.\n\n    Args:\n        df_train: Training DataFrame\n        df_val: Validation DataFrame\n        df_test: Test DataFrame\n        base_path: Base directory for saving\n        dataset_name: Name of dataset\n\n    Returns:\n        Dictionary with paths to saved files\n    \"\"\"\n    base_path = Path(base_path)\n    base_path.mkdir(parents=True, exist_ok=True)\n\n    paths = {}\n\n    # Save splits\n    for split_name, df in [(\"train\", df_train), (\"val\", df_val), (\"test\", df_test)]:\n        file_path = base_path / f\"{dataset_name}_{split_name}.parquet\"\n        df.write_parquet(file_path)\n        paths[split_name] = file_path\n\n    # Save metadata\n    metadata = {\n        \"dataset_name\": dataset_name,\n        \"n_train\": len(df_train),\n        \"n_val\": len(df_val),\n        \"n_test\": len(df_test),\n        \"n_total\": len(df_train) + len(df_val) + len(df_test),\n        \"columns\": df_train.columns,\n        \"created_at\": str(pl.datetime.now()),\n    }\n\n    metadata_path = base_path / f\"{dataset_name}_metadata.json\"\n    with open(metadata_path, \"w\") as f:\n        json.dump(metadata, f, indent=2)\n\n    paths[\"metadata\"] = metadata_path\n    return paths\n</code></pre>"},{"location":"api/astro_lab/","title":"astro_lab","text":"<p>``` </p>"},{"location":"api/astro_lab/#astro_lab","title":"astro_lab","text":"<p>Comprehensive Astronomical Data Analysis Framework</p> <p>A modern Python framework for astronomical data analysis, machine learning, and visualization that combines specialized astronomy libraries with cutting-edge ML tools.</p> <p>Modules:</p> Name Description <code>cli</code> <p>AstroLab CLI - Command Line Interface</p> <code>data</code> <p>AstroLab Data Module - High-Performance Astronomical Data Processing</p> <code>models</code> <p>AstroLab Models - Modern Graph Neural Networks for Astronomical Data</p> <code>tensors</code> <p>Astronomical TensorDict System</p> <code>training</code> <p>Training Module - Neural Network Training Infrastructure</p> <code>ui</code> <p>AstroLab UI Module</p> <code>utils</code> <p>AstroLab Utilities - Core Utility Functions</p> <code>widgets</code> <p>AstroLab Widgets - Interactive astronomical visualization and analysis.</p> <p>Functions:</p> Name Description <code>get_device</code> <p>Get the best available device (CUDA if available, else CPU).</p> <code>set_random_seed</code> <p>Set random seed for reproducibility.</p> <code>setup_logging</code> <p>Setup logging configuration.</p>"},{"location":"api/astro_lab/#astro_lab.get_device","title":"get_device","text":"<pre><code>get_device() -&gt; device\n</code></pre> <p>Get the best available device (CUDA if available, else CPU).</p> Source code in <code>src\\astro_lab\\utils\\__init__.py</code> <pre><code>def get_device() -&gt; torch.device:\n    \"\"\"Get the best available device (CUDA if available, else CPU).\"\"\"\n    return torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n</code></pre>"},{"location":"api/astro_lab/#astro_lab.set_random_seed","title":"set_random_seed","text":"<pre><code>set_random_seed(seed: int = 42) -&gt; None\n</code></pre> <p>Set random seed for reproducibility.</p> Source code in <code>src\\astro_lab\\utils\\__init__.py</code> <pre><code>def set_random_seed(seed: int = 42) -&gt; None:\n    \"\"\"Set random seed for reproducibility.\"\"\"\n    import random\n\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed(seed)\n        torch.cuda.manual_seed_all(seed)\n</code></pre>"},{"location":"api/astro_lab/#astro_lab.setup_logging","title":"setup_logging","text":"<pre><code>setup_logging(level: int = INFO) -&gt; None\n</code></pre> <p>Setup logging configuration.</p> Source code in <code>src\\astro_lab\\utils\\__init__.py</code> <pre><code>def setup_logging(level: int = logging.INFO) -&gt; None:\n    \"\"\"Setup logging configuration.\"\"\"\n    logging.basicConfig(\n        level=level, format=\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\"\n    )\n</code></pre>"},{"location":"api/astro_lab.models/","title":"astro_lab.models","text":""},{"location":"api/astro_lab.models/#astro_lab.models","title":"models","text":"<p>AstroLab Models - Modern Graph Neural Networks for Astronomical Data</p> <p>Modules:</p> Name Description <code>components</code> <p>Reusable components for AstroLab models.</p> <code>config</code> <p>Simple model configuration with dataclasses.</p> <code>core</code> <p>Core AstroLab models.</p> <code>encoders</code> <p>Encoder Modules for AstroLab Models</p> <code>factories</code> <p>Model factory functions for creating pre-configured astronomical models.</p> <code>utils</code> <p>Pure utility functions - no factories or classes.</p> <p>Classes:</p> Name Description <code>ALCDEFTemporalGNN</code> <p>Simplified Temporal GNN for ALCDEF lightcurve data.</p> <code>AstroPhotGNN</code> <p>Simplified GNN with AstroPhot integration for galaxy modeling.</p> <code>AstroSurveyGNN</code> <p>Simplified survey GNN using composition.</p> <code>ModelConfig</code> <p>Simple model configuration.</p> <code>TemporalGCN</code> <p>Simple Temporal Graph Convolutional Network.</p> <p>Functions:</p> Name Description <code>create_asteroid_period_detector</code> <p>Create asteroid period detector (robust to all kwargs).</p> <code>create_gaia_classifier</code> <p>Create Gaia classifier (robust to all kwargs).</p> <code>create_lsst_transient_detector</code> <p>Create LSST transient detector (robust to all kwargs).</p> <code>create_sdss_galaxy_model</code> <p>Create SDSS galaxy model (robust to all kwargs).</p> <code>get_predefined_config</code> <p>Get predefined config by name.</p>"},{"location":"api/astro_lab.models/#astro_lab.models.ALCDEFTemporalGNN","title":"ALCDEFTemporalGNN","text":"<p>               Bases: <code>Module</code></p> <p>Simplified Temporal GNN for ALCDEF lightcurve data.</p> <p>Methods:</p> Name Description <code>encode_lightcurve</code> <p>Encode lightcurve sequence with LSTM.</p> <code>forward</code> <p>Forward pass for lightcurve analysis.</p> Source code in <code>src\\astro_lab\\models\\core\\point_cloud_gnn.py</code> <pre><code>class ALCDEFTemporalGNN(nn.Module):\n    \"\"\"Simplified Temporal GNN for ALCDEF lightcurve data.\"\"\"\n\n    def __init__(\n        self,\n        input_dim: int = 1,  # Typically magnitude values\n        hidden_dim: int = 128,\n        output_dim: int = 1,\n        num_layers: int = 3,\n        task: str = \"period_detection\",\n        dropout: float = 0.1,\n        device: Optional[Union[str, torch.device]] = None,\n        **kwargs\n    ):\n        super().__init__()\n\n        self.device = torch.device(device or ('cuda' if torch.cuda.is_available() else 'cpu'))\n        self.hidden_dim = hidden_dim\n        self.task = task\n\n        # Lightcurve encoder - simple LSTM\n        self.lightcurve_encoder = nn.LSTM(\n            input_size=input_dim,\n            hidden_size=hidden_dim,\n            num_layers=2,\n            batch_first=True,\n            dropout=dropout if num_layers &gt; 1 else 0,\n        )\n\n        # Graph convolutions for temporal relationships\n        self.convs = nn.ModuleList([\n            create_conv_layer(\"gcn\", hidden_dim, hidden_dim)\n            for _ in range(num_layers)\n        ])\n\n        self.norms = nn.ModuleList([\n            nn.LayerNorm(hidden_dim) for _ in range(num_layers)\n        ])\n\n        self.dropout = nn.Dropout(dropout)\n\n        # Task-specific output heads\n        if task == \"period_detection\":\n            self.output_head = PeriodDetectionHead(hidden_dim, dropout)\n        elif task == \"shape_modeling\":\n            num_harmonics = kwargs.get(\"num_harmonics\", 10)\n            self.output_head = ShapeModelingHead(hidden_dim, num_harmonics, dropout)\n        elif task == \"classification\":\n            num_classes = kwargs.get(\"num_classes\", 2)\n            self.output_head = ClassificationHead(hidden_dim, num_classes, dropout)\n        else:\n            # Default to simple output head\n            self.output_head = create_output_head(\n                \"regression\",\n                input_dim=hidden_dim,\n                output_dim=output_dim,\n                dropout=dropout\n            )\n\n        self.to(self.device)\n\n    def encode_lightcurve(self, lightcurve_data: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"Encode lightcurve sequence with LSTM.\"\"\"\n        # lightcurve_data shape: (batch, seq_len, features)\n        if lightcurve_data.dim() == 2:\n            # Add feature dimension if needed\n            lightcurve_data = lightcurve_data.unsqueeze(-1)\n\n        # Encode with LSTM\n        lstm_out, (h_n, _) = self.lightcurve_encoder(lightcurve_data)\n\n        # Use last hidden state\n        # h_n shape: (num_layers, batch, hidden_dim)\n        encoded = h_n[-1]  # Take last layer: (batch, hidden_dim)\n\n        return encoded\n\n    def forward(\n        self,\n        lightcurve,\n        edge_index: torch.Tensor,\n        batch: Optional[torch.Tensor] = None,\n        return_embeddings: bool = False,\n    ) -&gt; Union[torch.Tensor, Dict[str, torch.Tensor]]:\n        \"\"\"Forward pass for lightcurve analysis.\"\"\"\n\n        # Move to device\n        edge_index = edge_index.to(self.device)\n        if batch is not None:\n            batch = batch.to(self.device)\n\n        # Handle different input formats\n        if hasattr(lightcurve, 'data'):\n            # LightcurveTensor\n            lc_data = lightcurve.data.to(self.device)\n        elif isinstance(lightcurve, torch.Tensor):\n            lc_data = lightcurve.to(self.device)\n        else:\n            raise ValueError(\"Unsupported lightcurve format\")\n\n        # Encode lightcurve\n        h = self.encode_lightcurve(lc_data)\n\n        # If we have multiple nodes per graph, expand the encoding\n        if edge_index.size(1) &gt; 0:\n            num_nodes = edge_index.max().item() + 1\n            if h.size(0) == 1 and num_nodes &gt; 1:\n                # Broadcast to all nodes\n                h = h.expand(num_nodes, -1)\n\n        # Apply graph convolutions\n        for conv, norm in zip(self.convs, self.norms):\n            h_prev = h\n            h = conv(h, edge_index)\n            h = norm(h)\n            h = F.relu(h)\n            h = self.dropout(h)\n\n            # Residual connection\n            if h_prev.shape == h.shape:\n                h = h + h_prev\n\n        # Pool if needed\n        if batch is not None:\n            pooled = global_mean_pool(h, batch)\n        else:\n            pooled = h.mean(dim=0, keepdim=True)\n\n        # Task-specific output\n        output = self.output_head(pooled)\n\n        if return_embeddings:\n            return {\"predictions\": output, \"embeddings\": pooled}\n        return output \n</code></pre>"},{"location":"api/astro_lab.models/#astro_lab.models.ALCDEFTemporalGNN.encode_lightcurve","title":"encode_lightcurve","text":"<pre><code>encode_lightcurve(lightcurve_data: Tensor) -&gt; Tensor\n</code></pre> <p>Encode lightcurve sequence with LSTM.</p> Source code in <code>src\\astro_lab\\models\\core\\point_cloud_gnn.py</code> <pre><code>def encode_lightcurve(self, lightcurve_data: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"Encode lightcurve sequence with LSTM.\"\"\"\n    # lightcurve_data shape: (batch, seq_len, features)\n    if lightcurve_data.dim() == 2:\n        # Add feature dimension if needed\n        lightcurve_data = lightcurve_data.unsqueeze(-1)\n\n    # Encode with LSTM\n    lstm_out, (h_n, _) = self.lightcurve_encoder(lightcurve_data)\n\n    # Use last hidden state\n    # h_n shape: (num_layers, batch, hidden_dim)\n    encoded = h_n[-1]  # Take last layer: (batch, hidden_dim)\n\n    return encoded\n</code></pre>"},{"location":"api/astro_lab.models/#astro_lab.models.ALCDEFTemporalGNN.forward","title":"forward","text":"<pre><code>forward(\n    lightcurve,\n    edge_index: Tensor,\n    batch: Optional[Tensor] = None,\n    return_embeddings: bool = False,\n) -&gt; Union[Tensor, Dict[str, Tensor]]\n</code></pre> <p>Forward pass for lightcurve analysis.</p> Source code in <code>src\\astro_lab\\models\\core\\point_cloud_gnn.py</code> <pre><code>def forward(\n    self,\n    lightcurve,\n    edge_index: torch.Tensor,\n    batch: Optional[torch.Tensor] = None,\n    return_embeddings: bool = False,\n) -&gt; Union[torch.Tensor, Dict[str, torch.Tensor]]:\n    \"\"\"Forward pass for lightcurve analysis.\"\"\"\n\n    # Move to device\n    edge_index = edge_index.to(self.device)\n    if batch is not None:\n        batch = batch.to(self.device)\n\n    # Handle different input formats\n    if hasattr(lightcurve, 'data'):\n        # LightcurveTensor\n        lc_data = lightcurve.data.to(self.device)\n    elif isinstance(lightcurve, torch.Tensor):\n        lc_data = lightcurve.to(self.device)\n    else:\n        raise ValueError(\"Unsupported lightcurve format\")\n\n    # Encode lightcurve\n    h = self.encode_lightcurve(lc_data)\n\n    # If we have multiple nodes per graph, expand the encoding\n    if edge_index.size(1) &gt; 0:\n        num_nodes = edge_index.max().item() + 1\n        if h.size(0) == 1 and num_nodes &gt; 1:\n            # Broadcast to all nodes\n            h = h.expand(num_nodes, -1)\n\n    # Apply graph convolutions\n    for conv, norm in zip(self.convs, self.norms):\n        h_prev = h\n        h = conv(h, edge_index)\n        h = norm(h)\n        h = F.relu(h)\n        h = self.dropout(h)\n\n        # Residual connection\n        if h_prev.shape == h.shape:\n            h = h + h_prev\n\n    # Pool if needed\n    if batch is not None:\n        pooled = global_mean_pool(h, batch)\n    else:\n        pooled = h.mean(dim=0, keepdim=True)\n\n    # Task-specific output\n    output = self.output_head(pooled)\n\n    if return_embeddings:\n        return {\"predictions\": output, \"embeddings\": pooled}\n    return output \n</code></pre>"},{"location":"api/astro_lab.models/#astro_lab.models.AstroPhotGNN","title":"AstroPhotGNN","text":"<p>               Bases: <code>Module</code></p> <p>Simplified GNN with AstroPhot integration for galaxy modeling.</p> <p>Methods:</p> Name Description <code>apply_parameter_constraints</code> <p>Apply physical constraints to component parameters.</p> <code>forward</code> <p>Forward pass for galaxy parameter prediction.</p> Source code in <code>src\\astro_lab\\models\\core\\astrophot_gnn.py</code> <pre><code>class AstroPhotGNN(nn.Module):\n    \"\"\"Simplified GNN with AstroPhot integration for galaxy modeling.\"\"\"\n\n    def __init__(\n        self,\n        input_dim: Optional[int] = None,\n        hidden_dim: int = 128,\n        output_dim: int = 12,  # Typical Sersic + disk parameters\n        num_layers: int = 3,\n        model_components: List[str] = None,\n        dropout: float = 0.1,\n        device: Optional[Union[str, torch.device]] = None,\n        **kwargs,\n    ):\n        super().__init__()\n\n        self.device = torch.device(\n            device or (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n        )\n        self.hidden_dim = hidden_dim\n        self.output_dim = output_dim\n\n        if model_components is None:\n            model_components = [\"sersic\", \"disk\"]\n        self.model_components = model_components\n\n        # Default input dimension for galaxy features\n        if input_dim is None:\n            input_dim = kwargs.get(\"galaxy_features_dim\", 20)\n\n        # Input projection\n        self.input_projection = nn.Linear(input_dim, hidden_dim)\n\n        # Graph processor\n        self.graph_processor = GraphProcessor(\n            hidden_dim=hidden_dim,\n            num_layers=num_layers,\n            conv_type=kwargs.get(\"conv_type\", \"gcn\"),\n            dropout=dropout,\n            **kwargs,\n        )\n\n        # Pooling\n        self.pooling = PoolingModule(kwargs.get(\"pooling\", \"mean\"))\n\n        # Component-specific heads\n        self.component_heads = nn.ModuleDict()\n        for component in model_components:\n            if component == \"sersic\":\n                # Sersic parameters: [Re, n, I_e, PA]\n                self.component_heads[component] = create_mlp(\n                    hidden_dim, 4, [hidden_dim // 2], dropout=dropout\n                )\n            elif component == \"disk\":\n                # Disk parameters: [Rd, I0, PA]\n                self.component_heads[component] = create_mlp(\n                    hidden_dim, 3, [hidden_dim // 2], dropout=dropout\n                )\n            elif component == \"bulge\":\n                # Bulge parameters: [Rb, Ib, q]\n                self.component_heads[component] = create_mlp(\n                    hidden_dim, 3, [hidden_dim // 2], dropout=dropout\n                )\n\n        # Global galaxy parameters\n        self.global_head = create_mlp(\n            hidden_dim, output_dim, [hidden_dim, hidden_dim // 2], dropout=dropout\n        )\n\n        self.to(self.device)\n\n    def apply_parameter_constraints(\n        self, params: torch.Tensor, component: str\n    ) -&gt; torch.Tensor:\n        \"\"\"Apply physical constraints to component parameters.\"\"\"\n        if component == \"sersic\":\n            # [Re, n, I_e, PA]\n            re = F.softplus(params[..., 0:1])  # Effective radius &gt; 0\n            n = torch.clamp(params[..., 1:2], 0.1, 8.0)  # Sersic index\n            ie = F.softplus(params[..., 2:3])  # Surface brightness &gt; 0\n            pa = torch.remainder(params[..., 3:4], 180.0)  # Position angle [0, 180)\n            return torch.cat([re, n, ie, pa], dim=-1)\n\n        elif component == \"disk\":\n            # [Rd, I0, PA]\n            rd = F.softplus(params[..., 0:1])  # Scale radius &gt; 0\n            i0 = F.softplus(params[..., 1:2])  # Central surface brightness &gt; 0\n            pa = torch.remainder(params[..., 2:3], 180.0)  # Position angle\n            return torch.cat([rd, i0, pa], dim=-1)\n\n        elif component == \"bulge\":\n            # [Rb, Ib, q]\n            rb = F.softplus(params[..., 0:1])  # Bulge radius &gt; 0\n            ib = F.softplus(params[..., 1:2])  # Bulge intensity &gt; 0\n            q = torch.sigmoid(params[..., 2:3])  # Axis ratio [0, 1]\n            return torch.cat([rb, ib, q], dim=-1)\n\n        return params\n\n    def forward(\n        self,\n        data,\n        edge_index: Optional[torch.Tensor] = None,\n        batch: Optional[torch.Tensor] = None,\n        return_components: bool = False,\n        **kwargs,\n    ) -&gt; Union[torch.Tensor, Dict[str, torch.Tensor]]:\n        \"\"\"Forward pass for galaxy parameter prediction.\"\"\"\n\n        # Handle different input formats\n        if hasattr(data, \"x\"):\n            x = data.x\n            if edge_index is None and hasattr(data, \"edge_index\"):\n                edge_index = data.edge_index\n            if batch is None and hasattr(data, \"batch\"):\n                batch = data.batch\n        elif isinstance(data, torch.Tensor):\n            x = data\n        else:\n            # Try to extract features\n            x = data\n\n        # Move to device\n        if isinstance(x, torch.Tensor):\n            x = x.to(self.device)\n        if edge_index is not None:\n            edge_index = edge_index.to(self.device)\n        if batch is not None:\n            batch = batch.to(self.device)\n\n        # Project input\n        h = self.input_projection(x)\n\n        # Process through graph layers\n        if edge_index is not None:\n            h = self.graph_processor(h, edge_index)\n\n        # Pool to galaxy level\n        h = self.pooling(h, batch)\n\n        if return_components:\n            # Return component-specific predictions\n            results = {}\n            for component, head in self.component_heads.items():\n                params = head(h)\n                results[component] = self.apply_parameter_constraints(params, component)\n            results[\"global\"] = self.global_head(h)\n            return results\n        else:\n            # Return global parameters only\n            return self.global_head(h)\n</code></pre>"},{"location":"api/astro_lab.models/#astro_lab.models.AstroPhotGNN.apply_parameter_constraints","title":"apply_parameter_constraints","text":"<pre><code>apply_parameter_constraints(params: Tensor, component: str) -&gt; Tensor\n</code></pre> <p>Apply physical constraints to component parameters.</p> Source code in <code>src\\astro_lab\\models\\core\\astrophot_gnn.py</code> <pre><code>def apply_parameter_constraints(\n    self, params: torch.Tensor, component: str\n) -&gt; torch.Tensor:\n    \"\"\"Apply physical constraints to component parameters.\"\"\"\n    if component == \"sersic\":\n        # [Re, n, I_e, PA]\n        re = F.softplus(params[..., 0:1])  # Effective radius &gt; 0\n        n = torch.clamp(params[..., 1:2], 0.1, 8.0)  # Sersic index\n        ie = F.softplus(params[..., 2:3])  # Surface brightness &gt; 0\n        pa = torch.remainder(params[..., 3:4], 180.0)  # Position angle [0, 180)\n        return torch.cat([re, n, ie, pa], dim=-1)\n\n    elif component == \"disk\":\n        # [Rd, I0, PA]\n        rd = F.softplus(params[..., 0:1])  # Scale radius &gt; 0\n        i0 = F.softplus(params[..., 1:2])  # Central surface brightness &gt; 0\n        pa = torch.remainder(params[..., 2:3], 180.0)  # Position angle\n        return torch.cat([rd, i0, pa], dim=-1)\n\n    elif component == \"bulge\":\n        # [Rb, Ib, q]\n        rb = F.softplus(params[..., 0:1])  # Bulge radius &gt; 0\n        ib = F.softplus(params[..., 1:2])  # Bulge intensity &gt; 0\n        q = torch.sigmoid(params[..., 2:3])  # Axis ratio [0, 1]\n        return torch.cat([rb, ib, q], dim=-1)\n\n    return params\n</code></pre>"},{"location":"api/astro_lab.models/#astro_lab.models.AstroPhotGNN.forward","title":"forward","text":"<pre><code>forward(\n    data,\n    edge_index: Optional[Tensor] = None,\n    batch: Optional[Tensor] = None,\n    return_components: bool = False,\n    **kwargs\n) -&gt; Union[Tensor, Dict[str, Tensor]]\n</code></pre> <p>Forward pass for galaxy parameter prediction.</p> Source code in <code>src\\astro_lab\\models\\core\\astrophot_gnn.py</code> <pre><code>def forward(\n    self,\n    data,\n    edge_index: Optional[torch.Tensor] = None,\n    batch: Optional[torch.Tensor] = None,\n    return_components: bool = False,\n    **kwargs,\n) -&gt; Union[torch.Tensor, Dict[str, torch.Tensor]]:\n    \"\"\"Forward pass for galaxy parameter prediction.\"\"\"\n\n    # Handle different input formats\n    if hasattr(data, \"x\"):\n        x = data.x\n        if edge_index is None and hasattr(data, \"edge_index\"):\n            edge_index = data.edge_index\n        if batch is None and hasattr(data, \"batch\"):\n            batch = data.batch\n    elif isinstance(data, torch.Tensor):\n        x = data\n    else:\n        # Try to extract features\n        x = data\n\n    # Move to device\n    if isinstance(x, torch.Tensor):\n        x = x.to(self.device)\n    if edge_index is not None:\n        edge_index = edge_index.to(self.device)\n    if batch is not None:\n        batch = batch.to(self.device)\n\n    # Project input\n    h = self.input_projection(x)\n\n    # Process through graph layers\n    if edge_index is not None:\n        h = self.graph_processor(h, edge_index)\n\n    # Pool to galaxy level\n    h = self.pooling(h, batch)\n\n    if return_components:\n        # Return component-specific predictions\n        results = {}\n        for component, head in self.component_heads.items():\n            params = head(h)\n            results[component] = self.apply_parameter_constraints(params, component)\n        results[\"global\"] = self.global_head(h)\n        return results\n    else:\n        # Return global parameters only\n        return self.global_head(h)\n</code></pre>"},{"location":"api/astro_lab.models/#astro_lab.models.AstroSurveyGNN","title":"AstroSurveyGNN","text":"<p>               Bases: <code>Module</code></p> <p>Simplified survey GNN using composition.</p> <p>Methods:</p> Name Description <code>forward</code> <p>Fully robust forward pass that handles all calling conventions.</p> <code>get_num_parameters</code> <p>Count trainable parameters.</p> Source code in <code>src\\astro_lab\\models\\core\\survey_gnn.py</code> <pre><code>class AstroSurveyGNN(nn.Module):\n    \"\"\"Simplified survey GNN using composition.\"\"\"\n\n    def __init__(\n        self,\n        input_dim: Optional[int] = None,\n        hidden_dim: int = 128,\n        output_dim: int = 1,\n        num_layers: int = 3,\n        conv_type: str = \"gcn\",\n        task: str = \"classification\",\n        use_photometry: bool = True,\n        use_astrometry: bool = True,\n        use_spectroscopy: bool = False,\n        pooling: str = \"mean\",\n        dropout: float = 0.1,\n        device: Optional[Union[str, torch.device]] = None,\n        **kwargs,\n    ):\n        super().__init__()\n\n        # Device management\n        self.device = torch.device(\n            device or (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n        )\n\n        # Store configuration\n        self.hidden_dim = hidden_dim\n        self.output_dim = output_dim\n        self.task = task\n        self.pooling = pooling\n\n        # Calculate input dimension if not provided\n        if input_dim is None:\n            calculated_dim = 0\n            if use_photometry:\n                calculated_dim += kwargs.get(\"photometry_dim\", 5)\n            if use_astrometry:\n                calculated_dim += kwargs.get(\"astrometry_dim\", 5)\n            if use_spectroscopy:\n                calculated_dim += kwargs.get(\"spectroscopy_dim\", 3)\n\n            # Default fallback\n            if calculated_dim == 0:\n                calculated_dim = 13  # Default total features\n            input_dim = calculated_dim\n\n        # Use composition instead of complex inheritance\n        self.feature_processor = FeatureProcessor(\n            input_dim=input_dim,\n            hidden_dim=hidden_dim,\n            use_photometry=use_photometry,\n            use_astrometry=use_astrometry,\n            use_spectroscopy=use_spectroscopy,\n            **kwargs,\n        )\n\n        self.graph_processor = GraphProcessor(\n            hidden_dim=hidden_dim,\n            num_layers=num_layers,\n            conv_type=conv_type,\n            dropout=dropout,\n            **kwargs,\n        )\n\n        # Pooling for graph-level tasks\n        if \"graph\" in task:\n            self.pooling_module = PoolingModule(pooling)\n        else:\n            self.pooling_module = None\n\n        # Output head based on task\n        head_type = task.replace(\"graph_\", \"\").replace(\"node_\", \"\")\n        self.output_head = create_output_head(\n            head_type,\n            input_dim=hidden_dim,\n            output_dim=output_dim,\n            dropout=dropout,\n            **kwargs,\n        )\n\n        # Move to device\n        self.to(self.device)\n\n    def forward(self, *args, **kwargs) -&gt; Union[torch.Tensor, Dict[str, torch.Tensor]]:\n        \"\"\"\n        Fully robust forward pass that handles all calling conventions.\n\n        This handles:\n        - Lightning calling model(batch)\n        - torch.compile calling model(batch)\n        - PyTorch Geometric calling model(batch.x, batch.edge_index)\n        - Manual calling model(data=batch) or model(batch=batch)\n        - Empty calls for model introspection\n        \"\"\"\n        try:\n            # Extract batch from various calling conventions\n            batch = None\n            edge_index = None\n            batch_idx = None\n\n            # Case 1: Positional argument (most common)\n            if args:\n                batch = args[0]\n                # Extract additional args if provided\n                if len(args) &gt; 1:\n                    edge_index = args[1]\n                if len(args) &gt; 2:\n                    batch_idx = args[2]\n\n            # Case 2: Keyword arguments\n            if batch is None:\n                batch = kwargs.get(\"batch\", kwargs.get(\"data\", kwargs.get(\"x\")))\n\n            if edge_index is None:\n                edge_index = kwargs.get(\"edge_index\")\n\n            if batch_idx is None:\n                batch_idx = kwargs.get(\"batch_index\", kwargs.get(\"batch\"))\n\n            # Case 3: Empty call for model introspection - create dummy data\n            if batch is None and not args and not kwargs:\n                # Create minimal dummy data for introspection\n                dummy_batch_size = 1\n                dummy_features = self.feature_processor.input_dim\n                x = torch.zeros((dummy_batch_size, dummy_features), device=self.device)\n                edge_index = torch.tensor(\n                    [[0], [0]], device=self.device, dtype=torch.long\n                )\n                batch_idx = torch.zeros(\n                    dummy_batch_size, device=self.device, dtype=torch.long\n                )\n\n                # Process with dummy data\n                features = self.feature_processor(x)\n                embeddings = self.graph_processor(features, edge_index)\n                if self.pooling_module is not None:\n                    pooled = self.pooling_module(embeddings, batch_idx)\n                else:\n                    pooled = embeddings\n                output = self.output_head(pooled)\n                return output\n\n            # Case 4: If still no batch with arguments, something is wrong\n            if batch is None:\n                raise ValueError(\n                    f\"No batch data provided. args: {[type(arg).__name__ for arg in args]}, kwargs: {list(kwargs.keys())}\"\n                )\n\n            # Handle different input formats\n            if hasattr(batch, \"x\") and hasattr(batch, \"edge_index\"):\n                # PyTorch Geometric Data object\n                x = batch.x\n                if edge_index is None:\n                    edge_index = batch.edge_index\n                if batch_idx is None and hasattr(batch, \"batch\"):\n                    batch_idx = batch.batch\n            elif isinstance(batch, torch.Tensor):\n                # Raw tensor\n                x = batch\n            elif isinstance(batch, dict):\n                # Dictionary format\n                x = batch.get(\"x\", batch)\n                if edge_index is None:\n                    edge_index = batch.get(\"edge_index\")\n                if batch_idx is None:\n                    batch_idx = batch.get(\"batch\")\n            else:\n                # Assume it's a tensor\n                x = batch\n\n            # Ensure tensors are on correct device\n            if isinstance(x, torch.Tensor):\n                x = x.to(self.device)\n            if edge_index is not None and isinstance(edge_index, torch.Tensor):\n                edge_index = edge_index.to(self.device)\n            if batch_idx is not None and isinstance(batch_idx, torch.Tensor):\n                batch_idx = batch_idx.to(self.device)\n\n            # Process features\n            features = self.feature_processor(x)\n\n            # Process through graph layers if edge_index available\n            if edge_index is not None:\n                embeddings = self.graph_processor(features, edge_index)\n            else:\n                embeddings = features\n\n            # Apply pooling if needed\n            if self.pooling_module is not None and batch_idx is not None:\n                pooled = self.pooling_module(embeddings, batch_idx)\n            else:\n                pooled = embeddings\n\n            # Generate output\n            output = self.output_head(pooled)\n\n            # Return embeddings if requested\n            if kwargs.get(\"return_embeddings\", False):\n                return {\"logits\": output, \"embeddings\": embeddings}\n\n            return output\n\n        except Exception as e:\n            # error reporting\n            import logging\n            import traceback\n\n            logger = logging.getLogger(__name__)\n            logger.error(f\"\u274c Forward pass failed: {e}\")\n            logger.error(f\"   Args: {[type(arg).__name__ for arg in args]}\")\n            logger.error(f\"   Kwargs: {list(kwargs.keys())}\")\n            logger.error(f\"   Traceback: {traceback.format_exc()}\")\n            raise\n\n    def get_num_parameters(self) -&gt; int:\n        \"\"\"Count trainable parameters.\"\"\"\n        return sum(p.numel() for p in self.parameters() if p.requires_grad)\n</code></pre>"},{"location":"api/astro_lab.models/#astro_lab.models.AstroSurveyGNN.forward","title":"forward","text":"<pre><code>forward(*args, **kwargs) -&gt; Union[Tensor, Dict[str, Tensor]]\n</code></pre> <p>Fully robust forward pass that handles all calling conventions.</p> <p>This handles: - Lightning calling model(batch) - torch.compile calling model(batch) - PyTorch Geometric calling model(batch.x, batch.edge_index) - Manual calling model(data=batch) or model(batch=batch) - Empty calls for model introspection</p> Source code in <code>src\\astro_lab\\models\\core\\survey_gnn.py</code> <pre><code>def forward(self, *args, **kwargs) -&gt; Union[torch.Tensor, Dict[str, torch.Tensor]]:\n    \"\"\"\n    Fully robust forward pass that handles all calling conventions.\n\n    This handles:\n    - Lightning calling model(batch)\n    - torch.compile calling model(batch)\n    - PyTorch Geometric calling model(batch.x, batch.edge_index)\n    - Manual calling model(data=batch) or model(batch=batch)\n    - Empty calls for model introspection\n    \"\"\"\n    try:\n        # Extract batch from various calling conventions\n        batch = None\n        edge_index = None\n        batch_idx = None\n\n        # Case 1: Positional argument (most common)\n        if args:\n            batch = args[0]\n            # Extract additional args if provided\n            if len(args) &gt; 1:\n                edge_index = args[1]\n            if len(args) &gt; 2:\n                batch_idx = args[2]\n\n        # Case 2: Keyword arguments\n        if batch is None:\n            batch = kwargs.get(\"batch\", kwargs.get(\"data\", kwargs.get(\"x\")))\n\n        if edge_index is None:\n            edge_index = kwargs.get(\"edge_index\")\n\n        if batch_idx is None:\n            batch_idx = kwargs.get(\"batch_index\", kwargs.get(\"batch\"))\n\n        # Case 3: Empty call for model introspection - create dummy data\n        if batch is None and not args and not kwargs:\n            # Create minimal dummy data for introspection\n            dummy_batch_size = 1\n            dummy_features = self.feature_processor.input_dim\n            x = torch.zeros((dummy_batch_size, dummy_features), device=self.device)\n            edge_index = torch.tensor(\n                [[0], [0]], device=self.device, dtype=torch.long\n            )\n            batch_idx = torch.zeros(\n                dummy_batch_size, device=self.device, dtype=torch.long\n            )\n\n            # Process with dummy data\n            features = self.feature_processor(x)\n            embeddings = self.graph_processor(features, edge_index)\n            if self.pooling_module is not None:\n                pooled = self.pooling_module(embeddings, batch_idx)\n            else:\n                pooled = embeddings\n            output = self.output_head(pooled)\n            return output\n\n        # Case 4: If still no batch with arguments, something is wrong\n        if batch is None:\n            raise ValueError(\n                f\"No batch data provided. args: {[type(arg).__name__ for arg in args]}, kwargs: {list(kwargs.keys())}\"\n            )\n\n        # Handle different input formats\n        if hasattr(batch, \"x\") and hasattr(batch, \"edge_index\"):\n            # PyTorch Geometric Data object\n            x = batch.x\n            if edge_index is None:\n                edge_index = batch.edge_index\n            if batch_idx is None and hasattr(batch, \"batch\"):\n                batch_idx = batch.batch\n        elif isinstance(batch, torch.Tensor):\n            # Raw tensor\n            x = batch\n        elif isinstance(batch, dict):\n            # Dictionary format\n            x = batch.get(\"x\", batch)\n            if edge_index is None:\n                edge_index = batch.get(\"edge_index\")\n            if batch_idx is None:\n                batch_idx = batch.get(\"batch\")\n        else:\n            # Assume it's a tensor\n            x = batch\n\n        # Ensure tensors are on correct device\n        if isinstance(x, torch.Tensor):\n            x = x.to(self.device)\n        if edge_index is not None and isinstance(edge_index, torch.Tensor):\n            edge_index = edge_index.to(self.device)\n        if batch_idx is not None and isinstance(batch_idx, torch.Tensor):\n            batch_idx = batch_idx.to(self.device)\n\n        # Process features\n        features = self.feature_processor(x)\n\n        # Process through graph layers if edge_index available\n        if edge_index is not None:\n            embeddings = self.graph_processor(features, edge_index)\n        else:\n            embeddings = features\n\n        # Apply pooling if needed\n        if self.pooling_module is not None and batch_idx is not None:\n            pooled = self.pooling_module(embeddings, batch_idx)\n        else:\n            pooled = embeddings\n\n        # Generate output\n        output = self.output_head(pooled)\n\n        # Return embeddings if requested\n        if kwargs.get(\"return_embeddings\", False):\n            return {\"logits\": output, \"embeddings\": embeddings}\n\n        return output\n\n    except Exception as e:\n        # error reporting\n        import logging\n        import traceback\n\n        logger = logging.getLogger(__name__)\n        logger.error(f\"\u274c Forward pass failed: {e}\")\n        logger.error(f\"   Args: {[type(arg).__name__ for arg in args]}\")\n        logger.error(f\"   Kwargs: {list(kwargs.keys())}\")\n        logger.error(f\"   Traceback: {traceback.format_exc()}\")\n        raise\n</code></pre>"},{"location":"api/astro_lab.models/#astro_lab.models.AstroSurveyGNN.get_num_parameters","title":"get_num_parameters","text":"<pre><code>get_num_parameters() -&gt; int\n</code></pre> <p>Count trainable parameters.</p> Source code in <code>src\\astro_lab\\models\\core\\survey_gnn.py</code> <pre><code>def get_num_parameters(self) -&gt; int:\n    \"\"\"Count trainable parameters.\"\"\"\n    return sum(p.numel() for p in self.parameters() if p.requires_grad)\n</code></pre>"},{"location":"api/astro_lab.models/#astro_lab.models.ModelConfig","title":"ModelConfig  <code>dataclass</code>","text":"<p>Simple model configuration.</p> <p>Methods:</p> Name Description <code>to_dict</code> <p>Convert to dictionary.</p> Source code in <code>src\\astro_lab\\models\\config.py</code> <pre><code>@dataclass\nclass ModelConfig:\n    \"\"\"Simple model configuration.\"\"\"\n\n    name: str\n    hidden_dim: int = 128\n    num_layers: int = 3\n    conv_type: str = \"gcn\"\n    dropout: float = 0.1\n    task: str = \"classification\"\n\n    # Survey-specific settings\n    use_photometry: bool = True\n    use_astrometry: bool = True\n    use_spectroscopy: bool = False\n\n    # Additional settings\n    output_dim: Optional[int] = None\n    pooling: str = \"mean\"\n    activation: str = \"relu\"\n\n    # GAT/Transformer specific\n    num_heads: int = 8\n\n    def to_dict(self) -&gt; Dict[str, Any]:\n        \"\"\"Convert to dictionary.\"\"\"\n        return {\n            \"name\": self.name,\n            \"hidden_dim\": self.hidden_dim,\n            \"num_layers\": self.num_layers,\n            \"conv_type\": self.conv_type,\n            \"dropout\": self.dropout,\n            \"task\": self.task,\n            \"use_photometry\": self.use_photometry,\n            \"use_astrometry\": self.use_astrometry,\n            \"use_spectroscopy\": self.use_spectroscopy,\n            \"output_dim\": self.output_dim,\n            \"pooling\": self.pooling,\n            \"activation\": self.activation,\n            \"num_heads\": self.num_heads,\n        }\n</code></pre>"},{"location":"api/astro_lab.models/#astro_lab.models.ModelConfig.to_dict","title":"to_dict","text":"<pre><code>to_dict() -&gt; Dict[str, Any]\n</code></pre> <p>Convert to dictionary.</p> Source code in <code>src\\astro_lab\\models\\config.py</code> <pre><code>def to_dict(self) -&gt; Dict[str, Any]:\n    \"\"\"Convert to dictionary.\"\"\"\n    return {\n        \"name\": self.name,\n        \"hidden_dim\": self.hidden_dim,\n        \"num_layers\": self.num_layers,\n        \"conv_type\": self.conv_type,\n        \"dropout\": self.dropout,\n        \"task\": self.task,\n        \"use_photometry\": self.use_photometry,\n        \"use_astrometry\": self.use_astrometry,\n        \"use_spectroscopy\": self.use_spectroscopy,\n        \"output_dim\": self.output_dim,\n        \"pooling\": self.pooling,\n        \"activation\": self.activation,\n        \"num_heads\": self.num_heads,\n    }\n</code></pre>"},{"location":"api/astro_lab.models/#astro_lab.models.TemporalGCN","title":"TemporalGCN","text":"<p>               Bases: <code>Module</code></p> <p>Simple Temporal Graph Convolutional Network.</p> <p>Methods:</p> Name Description <code>encode_snapshot</code> <p>Encode a single graph snapshot.</p> <code>forward</code> <p>Process temporal sequence of graphs.</p> Source code in <code>src\\astro_lab\\models\\core\\temporal_gnn.py</code> <pre><code>class TemporalGCN(nn.Module):\n    \"\"\"Simple Temporal Graph Convolutional Network.\"\"\"\n\n    def __init__(\n        self,\n        input_dim: int,\n        hidden_dim: int = 128,\n        output_dim: int = 1,\n        num_graph_layers: int = 3,\n        num_rnn_layers: int = 2,\n        rnn_type: str = \"lstm\",\n        conv_type: str = \"gcn\",\n        task: str = \"regression\",\n        dropout: float = 0.1,\n        device: Optional[Union[str, torch.device]] = None,\n        **kwargs\n    ):\n        super().__init__()\n\n        self.device = torch.device(device or ('cuda' if torch.cuda.is_available() else 'cpu'))\n        self.hidden_dim = hidden_dim\n        self.task = task\n\n        # Input projection\n        self.input_projection = nn.Linear(input_dim, hidden_dim)\n\n        # Graph processor for spatial features\n        self.graph_processor = GraphProcessor(\n            hidden_dim=hidden_dim,\n            num_layers=num_graph_layers,\n            conv_type=conv_type,\n            dropout=dropout,\n            **kwargs\n        )\n\n        # RNN for temporal processing\n        if rnn_type.lower() == \"lstm\":\n            self.rnn = nn.LSTM(\n                input_size=hidden_dim,\n                hidden_size=hidden_dim,\n                num_layers=num_rnn_layers,\n                dropout=dropout if num_rnn_layers &gt; 1 else 0,\n                batch_first=True,\n            )\n        elif rnn_type.lower() == \"gru\":\n            self.rnn = nn.GRU(\n                input_size=hidden_dim,\n                hidden_size=hidden_dim,\n                num_layers=num_rnn_layers,\n                dropout=dropout if num_rnn_layers &gt; 1 else 0,\n                batch_first=True,\n            )\n        else:\n            raise ValueError(f\"Unknown RNN type: {rnn_type}\")\n\n        # Pooling\n        self.pooling = PoolingModule(\"mean\")\n\n        # Output head\n        head_type = task.replace('graph_', '').replace('node_', '')\n        self.output_head = create_output_head(\n            head_type,\n            input_dim=hidden_dim,\n            output_dim=output_dim,\n            dropout=dropout,\n            **kwargs\n        )\n\n        self.to(self.device)\n\n    def encode_snapshot(\n        self,\n        x: torch.Tensor,\n        edge_index: torch.Tensor,\n        batch: Optional[torch.Tensor] = None,\n    ) -&gt; torch.Tensor:\n        \"\"\"Encode a single graph snapshot.\"\"\"\n        # Project input\n        h = self.input_projection(x)\n\n        # Process through graph layers\n        h = self.graph_processor(h, edge_index)\n\n        # Pool to graph level\n        h = self.pooling(h, batch)\n\n        return h\n\n    def forward(\n        self,\n        snapshot_sequence: Union[List[Dict[str, torch.Tensor]], Dict[str, torch.Tensor]],\n        return_embeddings: bool = False,\n    ) -&gt; Union[torch.Tensor, Dict[str, torch.Tensor]]:\n        \"\"\"Process temporal sequence of graphs.\"\"\"\n\n        # Handle single snapshot\n        if isinstance(snapshot_sequence, dict):\n            snapshot_sequence = [snapshot_sequence]\n\n        # Encode each snapshot\n        graph_embeddings = []\n        for snapshot in snapshot_sequence:\n            x = snapshot[\"x\"].to(self.device)\n            edge_index = snapshot[\"edge_index\"].to(self.device)\n            batch = snapshot.get(\"batch\")\n            if batch is not None:\n                batch = batch.to(self.device)\n\n            h = self.encode_snapshot(x, edge_index, batch)\n            graph_embeddings.append(h)\n\n        # Stack for temporal processing\n        if len(graph_embeddings) &gt; 1:\n            # Multiple snapshots - use RNN\n            graph_sequence = torch.stack(graph_embeddings, dim=1)\n            rnn_out, _ = self.rnn(graph_sequence)\n            final_embedding = rnn_out[:, -1, :]  # Last output\n        else:\n            # Single snapshot\n            final_embedding = graph_embeddings[0]\n\n        # Output prediction\n        output = self.output_head(final_embedding)\n\n        if return_embeddings:\n            return {\"predictions\": output, \"embeddings\": final_embedding}\n        return output \n</code></pre>"},{"location":"api/astro_lab.models/#astro_lab.models.TemporalGCN.encode_snapshot","title":"encode_snapshot","text":"<pre><code>encode_snapshot(\n    x: Tensor, edge_index: Tensor, batch: Optional[Tensor] = None\n) -&gt; Tensor\n</code></pre> <p>Encode a single graph snapshot.</p> Source code in <code>src\\astro_lab\\models\\core\\temporal_gnn.py</code> <pre><code>def encode_snapshot(\n    self,\n    x: torch.Tensor,\n    edge_index: torch.Tensor,\n    batch: Optional[torch.Tensor] = None,\n) -&gt; torch.Tensor:\n    \"\"\"Encode a single graph snapshot.\"\"\"\n    # Project input\n    h = self.input_projection(x)\n\n    # Process through graph layers\n    h = self.graph_processor(h, edge_index)\n\n    # Pool to graph level\n    h = self.pooling(h, batch)\n\n    return h\n</code></pre>"},{"location":"api/astro_lab.models/#astro_lab.models.TemporalGCN.forward","title":"forward","text":"<pre><code>forward(\n    snapshot_sequence: Union[List[Dict[str, Tensor]], Dict[str, Tensor]],\n    return_embeddings: bool = False,\n) -&gt; Union[Tensor, Dict[str, Tensor]]\n</code></pre> <p>Process temporal sequence of graphs.</p> Source code in <code>src\\astro_lab\\models\\core\\temporal_gnn.py</code> <pre><code>def forward(\n    self,\n    snapshot_sequence: Union[List[Dict[str, torch.Tensor]], Dict[str, torch.Tensor]],\n    return_embeddings: bool = False,\n) -&gt; Union[torch.Tensor, Dict[str, torch.Tensor]]:\n    \"\"\"Process temporal sequence of graphs.\"\"\"\n\n    # Handle single snapshot\n    if isinstance(snapshot_sequence, dict):\n        snapshot_sequence = [snapshot_sequence]\n\n    # Encode each snapshot\n    graph_embeddings = []\n    for snapshot in snapshot_sequence:\n        x = snapshot[\"x\"].to(self.device)\n        edge_index = snapshot[\"edge_index\"].to(self.device)\n        batch = snapshot.get(\"batch\")\n        if batch is not None:\n            batch = batch.to(self.device)\n\n        h = self.encode_snapshot(x, edge_index, batch)\n        graph_embeddings.append(h)\n\n    # Stack for temporal processing\n    if len(graph_embeddings) &gt; 1:\n        # Multiple snapshots - use RNN\n        graph_sequence = torch.stack(graph_embeddings, dim=1)\n        rnn_out, _ = self.rnn(graph_sequence)\n        final_embedding = rnn_out[:, -1, :]  # Last output\n    else:\n        # Single snapshot\n        final_embedding = graph_embeddings[0]\n\n    # Output prediction\n    output = self.output_head(final_embedding)\n\n    if return_embeddings:\n        return {\"predictions\": output, \"embeddings\": final_embedding}\n    return output \n</code></pre>"},{"location":"api/astro_lab.models/#astro_lab.models.create_asteroid_period_detector","title":"create_asteroid_period_detector","text":"<pre><code>create_asteroid_period_detector(\n    hidden_dim: int = 128, device: Optional[Union[str, device]] = None, **kwargs: Any\n) -&gt; ALCDEFTemporalGNN\n</code></pre> <p>Create asteroid period detector (robust to all kwargs).</p> <p>Parameters:</p> Name Type Description Default <code>hidden_dim</code> <code>int</code> <p>Hidden dimension size</p> <code>128</code> <code>device</code> <code>Optional[Union[str, device]]</code> <p>Device to place model on</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Additional model parameters</p> <code>{}</code> <p>Returns:</p> Type Description <code>ALCDEFTemporalGNN</code> <p>Configured ALCDEFTemporalGNN model</p> Source code in <code>src\\astro_lab\\models\\factories.py</code> <pre><code>def create_asteroid_period_detector(\n    hidden_dim: int = 128,\n    device: Optional[Union[str, torch.device]] = None,\n    **kwargs: Any,\n) -&gt; ALCDEFTemporalGNN:\n    \"\"\"\n    Create asteroid period detector (robust to all kwargs).\n\n    Args:\n        hidden_dim: Hidden dimension size\n        device: Device to place model on\n        **kwargs: Additional model parameters\n\n    Returns:\n        Configured ALCDEFTemporalGNN model\n    \"\"\"\n    config = get_predefined_config(\"asteroid_period\").to_dict()\n    config.update(kwargs)\n    config.setdefault(\"hidden_dim\", hidden_dim)\n    config.setdefault(\"device\", device)\n    config.setdefault(\"task\", \"period_detection\")\n\n    # Filter to only valid ALCDEFTemporalGNN parameters\n    filtered = filter_kwargs(ALCDEFTemporalGNN, **config)\n    return ALCDEFTemporalGNN(**filtered)\n</code></pre>"},{"location":"api/astro_lab.models/#astro_lab.models.create_gaia_classifier","title":"create_gaia_classifier","text":"<pre><code>create_gaia_classifier(\n    num_classes: Optional[int] = None,\n    hidden_dim: int = 128,\n    device: Optional[Union[str, device]] = None,\n    **kwargs: Any\n) -&gt; AstroSurveyGNN\n</code></pre> <p>Create Gaia classifier (robust to all kwargs).</p> <p>Parameters:</p> Name Type Description Default <code>num_classes</code> <code>Optional[int]</code> <p>Number of output classes (required for classification)</p> <code>None</code> <code>hidden_dim</code> <code>int</code> <p>Hidden dimension size</p> <code>128</code> <code>device</code> <code>Optional[Union[str, device]]</code> <p>Device to place model on</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Additional model parameters</p> <code>{}</code> <p>Returns:</p> Type Description <code>AstroSurveyGNN</code> <p>Configured AstroSurveyGNN model</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If num_classes is not provided or invalid</p> Source code in <code>src\\astro_lab\\models\\factories.py</code> <pre><code>def create_gaia_classifier(\n    num_classes: Optional[int] = None,\n    hidden_dim: int = 128,\n    device: Optional[Union[str, torch.device]] = None,\n    **kwargs: Any,\n) -&gt; AstroSurveyGNN:\n    \"\"\"\n    Create Gaia classifier (robust to all kwargs).\n\n    Args:\n        num_classes: Number of output classes (required for classification)\n        hidden_dim: Hidden dimension size\n        device: Device to place model on\n        **kwargs: Additional model parameters\n\n    Returns:\n        Configured AstroSurveyGNN model\n\n    Raises:\n        ValueError: If num_classes is not provided or invalid\n    \"\"\"\n    if num_classes is None:\n        raise ValueError(\n            \"num_classes must be specified for classification models. \"\n            \"It should be determined from your data.\"\n        )\n    if num_classes &lt; 2:\n        raise ValueError(f\"num_classes must be at least 2, got {num_classes}\")\n\n    config = get_predefined_config(\"gaia_classifier\").to_dict()\n    config.update(kwargs)\n    config[\"output_dim\"] = num_classes\n    config[\"hidden_dim\"] = hidden_dim\n    if device is not None:\n        config[\"device\"] = device\n    config[\"task\"] = \"classification\"\n\n    logger.info(f\"Creating Gaia classifier with {num_classes} classes\")\n\n    filtered = filter_kwargs_strict(AstroSurveyGNN, **config)\n    return AstroSurveyGNN(**filtered)\n</code></pre>"},{"location":"api/astro_lab.models/#astro_lab.models.create_lsst_transient_detector","title":"create_lsst_transient_detector","text":"<pre><code>create_lsst_transient_detector(\n    num_classes: Optional[int] = None,\n    hidden_dim: int = 128,\n    device: Optional[Union[str, device]] = None,\n    **kwargs: Any\n) -&gt; AstroSurveyGNN\n</code></pre> <p>Create LSST transient detector (robust to all kwargs).</p> <p>Parameters:</p> Name Type Description Default <code>num_classes</code> <code>Optional[int]</code> <p>Number of output classes (required)</p> <code>None</code> <code>hidden_dim</code> <code>int</code> <p>Hidden dimension size</p> <code>128</code> <code>device</code> <code>Optional[Union[str, device]]</code> <p>Device to place model on</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Additional model parameters</p> <code>{}</code> <p>Returns:</p> Type Description <code>AstroSurveyGNN</code> <p>Configured AstroSurveyGNN model</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If num_classes is not provided</p> Source code in <code>src\\astro_lab\\models\\factories.py</code> <pre><code>def create_lsst_transient_detector(\n    num_classes: Optional[int] = None,\n    hidden_dim: int = 128,\n    device: Optional[Union[str, torch.device]] = None,\n    **kwargs: Any,\n) -&gt; AstroSurveyGNN:\n    \"\"\"\n    Create LSST transient detector (robust to all kwargs).\n\n    Args:\n        num_classes: Number of output classes (required)\n        hidden_dim: Hidden dimension size\n        device: Device to place model on\n        **kwargs: Additional model parameters\n\n    Returns:\n        Configured AstroSurveyGNN model\n\n    Raises:\n        ValueError: If num_classes is not provided\n    \"\"\"\n    if num_classes is None:\n        raise ValueError(\n            \"num_classes must be specified for classification models. \"\n            \"It should be determined from your data.\"\n        )\n\n    config = get_predefined_config(\"lsst_transient\").to_dict()\n    config.update(kwargs)\n    config[\"output_dim\"] = num_classes\n    config[\"hidden_dim\"] = hidden_dim\n    config[\"device\"] = device\n    config[\"task\"] = \"classification\"\n\n    # Filter to only valid AstroSurveyGNN parameters\n    filtered = filter_kwargs(AstroSurveyGNN, **config)\n    return AstroSurveyGNN(**filtered)\n</code></pre>"},{"location":"api/astro_lab.models/#astro_lab.models.create_sdss_galaxy_model","title":"create_sdss_galaxy_model","text":"<pre><code>create_sdss_galaxy_model(\n    output_dim: Optional[int] = None,\n    hidden_dim: int = 256,\n    device: Optional[Union[str, device]] = None,\n    **kwargs: Any\n) -&gt; AstroSurveyGNN\n</code></pre> <p>Create SDSS galaxy model (robust to all kwargs).</p> <p>Parameters:</p> Name Type Description Default <code>output_dim</code> <code>Optional[int]</code> <p>Output dimension (required)</p> <code>None</code> <code>hidden_dim</code> <code>int</code> <p>Hidden dimension size</p> <code>256</code> <code>device</code> <code>Optional[Union[str, device]]</code> <p>Device to place model on</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Additional model parameters</p> <code>{}</code> <p>Returns:</p> Type Description <code>AstroSurveyGNN</code> <p>Configured AstroSurveyGNN model</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If output_dim is not provided</p> Source code in <code>src\\astro_lab\\models\\factories.py</code> <pre><code>def create_sdss_galaxy_model(\n    output_dim: Optional[int] = None,\n    hidden_dim: int = 256,\n    device: Optional[Union[str, torch.device]] = None,\n    **kwargs: Any,\n) -&gt; AstroSurveyGNN:\n    \"\"\"\n    Create SDSS galaxy model (robust to all kwargs).\n\n    Args:\n        output_dim: Output dimension (required)\n        hidden_dim: Hidden dimension size\n        device: Device to place model on\n        **kwargs: Additional model parameters\n\n    Returns:\n        Configured AstroSurveyGNN model\n\n    Raises:\n        ValueError: If output_dim is not provided\n    \"\"\"\n    if output_dim is None:\n        raise ValueError(\n            \"output_dim must be specified for regression models. \"\n            \"It depends on the number of properties you want to predict.\"\n        )\n\n    config = get_predefined_config(\"sdss_galaxy\").to_dict()\n    config.update(kwargs)\n    if output_dim is not None:\n        config[\"output_dim\"] = output_dim\n    config[\"hidden_dim\"] = hidden_dim\n    if device is not None:\n        config[\"device\"] = device\n    config[\"task\"] = \"regression\"\n\n    filtered = filter_kwargs_strict(AstroSurveyGNN, **config)\n    return AstroSurveyGNN(**filtered)\n</code></pre>"},{"location":"api/astro_lab.models/#astro_lab.models.get_predefined_config","title":"get_predefined_config","text":"<pre><code>get_predefined_config(name: str) -&gt; ModelConfig\n</code></pre> <p>Get predefined config by name.</p> Source code in <code>src\\astro_lab\\models\\config.py</code> <pre><code>def get_predefined_config(name: str) -&gt; ModelConfig:\n    \"\"\"Get predefined config by name.\"\"\"\n    if name not in CONFIGS:\n        available = list(CONFIGS.keys())\n        raise ValueError(f\"Unknown config: {name}. Available: {available}\")\n\n    # Return a copy to avoid modifications\n    config = CONFIGS[name]\n    return ModelConfig(**config.to_dict())\n</code></pre>"},{"location":"api/astro_lab.tensors/","title":"astro_lab.tensors","text":""},{"location":"api/astro_lab.tensors/#astro_lab.tensors","title":"tensors","text":""},{"location":"api/astro_lab.tensors/#astro_lab.tensors--astronomical-tensordict-system","title":"Astronomical TensorDict System","text":"<p>Modern tensor-based astronomical data processing using TensorDict architecture. Fully modernized for PyTorch 2.0+ and Lightning integration.</p> <p>This module provides specialized TensorDicts for astronomical data types: - Spatial coordinates with coordinate transformations - Photometric measurements across multiple bands - Spectroscopic data with wavelength operations - Time series and lightcurve analysis - Survey data coordination and management - Orbital mechanics and satellite tracking - Cosmological simulation data</p> <p>All TensorDicts include: - Native PyTorch integration - Memory-efficient hierarchical data structures - Zero-copy operations where possible - Astronomical metadata preservation - Lightning DataModule compatibility</p> <p>Classes:</p> Name Description <code>LightcurveTensorConfig</code> <p>Configuration for LightcurveTensorDict.</p> <code>OrbitTensorConfig</code> <p>Configuration for OrbitTensorDict.</p> <code>PhotometricTensorConfig</code> <p>Configuration for PhotometricTensorDict.</p> <code>SpatialTensorConfig</code> <p>Configuration for SpatialTensorDict.</p> <code>SpectralTensorConfig</code> <p>Configuration for SpectralTensorDict.</p> <code>SurveyTensorConfig</code> <p>Configuration for SurveyTensorDict.</p> <p>Functions:</p> Name Description <code>create_photometric_tensor</code> <p>Create PhotometricTensorDict from magnitude data.</p> <code>create_simulation_tensor</code> <p>Create SimulationTensorDict for N-body data.</p> <code>create_spatial_tensor</code> <p>Create SpatialTensorDict from coordinates.</p> <code>create_survey_tensor</code> <p>Create SurveyTensorDict from components.</p> <code>from_astrometric_data</code> <p>Create SpatialTensorDict from astrometric measurements.</p> <code>from_lightcurve_data</code> <p>Create LightcurveTensorDict from lightcurve data.</p> <code>from_orbital_elements</code> <p>Create OrbitTensorDict from orbital elements.</p>"},{"location":"api/astro_lab.tensors/#astro_lab.tensors.LightcurveTensorConfig","title":"LightcurveTensorConfig","text":"<p>               Bases: <code>BaseModel</code></p> <p>Configuration for LightcurveTensorDict.</p> Source code in <code>src\\astro_lab\\tensors\\__init__.py</code> <pre><code>class LightcurveTensorConfig(BaseModel):\n    \"\"\"Configuration for LightcurveTensorDict.\"\"\"\n\n    model_config = ConfigDict(validate_assignment=True, extra=\"allow\")\n    time_format: str = Field(default=\"mjd\", description=\"Time format\")\n    time_scale: str = Field(default=\"utc\", description=\"Time scale\")\n    bands: List[str] = Field(default_factory=lambda: [\"V\", \"I\"])\n</code></pre>"},{"location":"api/astro_lab.tensors/#astro_lab.tensors.OrbitTensorConfig","title":"OrbitTensorConfig","text":"<p>               Bases: <code>BaseModel</code></p> <p>Configuration for OrbitTensorDict.</p> Source code in <code>src\\astro_lab\\tensors\\__init__.py</code> <pre><code>class OrbitTensorConfig(BaseModel):\n    \"\"\"Configuration for OrbitTensorDict.\"\"\"\n\n    model_config = ConfigDict(validate_assignment=True, extra=\"allow\")\n    frame: str = Field(default=\"ecliptic\", description=\"Reference frame\")\n    units: Dict[str, str] = Field(\n        default_factory=lambda: {\"a\": \"au\", \"e\": \"dimensionless\", \"i\": \"degrees\"}\n    )\n</code></pre>"},{"location":"api/astro_lab.tensors/#astro_lab.tensors.PhotometricTensorConfig","title":"PhotometricTensorConfig","text":"<p>               Bases: <code>BaseModel</code></p> <p>Configuration for PhotometricTensorDict.</p> Source code in <code>src\\astro_lab\\tensors\\__init__.py</code> <pre><code>class PhotometricTensorConfig(BaseModel):\n    \"\"\"Configuration for PhotometricTensorDict.\"\"\"\n\n    model_config = ConfigDict(validate_assignment=True, extra=\"allow\")\n    bands: List[str] = Field(default_factory=lambda: [\"u\", \"g\", \"r\", \"i\", \"z\"])\n    magnitude_system: str = Field(default=\"AB\", description=\"Magnitude system\")\n    zeropoints: Optional[Dict[str, float]] = Field(default=None)\n</code></pre>"},{"location":"api/astro_lab.tensors/#astro_lab.tensors.SpatialTensorConfig","title":"SpatialTensorConfig","text":"<p>               Bases: <code>BaseModel</code></p> <p>Configuration for SpatialTensorDict.</p> Source code in <code>src\\astro_lab\\tensors\\__init__.py</code> <pre><code>class SpatialTensorConfig(BaseModel):\n    \"\"\"Configuration for SpatialTensorDict.\"\"\"\n\n    model_config = ConfigDict(validate_assignment=True, extra=\"allow\")\n    coordinate_system: str = Field(\n        default=\"icrs\", description=\"Coordinate reference system\"\n    )\n    units: Dict[str, str] = Field(\n        default_factory=lambda: {\"ra\": \"degrees\", \"dec\": \"degrees\", \"distance\": \"kpc\"}\n    )\n    epoch: Optional[str] = Field(default=\"J2000.0\", description=\"Coordinate epoch\")\n</code></pre>"},{"location":"api/astro_lab.tensors/#astro_lab.tensors.SpectralTensorConfig","title":"SpectralTensorConfig","text":"<p>               Bases: <code>BaseModel</code></p> <p>Configuration for SpectralTensorDict.</p> Source code in <code>src\\astro_lab\\tensors\\__init__.py</code> <pre><code>class SpectralTensorConfig(BaseModel):\n    \"\"\"Configuration for SpectralTensorDict.\"\"\"\n\n    model_config = ConfigDict(validate_assignment=True, extra=\"allow\")\n    wavelength_unit: str = Field(default=\"angstrom\", description=\"Wavelength units\")\n    flux_unit: str = Field(default=\"erg/s/cm2/A\", description=\"Flux units\")\n    spectral_resolution: Optional[float] = Field(default=None, description=\"R = \u03bb/\u0394\u03bb\")\n</code></pre>"},{"location":"api/astro_lab.tensors/#astro_lab.tensors.SurveyTensorConfig","title":"SurveyTensorConfig","text":"<p>               Bases: <code>BaseModel</code></p> <p>Configuration for SurveyTensorDict.</p> Source code in <code>src\\astro_lab\\tensors\\__init__.py</code> <pre><code>class SurveyTensorConfig(BaseModel):\n    \"\"\"Configuration for SurveyTensorDict.\"\"\"\n\n    model_config = ConfigDict(validate_assignment=True, extra=\"allow\")\n    survey_name: str = Field(..., description=\"Name of the survey\")\n    data_release: Optional[str] = Field(\n        default=None, description=\"Data release version\"\n    )\n    selection_function: Optional[str] = Field(\n        default=None, description=\"Selection function applied\"\n    )\n    created_at: str = Field(default_factory=lambda: datetime.datetime.now().isoformat())\n</code></pre>"},{"location":"api/astro_lab.tensors/#astro_lab.tensors.create_photometric_tensor","title":"create_photometric_tensor","text":"<pre><code>create_photometric_tensor(magnitudes, bands, **kwargs)\n</code></pre> <p>Create PhotometricTensorDict from magnitude data.</p> Source code in <code>src\\astro_lab\\tensors\\__init__.py</code> <pre><code>def create_photometric_tensor(magnitudes, bands, **kwargs):\n    \"\"\"Create PhotometricTensorDict from magnitude data.\"\"\"\n    import torch\n\n    if not isinstance(magnitudes, torch.Tensor):\n        magnitudes = torch.tensor(magnitudes, dtype=torch.float32)\n\n    return PhotometricTensorDict(magnitudes, bands, **kwargs)\n</code></pre>"},{"location":"api/astro_lab.tensors/#astro_lab.tensors.create_simulation_tensor","title":"create_simulation_tensor","text":"<pre><code>create_simulation_tensor(positions, features=None, **kwargs)\n</code></pre> <p>Create SimulationTensorDict for N-body data.</p> Source code in <code>src\\astro_lab\\tensors\\__init__.py</code> <pre><code>def create_simulation_tensor(positions, features=None, **kwargs):\n    \"\"\"Create SimulationTensorDict for N-body data.\"\"\"\n    return SimulationTensorDict(\n        positions=positions,\n        velocities=kwargs.get(\"velocities\", positions * 0),\n        masses=kwargs.get(\"masses\", positions.new_ones(positions.shape[0])),\n        **kwargs,\n    )\n</code></pre>"},{"location":"api/astro_lab.tensors/#astro_lab.tensors.create_spatial_tensor","title":"create_spatial_tensor","text":"<pre><code>create_spatial_tensor(coordinates, coordinate_system='icrs', **kwargs)\n</code></pre> <p>Create SpatialTensorDict from coordinates.</p> Source code in <code>src\\astro_lab\\tensors\\__init__.py</code> <pre><code>def create_spatial_tensor(coordinates, coordinate_system=\"icrs\", **kwargs):\n    \"\"\"Create SpatialTensorDict from coordinates.\"\"\"\n    import torch\n\n    if not isinstance(coordinates, torch.Tensor):\n        coordinates = torch.tensor(coordinates, dtype=torch.float32)\n\n    return SpatialTensorDict(coordinates, coordinate_system=coordinate_system, **kwargs)\n</code></pre>"},{"location":"api/astro_lab.tensors/#astro_lab.tensors.create_survey_tensor","title":"create_survey_tensor","text":"<pre><code>create_survey_tensor(spatial, photometric, survey_name, **kwargs)\n</code></pre> <p>Create SurveyTensorDict from components.</p> Source code in <code>src\\astro_lab\\tensors\\__init__.py</code> <pre><code>def create_survey_tensor(spatial, photometric, survey_name, **kwargs):\n    \"\"\"Create SurveyTensorDict from components.\"\"\"\n    return SurveyTensorDict(\n        spatial=spatial, photometric=photometric, survey_name=survey_name, **kwargs\n    )\n</code></pre>"},{"location":"api/astro_lab.tensors/#astro_lab.tensors.from_astrometric_data","title":"from_astrometric_data","text":"<pre><code>from_astrometric_data(ra, dec, parallax=None, pmra=None, pmdec=None, **kwargs)\n</code></pre> <p>Create SpatialTensorDict from astrometric measurements.</p> Source code in <code>src\\astro_lab\\tensors\\__init__.py</code> <pre><code>def from_astrometric_data(ra, dec, parallax=None, pmra=None, pmdec=None, **kwargs):\n    \"\"\"Create SpatialTensorDict from astrometric measurements.\"\"\"\n    import torch\n\n    # Convert to tensors\n    if not isinstance(ra, torch.Tensor):\n        ra = torch.tensor(ra, dtype=torch.float32)\n    if not isinstance(dec, torch.Tensor):\n        dec = torch.tensor(dec, dtype=torch.float32)\n\n    # Create coordinates tensor\n    coords = torch.stack([ra, dec, torch.zeros_like(ra)], dim=-1)\n\n    # Add distance from parallax if available\n    if parallax is not None:\n        if not isinstance(parallax, torch.Tensor):\n            parallax = torch.tensor(parallax, dtype=torch.float32)\n        distance = 1000.0 / (torch.abs(parallax) + 1e-6)  # mas to parsec\n        coords[..., 2] = distance\n\n    return SpatialTensorDict(coords, coordinate_system=\"icrs\", **kwargs)\n</code></pre>"},{"location":"api/astro_lab.tensors/#astro_lab.tensors.from_lightcurve_data","title":"from_lightcurve_data","text":"<pre><code>from_lightcurve_data(times, magnitudes, errors=None, **kwargs)\n</code></pre> <p>Create LightcurveTensorDict from lightcurve data.</p> Source code in <code>src\\astro_lab\\tensors\\__init__.py</code> <pre><code>def from_lightcurve_data(times, magnitudes, errors=None, **kwargs):\n    \"\"\"Create LightcurveTensorDict from lightcurve data.\"\"\"\n    import torch\n\n    if not isinstance(times, torch.Tensor):\n        times = torch.tensor(times, dtype=torch.float32)\n    if not isinstance(magnitudes, torch.Tensor):\n        magnitudes = torch.tensor(magnitudes, dtype=torch.float32)\n\n    # Ensure proper shape for LightcurveTensorDict\n    if magnitudes.dim() == 2:\n        magnitudes = magnitudes.unsqueeze(-1)  # Add band dimension\n\n    if errors is not None:\n        if not isinstance(errors, torch.Tensor):\n            errors = torch.tensor(errors, dtype=torch.float32)\n        if errors.dim() == 2:\n            errors = errors.unsqueeze(-1)\n\n    return LightcurveTensorDict(\n        times=times, magnitudes=magnitudes, bands=[\"V\"], errors=errors, **kwargs\n    )\n</code></pre>"},{"location":"api/astro_lab.tensors/#astro_lab.tensors.from_orbital_elements","title":"from_orbital_elements","text":"<pre><code>from_orbital_elements(elements, element_type='keplerian', **kwargs)\n</code></pre> <p>Create OrbitTensorDict from orbital elements.</p> Source code in <code>src\\astro_lab\\tensors\\__init__.py</code> <pre><code>def from_orbital_elements(elements, element_type=\"keplerian\", **kwargs):\n    \"\"\"Create OrbitTensorDict from orbital elements.\"\"\"\n    import torch\n\n    if not isinstance(elements, torch.Tensor):\n        elements = torch.tensor(elements, dtype=torch.float32)\n\n    return OrbitTensorDict(elements=elements, **kwargs)\n</code></pre>"},{"location":"api/astro_lab.training/","title":"astro_lab.training","text":""},{"location":"api/astro_lab.training/#astro_lab.training","title":"training","text":""},{"location":"api/astro_lab.training/#astro_lab.training--training-module-neural-network-training-infrastructure","title":"Training Module - Neural Network Training Infrastructure","text":"<p>Provides training infrastructure for neural network models including Lightning modules, MLflow logging, and Optuna optimization.</p> <p>Modules:</p> Name Description <code>config</code> <p>Training Configuration Management</p> <code>lightning_module</code> <p>AstroLightningModule - Unified PyTorch Lightning Module for Astronomical ML</p> <code>mlflow_logger</code> <p>MLflow Integration for AstroLab Training</p> <code>trainer</code> <p>AstroLab Trainer - High-Performance Training Interface</p> <p>Classes:</p> Name Description <code>AstroLightningModule</code> <p>Unified PyTorch Lightning Module for Astronomical Machine Learning.</p> <code>AstroMLflowLogger</code> <p>Optimized MLflow logger for astronomical models with 2025 system metrics integration.</p> <code>AstroTrainer</code> <p>High-performance trainer for astronomical ML models.</p> <code>TrainingConfig</code> <p>Complete training configuration integrating model and training settings.</p> <p>Functions:</p> Name Description <code>optimize_hyperparameters</code> <p>Run hyperparameter optimization.</p> <code>run_training</code> <p>Run training based on CLI arguments.</p> <code>train_from_config</code> <p>Train a model from configuration file.</p> <code>train_quick</code> <p>Quick training with dataset and model names.</p>"},{"location":"api/astro_lab.training/#astro_lab.training.AstroLightningModule","title":"AstroLightningModule","text":"<p>               Bases: <code>LightningModule</code></p> <p>Unified PyTorch Lightning Module for Astronomical Machine Learning.</p> <p>Features: - Lightning 2.0+ compatible architecture - Robust error handling with detailed logging - Automatic model creation from config - Support for classification, regression, and unsupervised tasks - Unified logging throughout - Modern metrics tracking with torchmetrics - Automatic class detection from data -  training features: gradient accumulation, gradient clipping - 2025 Best Practices: Compile mode, FSDP support, advanced schedulers - Fixed PyTorch Geometric compatibility</p> <p>Methods:</p> Name Description <code>configure_optimizers</code> <p>Configure optimizers and schedulers with 2025 best practices.</p> <code>forward</code> <p>Forward pass - handles different input formats.</p> <code>on_test_start</code> <p>Called at the start of testing.</p> <code>on_train_end</code> <p>Called at the end of training.</p> <code>on_train_start</code> <p>Called at the start of training.</p> <code>on_validation_start</code> <p>Called at the start of validation.</p> <code>predict_step</code> <p>Prediction step for inference.</p> <code>test_step</code> <p>Test step with comprehensive evaluation.</p> <code>training_step</code> <p>Training step with automatic optimization.</p> <code>validation_step</code> <p>Validation step - fails fast on errors.</p> <p>Attributes:</p> Name Type Description <code>model</code> <code>Module</code> <p>Lazy model getter - creates model if not initialized.</p> Source code in <code>src\\astro_lab\\training\\lightning_module.py</code> <pre><code>class AstroLightningModule(LightningModule):\n    \"\"\"\n    Unified PyTorch Lightning Module for Astronomical Machine Learning.\n\n    Features:\n    - Lightning 2.0+ compatible architecture\n    - Robust error handling with detailed logging\n    - Automatic model creation from config\n    - Support for classification, regression, and unsupervised tasks\n    - Unified logging throughout\n    - Modern metrics tracking with torchmetrics\n    - Automatic class detection from data\n    -  training features: gradient accumulation, gradient clipping\n    - 2025 Best Practices: Compile mode, FSDP support, advanced schedulers\n    - Fixed PyTorch Geometric compatibility\n    \"\"\"\n\n    def __init__(\n        self,\n        model: Optional[torch.nn.Module] = None,\n        model_config: Optional[ModelConfig] = None,\n        training_config: Optional[TrainingConfig] = None,\n        task_type: str = \"classification\",\n        learning_rate: float = 1e-3,\n        weight_decay: float = 1e-4,\n        projection_dim: int = 128,\n        temperature: float = 0.1,\n        num_classes: Optional[int] = None,\n        gradient_accumulation_steps: int = 1,\n        gradient_clip_val: float = 1.0,\n        gradient_clip_algorithm: str = \"norm\",\n        scheduler_type: str = \"cosine\",\n        warmup_steps: int = 0,\n        use_compile: bool = True,\n        use_ema: bool = False,\n        ema_decay: float = 0.999,\n        label_smoothing: float = 0.0,\n        **kwargs,\n    ):\n        super().__init__()\n\n        # Save hyperparameters for Lightning compatibility\n        self.save_hyperparameters(ignore=[\"model\"])\n\n        # Core configuration\n        self.task_type = task_type\n        self.learning_rate = learning_rate\n        self.weight_decay = weight_decay\n        self.projection_dim = projection_dim\n        self.temperature = temperature\n        self.num_classes = num_classes  # Will be set automatically if None\n        self.model_config = model_config\n        self.training_config = training_config\n\n        #  training options\n        self.gradient_accumulation_steps = gradient_accumulation_steps\n        self.gradient_clip_val = gradient_clip_val\n        self.gradient_clip_algorithm = gradient_clip_algorithm\n        self.scheduler_type = scheduler_type\n        self.warmup_steps = warmup_steps\n        self.use_compile = use_compile\n        self.use_ema = use_ema\n        self.ema_decay = ema_decay\n        self.label_smoothing = label_smoothing\n\n        # Use automatic optimization (default) instead of manual\n        self.automatic_optimization = True\n\n        # LAZY INITIALIZATION: Don't create model until we know num_classes\n        self._model = model  # Store the provided model\n        self._model_initialized = False  # Track if model is initialized\n\n        # Initialize EMA if requested\n        if self.use_ema:\n            self._init_ema()\n\n        # Initialize projection head for unsupervised learning\n        self.projection_head = None\n        if task_type == \"unsupervised\":\n            self.projection_head = self._auto_create_projection_head()\n\n        # Initialize metrics placeholders (will be set up after class detection)\n        self._setup_metrics()\n        self.metrics_initialized = False  # Will be set to True after setup\n\n        # Performance tracking\n        self._step_times = []\n        self._memory_usage = []\n\n    @property\n    def model(self) -&gt; torch.nn.Module:\n        \"\"\"Lazy model getter - creates model if not initialized.\"\"\"\n        if not self._model_initialized:\n            self._initialize_model(self._model)\n        return self._model\n\n    @model.setter\n    def model(self, value: torch.nn.Module):\n        \"\"\"Model setter.\"\"\"\n        self._model = value\n        self._model_initialized = True\n\n    def _init_ema(self):\n        \"\"\"Initialize Exponential Moving Average of model weights.\"\"\"\n        self.ema_model = torch.nn.ParameterDict()\n        # Don't access self.model here as it may not be initialized yet\n\n    def _update_ema(self):\n        \"\"\"Update EMA weights.\"\"\"\n        if self.use_ema and self._model_initialized:\n            with torch.no_grad():\n                for name, param in self.model.named_parameters():\n                    if param.requires_grad:\n                        key = name.replace(\".\", \"_\")\n                        if key not in self.ema_model:\n                            # Initialize EMA parameter on first access\n                            self.ema_model[key] = param.data.clone()\n                        else:\n                            ema_param = self.ema_model[key]\n                            ema_param.mul_(self.ema_decay).add_(\n                                param.data, alpha=1 - self.ema_decay\n                            )\n\n    def _initialize_model(self, model: Optional[torch.nn.Module]) -&gt; None:\n        \"\"\"Initialize model with compile support and error handling.\"\"\"\n        if self._model_initialized and self._model is not None:\n            return  # Already initialized\n\n        if model is None:\n            model = self._create_default_model()\n\n        self._model = model\n        self._model_initialized = True\n\n        # Skip torch.compile on Windows due to cl.exe and triton issues\n        import platform\n\n        skip_compile_windows = platform.system() == \"Windows\"\n\n        # Apply torch.compile with robust error handling\n        if self.use_compile and not skip_compile_windows:\n            try:\n                # Use inductor backend for best performance with dynamic shapes\n                self._model = torch.compile(\n                    self._model,\n                    backend=\"inductor\",  # Better than eager for performance\n                    mode=\"reduce-overhead\",  # Stable mode\n                    fullgraph=False,  # Allow fallback for unsupported operations\n                    dynamic=True,  # Enable dynamic shapes for graphs\n                )\n                logger.info(\"\u2705 Model compiled successfully with inductor backend\")\n            except Exception as e:\n                logger.warning(f\"\u26a0\ufe0f torch.compile failed: {e}\")\n                logger.info(\"\ud83d\udd04 Using uncompiled model\")\n                # Keep the original model without compilation\n                pass\n        else:\n            if skip_compile_windows:\n                logger.info(\n                    \"\ud83d\udd04 Skipping torch.compile on Windows (cl.exe/triton compatibility)\"\n                )\n            else:\n                logger.info(\"\ud83d\udcdd Model training without torch.compile\")\n\n    def _load_num_classes_from_metadata(self) -&gt; Optional[int]:\n        \"\"\"Load number of classes from dataset metadata files.\"\"\"\n        try:\n            # Try common metadata file locations\n            metadata_paths = [\n                \"data/processed/gaia/gaia_tensor_metadata.json\",\n                \"data/processed/gaia/gaia_metadata.json\",\n                \"data/processed/gaia_metadata.json\",\n            ]\n\n            for metadata_path in metadata_paths:\n                path = Path(metadata_path)\n                if path.exists():\n                    with open(path, \"r\") as f:\n                        metadata = json.load(f)\n\n                    # Check for classification info\n                    if \"classification\" in metadata:\n                        num_classes = metadata[\"classification\"].get(\"num_classes\")\n                        if num_classes:\n                            return int(num_classes)\n\n                    # Fallback: check for direct num_classes field\n                    if \"num_classes\" in metadata:\n                        num_classes = metadata[\"num_classes\"]\n                        return int(num_classes)\n\n            return None\n\n        except Exception:\n            return None\n\n    def _setup_metrics(self) -&gt; None:\n        \"\"\"Setup torchmetrics for performance tracking. \ud83d\udcca\"\"\"\n        # Initialize as None - will be set up later when we know num_classes\n        self.train_acc = None\n        self.val_acc = None\n        self.test_acc = None\n        self.train_f1 = None\n        self.val_f1 = None\n        self.test_f1 = None\n\n    def _create_metrics_for_classes(self, num_classes: int) -&gt; None:\n        \"\"\"Create metrics once we know the number of classes.\"\"\"\n        if self.task_type == \"classification\" and not self.metrics_initialized:\n            # Ensure we have at least 2 classes\n            num_classes = max(num_classes, 2)\n            device = self.device if hasattr(self, \"device\") else \"cpu\"\n\n            # Always use multiclass metrics for consistency\n            self.train_acc = MulticlassAccuracy(num_classes=num_classes).to(device)\n            self.val_acc = MulticlassAccuracy(num_classes=num_classes).to(device)\n            self.test_acc = MulticlassAccuracy(num_classes=num_classes).to(device)\n            self.train_f1 = F1Score(task=\"multiclass\", num_classes=num_classes).to(\n                device\n            )\n            self.val_f1 = F1Score(task=\"multiclass\", num_classes=num_classes).to(device)\n            self.test_f1 = F1Score(task=\"multiclass\", num_classes=num_classes).to(\n                device\n            )\n\n            self.metrics_initialized = True\n            logger.info(f\"\u2705 Metrics initialized for {num_classes} classes\")\n\n    def _detect_num_classes_from_data(self, batch) -&gt; int:\n        \"\"\"Detect number of classes from a batch of data.\"\"\"\n        try:\n            # Handle PyTorch Geometric Data objects\n            if hasattr(batch, \"y\"):\n                targets = batch.y\n            elif isinstance(batch, list) and len(batch) &gt; 0:\n                # Handle list of data objects\n                if hasattr(batch[0], \"y\"):\n                    targets = batch[0].y\n                else:\n                    targets = batch[1] if len(batch) &gt; 1 else None\n            elif isinstance(batch, dict):\n                targets = batch.get(\"target\") or batch.get(\"y\") or batch.get(\"labels\")\n            elif isinstance(batch, (list, tuple)) and len(batch) &gt; 1:\n                targets = batch[1]\n            else:\n                targets = None\n\n            if targets is not None:\n                if targets.dim() &gt; 1:\n                    targets = targets.flatten()\n                num_classes = int(targets.max().item()) + 1\n                # Ensure at least 2 classes\n                return max(num_classes, 2)\n            else:\n                # Use datamodule info if available\n                if (\n                    hasattr(self.trainer, \"datamodule\")\n                    and self.trainer.datamodule is not None\n                ):\n                    dm = self.trainer.datamodule\n                    if hasattr(dm, \"num_classes\") and dm.num_classes:\n                        return dm.num_classes\n                return 4  # Default fallback\n\n        except Exception as e:\n            logger.warning(f\"Could not detect classes from data: {e}\")\n            return 4  # Default fallback\n\n    def _create_default_model(self) -&gt; torch.nn.Module:\n        \"\"\"Create a default model if none provided.\"\"\"\n        try:\n            # Ensure we have num_classes before creating the model\n            if self.num_classes is None:\n                # Try to get from datamodule first\n                if (\n                    hasattr(self, \"trainer\")\n                    and hasattr(self.trainer, \"datamodule\")\n                    and self.trainer.datamodule is not None\n                ):\n                    dm = self.trainer.datamodule\n                    if hasattr(dm, \"num_classes\") and dm.num_classes:\n                        self.num_classes = dm.num_classes\n\n                # If still None, use default\n                if self.num_classes is None:\n                    logger.warning(\n                        \"Creating model without known num_classes, using default 4\"\n                    )\n                    self.num_classes = 4\n\n            if self.model_config:\n                # Update model config with correct number of classes\n                self.model_config.output_dim = self.num_classes\n                return self._create_model_from_config(self.model_config)\n            else:\n                # Create a simple default model\n                return AstroSurveyGNN(\n                    input_dim=16,  # Default input dimension\n                    hidden_dim=64,\n                    output_dim=self.num_classes,\n                    num_layers=3,\n                    dropout=0.1,\n                )\n        except Exception as e:\n            logger.error(f\"Failed to create default model: {e}\")\n            # Ultimate fallback\n            return torch.nn.Sequential(\n                torch.nn.Linear(16, 64),\n                torch.nn.ReLU(),\n                torch.nn.Dropout(0.1),\n                torch.nn.Linear(64, self.num_classes or 4),\n            )\n\n    def _create_model_from_config(self, config: ModelConfig) -&gt; torch.nn.Module:\n        \"\"\"Create model from configuration.\"\"\"\n        try:\n            # IMPORTANT: Ensure we have num_classes for classification models\n            if config.task == \"classification\" and self.num_classes is None:\n                raise ValueError(\n                    \"Cannot create classification model without knowing num_classes. \"\n                    \"Please ensure datamodule.setup() was called first.\"\n                )\n\n            # Ensure output_dim is set correctly\n            if self.num_classes is not None:\n                config.output_dim = self.num_classes\n\n            # Use the simplified ModelConfig fields\n            if (\n                config.task == \"classification\"\n                and config.name\n                and \"gaia\" in config.name.lower()\n            ):\n                # CRITICAL: Pass num_classes explicitly\n                if self.num_classes is None:\n                    raise ValueError(\"num_classes must be set for Gaia classifier\")\n\n                return create_gaia_classifier(\n                    hidden_dim=config.hidden_dim,\n                    num_classes=self.num_classes,  # Explicit parameter\n                )\n            elif config.name and \"astrophot\" in config.name.lower():\n                return AstroPhotGNN(\n                    hidden_dim=config.hidden_dim,\n                    output_dim=config.output_dim or self.num_classes or 12,\n                )\n            elif config.name and \"temporal\" in config.name.lower():\n                return ALCDEFTemporalGNN(\n                    hidden_dim=config.hidden_dim,\n                    task=config.task or \"period_detection\",\n                )\n            else:\n                # Default survey GNN\n                return AstroSurveyGNN(\n                    hidden_dim=config.hidden_dim,\n                    output_dim=config.output_dim or self.num_classes or 4,\n                    num_layers=config.num_layers,\n                    conv_type=config.conv_type,\n                    task=config.task or \"classification\",\n                    dropout=config.dropout,\n                )\n\n        except Exception as e:\n            logger.error(f\"Failed to create model from config: {e}\")\n            logger.error(f\"Config: {config}\")\n            logger.error(f\"num_classes: {self.num_classes}\")\n            raise  # Re-raise to see the actual error\n\n    def _auto_create_projection_head(self) -&gt; Optional[torch.nn.Module]:\n        \"\"\"Create projection head for unsupervised learning.\"\"\"\n        try:\n            return torch.nn.Sequential(\n                torch.nn.Linear(self.projection_dim, self.projection_dim),\n                torch.nn.ReLU(),\n                torch.nn.Linear(self.projection_dim, self.projection_dim),\n            )\n        except Exception as e:\n            logger.error(f\"Failed to create projection head: {e}\")\n            return None\n\n    def forward(\n        self, batch: Union[torch.Tensor, Dict[str, torch.Tensor], Any]\n    ) -&gt; torch.Tensor:\n        \"\"\"\n        Forward pass - handles different input formats.\n\n        Args:\n            batch: Input batch (can be PyTorch Geometric Data, tensor, or dict)\n\n        Returns:\n            Model output tensor\n        \"\"\"\n        try:\n            return self.model(batch)\n        except TypeError as e:\n            # Fallback: call model.forward directly (for torch.compile/ScriptModule)\n            if hasattr(self.model, \"forward\"):\n                return self.model.forward(batch)\n            else:\n                logger.error(f\"Forward pass failed - no forward method: {e}\")\n                logger.error(f\"Batch type: {type(batch)}\")\n                raise e\n        except Exception as e:\n            logger.error(f\"Forward pass failed: {e}\")\n            logger.error(f\"Batch type: {type(batch)}\")\n            raise\n\n    def _compute_loss(\n        self, outputs: torch.Tensor, targets: torch.Tensor\n    ) -&gt; torch.Tensor:\n        \"\"\"Compute loss based on task type.\"\"\"\n        try:\n            # Ensure both tensors are on the same device\n            if outputs.device != targets.device:\n                targets = targets.to(outputs.device)\n\n            if self.task_type == \"classification\":\n                # Ensure targets are the right type and shape\n                if targets.dtype != torch.long:\n                    targets = targets.long()\n                if targets.dim() &gt; 1:\n                    targets = targets.squeeze(-1)\n\n                # Handle shape mismatch for classification\n                if outputs.dim() &gt; 1 and outputs.size(-1) == 1:\n                    # Binary classification with single output\n                    outputs = outputs.squeeze(-1)\n                    targets = targets.float()\n                    return F.binary_cross_entropy_with_logits(outputs, targets)\n                else:\n                    # Multi-class classification\n                    if self.label_smoothing &gt; 0:\n                        return F.cross_entropy(\n                            outputs, targets, label_smoothing=self.label_smoothing\n                        )\n                    else:\n                        return F.cross_entropy(outputs, targets)\n            elif self.task_type == \"regression\":\n                return F.mse_loss(outputs, targets)\n            elif self.task_type == \"unsupervised\":\n                # Contrastive loss or similar\n                if self.projection_head is not None:\n                    projected = self.projection_head(outputs)\n                    # Simple contrastive loss (placeholder)\n                    return F.mse_loss(projected, torch.zeros_like(projected))\n                else:\n                    return F.mse_loss(outputs, targets)\n            else:\n                # Default to MSE\n                return F.mse_loss(outputs, targets)\n        except Exception as e:\n            logger.error(f\"Loss computation failed: {e}\")\n            logger.error(\n                f\"Outputs shape: {outputs.shape if hasattr(outputs, 'shape') else 'unknown'}\"\n            )\n            logger.error(\n                f\"Targets shape: {targets.shape if hasattr(targets, 'shape') else 'unknown'}\"\n            )\n            raise  # Re-raise instead of returning dummy loss\n\n    def _compute_step(self, batch: Any, stage: str) -&gt; Dict[str, torch.Tensor]:\n        \"\"\"\n        Unified step computation for train/val/test.\n\n        Args:\n            batch: Input batch (PyTorch Geometric Data object or list)\n            stage: One of 'train', 'val', 'test'\n\n        Returns:\n            Dictionary with loss and other metrics\n        \"\"\"\n        # Ensure we have a PyTorch Geometric Data object\n        if not hasattr(batch, \"x\") or not hasattr(batch, \"edge_index\"):\n            raise ValueError(\n                f\"Expected PyTorch Geometric Data object with 'x' and 'edge_index', got {type(batch)}\"\n            )\n\n        # Initialize metrics if not done yet\n        if not self.metrics_initialized and hasattr(batch, \"y\"):\n            num_classes = self._detect_num_classes_from_data(batch)\n            self.num_classes = num_classes\n            self._create_metrics_for_classes(num_classes)\n\n            # IMPORTANT: Ensure model is created/updated with correct num_classes\n            if not self._model_initialized:\n                # Create model now that we know num_classes\n                self._initialize_model(None)\n            elif hasattr(self._model, \"output_dim\"):\n                # Validate that model output matches detected classes\n                model_output_dim = self._model.output_dim\n                if model_output_dim != num_classes:\n                    raise ValueError(\n                        f\"Model output dimension ({model_output_dim}) does not match \"\n                        f\"number of classes in data ({num_classes}). \"\n                        f\"This usually happens when the model was created with wrong num_classes. \"\n                        f\"Labels in data: {torch.unique(batch.y).tolist()}\"\n                    )\n\n        # Get the appropriate mask for this stage\n        if stage == \"train\" and hasattr(batch, \"train_mask\"):\n            mask = batch.train_mask\n        elif stage == \"val\" and hasattr(batch, \"val_mask\"):\n            mask = batch.val_mask\n        elif stage == \"test\" and hasattr(batch, \"test_mask\"):\n            mask = batch.test_mask\n        else:\n            mask = None\n\n        # Forward pass\n        outputs = self.forward(batch)\n\n        # Apply mask if available and valid\n        if mask is not None and mask.sum() &gt; 0:\n            outputs_masked = outputs[mask]\n            targets_masked = batch.y[mask]\n        else:\n            outputs_masked = outputs\n            targets_masked = batch.y\n\n        # Ensure we have valid targets\n        if targets_masked.numel() == 0:\n            raise RuntimeError(\n                f\"No valid targets for stage {stage} - all masks are empty or False\"\n            )\n\n        # Additional validation for classification\n        if self.task_type == \"classification\":\n            # Check that all labels are within valid range\n            unique_labels = torch.unique(targets_masked)\n            max_label = unique_labels.max().item()\n            min_label = unique_labels.min().item()\n\n            if min_label &lt; 0:\n                raise ValueError(\n                    f\"Found negative label: {min_label}. Labels must be &gt;= 0\"\n                )\n\n            if max_label &gt;= self.num_classes:\n                raise ValueError(\n                    f\"Found label {max_label} but model expects {self.num_classes} classes (0 to {self.num_classes - 1}). \"\n                    f\"All unique labels in batch: {unique_labels.tolist()}\"\n                )\n\n        # Compute loss\n        loss = self._compute_loss(outputs_masked, targets_masked)\n\n        # Log metrics\n        self._log_step_metrics(outputs_masked, targets_masked, loss, stage)\n\n        return {\"loss\": loss, \"outputs\": outputs_masked, \"targets\": targets_masked}\n\n    def _log_step_metrics(\n        self,\n        outputs: torch.Tensor,\n        targets: torch.Tensor,\n        loss: torch.Tensor,\n        stage: str,\n    ) -&gt; None:\n        \"\"\"Log metrics for the current step.\"\"\"\n        try:\n            # Log loss\n            self.log(\n                f\"{stage}_loss\",\n                loss,\n                on_step=(stage == \"train\"),\n                on_epoch=True,\n                prog_bar=True,\n            )\n\n            # Log accuracy for classification\n            if self.task_type == \"classification\" and self.metrics_initialized:\n                # Ensure targets and outputs are on the same device and correct shape\n                if outputs.device != targets.device:\n                    targets = targets.to(outputs.device)\n\n                # Always use logits for multiclass metrics\n                preds = outputs\n                targets = targets.long()\n\n                # Ensure metrics are on the correct device\n                if stage == \"train\" and self.train_acc is not None:\n                    self.train_acc = self.train_acc.to(preds.device)\n                    acc = self.train_acc(preds, targets)\n                    self.log(\"train_acc\", acc, on_step=False, on_epoch=True)\n                elif stage == \"val\" and self.val_acc is not None:\n                    self.val_acc = self.val_acc.to(preds.device)\n                    acc = self.val_acc(preds, targets)\n                    self.log(\n                        \"val_acc\", acc, on_step=False, on_epoch=True, prog_bar=True\n                    )\n                elif stage == \"test\" and self.test_acc is not None:\n                    self.test_acc = self.test_acc.to(preds.device)\n                    acc = self.test_acc(preds, targets)\n                    self.log(\"test_acc\", acc, on_step=False, on_epoch=True)\n\n        except Exception as e:\n            logger.warning(f\"Metric logging failed for stage {stage}: {e}\")\n            # Continue execution - metric logging failure shouldn't crash training\n\n    def training_step(self, batch: Any, batch_idx: int) -&gt; torch.Tensor:\n        \"\"\"\n        Training step with automatic optimization.\n\n        Updated for 2025 best practices with proper PyTorch Geometric handling.\n        \"\"\"\n        try:\n            # Compute step\n            result = self._compute_step(batch, \"train\")\n            loss = result[\"loss\"]\n\n            # Update EMA if enabled\n            if self.use_ema:\n                self._update_ema()\n\n            return loss\n\n        except Exception as e:\n            logger.error(f\"Training step {batch_idx} failed: {e}\")\n            logger.error(f\"   Batch type: {type(batch)}\")\n            if hasattr(batch, \"x\"):\n                logger.error(f\"   Batch x shape: {batch.x.shape}\")\n            raise  # Re-raise to fail fast and fix the underlying issue\n\n    def validation_step(self, batch: Any, batch_idx: int) -&gt; torch.Tensor:\n        \"\"\"Validation step - fails fast on errors.\"\"\"\n        try:\n            result = self._compute_step(batch, \"val\")\n            return result[\"loss\"]\n        except Exception as e:\n            logger.error(f\"\u274c Validation step {batch_idx} failed: {e}\")\n            logger.error(f\"   Batch type: {type(batch)}\")\n            if hasattr(batch, \"x\"):\n                logger.error(f\"   Batch x shape: {batch.x.shape}\")\n            raise  # Re-raise to fail fast - validation errors indicate serious problems\n\n    def test_step(self, batch: Any, batch_idx: int) -&gt; torch.Tensor:\n        \"\"\"Test step with comprehensive evaluation.\"\"\"\n        try:\n            result = self._compute_step(batch, \"test\")\n            return result[\"loss\"]\n        except Exception as e:\n            logger.error(f\"\u274c Test step {batch_idx} failed: {e}\")\n            logger.error(f\"   Batch type: {type(batch)}\")\n            if hasattr(batch, \"x\"):\n                logger.error(f\"   Batch x shape: {batch.x.shape}\")\n            raise\n\n    def configure_optimizers(self) -&gt; Dict[str, Any]:\n        \"\"\"\n        Configure optimizers and schedulers with 2025 best practices.\n\n        Includes:\n        - AdamW with decoupled weight decay\n        - Multiple scheduler options\n        - Warmup support\n        - Gradient accumulation awareness\n        \"\"\"\n        # Get model parameters robustly (works with torch.compile)\n        try:\n            # Try to get parameters from the model directly\n            if hasattr(self.model, \"parameters\") and callable(self.model.parameters):\n                model_params = list(self.model.parameters())\n            else:\n                # Fallback to Lightning module parameters\n                model_params = list(self.parameters())\n        except Exception:\n            # Ultimate fallback\n            model_params = list(self.parameters())\n\n        if not model_params:\n            raise ValueError(\n                f\"No parameters found in model. Model type: {type(self.model)}\"\n            )\n\n        trainable_params = [p for p in model_params if p.requires_grad]\n        if not trainable_params:\n            raise ValueError(\n                \"No trainable parameters found - all parameters are frozen\"\n            )\n\n        logger.info(f\"\ud83c\udfaf Found {len(trainable_params)} trainable parameter groups\")\n\n        # Configure optimizer\n        optimizer = AdamW(\n            trainable_params,\n            lr=self.learning_rate,\n            weight_decay=self.weight_decay,\n            betas=(0.9, 0.999),\n            eps=1e-8,\n        )\n\n        # Configure scheduler\n        config: Dict[str, Any] = {\"optimizer\": optimizer}\n\n        if self.scheduler_type == \"cosine\":\n            # Safe trainer access - use default if trainer not attached yet\n            max_epochs = 100\n            try:\n                if (\n                    self.trainer\n                    and hasattr(self.trainer, \"max_epochs\")\n                    and self.trainer.max_epochs\n                ):\n                    max_epochs = self.trainer.max_epochs\n            except (RuntimeError, AttributeError):\n                # Trainer not attached yet - use default\n                pass\n\n            scheduler = CosineAnnealingLR(optimizer, T_max=max_epochs, eta_min=1e-6)\n            config[\"lr_scheduler\"] = {\n                \"scheduler\": scheduler,\n                \"interval\": \"epoch\",\n                \"frequency\": 1,\n            }\n        elif self.scheduler_type == \"onecycle\":\n            # Safe trainer access for stepping batches\n            total_steps = 1000\n            try:\n                if (\n                    self.trainer\n                    and hasattr(self.trainer, \"estimated_stepping_batches\")\n                    and self.trainer.estimated_stepping_batches\n                ):\n                    total_steps = int(self.trainer.estimated_stepping_batches)\n            except (RuntimeError, AttributeError):\n                # Trainer not attached yet - use default\n                pass\n\n            scheduler = OneCycleLR(\n                optimizer,\n                max_lr=self.learning_rate,\n                total_steps=total_steps,\n                pct_start=0.3,\n                anneal_strategy=\"cos\",\n            )\n            config[\"lr_scheduler\"] = {\n                \"scheduler\": scheduler,\n                \"interval\": \"step\",\n                \"frequency\": 1,\n            }\n        elif self.scheduler_type == \"plateau\":\n            scheduler = ReduceLROnPlateau(\n                optimizer, mode=\"min\", factor=0.5, patience=10, min_lr=1e-6\n            )\n            config[\"lr_scheduler\"] = {\n                \"scheduler\": scheduler,\n                \"monitor\": \"val_loss\",\n                \"interval\": \"epoch\",\n                \"frequency\": 1,\n            }\n        elif self.scheduler_type == \"cosine_warm_restarts\":\n            scheduler = CosineAnnealingWarmRestarts(\n                optimizer, T_0=10, T_mult=2, eta_min=1e-6\n            )\n            config[\"lr_scheduler\"] = {\n                \"scheduler\": scheduler,\n                \"interval\": \"epoch\",\n                \"frequency\": 1,\n            }\n\n        return config\n\n    def predict_step(self, batch: Any, batch_idx: int) -&gt; torch.Tensor:\n        \"\"\"Prediction step for inference.\"\"\"\n        try:\n            outputs = self.forward(batch)\n\n            # Apply softmax for classification\n            if self.task_type == \"classification\":\n                outputs = F.softmax(outputs, dim=-1)\n\n            return outputs\n\n        except Exception as e:\n            logger.error(f\"Prediction step failed: {e}\")\n            raise\n\n    def on_train_start(self) -&gt; None:\n        \"\"\"Called at the start of training.\"\"\"\n        try:\n            # Get datamodule info if available\n            if (\n                hasattr(self.trainer, \"datamodule\")\n                and self.trainer.datamodule is not None  # type: ignore\n            ):\n                dm = self.trainer.datamodule  # type: ignore\n                if (\n                    hasattr(dm, \"num_classes\")\n                    and dm.num_classes\n                    and not self.metrics_initialized\n                ):\n                    self.num_classes = dm.num_classes\n                    self._create_metrics_for_classes(dm.num_classes)\n\n            # Log model info\n            total_params = sum(p.numel() for p in self.parameters())\n            trainable_params = sum(\n                p.numel() for p in self.parameters() if p.requires_grad\n            )\n\n            logger.info(\"\ud83d\ude80 Training started\")\n            logger.info(f\"\ud83d\udcca Total parameters: {total_params:,}\")\n            logger.info(f\"\ud83c\udfaf Trainable parameters: {trainable_params:,}\")\n            logger.info(f\"\ud83c\udfa8 Task type: {self.task_type}\")\n            logger.info(f\"\ud83d\udd22 Number of classes: {self.num_classes}\")\n\n        except Exception as e:\n            logger.error(f\"Error in on_train_start: {e}\")\n\n    def on_train_end(self) -&gt; None:\n        \"\"\"Called at the end of training.\"\"\"\n        logger.info(\"\ud83c\udfc1 Training completed\")\n\n    def on_validation_start(self) -&gt; None:\n        \"\"\"Called at the start of validation.\"\"\"\n        pass\n\n    def on_test_start(self) -&gt; None:\n        \"\"\"Called at the start of testing.\"\"\"\n        pass\n</code></pre>"},{"location":"api/astro_lab.training/#astro_lab.training.AstroLightningModule.model","title":"model  <code>property</code> <code>writable</code>","text":"<pre><code>model: Module\n</code></pre> <p>Lazy model getter - creates model if not initialized.</p>"},{"location":"api/astro_lab.training/#astro_lab.training.AstroLightningModule.configure_optimizers","title":"configure_optimizers","text":"<pre><code>configure_optimizers() -&gt; Dict[str, Any]\n</code></pre> <p>Configure optimizers and schedulers with 2025 best practices.</p> <p>Includes: - AdamW with decoupled weight decay - Multiple scheduler options - Warmup support - Gradient accumulation awareness</p> Source code in <code>src\\astro_lab\\training\\lightning_module.py</code> <pre><code>def configure_optimizers(self) -&gt; Dict[str, Any]:\n    \"\"\"\n    Configure optimizers and schedulers with 2025 best practices.\n\n    Includes:\n    - AdamW with decoupled weight decay\n    - Multiple scheduler options\n    - Warmup support\n    - Gradient accumulation awareness\n    \"\"\"\n    # Get model parameters robustly (works with torch.compile)\n    try:\n        # Try to get parameters from the model directly\n        if hasattr(self.model, \"parameters\") and callable(self.model.parameters):\n            model_params = list(self.model.parameters())\n        else:\n            # Fallback to Lightning module parameters\n            model_params = list(self.parameters())\n    except Exception:\n        # Ultimate fallback\n        model_params = list(self.parameters())\n\n    if not model_params:\n        raise ValueError(\n            f\"No parameters found in model. Model type: {type(self.model)}\"\n        )\n\n    trainable_params = [p for p in model_params if p.requires_grad]\n    if not trainable_params:\n        raise ValueError(\n            \"No trainable parameters found - all parameters are frozen\"\n        )\n\n    logger.info(f\"\ud83c\udfaf Found {len(trainable_params)} trainable parameter groups\")\n\n    # Configure optimizer\n    optimizer = AdamW(\n        trainable_params,\n        lr=self.learning_rate,\n        weight_decay=self.weight_decay,\n        betas=(0.9, 0.999),\n        eps=1e-8,\n    )\n\n    # Configure scheduler\n    config: Dict[str, Any] = {\"optimizer\": optimizer}\n\n    if self.scheduler_type == \"cosine\":\n        # Safe trainer access - use default if trainer not attached yet\n        max_epochs = 100\n        try:\n            if (\n                self.trainer\n                and hasattr(self.trainer, \"max_epochs\")\n                and self.trainer.max_epochs\n            ):\n                max_epochs = self.trainer.max_epochs\n        except (RuntimeError, AttributeError):\n            # Trainer not attached yet - use default\n            pass\n\n        scheduler = CosineAnnealingLR(optimizer, T_max=max_epochs, eta_min=1e-6)\n        config[\"lr_scheduler\"] = {\n            \"scheduler\": scheduler,\n            \"interval\": \"epoch\",\n            \"frequency\": 1,\n        }\n    elif self.scheduler_type == \"onecycle\":\n        # Safe trainer access for stepping batches\n        total_steps = 1000\n        try:\n            if (\n                self.trainer\n                and hasattr(self.trainer, \"estimated_stepping_batches\")\n                and self.trainer.estimated_stepping_batches\n            ):\n                total_steps = int(self.trainer.estimated_stepping_batches)\n        except (RuntimeError, AttributeError):\n            # Trainer not attached yet - use default\n            pass\n\n        scheduler = OneCycleLR(\n            optimizer,\n            max_lr=self.learning_rate,\n            total_steps=total_steps,\n            pct_start=0.3,\n            anneal_strategy=\"cos\",\n        )\n        config[\"lr_scheduler\"] = {\n            \"scheduler\": scheduler,\n            \"interval\": \"step\",\n            \"frequency\": 1,\n        }\n    elif self.scheduler_type == \"plateau\":\n        scheduler = ReduceLROnPlateau(\n            optimizer, mode=\"min\", factor=0.5, patience=10, min_lr=1e-6\n        )\n        config[\"lr_scheduler\"] = {\n            \"scheduler\": scheduler,\n            \"monitor\": \"val_loss\",\n            \"interval\": \"epoch\",\n            \"frequency\": 1,\n        }\n    elif self.scheduler_type == \"cosine_warm_restarts\":\n        scheduler = CosineAnnealingWarmRestarts(\n            optimizer, T_0=10, T_mult=2, eta_min=1e-6\n        )\n        config[\"lr_scheduler\"] = {\n            \"scheduler\": scheduler,\n            \"interval\": \"epoch\",\n            \"frequency\": 1,\n        }\n\n    return config\n</code></pre>"},{"location":"api/astro_lab.training/#astro_lab.training.AstroLightningModule.forward","title":"forward","text":"<pre><code>forward(batch: Union[Tensor, Dict[str, Tensor], Any]) -&gt; Tensor\n</code></pre> <p>Forward pass - handles different input formats.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>Union[Tensor, Dict[str, Tensor], Any]</code> <p>Input batch (can be PyTorch Geometric Data, tensor, or dict)</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Model output tensor</p> Source code in <code>src\\astro_lab\\training\\lightning_module.py</code> <pre><code>def forward(\n    self, batch: Union[torch.Tensor, Dict[str, torch.Tensor], Any]\n) -&gt; torch.Tensor:\n    \"\"\"\n    Forward pass - handles different input formats.\n\n    Args:\n        batch: Input batch (can be PyTorch Geometric Data, tensor, or dict)\n\n    Returns:\n        Model output tensor\n    \"\"\"\n    try:\n        return self.model(batch)\n    except TypeError as e:\n        # Fallback: call model.forward directly (for torch.compile/ScriptModule)\n        if hasattr(self.model, \"forward\"):\n            return self.model.forward(batch)\n        else:\n            logger.error(f\"Forward pass failed - no forward method: {e}\")\n            logger.error(f\"Batch type: {type(batch)}\")\n            raise e\n    except Exception as e:\n        logger.error(f\"Forward pass failed: {e}\")\n        logger.error(f\"Batch type: {type(batch)}\")\n        raise\n</code></pre>"},{"location":"api/astro_lab.training/#astro_lab.training.AstroLightningModule.on_test_start","title":"on_test_start","text":"<pre><code>on_test_start() -&gt; None\n</code></pre> <p>Called at the start of testing.</p> Source code in <code>src\\astro_lab\\training\\lightning_module.py</code> <pre><code>def on_test_start(self) -&gt; None:\n    \"\"\"Called at the start of testing.\"\"\"\n    pass\n</code></pre>"},{"location":"api/astro_lab.training/#astro_lab.training.AstroLightningModule.on_train_end","title":"on_train_end","text":"<pre><code>on_train_end() -&gt; None\n</code></pre> <p>Called at the end of training.</p> Source code in <code>src\\astro_lab\\training\\lightning_module.py</code> <pre><code>def on_train_end(self) -&gt; None:\n    \"\"\"Called at the end of training.\"\"\"\n    logger.info(\"\ud83c\udfc1 Training completed\")\n</code></pre>"},{"location":"api/astro_lab.training/#astro_lab.training.AstroLightningModule.on_train_start","title":"on_train_start","text":"<pre><code>on_train_start() -&gt; None\n</code></pre> <p>Called at the start of training.</p> Source code in <code>src\\astro_lab\\training\\lightning_module.py</code> <pre><code>def on_train_start(self) -&gt; None:\n    \"\"\"Called at the start of training.\"\"\"\n    try:\n        # Get datamodule info if available\n        if (\n            hasattr(self.trainer, \"datamodule\")\n            and self.trainer.datamodule is not None  # type: ignore\n        ):\n            dm = self.trainer.datamodule  # type: ignore\n            if (\n                hasattr(dm, \"num_classes\")\n                and dm.num_classes\n                and not self.metrics_initialized\n            ):\n                self.num_classes = dm.num_classes\n                self._create_metrics_for_classes(dm.num_classes)\n\n        # Log model info\n        total_params = sum(p.numel() for p in self.parameters())\n        trainable_params = sum(\n            p.numel() for p in self.parameters() if p.requires_grad\n        )\n\n        logger.info(\"\ud83d\ude80 Training started\")\n        logger.info(f\"\ud83d\udcca Total parameters: {total_params:,}\")\n        logger.info(f\"\ud83c\udfaf Trainable parameters: {trainable_params:,}\")\n        logger.info(f\"\ud83c\udfa8 Task type: {self.task_type}\")\n        logger.info(f\"\ud83d\udd22 Number of classes: {self.num_classes}\")\n\n    except Exception as e:\n        logger.error(f\"Error in on_train_start: {e}\")\n</code></pre>"},{"location":"api/astro_lab.training/#astro_lab.training.AstroLightningModule.on_validation_start","title":"on_validation_start","text":"<pre><code>on_validation_start() -&gt; None\n</code></pre> <p>Called at the start of validation.</p> Source code in <code>src\\astro_lab\\training\\lightning_module.py</code> <pre><code>def on_validation_start(self) -&gt; None:\n    \"\"\"Called at the start of validation.\"\"\"\n    pass\n</code></pre>"},{"location":"api/astro_lab.training/#astro_lab.training.AstroLightningModule.predict_step","title":"predict_step","text":"<pre><code>predict_step(batch: Any, batch_idx: int) -&gt; Tensor\n</code></pre> <p>Prediction step for inference.</p> Source code in <code>src\\astro_lab\\training\\lightning_module.py</code> <pre><code>def predict_step(self, batch: Any, batch_idx: int) -&gt; torch.Tensor:\n    \"\"\"Prediction step for inference.\"\"\"\n    try:\n        outputs = self.forward(batch)\n\n        # Apply softmax for classification\n        if self.task_type == \"classification\":\n            outputs = F.softmax(outputs, dim=-1)\n\n        return outputs\n\n    except Exception as e:\n        logger.error(f\"Prediction step failed: {e}\")\n        raise\n</code></pre>"},{"location":"api/astro_lab.training/#astro_lab.training.AstroLightningModule.test_step","title":"test_step","text":"<pre><code>test_step(batch: Any, batch_idx: int) -&gt; Tensor\n</code></pre> <p>Test step with comprehensive evaluation.</p> Source code in <code>src\\astro_lab\\training\\lightning_module.py</code> <pre><code>def test_step(self, batch: Any, batch_idx: int) -&gt; torch.Tensor:\n    \"\"\"Test step with comprehensive evaluation.\"\"\"\n    try:\n        result = self._compute_step(batch, \"test\")\n        return result[\"loss\"]\n    except Exception as e:\n        logger.error(f\"\u274c Test step {batch_idx} failed: {e}\")\n        logger.error(f\"   Batch type: {type(batch)}\")\n        if hasattr(batch, \"x\"):\n            logger.error(f\"   Batch x shape: {batch.x.shape}\")\n        raise\n</code></pre>"},{"location":"api/astro_lab.training/#astro_lab.training.AstroLightningModule.training_step","title":"training_step","text":"<pre><code>training_step(batch: Any, batch_idx: int) -&gt; Tensor\n</code></pre> <p>Training step with automatic optimization.</p> <p>Updated for 2025 best practices with proper PyTorch Geometric handling.</p> Source code in <code>src\\astro_lab\\training\\lightning_module.py</code> <pre><code>def training_step(self, batch: Any, batch_idx: int) -&gt; torch.Tensor:\n    \"\"\"\n    Training step with automatic optimization.\n\n    Updated for 2025 best practices with proper PyTorch Geometric handling.\n    \"\"\"\n    try:\n        # Compute step\n        result = self._compute_step(batch, \"train\")\n        loss = result[\"loss\"]\n\n        # Update EMA if enabled\n        if self.use_ema:\n            self._update_ema()\n\n        return loss\n\n    except Exception as e:\n        logger.error(f\"Training step {batch_idx} failed: {e}\")\n        logger.error(f\"   Batch type: {type(batch)}\")\n        if hasattr(batch, \"x\"):\n            logger.error(f\"   Batch x shape: {batch.x.shape}\")\n        raise  # Re-raise to fail fast and fix the underlying issue\n</code></pre>"},{"location":"api/astro_lab.training/#astro_lab.training.AstroLightningModule.validation_step","title":"validation_step","text":"<pre><code>validation_step(batch: Any, batch_idx: int) -&gt; Tensor\n</code></pre> <p>Validation step - fails fast on errors.</p> Source code in <code>src\\astro_lab\\training\\lightning_module.py</code> <pre><code>def validation_step(self, batch: Any, batch_idx: int) -&gt; torch.Tensor:\n    \"\"\"Validation step - fails fast on errors.\"\"\"\n    try:\n        result = self._compute_step(batch, \"val\")\n        return result[\"loss\"]\n    except Exception as e:\n        logger.error(f\"\u274c Validation step {batch_idx} failed: {e}\")\n        logger.error(f\"   Batch type: {type(batch)}\")\n        if hasattr(batch, \"x\"):\n            logger.error(f\"   Batch x shape: {batch.x.shape}\")\n        raise  # Re-raise to fail fast - validation errors indicate serious problems\n</code></pre>"},{"location":"api/astro_lab.training/#astro_lab.training.AstroMLflowLogger","title":"AstroMLflowLogger","text":"<p>               Bases: <code>MLFlowLogger</code></p> <p>Optimized MLflow logger for astronomical models with 2025 system metrics integration.</p> <p>Methods:</p> Name Description <code>end_run</code> <p>End MLflow run with cleanup.</p> <code>log_checkpoint_info</code> <p>Log checkpoint directory information.</p> <code>log_config_info</code> <p>Log configuration information from the new config system.</p> <code>log_dataset_info</code> <p>Log dataset information efficiently.</p> <code>log_final_model</code> <p>Log final trained model with metadata.</p> <code>log_hyperparameters</code> <p>Log hyperparameters with structured organization.</p> <code>log_model_architecture</code> <p>Log model architecture with optimized metrics.</p> <code>log_predictions</code> <p>Log prediction results with description.</p> <code>log_survey_info</code> <p>Log astronomical survey information with data config integration.</p> <code>log_training_environment</code> <p>Log comprehensive training environment information.</p> <code>log_visualization</code> <p>Log visualization plots.</p> <code>start_system_metrics_logging</code> <p>Start automatic system metrics logging in background thread.</p> <code>stop_system_metrics_logging</code> <p>Stop system metrics logging.</p> Source code in <code>src\\astro_lab\\training\\mlflow_logger.py</code> <pre><code>class AstroMLflowLogger(MLFlowLogger):\n    \"\"\"Optimized MLflow logger for astronomical models with 2025 system metrics integration.\"\"\"\n\n    def __init__(\n        self,\n        experiment_name: str = \"astro_experiments\",\n        tracking_uri: Optional[str] = None,\n        tags: Optional[Dict[str, str]] = None,\n        artifact_location: Optional[str] = None,\n        run_name: Optional[str] = None,\n        enable_system_metrics: bool = True,\n        system_metrics_interval: int = 30,  # seconds\n        **kwargs,\n    ):\n        # Set up MLflow tracking URI and artifact location\n        if tracking_uri is None:\n            # Use data_config system for organized MLflow storage\n            data_config.ensure_experiment_directories(experiment_name)\n            exp_paths = data_config.get_experiment_paths(experiment_name)\n            tracking_uri = f\"file://{exp_paths['mlruns'].absolute()}\"\n\n        if artifact_location is None:\n            # Use data_config system for organized artifact storage\n            data_config.ensure_experiment_directories(experiment_name)\n            exp_paths = data_config.get_experiment_paths(experiment_name)\n            artifact_location = f\"file://{exp_paths['artifacts'].absolute()}\"\n\n        # Set environment variable for consistency\n        os.environ[\"MLFLOW_TRACKING_URI\"] = tracking_uri\n\n        # Setup MLflow experiment first\n        setup_mlflow_experiment(experiment_name, tracking_uri, artifact_location)\n\n        # Combine user tags with astro tags\n        astro_tags = {\n            \"framework\": \"astro-lab\",\n            \"domain\": \"astronomy\",\n            \"version\": \"0.3.0\",\n            \"data_config_version\": \"2.0\",  # Track config system version\n            \"tracking_uri\": tracking_uri,\n            \"artifact_location\": artifact_location,\n            \"system_metrics_enabled\": str(enable_system_metrics),\n        }\n        if tags:\n            astro_tags.update(tags)\n\n        super().__init__(\n            experiment_name=experiment_name,\n            tracking_uri=tracking_uri,\n            tags=astro_tags,  # Pass tags to parent\n            artifact_location=artifact_location,\n            run_name=run_name,\n            **kwargs,\n        )\n\n        # System metrics monitoring setup\n        self.enable_system_metrics = enable_system_metrics\n        self.system_metrics_interval = system_metrics_interval\n        self._system_metrics_thread = None\n        self._stop_system_metrics = threading.Event()\n\n        # Store paths for later use\n        self.experiment_name = experiment_name\n        self.tracking_uri = tracking_uri\n\n    def start_system_metrics_logging(self) -&gt; None:\n        \"\"\"Start automatic system metrics logging in background thread.\"\"\"\n        if not self.enable_system_metrics:\n            return\n\n        if self._system_metrics_thread and self._system_metrics_thread.is_alive():\n            return  # Already running\n\n        self._stop_system_metrics.clear()\n        self._system_metrics_thread = threading.Thread(\n            target=self._log_system_metrics_loop, daemon=True\n        )\n        self._system_metrics_thread.start()\n        print(\n            f\"\ud83d\udda5\ufe0f  Started system metrics logging (interval: {self.system_metrics_interval}s)\"\n        )\n\n    def stop_system_metrics_logging(self) -&gt; None:\n        \"\"\"Stop system metrics logging.\"\"\"\n        if self._system_metrics_thread and self._system_metrics_thread.is_alive():\n            self._stop_system_metrics.set()\n            self._system_metrics_thread.join(timeout=5)\n            print(\"\ud83d\udda5\ufe0f  Stopped system metrics logging\")\n\n    def _log_system_metrics_loop(self) -&gt; None:\n        \"\"\"Background loop for logging system metrics.\"\"\"\n        while not self._stop_system_metrics.wait(self.system_metrics_interval):\n            try:\n                self._log_system_metrics_snapshot()\n            except Exception as e:\n                print(f\"Warning: Failed to log system metrics: {e}\")\n\n    def _log_system_metrics_snapshot(self) -&gt; None:\n        \"\"\"Log a single snapshot of system metrics using 2025 best practices.\"\"\"\n        try:\n            timestamp = time.time()\n\n            # CPU Metrics (using slash notation for grouping)\n            cpu_percent = psutil.cpu_percent(interval=1)\n            mlflow.log_metric(\n                \"system/cpu/utilization_percent\",\n                float(cpu_percent),\n                step=int(timestamp),\n            )\n\n            # Memory Metrics\n            memory = psutil.virtual_memory()\n            mlflow.log_metric(\n                \"system/memory/used_gb\",\n                float(memory.used / (1024**3)),\n                step=int(timestamp),\n            )\n            mlflow.log_metric(\n                \"system/memory/available_gb\",\n                float(memory.available / (1024**3)),\n                step=int(timestamp),\n            )\n            mlflow.log_metric(\n                \"system/memory/utilization_percent\",\n                float(memory.percent),\n                step=int(timestamp),\n            )\n\n            # Disk Metrics\n            disk = psutil.disk_usage(\"/\")\n            mlflow.log_metric(\n                \"system/disk/used_gb\", float(disk.used / (1024**3)), step=int(timestamp)\n            )\n            mlflow.log_metric(\n                \"system/disk/free_gb\", float(disk.free / (1024**3)), step=int(timestamp)\n            )\n            mlflow.log_metric(\n                \"system/disk/utilization_percent\",\n                float((disk.used / disk.total) * 100),\n                step=int(timestamp),\n            )\n\n            # Network I/O\n            net_io = psutil.net_io_counters()\n            if (\n                net_io\n                and hasattr(net_io, \"bytes_sent\")\n                and hasattr(net_io, \"bytes_recv\")\n            ):\n                mlflow.log_metric(\n                    \"system/network/bytes_sent_mb\",\n                    float(getattr(net_io, \"bytes_sent\", 0) / (1024**2)),\n                    step=int(timestamp),\n                )\n                mlflow.log_metric(\n                    \"system/network/bytes_recv_mb\",\n                    float(getattr(net_io, \"bytes_recv\", 0) / (1024**2)),\n                    step=int(timestamp),\n                )\n\n            # GPU Metrics (if available)\n            if torch.cuda.is_available():\n                try:\n                    # PyTorch GPU metrics\n                    for i in range(torch.cuda.device_count()):\n                        # Memory usage\n                        memory_allocated = torch.cuda.memory_allocated(i) / (\n                            1024**3\n                        )  # GB\n                        memory_reserved = torch.cuda.memory_reserved(i) / (\n                            1024**3\n                        )  # GB\n                        max_memory = torch.cuda.max_memory_allocated(i) / (\n                            1024**3\n                        )  # GB\n\n                        mlflow.log_metric(\n                            f\"system/gpu_{i}/memory_allocated_gb\",\n                            memory_allocated,\n                            step=int(timestamp),\n                        )\n                        mlflow.log_metric(\n                            f\"system/gpu_{i}/memory_reserved_gb\",\n                            memory_reserved,\n                            step=int(timestamp),\n                        )\n                        mlflow.log_metric(\n                            f\"system/gpu_{i}/max_memory_gb\",\n                            max_memory,\n                            step=int(timestamp),\n                        )\n\n                        # GPU utilization (if GPUtil available)\n                        if \"GPUtil\" in globals():\n                            gpus = GPUtil.getGPUs()\n                            if i &lt; len(gpus):\n                                gpu = gpus[i]\n                                mlflow.log_metric(\n                                    f\"system/gpu_{i}/utilization_percent\",\n                                    gpu.load * 100,\n                                    step=int(timestamp),\n                                )\n                                mlflow.log_metric(\n                                    f\"system/gpu_{i}/temperature_c\",\n                                    gpu.temperature,\n                                    step=int(timestamp),\n                                )\n                                mlflow.log_metric(\n                                    f\"system/gpu_{i}/memory_utilization_percent\",\n                                    gpu.memoryUtil * 100,\n                                    step=int(timestamp),\n                                )\n                except Exception as gpu_error:\n                    print(f\"Warning: GPU metrics logging failed: {gpu_error}\")\n\n        except Exception as e:\n            print(f\"Warning: System metrics snapshot failed: {e}\")\n\n    def log_training_environment(self) -&gt; None:\n        \"\"\"Log comprehensive training environment information.\"\"\"\n        try:\n            env_info = {\n                \"python_version\": f\"{psutil.sys.version_info.major}.{psutil.sys.version_info.minor}.{psutil.sys.version_info.micro}\",\n                \"cpu_count\": psutil.cpu_count(),\n                \"total_memory_gb\": round(psutil.virtual_memory().total / (1024**3), 2),\n                \"platform\": psutil.os.name,\n            }\n\n            # GPU Information\n            if torch.cuda.is_available():\n                env_info.update(\n                    {\n                        \"cuda_available\": True,\n                        \"cuda_version\": \"unknown\",\n                        \"gpu_count\": torch.cuda.device_count(),\n                        \"gpu_names\": [\n                            torch.cuda.get_device_name(i)\n                            for i in range(torch.cuda.device_count())\n                        ],\n                    }\n                )\n            else:\n                env_info[\"cuda_available\"] = False\n\n            # Log as parameters\n            for key, value in env_info.items():\n                if isinstance(value, list):\n                    mlflow.log_param(f\"env_{key}\", \", \".join(map(str, value)))\n                else:\n                    mlflow.log_param(f\"env_{key}\", value)\n\n            # Save detailed environment info\n            env_file = \"training_environment.json\"\n            with open(env_file, \"w\") as f:\n                json.dump(env_info, f, indent=2, default=str)\n\n            mlflow.log_artifact(env_file, \"environment\")\n            Path(env_file).unlink()  # Cleanup\n\n        except Exception as e:\n            print(f\"Warning: Could not log training environment: {e}\")\n\n    def log_model_architecture(self, model: torch.nn.Module) -&gt; None:\n        \"\"\"Log model architecture with optimized metrics.\"\"\"\n        try:\n            # Calculate model statistics\n            total_params = sum(p.numel() for p in model.parameters())\n            trainable_params = sum(\n                p.numel() for p in model.parameters() if p.requires_grad\n            )\n            model_size_mb = total_params * 4 / (1024 * 1024)  # Assuming float32\n\n            architecture_info = {\n                \"model_class\": model.__class__.__name__,\n                \"total_parameters\": total_params,\n                \"trainable_parameters\": trainable_params,\n                \"model_size_mb\": round(model_size_mb, 2),\n                \"num_layers\": len(list(model.named_modules())),\n            }\n\n            # Log as metrics for easy comparison\n            for key, value in architecture_info.items():\n                if isinstance(value, (int, float)):\n                    mlflow.log_metric(f\"model_{key}\", value)\n                else:\n                    mlflow.log_param(f\"model_{key}\", str(value))\n\n            # Save detailed architecture\n            arch_file = \"model_architecture.json\"\n            with open(arch_file, \"w\") as f:\n                json.dump(architecture_info, f, indent=2)\n\n            mlflow.log_artifact(arch_file)\n            Path(arch_file).unlink()  # Cleanup\n\n        except Exception as e:\n            print(f\"Warning: Could not log model architecture: {e}\")\n\n    def log_hyperparameters(self, params: Dict[str, Any]) -&gt; None:\n        \"\"\"Log hyperparameters with structured organization.\"\"\"\n        # Organize parameters by category\n        categories = {\n            \"model\": [\"hidden_dim\", \"num_layers\", \"dropout\", \"num_classes\"],\n            \"training\": [\"learning_rate\", \"weight_decay\", \"scheduler\", \"batch_size\"],\n            \"optimization\": [\n                \"precision\",\n                \"gradient_clip_val\",\n                \"accumulate_grad_batches\",\n            ],\n            \"hardware\": [\"accelerator\", \"devices\", \"enable_swa\"],\n            \"data\": [\"survey\", \"k_neighbors\", \"distance_threshold\", \"num_features\"],\n        }\n\n        # Log categorized parameters\n        for category, param_names in categories.items():\n            for param_name in param_names:\n                if param_name in params:\n                    mlflow.log_param(f\"{category}_{param_name}\", params[param_name])\n\n        # Log remaining parameters\n        logged_params = {\n            param for param_list in categories.values() for param in param_list\n        }\n        for key, value in params.items():\n            if key not in logged_params:\n                mlflow.log_param(key, value)\n\n    def log_config_info(self, config: Dict[str, Any]) -&gt; None:\n        \"\"\"Log configuration information from the new config system.\"\"\"\n        try:\n            # Log data paths\n            mlflow.log_param(\"data_base_dir\", str(data_config.base_dir))\n            mlflow.log_param(\"mlruns_dir\", str(data_config.mlruns_dir))\n            mlflow.log_param(\"checkpoints_dir\", str(data_config.checkpoints_dir))\n\n            # Log config sections\n            if \"training\" in config:\n                training_config = config[\"training\"]\n                mlflow.log_param(\"max_epochs\", training_config.get(\"max_epochs\"))\n                mlflow.log_param(\"batch_size\", training_config.get(\"batch_size\"))\n                mlflow.log_param(\"learning_rate\", training_config.get(\"learning_rate\"))\n                mlflow.log_param(\"accelerator\", training_config.get(\"accelerator\"))\n                mlflow.log_param(\"precision\", training_config.get(\"precision\"))\n\n            if \"model\" in config:\n                model_config = config[\"model\"]\n                mlflow.log_param(\"model_type\", model_config.get(\"type\"))\n                mlflow.log_param(\"hidden_dim\", model_config.get(\"hidden_dim\"))\n                mlflow.log_param(\"num_layers\", model_config.get(\"num_layers\"))\n                mlflow.log_param(\"dropout\", model_config.get(\"dropout\"))\n\n            # Save full config as artifact\n            config_file = \"experiment_config.json\"\n            with open(config_file, \"w\") as f:\n                json.dump(config, f, indent=2, default=str)\n\n            mlflow.log_artifact(config_file, \"config\")\n            Path(config_file).unlink()  # Cleanup\n\n        except Exception as e:\n            print(f\"Warning: Could not log config info: {e}\")\n\n    def log_dataset_info(self, dataset_info: Dict[str, Any]) -&gt; None:\n        \"\"\"Log dataset information efficiently.\"\"\"\n        if not dataset_info:\n            return\n\n        # Log key metrics\n        metrics_to_log = [\"dataset_size\", \"num_features\", \"num_classes\", \"num_graphs\"]\n        for metric in metrics_to_log:\n            if metric in dataset_info:\n                value = dataset_info[metric]\n                if isinstance(value, (int, float)) and value &gt; 0:\n                    mlflow.log_metric(metric, value)\n\n        # Log dataset metadata\n        metadata_file = \"dataset_info.json\"\n        with open(metadata_file, \"w\") as f:\n            json.dump(dataset_info, f, indent=2, default=str)\n\n        mlflow.log_artifact(metadata_file, \"data\")\n        Path(metadata_file).unlink()\n\n    def log_survey_info(self, survey: str, bands: Optional[list] = None) -&gt; None:\n        \"\"\"Log astronomical survey information with data config integration.\"\"\"\n        mlflow.set_tag(\"survey\", survey)\n\n        # Log survey-specific paths\n        survey_raw_dir = data_config.get_survey_raw_dir(survey)\n        survey_processed_dir = data_config.get_survey_processed_dir(survey)\n\n        mlflow.log_param(\"survey_raw_dir\", str(survey_raw_dir))\n        mlflow.log_param(\"survey_processed_dir\", str(survey_processed_dir))\n\n        if bands:\n            mlflow.set_tag(\"bands\", \",\".join(map(str, bands)))\n            mlflow.log_metric(\"num_bands\", len(bands))\n\n    def log_final_model(\n        self, model: torch.nn.Module, model_name: str = \"astro_model\"\n    ) -&gt; Optional[Any]:\n        \"\"\"Log final trained model with metadata.\"\"\"\n        try:\n            # Log model with PyTorch Lightning integration\n            model_info = mlflow.pytorch.log_model(\n                pytorch_model=model,\n                artifact_path=model_name,\n                conda_env=self._get_conda_env(),\n                code_paths=[\"src/astro_lab/\"],  # Include source code\n            )\n\n            # Register model if in production mode\n            if os.getenv(\"MLFLOW_REGISTER_MODEL\", \"false\").lower() == \"true\":\n                model_uri = f\"runs:/{mlflow.active_run().info.run_id}/{model_name}\"\n                model_version = mlflow.register_model(\n                    model_uri=model_uri,\n                    name=f\"astrolab_{model.__class__.__name__.lower()}\",\n                    tags={\"domain\": \"astronomy\", \"framework\": \"astrolab\"},\n                )\n                mlflow.log_param(\"registered_model_version\", model_version.version)\n\n            return model_info\n\n        except Exception as e:\n            print(f\"Warning: Could not log model: {e}\")\n            return None\n\n    def _get_conda_env(self) -&gt; Dict[str, Any]:\n        \"\"\"Get conda environment for model deployment.\"\"\"\n        return {\n            \"channels\": [\"conda-forge\", \"pytorch\", \"pyg\"],\n            \"dependencies\": [\n                \"python=3.11\",\n                \"pytorch\",\n                \"torch-geometric\",\n                \"lightning\",\n                \"astropy\",\n                \"pyyaml\",  # For config loading\n                {\n                    \"pip\": [\n                        \"mlflow\",\n                        \"optuna\",\n                        \"astroquery\",\n                    ]\n                },\n            ],\n        }\n\n    def log_predictions(\n        self, predictions_file: str, description: str = \"Model predictions\"\n    ) -&gt; None:\n        \"\"\"Log prediction results with description.\"\"\"\n        if Path(predictions_file).exists():\n            mlflow.log_artifact(predictions_file, \"predictions\")\n            mlflow.set_tag(\"predictions_logged\", description)\n\n    def log_visualization(self, plot_path: str, plot_type: str = \"plot\") -&gt; None:\n        \"\"\"Log visualization plots.\"\"\"\n        if Path(plot_path).exists():\n            mlflow.log_artifact(plot_path, f\"plots/{plot_type}\")\n\n    def log_checkpoint_info(self, checkpoint_dir: Path) -&gt; None:\n        \"\"\"Log checkpoint directory information.\"\"\"\n        mlflow.log_param(\"checkpoint_dir\", str(checkpoint_dir))\n\n        # Log checkpoint files if they exist\n        if checkpoint_dir.exists():\n            checkpoints = list(checkpoint_dir.glob(\"*.ckpt\"))\n            mlflow.log_metric(\"num_checkpoints\", len(checkpoints))\n\n            if checkpoints:\n                # Log latest checkpoint info\n                latest_checkpoint = max(checkpoints, key=lambda x: x.stat().st_mtime)\n                mlflow.log_param(\"latest_checkpoint\", latest_checkpoint.name)\n                mlflow.log_metric(\n                    \"checkpoint_size_mb\",\n                    latest_checkpoint.stat().st_size / (1024 * 1024),\n                )\n\n    def end_run(self) -&gt; None:\n        \"\"\"End MLflow run with cleanup.\"\"\"\n        try:\n            # Stop system metrics logging\n            self.stop_system_metrics_logging()\n\n            # Log final run status\n            mlflow.set_tag(\"run_status\", \"completed\")\n            mlflow.set_tag(\"data_config_base_dir\", str(data_config.base_dir))\n            mlflow.end_run()\n        except Exception as e:\n            print(f\"Warning: Error ending MLflow run: {e}\")\n</code></pre>"},{"location":"api/astro_lab.training/#astro_lab.training.AstroMLflowLogger.end_run","title":"end_run","text":"<pre><code>end_run() -&gt; None\n</code></pre> <p>End MLflow run with cleanup.</p> Source code in <code>src\\astro_lab\\training\\mlflow_logger.py</code> <pre><code>def end_run(self) -&gt; None:\n    \"\"\"End MLflow run with cleanup.\"\"\"\n    try:\n        # Stop system metrics logging\n        self.stop_system_metrics_logging()\n\n        # Log final run status\n        mlflow.set_tag(\"run_status\", \"completed\")\n        mlflow.set_tag(\"data_config_base_dir\", str(data_config.base_dir))\n        mlflow.end_run()\n    except Exception as e:\n        print(f\"Warning: Error ending MLflow run: {e}\")\n</code></pre>"},{"location":"api/astro_lab.training/#astro_lab.training.AstroMLflowLogger.log_checkpoint_info","title":"log_checkpoint_info","text":"<pre><code>log_checkpoint_info(checkpoint_dir: Path) -&gt; None\n</code></pre> <p>Log checkpoint directory information.</p> Source code in <code>src\\astro_lab\\training\\mlflow_logger.py</code> <pre><code>def log_checkpoint_info(self, checkpoint_dir: Path) -&gt; None:\n    \"\"\"Log checkpoint directory information.\"\"\"\n    mlflow.log_param(\"checkpoint_dir\", str(checkpoint_dir))\n\n    # Log checkpoint files if they exist\n    if checkpoint_dir.exists():\n        checkpoints = list(checkpoint_dir.glob(\"*.ckpt\"))\n        mlflow.log_metric(\"num_checkpoints\", len(checkpoints))\n\n        if checkpoints:\n            # Log latest checkpoint info\n            latest_checkpoint = max(checkpoints, key=lambda x: x.stat().st_mtime)\n            mlflow.log_param(\"latest_checkpoint\", latest_checkpoint.name)\n            mlflow.log_metric(\n                \"checkpoint_size_mb\",\n                latest_checkpoint.stat().st_size / (1024 * 1024),\n            )\n</code></pre>"},{"location":"api/astro_lab.training/#astro_lab.training.AstroMLflowLogger.log_config_info","title":"log_config_info","text":"<pre><code>log_config_info(config: Dict[str, Any]) -&gt; None\n</code></pre> <p>Log configuration information from the new config system.</p> Source code in <code>src\\astro_lab\\training\\mlflow_logger.py</code> <pre><code>def log_config_info(self, config: Dict[str, Any]) -&gt; None:\n    \"\"\"Log configuration information from the new config system.\"\"\"\n    try:\n        # Log data paths\n        mlflow.log_param(\"data_base_dir\", str(data_config.base_dir))\n        mlflow.log_param(\"mlruns_dir\", str(data_config.mlruns_dir))\n        mlflow.log_param(\"checkpoints_dir\", str(data_config.checkpoints_dir))\n\n        # Log config sections\n        if \"training\" in config:\n            training_config = config[\"training\"]\n            mlflow.log_param(\"max_epochs\", training_config.get(\"max_epochs\"))\n            mlflow.log_param(\"batch_size\", training_config.get(\"batch_size\"))\n            mlflow.log_param(\"learning_rate\", training_config.get(\"learning_rate\"))\n            mlflow.log_param(\"accelerator\", training_config.get(\"accelerator\"))\n            mlflow.log_param(\"precision\", training_config.get(\"precision\"))\n\n        if \"model\" in config:\n            model_config = config[\"model\"]\n            mlflow.log_param(\"model_type\", model_config.get(\"type\"))\n            mlflow.log_param(\"hidden_dim\", model_config.get(\"hidden_dim\"))\n            mlflow.log_param(\"num_layers\", model_config.get(\"num_layers\"))\n            mlflow.log_param(\"dropout\", model_config.get(\"dropout\"))\n\n        # Save full config as artifact\n        config_file = \"experiment_config.json\"\n        with open(config_file, \"w\") as f:\n            json.dump(config, f, indent=2, default=str)\n\n        mlflow.log_artifact(config_file, \"config\")\n        Path(config_file).unlink()  # Cleanup\n\n    except Exception as e:\n        print(f\"Warning: Could not log config info: {e}\")\n</code></pre>"},{"location":"api/astro_lab.training/#astro_lab.training.AstroMLflowLogger.log_dataset_info","title":"log_dataset_info","text":"<pre><code>log_dataset_info(dataset_info: Dict[str, Any]) -&gt; None\n</code></pre> <p>Log dataset information efficiently.</p> Source code in <code>src\\astro_lab\\training\\mlflow_logger.py</code> <pre><code>def log_dataset_info(self, dataset_info: Dict[str, Any]) -&gt; None:\n    \"\"\"Log dataset information efficiently.\"\"\"\n    if not dataset_info:\n        return\n\n    # Log key metrics\n    metrics_to_log = [\"dataset_size\", \"num_features\", \"num_classes\", \"num_graphs\"]\n    for metric in metrics_to_log:\n        if metric in dataset_info:\n            value = dataset_info[metric]\n            if isinstance(value, (int, float)) and value &gt; 0:\n                mlflow.log_metric(metric, value)\n\n    # Log dataset metadata\n    metadata_file = \"dataset_info.json\"\n    with open(metadata_file, \"w\") as f:\n        json.dump(dataset_info, f, indent=2, default=str)\n\n    mlflow.log_artifact(metadata_file, \"data\")\n    Path(metadata_file).unlink()\n</code></pre>"},{"location":"api/astro_lab.training/#astro_lab.training.AstroMLflowLogger.log_final_model","title":"log_final_model","text":"<pre><code>log_final_model(model: Module, model_name: str = 'astro_model') -&gt; Optional[Any]\n</code></pre> <p>Log final trained model with metadata.</p> Source code in <code>src\\astro_lab\\training\\mlflow_logger.py</code> <pre><code>def log_final_model(\n    self, model: torch.nn.Module, model_name: str = \"astro_model\"\n) -&gt; Optional[Any]:\n    \"\"\"Log final trained model with metadata.\"\"\"\n    try:\n        # Log model with PyTorch Lightning integration\n        model_info = mlflow.pytorch.log_model(\n            pytorch_model=model,\n            artifact_path=model_name,\n            conda_env=self._get_conda_env(),\n            code_paths=[\"src/astro_lab/\"],  # Include source code\n        )\n\n        # Register model if in production mode\n        if os.getenv(\"MLFLOW_REGISTER_MODEL\", \"false\").lower() == \"true\":\n            model_uri = f\"runs:/{mlflow.active_run().info.run_id}/{model_name}\"\n            model_version = mlflow.register_model(\n                model_uri=model_uri,\n                name=f\"astrolab_{model.__class__.__name__.lower()}\",\n                tags={\"domain\": \"astronomy\", \"framework\": \"astrolab\"},\n            )\n            mlflow.log_param(\"registered_model_version\", model_version.version)\n\n        return model_info\n\n    except Exception as e:\n        print(f\"Warning: Could not log model: {e}\")\n        return None\n</code></pre>"},{"location":"api/astro_lab.training/#astro_lab.training.AstroMLflowLogger.log_hyperparameters","title":"log_hyperparameters","text":"<pre><code>log_hyperparameters(params: Dict[str, Any]) -&gt; None\n</code></pre> <p>Log hyperparameters with structured organization.</p> Source code in <code>src\\astro_lab\\training\\mlflow_logger.py</code> <pre><code>def log_hyperparameters(self, params: Dict[str, Any]) -&gt; None:\n    \"\"\"Log hyperparameters with structured organization.\"\"\"\n    # Organize parameters by category\n    categories = {\n        \"model\": [\"hidden_dim\", \"num_layers\", \"dropout\", \"num_classes\"],\n        \"training\": [\"learning_rate\", \"weight_decay\", \"scheduler\", \"batch_size\"],\n        \"optimization\": [\n            \"precision\",\n            \"gradient_clip_val\",\n            \"accumulate_grad_batches\",\n        ],\n        \"hardware\": [\"accelerator\", \"devices\", \"enable_swa\"],\n        \"data\": [\"survey\", \"k_neighbors\", \"distance_threshold\", \"num_features\"],\n    }\n\n    # Log categorized parameters\n    for category, param_names in categories.items():\n        for param_name in param_names:\n            if param_name in params:\n                mlflow.log_param(f\"{category}_{param_name}\", params[param_name])\n\n    # Log remaining parameters\n    logged_params = {\n        param for param_list in categories.values() for param in param_list\n    }\n    for key, value in params.items():\n        if key not in logged_params:\n            mlflow.log_param(key, value)\n</code></pre>"},{"location":"api/astro_lab.training/#astro_lab.training.AstroMLflowLogger.log_model_architecture","title":"log_model_architecture","text":"<pre><code>log_model_architecture(model: Module) -&gt; None\n</code></pre> <p>Log model architecture with optimized metrics.</p> Source code in <code>src\\astro_lab\\training\\mlflow_logger.py</code> <pre><code>def log_model_architecture(self, model: torch.nn.Module) -&gt; None:\n    \"\"\"Log model architecture with optimized metrics.\"\"\"\n    try:\n        # Calculate model statistics\n        total_params = sum(p.numel() for p in model.parameters())\n        trainable_params = sum(\n            p.numel() for p in model.parameters() if p.requires_grad\n        )\n        model_size_mb = total_params * 4 / (1024 * 1024)  # Assuming float32\n\n        architecture_info = {\n            \"model_class\": model.__class__.__name__,\n            \"total_parameters\": total_params,\n            \"trainable_parameters\": trainable_params,\n            \"model_size_mb\": round(model_size_mb, 2),\n            \"num_layers\": len(list(model.named_modules())),\n        }\n\n        # Log as metrics for easy comparison\n        for key, value in architecture_info.items():\n            if isinstance(value, (int, float)):\n                mlflow.log_metric(f\"model_{key}\", value)\n            else:\n                mlflow.log_param(f\"model_{key}\", str(value))\n\n        # Save detailed architecture\n        arch_file = \"model_architecture.json\"\n        with open(arch_file, \"w\") as f:\n            json.dump(architecture_info, f, indent=2)\n\n        mlflow.log_artifact(arch_file)\n        Path(arch_file).unlink()  # Cleanup\n\n    except Exception as e:\n        print(f\"Warning: Could not log model architecture: {e}\")\n</code></pre>"},{"location":"api/astro_lab.training/#astro_lab.training.AstroMLflowLogger.log_predictions","title":"log_predictions","text":"<pre><code>log_predictions(predictions_file: str, description: str = 'Model predictions') -&gt; None\n</code></pre> <p>Log prediction results with description.</p> Source code in <code>src\\astro_lab\\training\\mlflow_logger.py</code> <pre><code>def log_predictions(\n    self, predictions_file: str, description: str = \"Model predictions\"\n) -&gt; None:\n    \"\"\"Log prediction results with description.\"\"\"\n    if Path(predictions_file).exists():\n        mlflow.log_artifact(predictions_file, \"predictions\")\n        mlflow.set_tag(\"predictions_logged\", description)\n</code></pre>"},{"location":"api/astro_lab.training/#astro_lab.training.AstroMLflowLogger.log_survey_info","title":"log_survey_info","text":"<pre><code>log_survey_info(survey: str, bands: Optional[list] = None) -&gt; None\n</code></pre> <p>Log astronomical survey information with data config integration.</p> Source code in <code>src\\astro_lab\\training\\mlflow_logger.py</code> <pre><code>def log_survey_info(self, survey: str, bands: Optional[list] = None) -&gt; None:\n    \"\"\"Log astronomical survey information with data config integration.\"\"\"\n    mlflow.set_tag(\"survey\", survey)\n\n    # Log survey-specific paths\n    survey_raw_dir = data_config.get_survey_raw_dir(survey)\n    survey_processed_dir = data_config.get_survey_processed_dir(survey)\n\n    mlflow.log_param(\"survey_raw_dir\", str(survey_raw_dir))\n    mlflow.log_param(\"survey_processed_dir\", str(survey_processed_dir))\n\n    if bands:\n        mlflow.set_tag(\"bands\", \",\".join(map(str, bands)))\n        mlflow.log_metric(\"num_bands\", len(bands))\n</code></pre>"},{"location":"api/astro_lab.training/#astro_lab.training.AstroMLflowLogger.log_training_environment","title":"log_training_environment","text":"<pre><code>log_training_environment() -&gt; None\n</code></pre> <p>Log comprehensive training environment information.</p> Source code in <code>src\\astro_lab\\training\\mlflow_logger.py</code> <pre><code>def log_training_environment(self) -&gt; None:\n    \"\"\"Log comprehensive training environment information.\"\"\"\n    try:\n        env_info = {\n            \"python_version\": f\"{psutil.sys.version_info.major}.{psutil.sys.version_info.minor}.{psutil.sys.version_info.micro}\",\n            \"cpu_count\": psutil.cpu_count(),\n            \"total_memory_gb\": round(psutil.virtual_memory().total / (1024**3), 2),\n            \"platform\": psutil.os.name,\n        }\n\n        # GPU Information\n        if torch.cuda.is_available():\n            env_info.update(\n                {\n                    \"cuda_available\": True,\n                    \"cuda_version\": \"unknown\",\n                    \"gpu_count\": torch.cuda.device_count(),\n                    \"gpu_names\": [\n                        torch.cuda.get_device_name(i)\n                        for i in range(torch.cuda.device_count())\n                    ],\n                }\n            )\n        else:\n            env_info[\"cuda_available\"] = False\n\n        # Log as parameters\n        for key, value in env_info.items():\n            if isinstance(value, list):\n                mlflow.log_param(f\"env_{key}\", \", \".join(map(str, value)))\n            else:\n                mlflow.log_param(f\"env_{key}\", value)\n\n        # Save detailed environment info\n        env_file = \"training_environment.json\"\n        with open(env_file, \"w\") as f:\n            json.dump(env_info, f, indent=2, default=str)\n\n        mlflow.log_artifact(env_file, \"environment\")\n        Path(env_file).unlink()  # Cleanup\n\n    except Exception as e:\n        print(f\"Warning: Could not log training environment: {e}\")\n</code></pre>"},{"location":"api/astro_lab.training/#astro_lab.training.AstroMLflowLogger.log_visualization","title":"log_visualization","text":"<pre><code>log_visualization(plot_path: str, plot_type: str = 'plot') -&gt; None\n</code></pre> <p>Log visualization plots.</p> Source code in <code>src\\astro_lab\\training\\mlflow_logger.py</code> <pre><code>def log_visualization(self, plot_path: str, plot_type: str = \"plot\") -&gt; None:\n    \"\"\"Log visualization plots.\"\"\"\n    if Path(plot_path).exists():\n        mlflow.log_artifact(plot_path, f\"plots/{plot_type}\")\n</code></pre>"},{"location":"api/astro_lab.training/#astro_lab.training.AstroMLflowLogger.start_system_metrics_logging","title":"start_system_metrics_logging","text":"<pre><code>start_system_metrics_logging() -&gt; None\n</code></pre> <p>Start automatic system metrics logging in background thread.</p> Source code in <code>src\\astro_lab\\training\\mlflow_logger.py</code> <pre><code>def start_system_metrics_logging(self) -&gt; None:\n    \"\"\"Start automatic system metrics logging in background thread.\"\"\"\n    if not self.enable_system_metrics:\n        return\n\n    if self._system_metrics_thread and self._system_metrics_thread.is_alive():\n        return  # Already running\n\n    self._stop_system_metrics.clear()\n    self._system_metrics_thread = threading.Thread(\n        target=self._log_system_metrics_loop, daemon=True\n    )\n    self._system_metrics_thread.start()\n    print(\n        f\"\ud83d\udda5\ufe0f  Started system metrics logging (interval: {self.system_metrics_interval}s)\"\n    )\n</code></pre>"},{"location":"api/astro_lab.training/#astro_lab.training.AstroMLflowLogger.stop_system_metrics_logging","title":"stop_system_metrics_logging","text":"<pre><code>stop_system_metrics_logging() -&gt; None\n</code></pre> <p>Stop system metrics logging.</p> Source code in <code>src\\astro_lab\\training\\mlflow_logger.py</code> <pre><code>def stop_system_metrics_logging(self) -&gt; None:\n    \"\"\"Stop system metrics logging.\"\"\"\n    if self._system_metrics_thread and self._system_metrics_thread.is_alive():\n        self._stop_system_metrics.set()\n        self._system_metrics_thread.join(timeout=5)\n        print(\"\ud83d\udda5\ufe0f  Stopped system metrics logging\")\n</code></pre>"},{"location":"api/astro_lab.training/#astro_lab.training.AstroTrainer","title":"AstroTrainer","text":"<p>               Bases: <code>Trainer</code></p> <p>High-performance trainer for astronomical ML models.</p> <p>Modern Lightning-based trainer with MLflow integration and astronomical optimizations. Inherits directly from Lightning Trainer for maximum compatibility. Updated for Lightning 2.0+ and modern ML practices.</p> <p>Methods:</p> Name Description <code>cleanup_old_checkpoints</code> <p>Clean up old checkpoints, keeping only the last N.</p> <code>fit</code> <p>Fit the model with modern Lightning 2.0+ compatibility.</p> <code>get_metrics</code> <p>Get training metrics.</p> <code>get_results_summary</code> <p>Get comprehensive results summary.</p> <code>list_checkpoints</code> <p>List all checkpoints in checkpoint directory.</p> <code>load_best_model</code> <p>Load the best model from checkpoint.</p> <code>load_from_checkpoint</code> <p>Load model from checkpoint.</p> <code>load_last_model</code> <p>Load the last model from checkpoint.</p> <code>optimize_hyperparameters</code> <p>Optimize hyperparameters using Optuna directly in AstroTrainer.</p> <code>predict</code> <p>Make predictions with modern Lightning 2.0+ compatibility.</p> <code>resume_from_checkpoint</code> <p>Resume training from checkpoint.</p> <code>save_best_models_to_results</code> <p>Save best models to results directory with organized structure.</p> <code>save_model</code> <p>Save model to path.</p> <code>test</code> <p>Test the model with modern Lightning 2.0+ compatibility.</p> <p>Attributes:</p> Name Type Description <code>lightning_module</code> <code>Optional[AstroLightningModule]</code> <p>Get the lightning module.</p> Source code in <code>src\\astro_lab\\training\\trainer.py</code> <pre><code>class AstroTrainer(Trainer):\n    \"\"\"\n    High-performance trainer for astronomical ML models.\n\n    Modern Lightning-based trainer with MLflow integration and astronomical optimizations.\n    Inherits directly from Lightning Trainer for maximum compatibility.\n    Updated for Lightning 2.0+ and modern ML practices.\n    \"\"\"\n\n    def __init__(\n        self,\n        lightning_module: Optional[AstroLightningModule] = None,\n        training_config: Optional[TrainingConfig] = None,\n        **kwargs,\n    ):\n        \"\"\"\n        Initialize AstroTrainer with modern Lightning 2.0+ compatibility.\n\n        Args:\n            lightning_module: Pre-configured Lightning module\n            training_config: Training configuration\n            **kwargs: Additional trainer parameters\n        \"\"\"\n        # Simple initialization without complex context manager\n        # Store configurations\n        self.training_config = training_config\n        self._lightning_module = lightning_module\n        self._optuna_study = None  # Will be set during hyperparameter optimization\n\n        # Create default training config if none provided\n        if training_config is None:\n            # Only pass valid ModelConfig fields after refactoring.\n            model_config = ModelConfig(name=\"default_model\")\n\n            training_config = TrainingConfig(\n                name=\"default_training\", model=model_config\n            )\n            self.training_config = training_config\n\n        # Validate training config\n        assert isinstance(self.training_config, TrainingConfig), (\n            \"training_config must be a TrainingConfig instance\"\n        )\n\n        # Get model config from training config\n        model_config = self.training_config.model\n\n        # Create Lightning module if not provided\n        if lightning_module is None:\n            lightning_module = AstroLightningModule(\n                model_config=model_config, training_config=training_config\n            )\n\n        self.astro_module = lightning_module\n        self.experiment_name = self.training_config.logging.experiment_name\n        self.survey = model_config.name if hasattr(model_config, \"name\") else \"unknown\"\n\n        # Extract hardware configuration\n        accelerator = self.training_config.hardware.accelerator\n        devices = self.training_config.hardware.devices\n        precision = self.training_config.hardware.precision\n\n        # Setup checkpoint directory\n        self.checkpoint_dir = self._setup_checkpoint_dir(\n            checkpoint_dir=None,  # Use default from data_config\n            experiment_name=self.experiment_name,\n        )\n\n        # Setup callbacks and logger\n        callbacks = self._setup_astro_callbacks(\n            enable_swa=self.training_config.callbacks.swa,\n            patience=self.training_config.callbacks.early_stopping_patience,\n            monitor=self.training_config.callbacks.monitor,\n            mode=self.training_config.callbacks.mode,\n            checkpoint_dir=self.checkpoint_dir,\n        )\n        logger_instance = self._setup_astro_logger()\n\n        # Filter kwargs to avoid conflicts\n        filtered_kwargs = {\n            k: v\n            for k, v in kwargs.items()\n            if k not in [\"accelerator\", \"devices\", \"precision\", \"max_epochs\"]\n        }\n\n        # Determine if we should disable Lightning's default logging\n        disable_lightning_logs = (\n            getattr(self.training_config.logging, \"use_mlflow\", False)\n            and logger_instance is not None\n        )\n\n        # Remove UI parameters from filtered_kwargs to avoid duplication - these are always set\n        ui_params = [\n            \"enable_progress_bar\",\n            \"enable_model_summary\",\n            \"enable_checkpointing\",\n        ]\n        for param in ui_params:\n            filtered_kwargs.pop(param, None)\n\n        # Initialize parent Trainer with modern parameters\n        super().__init__(\n            max_epochs=self.training_config.scheduler.max_epochs,\n            accelerator=accelerator,\n            devices=devices,\n            precision=precision,\n            callbacks=callbacks,\n            logger=logger_instance,\n            # UI parameters - always enabled, not configurable\n            enable_progress_bar=True,\n            enable_model_summary=True,\n            enable_checkpointing=True,\n            # Disable default Lightning logs directory when using MLflow\n            default_root_dir=None if disable_lightning_logs else None,\n            **filtered_kwargs,\n        )\n\n    def _setup_checkpoint_dir(\n        self, checkpoint_dir: Optional[Union[str, Path]], experiment_name: str\n    ) -&gt; Path:\n        \"\"\"Setup checkpoint directory using data_config system.\"\"\"\n        if checkpoint_dir is None:\n            # Use data_config system for organized checkpoint management\n            data_config.ensure_experiment_directories(experiment_name)\n            checkpoint_path = data_config.checkpoints_dir / experiment_name\n        else:\n            checkpoint_path = Path(checkpoint_dir)\n            # Create directory if it doesn't exist\n            checkpoint_path.mkdir(parents=True, exist_ok=True)\n\n        return checkpoint_path\n\n    def _setup_astro_callbacks(\n        self,\n        enable_swa: bool,\n        patience: int,\n        monitor: str,\n        mode: str,\n        checkpoint_dir: Path,\n    ) -&gt; List:\n        \"\"\"Setup training callbacks with astronomical ML optimizations.\"\"\"\n        callbacks = []\n\n        # Model checkpointing - save best model in specified directory\n        # Create descriptive filename with experiment name and timestamp\n        from datetime import datetime\n\n        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n        experiment_short = self.experiment_name.replace(\"_\", \"-\")[:20]  # Limit length\n\n        # Single optimized checkpoint callback - only best model\n        checkpoint_callback = ModelCheckpoint(\n            dirpath=str(checkpoint_dir),\n            monitor=monitor,\n            mode=mode,\n            save_top_k=1,  # Keep only the best model\n            save_last=True,  # Save last checkpoint as 'last.ckpt'\n            filename=f\"{experiment_short}_best_{{epoch:02d}}_{{val_loss:.3f}}_{timestamp}\",\n            auto_insert_metric_name=False,\n            verbose=False,  # Changed to False\n        )\n        callbacks.append(checkpoint_callback)\n\n        # Early stopping\n        early_stopping = EarlyStopping(\n            monitor=monitor,\n            mode=mode,\n            patience=patience,\n            verbose=False,  # Changed to False\n        )\n        callbacks.append(early_stopping)\n\n        # Learning rate monitoring\n        lr_monitor = LearningRateMonitor(logging_interval=\"step\")\n        callbacks.append(lr_monitor)\n\n        # Rich progress bar for better UX\n        progress_bar = RichProgressBar()\n        callbacks.append(progress_bar)\n\n        # Stochastic Weight Averaging (optional)\n        if enable_swa:\n            swa = StochasticWeightAveraging(swa_lrs=1e-2)\n            callbacks.append(swa)\n\n        return callbacks\n\n    def _setup_astro_logger(self):\n        \"\"\"Setup logging with MLflow integration.\"\"\"\n        if getattr(self.training_config.logging, \"use_mlflow\", True):  # Default to True\n            try:\n                tracking_uri = getattr(\n                    self.training_config.logging, \"tracking_uri\", None\n                )\n                if not tracking_uri:\n                    # Use data_config system for organized MLflow storage\n                    data_config.ensure_experiment_directories(self.experiment_name)\n                    exp_paths = data_config.get_experiment_paths(self.experiment_name)\n                    tracking_uri = f\"file://{exp_paths['mlruns'].absolute()}\"\n\n                logger_instance = AstroMLflowLogger(\n                    experiment_name=self.experiment_name,\n                    tracking_uri=tracking_uri,\n                    artifact_location=getattr(\n                        self.training_config.logging, \"tracking_uri\", None\n                    ),\n                    enable_system_metrics=getattr(\n                        self.training_config.logging, \"enable_system_metrics\", True\n                    ),\n                )\n                return logger_instance\n            except Exception as e:\n                logger.error(f\"MLflow logger setup failed: {e}\")\n                return None\n        else:\n            return None\n\n    def fit(\n        self,\n        train_dataloader: Optional[DataLoader] = None,\n        val_dataloader: Optional[DataLoader] = None,\n        datamodule: Any = None,\n        ckpt_path: Optional[Union[str, Path]] = None,\n    ) -&gt; None:\n        \"\"\"\n        Fit the model with modern Lightning 2.0+ compatibility.\n\n        Args:\n            train_dataloader: Training data loader\n            val_dataloader: Validation data loader\n            datamodule: Lightning DataModule\n            ckpt_path: Checkpoint path for resuming\n        \"\"\"\n        try:\n            # Use the lightning module directly - Lightning expects the LightningModule as model parameter\n            if datamodule is not None:\n                # Use DataModule (recommended approach)\n                # Pass the LightningModule as model parameter to the parent Trainer\n                super().fit(\n                    model=self.astro_module,  # This is the LightningModule\n                    datamodule=datamodule,\n                    ckpt_path=ckpt_path,\n                )\n            elif train_dataloader is not None or val_dataloader is not None:\n                # Use individual dataloaders\n                super().fit(\n                    model=self.astro_module,  # This is the LightningModule\n                    train_dataloaders=train_dataloader,\n                    val_dataloaders=val_dataloader,\n                    ckpt_path=ckpt_path,\n                )\n            else:\n                # No dataloaders provided - this should not happen with proper DataModule\n                raise ValueError(\n                    \"No dataloaders or datamodule provided. Cannot train without data.\"\n                )\n\n            # Cleanup after training\n            self._cleanup_after_training()\n\n        except Exception as e:\n            logger.error(f\"Training failed: {e}\")\n            # Cleanup even on error\n            self._cleanup_after_training()\n            raise\n\n    def _cleanup_after_training(self):\n        \"\"\"Clean up memory after training to prevent leaks.\"\"\"\n        try:\n            # Clear CUDA cache\n            if torch.cuda.is_available():\n                torch.cuda.empty_cache()\n\n            # Force garbage collection\n            gc.collect()\n\n            # Clear any cached tensors in the module\n            if hasattr(self.astro_module, \"model\"):\n                for param in self.astro_module.model.parameters():\n                    if param.grad is not None:\n                        param.grad.detach_()\n                        param.grad.zero_()\n\n            # Automatically save results and create plots if enabled\n            if getattr(self.training_config.logging, \"save_results_to_disk\", True):\n                self._save_training_results()\n\n        except Exception as e:\n            logger.error(f\"Memory cleanup failed: {e}\")\n\n    def _save_training_results(self):\n        \"\"\"Save training results to organized directory structure.\"\"\"\n        try:\n            # Get survey name for organization\n            survey = getattr(self, \"survey\", \"gaia\")\n\n            # Create organized results structure\n            results_structure = data_config.ensure_results_directories(\n                survey, self.experiment_name\n            )\n\n            # Save best models if enabled\n            if getattr(self.training_config.logging, \"save_best_models\", True):\n                top_k = getattr(self.training_config.logging, \"top_k_models\", 3)\n                saved_models = self.save_best_models_to_results(top_k)\n                logger.info(f\"\u2705 Saved {len(saved_models)} best models to results\")\n\n            # Create MLflow and Optuna plots using their built-in functions\n            if getattr(self.training_config.logging, \"create_plots\", True):\n                self._create_mlflow_optuna_plots(results_structure[\"plots\"])\n\n            # Create comprehensive results summary\n            self._create_results_summary(results_structure[\"base\"])\n\n            logger.info(f\"\ud83d\udcca Training results saved to: {results_structure['base']}\")\n\n        except Exception as e:\n            logger.error(f\"Failed to save training results: {e}\")\n\n    def _create_mlflow_optuna_plots(self, plots_dir: Path):\n        \"\"\"Create plots using MLflow and Optuna built-in visualization functions (2025 Best Practices).\"\"\"\n        try:\n            import mlflow\n            import optuna\n\n            # Create plots directory\n            plots_dir.mkdir(parents=True, exist_ok=True)\n\n            # 1. MLflow built-in plots (if we have an active run)\n            if mlflow.active_run():\n                self._create_mlflow_plots(plots_dir)\n\n            # 2. Optuna plots (if we have optimization results)\n            if hasattr(self, \"_optuna_study\") and self._optuna_study is not None:\n                self._create_optuna_plots(plots_dir)\n\n            logger.info(f\"\ud83d\udcc8 MLflow/Optuna plots created in: {plots_dir}\")\n\n        except Exception as e:\n            logger.error(f\"Failed to create MLflow/Optuna plots: {e}\")\n\n    def _create_mlflow_plots(self, plots_dir: Path):\n        \"\"\"Create plots using MLflow's built-in visualization functions (2025 Best Practices).\"\"\"\n        try:\n            import mlflow\n\n            # Get current run\n            run = mlflow.active_run()\n            if not run:\n                return\n\n            # MLflow automatically creates these plots in the UI\n            # We can also export them programmatically\n            logger.info(f\"\ud83d\udcca MLflow run {run.info.run_id} has built-in visualizations\")\n            logger.info(\n                f\"   View at: mlflow ui --backend-store-uri {mlflow.get_tracking_uri()}\"\n            )\n\n            # Create a simple summary of available metrics\n            client = mlflow.tracking.MlflowClient()\n            metrics = client.get_metric_history(run.info.run_id, \"val_loss\")\n\n            if metrics:\n                summary_file = plots_dir / \"mlflow_summary.txt\"\n                with open(summary_file, \"w\") as f:\n                    f.write(\"MLflow Run Summary\\n\")\n                    f.write(f\"Run ID: {run.info.run_id}\\n\")\n                    f.write(f\"Experiment: {run.info.experiment_id}\\n\")\n                    f.write(f\"Status: {run.info.status}\\n\")\n                    f.write(f\"Validation Loss Metrics: {len(metrics)}\\n\")\n                    f.write(f\"Best Loss: {min(m.value for m in metrics):.4f}\\n\")\n                    f.write(f\"Last Loss: {metrics[-1].value:.4f}\\n\")\n                    f.write(\n                        f\"\\nView plots at: mlflow ui --backend-store-uri {mlflow.get_tracking_uri()}\\n\"\n                    )\n                    f.write(\"   - Training curves (loss, accuracy, learning rate)\\n\")\n                    f.write(\"   - System metrics (CPU, GPU, memory)\\n\")\n                    f.write(\"   - Parameter tracking and comparison\\n\")\n                    f.write(\"   - Model artifacts and versions\\n\")\n\n        except Exception as e:\n            logger.warning(f\"Could not create MLflow plots: {e}\")\n\n    def _create_optuna_plots(self, plots_dir: Path):\n        \"\"\"Create plots using Optuna's built-in visualization functions (2025 Best Practices).\"\"\"\n        try:\n            import optuna\n\n            study = getattr(self, \"_optuna_study\", None)\n            if not study:\n                return\n\n            # Create Optuna plots using their built-in functions (2025 Best Practices)\n            # These are interactive HTML plots that can be viewed in browser\n\n            # 1. Optimization History - shows how the objective value improved over trials\n            fig1 = optuna.visualization.plot_optimization_history(study)\n            fig1.write_html(str(plots_dir / \"optuna_optimization_history.html\"))\n            fig1.write_image(str(plots_dir / \"optuna_optimization_history.png\"))\n\n            # 2. Parameter Importances - shows which hyperparameters are most important\n            fig2 = optuna.visualization.plot_param_importances(study)\n            fig2.write_html(str(plots_dir / \"optuna_param_importances.html\"))\n            fig2.write_image(str(plots_dir / \"optuna_param_importances.png\"))\n\n            # 3. Parallel Coordinate Plot - shows relationships between parameters\n            fig3 = optuna.visualization.plot_parallel_coordinate(study)\n            fig3.write_html(str(plots_dir / \"optuna_parallel_coordinate.html\"))\n\n            # 4. Contour Plot - shows 2D parameter relationships\n            fig4 = optuna.visualization.plot_contour(study)\n            fig4.write_html(str(plots_dir / \"optuna_contour.html\"))\n\n            # 5. Slice Plot - shows individual parameter distributions\n            fig5 = optuna.visualization.plot_slice(study)\n            fig5.write_html(str(plots_dir / \"optuna_slice.html\"))\n\n            # 6. Timeline Plot - shows trial duration and timing\n            fig6 = optuna.visualization.plot_timeline(study)\n            fig6.write_html(str(plots_dir / \"optuna_timeline.html\"))\n\n            # Create a summary file with instructions\n            summary_file = plots_dir / \"optuna_visualization_guide.md\"\n            with open(summary_file, \"w\") as f:\n                f.write(\"# Optuna Visualization Guide\\n\\n\")\n                f.write(f\"Study Name: {study.study_name}\\n\")\n                f.write(f\"Number of Trials: {len(study.trials)}\\n\")\n                f.write(f\"Best Value: {study.best_value:.4f}\\n\")\n                f.write(f\"Best Parameters: {study.best_params}\\n\\n\")\n\n                f.write(\"## Available Plots\\n\\n\")\n                f.write(\n                    \"1. **optimization_history.html** - How objective value improved over trials\\n\"\n                )\n                f.write(\n                    \"2. **param_importances.html** - Which hyperparameters matter most\\n\"\n                )\n                f.write(\n                    \"3. **parallel_coordinate.html** - Parameter relationships and correlations\\n\"\n                )\n                f.write(\"4. **contour.html** - 2D parameter space visualization\\n\")\n                f.write(\"5. **slice.html** - Individual parameter distributions\\n\")\n                f.write(\"6. **timeline.html** - Trial timing and duration analysis\\n\\n\")\n\n                f.write(\"## How to View\\n\\n\")\n                f.write(\n                    \"Open any `.html` file in your web browser for interactive visualizations.\\n\"\n                )\n                f.write(\n                    \"The plots are interactive - you can zoom, hover, and explore the data.\\n\\n\"\n                )\n\n                f.write(\"## Best Practices (2025)\\n\\n\")\n                f.write(\"- Use HTML plots for detailed analysis (interactive)\\n\")\n                f.write(\"- Use PNG plots for reports and documentation\\n\")\n                f.write(\"- Parameter importance helps focus optimization efforts\\n\")\n                f.write(\"- Parallel coordinate plots reveal parameter interactions\\n\")\n                f.write(\"- Timeline plots help identify performance bottlenecks\\n\")\n\n            logger.info(f\"\ud83d\udcca Created {len(study.trials)} Optuna visualization plots\")\n            logger.info(\"   HTML plots: Open in browser for interactive analysis\")\n            logger.info(\"   PNG plots: Use for reports and documentation\")\n\n        except Exception as e:\n            logger.warning(f\"Could not create Optuna plots: {e}\")\n\n    def _create_results_summary(self, results_dir: Path):\n        \"\"\"Create comprehensive results summary.\"\"\"\n        try:\n            from datetime import datetime\n\n            summary_file = results_dir / \"training_summary.md\"\n\n            with open(summary_file, \"w\", encoding=\"utf-8\") as f:\n                f.write(f\"# Training Summary: {self.experiment_name}\\n\\n\")\n                f.write(\n                    f\"**Generated:** {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\"\n                )\n                f.write(f\"**Survey:** {getattr(self, 'survey', 'unknown')}\\n\\n\")\n\n                # Model information\n                f.write(\"## Model Information\\n\\n\")\n                f.write(f\"- **Model Type:** {type(self.astro_module.model).__name__}\\n\")\n                f.write(\n                    f\"- **Task Type:** {getattr(self.astro_module, 'task_type', 'unknown')}\\n\"\n                )\n                f.write(\n                    f\"- **Number of Classes:** {getattr(self.astro_module, 'num_classes', 'N/A')}\\n\"\n                )\n                f.write(\n                    f\"- **Parameters:** {sum(p.numel() for p in self.astro_module.model.parameters()):,}\\n\"\n                )\n                f.write(\n                    f\"- **Trainable Parameters:** {sum(p.numel() for p in self.astro_module.model.parameters() if p.requires_grad):,}\\n\\n\"\n                )\n\n                # Training configuration\n                f.write(\"## Training Configuration\\n\\n\")\n                if self.training_config:\n                    f.write(\n                        f\"- **Max Epochs:** {self.training_config.scheduler.max_epochs}\\n\"\n                    )\n                    f.write(\n                        f\"- **Learning Rate:** {self.training_config.scheduler.learning_rate}\\n\"\n                    )\n                    f.write(\n                        f\"- **Batch Size:** {getattr(self.training_config.data, 'batch_size', 'N/A')}\\n\"\n                    )\n                    f.write(\n                        f\"- **Hardware:** {self.training_config.hardware.accelerator}\\n\"\n                    )\n                    f.write(\n                        f\"- **Precision:** {self.training_config.hardware.precision}\\n\\n\"\n                    )\n\n                # Results\n                f.write(\"## Results\\n\\n\")\n                f.write(f\"- **Best Model Path:** {self.best_model_path or 'N/A'}\\n\")\n                f.write(f\"- **Last Model Path:** {self.last_model_path or 'N/A'}\\n\")\n                f.write(f\"- **Checkpoint Directory:** {self.checkpoint_dir}\\n\\n\")\n\n                # MLflow information\n                f.write(\"## MLflow Integration\\n\\n\")\n                f.write(f\"- **Experiment Name:** {self.experiment_name}\\n\")\n                f.write(\n                    f\"- **MLflow Enabled:** {getattr(self.training_config.logging, 'use_mlflow', True)}\\n\"\n                )\n                if hasattr(self, \"logger\") and self.logger:\n                    f.write(f\"- **Logger Type:** {type(self.logger).__name__}\\n\")\n\n                # Directory structure\n                f.write(\"\\n## Directory Structure\\n\\n\")\n                f.write(\"```\\n\")\n                f.write(f\"{results_dir}/\\n\")\n                f.write(\"\u251c\u2500\u2500 models/          # Saved best models\\n\")\n                f.write(\"\u251c\u2500\u2500 plots/           # Training visualizations\\n\")\n                f.write(\"\u2502   \u251c\u2500\u2500 training_curves.png\\n\")\n                f.write(\"\u2502   \u251c\u2500\u2500 model_architecture.png\\n\")\n                f.write(\"\u2502   \u251c\u2500\u2500 confusion_matrix.png\\n\")\n                f.write(\"\u2502   \u2514\u2500\u2500 feature_importance.png\\n\")\n                f.write(\"\u2514\u2500\u2500 training_summary.md  # This file\\n\")\n                f.write(\"```\\n\")\n\n            logger.info(f\"\ud83d\udcdd Training summary created: {summary_file}\")\n\n        except Exception as e:\n            logger.error(f\"Failed to create results summary: {e}\")\n\n    def test(\n        self,\n        test_dataloader: Optional[DataLoader] = None,\n        datamodule: Any = None,\n    ) -&gt; List[Mapping[str, float]]:\n        \"\"\"\n        Test the model with modern Lightning 2.0+ compatibility.\n\n        Args:\n            test_dataloader: Test data loader\n            datamodule: Lightning DataModule\n\n        Returns:\n            Test results\n        \"\"\"\n        try:\n            if datamodule is not None:\n                results = super().test(\n                    model=self.astro_module,\n                    datamodule=datamodule,\n                )\n            else:\n                results = super().test(\n                    model=self.astro_module,\n                    dataloaders=test_dataloader,\n                )\n\n            # Cleanup after testing\n            self._cleanup_after_training()\n            return results\n\n        except Exception as e:\n            logger.error(f\"Testing failed: {e}\")\n            self._cleanup_after_training()\n            raise\n\n    def predict(\n        self,\n        predict_dataloader: Optional[DataLoader] = None,\n        datamodule: Any = None,\n    ) -&gt; Optional[List[Any]]:\n        \"\"\"\n        Make predictions with modern Lightning 2.0+ compatibility.\n\n        Args:\n            predict_dataloader: Prediction data loader\n            datamodule: Lightning DataModule\n\n        Returns:\n            Predictions\n        \"\"\"\n        try:\n            if datamodule is not None:\n                return super().predict(\n                    model=self.astro_module,\n                    datamodule=datamodule,\n                )\n            else:\n                return super().predict(\n                    model=self.astro_module,\n                    dataloaders=predict_dataloader,\n                )\n        except Exception as e:\n            logger.error(f\"Prediction failed: {e}\")\n            raise\n\n    def get_metrics(self) -&gt; Dict[str, Any]:\n        \"\"\"Get training metrics.\"\"\"\n        return {\n            \"best_model_path\": self.best_model_path,\n            \"last_model_path\": self.last_model_path,\n            \"experiment_name\": self.experiment_name,\n            \"survey\": self.survey,\n        }\n\n    @property\n    def best_model_path(self) -&gt; Optional[str]:\n        # Return the best model path from the ModelCheckpoint callback if available\n        cb = getattr(self, \"checkpoint_callback\", None)\n        if cb is not None and hasattr(cb, \"best_model_path\"):\n            return getattr(cb, \"best_model_path\", None)\n        # Fallback: try to find latest .ckpt file in checkpoint_dir\n        if hasattr(self, \"checkpoint_dir\") and self.checkpoint_dir:\n            ckpts = list(self.checkpoint_dir.glob(\"*.ckpt\"))\n            if ckpts:\n                return str(\n                    sorted(ckpts, key=lambda x: x.stat().st_mtime, reverse=True)[0]\n                )\n        return None\n\n    @property\n    def last_model_path(self) -&gt; Optional[str]:\n        # Return the last model path from the ModelCheckpoint callback if available\n        cb = getattr(self, \"checkpoint_callback\", None)\n        if cb is not None and hasattr(cb, \"last_model_path\"):\n            return getattr(cb, \"last_model_path\", None)\n        # Fallback: try to find last .ckpt file in checkpoint_dir\n        if hasattr(self, \"checkpoint_dir\") and self.checkpoint_dir:\n            ckpts = list(self.checkpoint_dir.glob(\"*.ckpt\"))\n            if ckpts:\n                return str(sorted(ckpts, key=lambda x: x.stat().st_mtime)[-1])\n        return None\n\n    def load_best_model(self) -&gt; AstroLightningModule:\n        \"\"\"Load the best model from checkpoint.\"\"\"\n        if self.best_model_path is None:\n            raise ValueError(\"No best model checkpoint found\")\n\n        return AstroLightningModule.load_from_checkpoint(self.best_model_path)\n\n    def load_last_model(self) -&gt; AstroLightningModule:\n        \"\"\"Load the last model from checkpoint.\"\"\"\n        if self.last_model_path is None:\n            raise ValueError(\"No last model checkpoint found\")\n\n        return AstroLightningModule.load_from_checkpoint(self.last_model_path)\n\n    def resume_from_checkpoint(self, checkpoint_path: Union[str, Path]) -&gt; None:\n        \"\"\"Resume training from checkpoint.\"\"\"\n        self.fit(ckpt_path=str(checkpoint_path))\n\n    def optimize_hyperparameters(\n        self,\n        train_dataloader: DataLoader,\n        val_dataloader: DataLoader,\n        n_trials: int = 50,\n        timeout: Optional[int] = None,\n        search_space: Optional[Dict[str, Any]] = None,\n        monitor: str = \"val_loss\",\n        **kwargs: Any,\n    ) -&gt; Dict[str, Any]:\n        \"\"\"\n        Optimize hyperparameters using Optuna directly in AstroTrainer.\n\n        Args:\n            train_dataloader: Training data loader\n            val_dataloader: Validation data loader\n            n_trials: Number of optimization trials\n            timeout: Optimization timeout in seconds\n            search_space: Custom hyperparameter search space\n            monitor: Metric to optimize\n            **kwargs: Additional arguments\n\n        Returns:\n            Dictionary with best parameters and results\n        \"\"\"\n        # Optuna direction and pruner are valid for create_study, but use kwargs to avoid linter errors\n        study_kwargs = dict(\n            direction=\"minimize\" if \"loss\" in monitor else \"maximize\",\n            pruner=optuna.pruners.MedianPruner(\n                n_startup_trials=10,  # Let more trials complete first\n                n_warmup_steps=5,  # Wait longer before pruning\n            ),\n        )\n        study = optuna.create_study(**study_kwargs)\n\n        def objective(trial):\n            # Suggest hyperparameters\n            if search_space:\n                # Use custom search space from config\n                params = {}\n                for name, config in search_space.items():\n                    if config[\"type\"] == \"float\" or config[\"type\"] == \"uniform\":\n                        params[name] = trial.suggest_float(\n                            name, config[\"low\"], config[\"high\"]\n                        )\n                    elif config[\"type\"] == \"loguniform\":\n                        params[name] = trial.suggest_float(\n                            name, config[\"low\"], config[\"high\"], log=True\n                        )\n                    elif config[\"type\"] == \"int\":\n                        params[name] = trial.suggest_int(\n                            name, config[\"low\"], config[\"high\"]\n                        )\n                    elif config[\"type\"] == \"categorical\":\n                        params[name] = trial.suggest_categorical(\n                            name, config[\"choices\"]\n                        )\n                    else:\n                        logger.warning(\n                            f\"Unknown parameter type '{config['type']}' for {name}, skipping\"\n                        )\n\n                logger.debug(f\"Trial {trial.number} params from config: {params}\")\n            else:\n                # Default search space\n                params = {\n                    \"learning_rate\": trial.suggest_float(\n                        \"learning_rate\", 1e-5, 1e-2, log=True\n                    ),\n                    \"hidden_dim\": trial.suggest_int(\"hidden_dim\", 64, 512),\n                    \"num_layers\": trial.suggest_int(\"num_layers\", 2, 6),\n                    \"dropout\": trial.suggest_float(\"dropout\", 0.1, 0.5),\n                    \"weight_decay\": trial.suggest_float(\n                        \"weight_decay\", 1e-6, 1e-3, log=True\n                    ),\n                }\n\n                logger.debug(f\"Trial {trial.number} params from default: {params}\")\n\n            # Create model with trial parameters\n            model_config = self.astro_module.model_config\n            if model_config:\n                # Update existing config\n                model_config.hidden_dim = params.get(\n                    \"hidden_dim\", model_config.hidden_dim\n                )\n                model_config.num_layers = params.get(\n                    \"num_layers\", model_config.num_layers\n                )\n                model_config.dropout = params.get(\"dropout\", model_config.dropout)\n\n                # Create new model\n                model = self.astro_module._create_model_from_config(model_config)\n            else:\n                # Create from scratch using new import path\n                from astro_lab.models.core.survey_gnn import AstroSurveyGNN\n\n                # Defensive: ensure input_dim and output_dim are integers\n                input_dim = getattr(self.astro_module.model, \"input_dim\", None)\n                if (\n                    input_dim is not None\n                    and not isinstance(input_dim, int)\n                    and hasattr(input_dim, \"item\")\n                ):\n                    input_dim = int(input_dim.item())\n                output_dim = getattr(self.astro_module.model, \"output_dim\", 1)\n                if (\n                    output_dim is not None\n                    and not isinstance(output_dim, int)\n                    and hasattr(output_dim, \"item\")\n                ):\n                    output_dim = int(output_dim.item())\n                model = AstroSurveyGNN(\n                    input_dim=input_dim,\n                    hidden_dim=params.get(\"hidden_dim\", 128),\n                    output_dim=output_dim,\n                    num_layers=params.get(\"num_layers\", 3),\n                    dropout=params.get(\"dropout\", 0.2),\n                    conv_type=\"gcn\",\n                )\n\n            # Create new Lightning module for this trial\n            trial_module = AstroLightningModule(\n                model=model,\n                task_type=self.astro_module.task_type,\n                learning_rate=params.get(\"learning_rate\", 1e-3),\n                weight_decay=params.get(\"weight_decay\", 1e-4),\n                num_classes=self.astro_module.num_classes,\n            )\n\n            # Create trial trainer with minimal callbacks\n            trial_trainer = Trainer(\n                max_epochs=50,  # Longer for better optimization\n                accelerator=self.accelerator,\n                devices=1,  # Single device for optimization\n                enable_checkpointing=False,\n                enable_progress_bar=False,\n                callbacks=[\n                    EarlyStopping(\n                        monitor=monitor,\n                        patience=8,\n                        mode=\"min\" if \"loss\" in monitor else \"max\",\n                    ),\n                    PyTorchLightningPruningCallback(trial, monitor=monitor),\n                ],\n                logger=False,  # Disable logging during optimization\n            )\n\n            # Train and evaluate\n            try:\n                trial_trainer.fit(trial_module, train_dataloader, val_dataloader)\n\n                # Get validation metric\n                val_metric = trial_trainer.callback_metrics.get(monitor, float(\"inf\"))\n                # Always return a float for Optuna compatibility\n                if not isinstance(val_metric, (float, int)) and hasattr(\n                    val_metric, \"item\"\n                ):\n                    return float(val_metric.item())\n                return float(val_metric)\n\n            except optuna.TrialPruned:\n                raise\n            except Exception as e:\n                logger.error(f\"Trial failed: {e}\")\n                return float(\"inf\") if \"loss\" in monitor else float(\"-inf\")\n\n        # Run optimization\n        logger.info(f\"Starting hyperparameter optimization with {n_trials} trials...\")\n        study.optimize(objective, n_trials=n_trials, timeout=timeout)\n\n        # Store the study for later plotting\n        self._optuna_study = study\n\n        # Get best parameters\n        best_params = study.best_params\n        best_value = study.best_value\n\n        logger.info(\"Optimization complete!\")\n        logger.info(f\"Best value: {best_value:.4f}\")\n        logger.info(f\"Best parameters: {best_params}\")\n\n        # Return results\n        return {\n            \"best_params\": best_params,\n            \"best_value\": best_value,\n            \"n_trials\": len(study.trials),\n            \"study\": study,\n        }\n\n    def load_from_checkpoint(self, checkpoint_path: str) -&gt; AstroLightningModule:\n        \"\"\"Load model from checkpoint.\"\"\"\n        return AstroLightningModule.load_from_checkpoint(checkpoint_path)\n\n    def save_model(self, path: str) -&gt; None:\n        \"\"\"Save model to path.\"\"\"\n        if self.astro_module is not None:\n            torch.save(self.astro_module.state_dict(), path)\n\n    def list_checkpoints(self) -&gt; List[Path]:\n        \"\"\"List all checkpoints in checkpoint directory.\"\"\"\n        if not self.checkpoint_dir.exists():\n            return []\n\n        checkpoints = list(self.checkpoint_dir.glob(\"*.ckpt\"))\n        return sorted(checkpoints, key=lambda x: x.stat().st_mtime, reverse=True)\n\n    def cleanup_old_checkpoints(self, keep_last_n: int = 5) -&gt; None:\n        \"\"\"Clean up old checkpoints, keeping only the last N.\"\"\"\n        checkpoints = self.list_checkpoints()\n\n        if len(checkpoints) &gt; keep_last_n:\n            checkpoints_to_remove = checkpoints[keep_last_n:]\n            for checkpoint in checkpoints_to_remove:\n                try:\n                    checkpoint.unlink()\n                except Exception as e:\n                    logger.error(f\"Failed to remove checkpoint {checkpoint.name}: {e}\")\n\n    def save_best_models_to_results(self, top_k: int = 3) -&gt; Dict[str, Path]:\n        \"\"\"\n        Save best models to results directory with organized structure.\n\n        Args:\n            top_k: Number of best models to save\n\n        Returns:\n            Dictionary mapping model names to saved paths\n        \"\"\"\n        try:\n            # Get survey name for better organization\n            survey = getattr(self, \"survey\", \"gaia\")\n\n            # Create organized results structure\n            results_dir = Path(f\"./results/{survey}\")\n            models_dir = results_dir / \"models\"\n            plots_dir = results_dir / \"plots\"\n\n            models_dir.mkdir(parents=True, exist_ok=True)\n            plots_dir.mkdir(parents=True, exist_ok=True)\n\n            # Get all checkpoints\n            checkpoint_dir = Path(f\"./experiments/{survey}/checkpoints\")\n            if not checkpoint_dir.exists():\n                logger.error(f\"No checkpoint directory found: {checkpoint_dir}\")\n                return {}\n\n            checkpoints = list(checkpoint_dir.glob(\"*.ckpt\"))\n            if not checkpoints:\n                logger.error(f\"No checkpoints found in: {checkpoint_dir}\")\n                return {}\n\n            # Sort by validation loss (best first)\n            def extract_val_loss(checkpoint_path):\n                try:\n                    # Extract validation loss from filename\n                    filename = checkpoint_path.stem\n                    if \"val_loss=\" in filename:\n                        loss_str = filename.split(\"val_loss=\")[1].split(\"_\")[0]\n                        return float(loss_str)\n                    return float(\"inf\")  # Put files without loss info at the end\n                except:\n                    return float(\"inf\")\n\n            checkpoints.sort(key=extract_val_loss)\n\n            # Save top-k models with descriptive names\n            saved_models = {}\n            timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n\n            for i, checkpoint_path in enumerate(checkpoints[:top_k]):\n                # Create descriptive filename\n                val_loss = extract_val_loss(checkpoint_path)\n                if val_loss == float(\"inf\"):\n                    model_name = f\"{survey}_model_{i + 1}_{timestamp}\"\n                else:\n                    model_name = (\n                        f\"{survey}_model_{i + 1}_val_loss_{val_loss:.4f}_{timestamp}\"\n                    )\n\n                # Save to results/models\n                target_path = models_dir / f\"{model_name}.ckpt\"\n\n                try:\n                    shutil.copy2(checkpoint_path, target_path)\n                    saved_models[model_name] = target_path\n                except Exception as e:\n                    logger.error(f\"Failed to save model {i + 1}: {e}\")\n\n            # Create README with model information\n            self._create_models_readme(saved_models, checkpoints[:top_k])\n\n            logger.info(f\"Saved {len(saved_models)} models to: {models_dir}\")\n            return saved_models\n\n        except Exception as e:\n            logger.error(f\"Failed to save models to results: {e}\")\n            return {}\n\n    def _create_models_readme(\n        self, saved_models: Dict[str, Path], checkpoints_info: List\n    ):\n        \"\"\"Create README file with model information.\"\"\"\n        try:\n            results_dir = data_config.results_dir / self.experiment_name\n            readme_path = results_dir / \"README.md\"\n\n            with open(readme_path, \"w\", encoding=\"utf-8\") as f:\n                f.write(f\"# {self.experiment_name} - Model Results\\n\\n\")\n                f.write(f\"**Survey:** {self.survey}\\n\")\n                f.write(f\"**Experiment:** {self.experiment_name}\\n\")\n                f.write(f\"**Date:** {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\\n\")\n\n                f.write(\"## Saved Models\\n\\n\")\n                for model_name, model_path in saved_models.items():\n                    f.write(f\"- **{model_name}**: `{model_path.name}`\\n\")\n\n                f.write(\"\\n## Training Information\\n\\n\")\n                f.write(f\"- **Model Type:** {type(self.astro_module.model).__name__}\\n\")\n                f.write(f\"- **Task Type:** {self.astro_module.task_type}\\n\")\n                f.write(\n                    f\"- **Number of Classes:** {getattr(self.astro_module, 'num_classes', 'N/A')}\\n\"\n                )\n\n                if self.training_config:\n                    f.write(\n                        f\"- **Max Epochs:** {self.training_config.scheduler.max_epochs}\\n\"\n                    )\n                    f.write(\n                        f\"- **Learning Rate:** {getattr(self.astro_module, 'learning_rate', 'N/A')}\\n\"\n                    )\n\n                f.write(\"\\n## Checkpoints\\n\\n\")\n                for i, checkpoint in enumerate(checkpoints_info[:10]):  # Show top 10\n                    f.write(f\"{i + 1}. `{checkpoint.name}`\\n\")\n\n        except Exception as e:\n            logger.error(f\"Failed to create README: {e}\")\n\n    def get_results_summary(self) -&gt; Dict[str, Any]:\n        \"\"\"Get comprehensive results summary.\"\"\"\n        return {\n            \"experiment_name\": self.experiment_name,\n            \"survey\": self.survey,\n            \"best_model_path\": self.best_model_path,\n            \"last_model_path\": self.last_model_path,\n            \"checkpoint_dir\": str(self.checkpoint_dir),\n            \"results_structure\": {\n                \"base\": str(data_config.results_dir / self.experiment_name),\n                \"checkpoints\": str(self.checkpoint_dir),\n                \"logs\": str(data_config.logs_dir / self.experiment_name),\n            },\n            \"model_info\": {\n                \"type\": type(self.astro_module.model).__name__,\n                \"task_type\": self.astro_module.task_type,\n                \"num_classes\": getattr(self.astro_module, \"num_classes\", \"N/A\"),\n            },\n            \"training_info\": {\n                \"max_epochs\": self.training_config.scheduler.max_epochs\n                if self.training_config\n                else \"N/A\",\n                \"learning_rate\": getattr(self.astro_module, \"learning_rate\", \"N/A\"),\n            },\n        }\n\n    @property\n    def lightning_module(self) -&gt; Optional[AstroLightningModule]:\n        \"\"\"Get the lightning module.\"\"\"\n        return self.astro_module\n</code></pre>"},{"location":"api/astro_lab.training/#astro_lab.training.AstroTrainer.lightning_module","title":"lightning_module  <code>property</code>","text":"<pre><code>lightning_module: Optional[AstroLightningModule]\n</code></pre> <p>Get the lightning module.</p>"},{"location":"api/astro_lab.training/#astro_lab.training.AstroTrainer.cleanup_old_checkpoints","title":"cleanup_old_checkpoints","text":"<pre><code>cleanup_old_checkpoints(keep_last_n: int = 5) -&gt; None\n</code></pre> <p>Clean up old checkpoints, keeping only the last N.</p> Source code in <code>src\\astro_lab\\training\\trainer.py</code> <pre><code>def cleanup_old_checkpoints(self, keep_last_n: int = 5) -&gt; None:\n    \"\"\"Clean up old checkpoints, keeping only the last N.\"\"\"\n    checkpoints = self.list_checkpoints()\n\n    if len(checkpoints) &gt; keep_last_n:\n        checkpoints_to_remove = checkpoints[keep_last_n:]\n        for checkpoint in checkpoints_to_remove:\n            try:\n                checkpoint.unlink()\n            except Exception as e:\n                logger.error(f\"Failed to remove checkpoint {checkpoint.name}: {e}\")\n</code></pre>"},{"location":"api/astro_lab.training/#astro_lab.training.AstroTrainer.fit","title":"fit","text":"<pre><code>fit(\n    train_dataloader: Optional[DataLoader] = None,\n    val_dataloader: Optional[DataLoader] = None,\n    datamodule: Any = None,\n    ckpt_path: Optional[Union[str, Path]] = None,\n) -&gt; None\n</code></pre> <p>Fit the model with modern Lightning 2.0+ compatibility.</p> <p>Parameters:</p> Name Type Description Default <code>train_dataloader</code> <code>Optional[DataLoader]</code> <p>Training data loader</p> <code>None</code> <code>val_dataloader</code> <code>Optional[DataLoader]</code> <p>Validation data loader</p> <code>None</code> <code>datamodule</code> <code>Any</code> <p>Lightning DataModule</p> <code>None</code> <code>ckpt_path</code> <code>Optional[Union[str, Path]]</code> <p>Checkpoint path for resuming</p> <code>None</code> Source code in <code>src\\astro_lab\\training\\trainer.py</code> <pre><code>def fit(\n    self,\n    train_dataloader: Optional[DataLoader] = None,\n    val_dataloader: Optional[DataLoader] = None,\n    datamodule: Any = None,\n    ckpt_path: Optional[Union[str, Path]] = None,\n) -&gt; None:\n    \"\"\"\n    Fit the model with modern Lightning 2.0+ compatibility.\n\n    Args:\n        train_dataloader: Training data loader\n        val_dataloader: Validation data loader\n        datamodule: Lightning DataModule\n        ckpt_path: Checkpoint path for resuming\n    \"\"\"\n    try:\n        # Use the lightning module directly - Lightning expects the LightningModule as model parameter\n        if datamodule is not None:\n            # Use DataModule (recommended approach)\n            # Pass the LightningModule as model parameter to the parent Trainer\n            super().fit(\n                model=self.astro_module,  # This is the LightningModule\n                datamodule=datamodule,\n                ckpt_path=ckpt_path,\n            )\n        elif train_dataloader is not None or val_dataloader is not None:\n            # Use individual dataloaders\n            super().fit(\n                model=self.astro_module,  # This is the LightningModule\n                train_dataloaders=train_dataloader,\n                val_dataloaders=val_dataloader,\n                ckpt_path=ckpt_path,\n            )\n        else:\n            # No dataloaders provided - this should not happen with proper DataModule\n            raise ValueError(\n                \"No dataloaders or datamodule provided. Cannot train without data.\"\n            )\n\n        # Cleanup after training\n        self._cleanup_after_training()\n\n    except Exception as e:\n        logger.error(f\"Training failed: {e}\")\n        # Cleanup even on error\n        self._cleanup_after_training()\n        raise\n</code></pre>"},{"location":"api/astro_lab.training/#astro_lab.training.AstroTrainer.get_metrics","title":"get_metrics","text":"<pre><code>get_metrics() -&gt; Dict[str, Any]\n</code></pre> <p>Get training metrics.</p> Source code in <code>src\\astro_lab\\training\\trainer.py</code> <pre><code>def get_metrics(self) -&gt; Dict[str, Any]:\n    \"\"\"Get training metrics.\"\"\"\n    return {\n        \"best_model_path\": self.best_model_path,\n        \"last_model_path\": self.last_model_path,\n        \"experiment_name\": self.experiment_name,\n        \"survey\": self.survey,\n    }\n</code></pre>"},{"location":"api/astro_lab.training/#astro_lab.training.AstroTrainer.get_results_summary","title":"get_results_summary","text":"<pre><code>get_results_summary() -&gt; Dict[str, Any]\n</code></pre> <p>Get comprehensive results summary.</p> Source code in <code>src\\astro_lab\\training\\trainer.py</code> <pre><code>def get_results_summary(self) -&gt; Dict[str, Any]:\n    \"\"\"Get comprehensive results summary.\"\"\"\n    return {\n        \"experiment_name\": self.experiment_name,\n        \"survey\": self.survey,\n        \"best_model_path\": self.best_model_path,\n        \"last_model_path\": self.last_model_path,\n        \"checkpoint_dir\": str(self.checkpoint_dir),\n        \"results_structure\": {\n            \"base\": str(data_config.results_dir / self.experiment_name),\n            \"checkpoints\": str(self.checkpoint_dir),\n            \"logs\": str(data_config.logs_dir / self.experiment_name),\n        },\n        \"model_info\": {\n            \"type\": type(self.astro_module.model).__name__,\n            \"task_type\": self.astro_module.task_type,\n            \"num_classes\": getattr(self.astro_module, \"num_classes\", \"N/A\"),\n        },\n        \"training_info\": {\n            \"max_epochs\": self.training_config.scheduler.max_epochs\n            if self.training_config\n            else \"N/A\",\n            \"learning_rate\": getattr(self.astro_module, \"learning_rate\", \"N/A\"),\n        },\n    }\n</code></pre>"},{"location":"api/astro_lab.training/#astro_lab.training.AstroTrainer.list_checkpoints","title":"list_checkpoints","text":"<pre><code>list_checkpoints() -&gt; List[Path]\n</code></pre> <p>List all checkpoints in checkpoint directory.</p> Source code in <code>src\\astro_lab\\training\\trainer.py</code> <pre><code>def list_checkpoints(self) -&gt; List[Path]:\n    \"\"\"List all checkpoints in checkpoint directory.\"\"\"\n    if not self.checkpoint_dir.exists():\n        return []\n\n    checkpoints = list(self.checkpoint_dir.glob(\"*.ckpt\"))\n    return sorted(checkpoints, key=lambda x: x.stat().st_mtime, reverse=True)\n</code></pre>"},{"location":"api/astro_lab.training/#astro_lab.training.AstroTrainer.load_best_model","title":"load_best_model","text":"<pre><code>load_best_model() -&gt; AstroLightningModule\n</code></pre> <p>Load the best model from checkpoint.</p> Source code in <code>src\\astro_lab\\training\\trainer.py</code> <pre><code>def load_best_model(self) -&gt; AstroLightningModule:\n    \"\"\"Load the best model from checkpoint.\"\"\"\n    if self.best_model_path is None:\n        raise ValueError(\"No best model checkpoint found\")\n\n    return AstroLightningModule.load_from_checkpoint(self.best_model_path)\n</code></pre>"},{"location":"api/astro_lab.training/#astro_lab.training.AstroTrainer.load_from_checkpoint","title":"load_from_checkpoint","text":"<pre><code>load_from_checkpoint(checkpoint_path: str) -&gt; AstroLightningModule\n</code></pre> <p>Load model from checkpoint.</p> Source code in <code>src\\astro_lab\\training\\trainer.py</code> <pre><code>def load_from_checkpoint(self, checkpoint_path: str) -&gt; AstroLightningModule:\n    \"\"\"Load model from checkpoint.\"\"\"\n    return AstroLightningModule.load_from_checkpoint(checkpoint_path)\n</code></pre>"},{"location":"api/astro_lab.training/#astro_lab.training.AstroTrainer.load_last_model","title":"load_last_model","text":"<pre><code>load_last_model() -&gt; AstroLightningModule\n</code></pre> <p>Load the last model from checkpoint.</p> Source code in <code>src\\astro_lab\\training\\trainer.py</code> <pre><code>def load_last_model(self) -&gt; AstroLightningModule:\n    \"\"\"Load the last model from checkpoint.\"\"\"\n    if self.last_model_path is None:\n        raise ValueError(\"No last model checkpoint found\")\n\n    return AstroLightningModule.load_from_checkpoint(self.last_model_path)\n</code></pre>"},{"location":"api/astro_lab.training/#astro_lab.training.AstroTrainer.optimize_hyperparameters","title":"optimize_hyperparameters","text":"<pre><code>optimize_hyperparameters(\n    train_dataloader: DataLoader,\n    val_dataloader: DataLoader,\n    n_trials: int = 50,\n    timeout: Optional[int] = None,\n    search_space: Optional[Dict[str, Any]] = None,\n    monitor: str = \"val_loss\",\n    **kwargs: Any\n) -&gt; Dict[str, Any]\n</code></pre> <p>Optimize hyperparameters using Optuna directly in AstroTrainer.</p> <p>Parameters:</p> Name Type Description Default <code>train_dataloader</code> <code>DataLoader</code> <p>Training data loader</p> required <code>val_dataloader</code> <code>DataLoader</code> <p>Validation data loader</p> required <code>n_trials</code> <code>int</code> <p>Number of optimization trials</p> <code>50</code> <code>timeout</code> <code>Optional[int]</code> <p>Optimization timeout in seconds</p> <code>None</code> <code>search_space</code> <code>Optional[Dict[str, Any]]</code> <p>Custom hyperparameter search space</p> <code>None</code> <code>monitor</code> <code>str</code> <p>Metric to optimize</p> <code>'val_loss'</code> <code>**kwargs</code> <code>Any</code> <p>Additional arguments</p> <code>{}</code> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dictionary with best parameters and results</p> Source code in <code>src\\astro_lab\\training\\trainer.py</code> <pre><code>def optimize_hyperparameters(\n    self,\n    train_dataloader: DataLoader,\n    val_dataloader: DataLoader,\n    n_trials: int = 50,\n    timeout: Optional[int] = None,\n    search_space: Optional[Dict[str, Any]] = None,\n    monitor: str = \"val_loss\",\n    **kwargs: Any,\n) -&gt; Dict[str, Any]:\n    \"\"\"\n    Optimize hyperparameters using Optuna directly in AstroTrainer.\n\n    Args:\n        train_dataloader: Training data loader\n        val_dataloader: Validation data loader\n        n_trials: Number of optimization trials\n        timeout: Optimization timeout in seconds\n        search_space: Custom hyperparameter search space\n        monitor: Metric to optimize\n        **kwargs: Additional arguments\n\n    Returns:\n        Dictionary with best parameters and results\n    \"\"\"\n    # Optuna direction and pruner are valid for create_study, but use kwargs to avoid linter errors\n    study_kwargs = dict(\n        direction=\"minimize\" if \"loss\" in monitor else \"maximize\",\n        pruner=optuna.pruners.MedianPruner(\n            n_startup_trials=10,  # Let more trials complete first\n            n_warmup_steps=5,  # Wait longer before pruning\n        ),\n    )\n    study = optuna.create_study(**study_kwargs)\n\n    def objective(trial):\n        # Suggest hyperparameters\n        if search_space:\n            # Use custom search space from config\n            params = {}\n            for name, config in search_space.items():\n                if config[\"type\"] == \"float\" or config[\"type\"] == \"uniform\":\n                    params[name] = trial.suggest_float(\n                        name, config[\"low\"], config[\"high\"]\n                    )\n                elif config[\"type\"] == \"loguniform\":\n                    params[name] = trial.suggest_float(\n                        name, config[\"low\"], config[\"high\"], log=True\n                    )\n                elif config[\"type\"] == \"int\":\n                    params[name] = trial.suggest_int(\n                        name, config[\"low\"], config[\"high\"]\n                    )\n                elif config[\"type\"] == \"categorical\":\n                    params[name] = trial.suggest_categorical(\n                        name, config[\"choices\"]\n                    )\n                else:\n                    logger.warning(\n                        f\"Unknown parameter type '{config['type']}' for {name}, skipping\"\n                    )\n\n            logger.debug(f\"Trial {trial.number} params from config: {params}\")\n        else:\n            # Default search space\n            params = {\n                \"learning_rate\": trial.suggest_float(\n                    \"learning_rate\", 1e-5, 1e-2, log=True\n                ),\n                \"hidden_dim\": trial.suggest_int(\"hidden_dim\", 64, 512),\n                \"num_layers\": trial.suggest_int(\"num_layers\", 2, 6),\n                \"dropout\": trial.suggest_float(\"dropout\", 0.1, 0.5),\n                \"weight_decay\": trial.suggest_float(\n                    \"weight_decay\", 1e-6, 1e-3, log=True\n                ),\n            }\n\n            logger.debug(f\"Trial {trial.number} params from default: {params}\")\n\n        # Create model with trial parameters\n        model_config = self.astro_module.model_config\n        if model_config:\n            # Update existing config\n            model_config.hidden_dim = params.get(\n                \"hidden_dim\", model_config.hidden_dim\n            )\n            model_config.num_layers = params.get(\n                \"num_layers\", model_config.num_layers\n            )\n            model_config.dropout = params.get(\"dropout\", model_config.dropout)\n\n            # Create new model\n            model = self.astro_module._create_model_from_config(model_config)\n        else:\n            # Create from scratch using new import path\n            from astro_lab.models.core.survey_gnn import AstroSurveyGNN\n\n            # Defensive: ensure input_dim and output_dim are integers\n            input_dim = getattr(self.astro_module.model, \"input_dim\", None)\n            if (\n                input_dim is not None\n                and not isinstance(input_dim, int)\n                and hasattr(input_dim, \"item\")\n            ):\n                input_dim = int(input_dim.item())\n            output_dim = getattr(self.astro_module.model, \"output_dim\", 1)\n            if (\n                output_dim is not None\n                and not isinstance(output_dim, int)\n                and hasattr(output_dim, \"item\")\n            ):\n                output_dim = int(output_dim.item())\n            model = AstroSurveyGNN(\n                input_dim=input_dim,\n                hidden_dim=params.get(\"hidden_dim\", 128),\n                output_dim=output_dim,\n                num_layers=params.get(\"num_layers\", 3),\n                dropout=params.get(\"dropout\", 0.2),\n                conv_type=\"gcn\",\n            )\n\n        # Create new Lightning module for this trial\n        trial_module = AstroLightningModule(\n            model=model,\n            task_type=self.astro_module.task_type,\n            learning_rate=params.get(\"learning_rate\", 1e-3),\n            weight_decay=params.get(\"weight_decay\", 1e-4),\n            num_classes=self.astro_module.num_classes,\n        )\n\n        # Create trial trainer with minimal callbacks\n        trial_trainer = Trainer(\n            max_epochs=50,  # Longer for better optimization\n            accelerator=self.accelerator,\n            devices=1,  # Single device for optimization\n            enable_checkpointing=False,\n            enable_progress_bar=False,\n            callbacks=[\n                EarlyStopping(\n                    monitor=monitor,\n                    patience=8,\n                    mode=\"min\" if \"loss\" in monitor else \"max\",\n                ),\n                PyTorchLightningPruningCallback(trial, monitor=monitor),\n            ],\n            logger=False,  # Disable logging during optimization\n        )\n\n        # Train and evaluate\n        try:\n            trial_trainer.fit(trial_module, train_dataloader, val_dataloader)\n\n            # Get validation metric\n            val_metric = trial_trainer.callback_metrics.get(monitor, float(\"inf\"))\n            # Always return a float for Optuna compatibility\n            if not isinstance(val_metric, (float, int)) and hasattr(\n                val_metric, \"item\"\n            ):\n                return float(val_metric.item())\n            return float(val_metric)\n\n        except optuna.TrialPruned:\n            raise\n        except Exception as e:\n            logger.error(f\"Trial failed: {e}\")\n            return float(\"inf\") if \"loss\" in monitor else float(\"-inf\")\n\n    # Run optimization\n    logger.info(f\"Starting hyperparameter optimization with {n_trials} trials...\")\n    study.optimize(objective, n_trials=n_trials, timeout=timeout)\n\n    # Store the study for later plotting\n    self._optuna_study = study\n\n    # Get best parameters\n    best_params = study.best_params\n    best_value = study.best_value\n\n    logger.info(\"Optimization complete!\")\n    logger.info(f\"Best value: {best_value:.4f}\")\n    logger.info(f\"Best parameters: {best_params}\")\n\n    # Return results\n    return {\n        \"best_params\": best_params,\n        \"best_value\": best_value,\n        \"n_trials\": len(study.trials),\n        \"study\": study,\n    }\n</code></pre>"},{"location":"api/astro_lab.training/#astro_lab.training.AstroTrainer.predict","title":"predict","text":"<pre><code>predict(\n    predict_dataloader: Optional[DataLoader] = None, datamodule: Any = None\n) -&gt; Optional[List[Any]]\n</code></pre> <p>Make predictions with modern Lightning 2.0+ compatibility.</p> <p>Parameters:</p> Name Type Description Default <code>predict_dataloader</code> <code>Optional[DataLoader]</code> <p>Prediction data loader</p> <code>None</code> <code>datamodule</code> <code>Any</code> <p>Lightning DataModule</p> <code>None</code> <p>Returns:</p> Type Description <code>Optional[List[Any]]</code> <p>Predictions</p> Source code in <code>src\\astro_lab\\training\\trainer.py</code> <pre><code>def predict(\n    self,\n    predict_dataloader: Optional[DataLoader] = None,\n    datamodule: Any = None,\n) -&gt; Optional[List[Any]]:\n    \"\"\"\n    Make predictions with modern Lightning 2.0+ compatibility.\n\n    Args:\n        predict_dataloader: Prediction data loader\n        datamodule: Lightning DataModule\n\n    Returns:\n        Predictions\n    \"\"\"\n    try:\n        if datamodule is not None:\n            return super().predict(\n                model=self.astro_module,\n                datamodule=datamodule,\n            )\n        else:\n            return super().predict(\n                model=self.astro_module,\n                dataloaders=predict_dataloader,\n            )\n    except Exception as e:\n        logger.error(f\"Prediction failed: {e}\")\n        raise\n</code></pre>"},{"location":"api/astro_lab.training/#astro_lab.training.AstroTrainer.resume_from_checkpoint","title":"resume_from_checkpoint","text":"<pre><code>resume_from_checkpoint(checkpoint_path: Union[str, Path]) -&gt; None\n</code></pre> <p>Resume training from checkpoint.</p> Source code in <code>src\\astro_lab\\training\\trainer.py</code> <pre><code>def resume_from_checkpoint(self, checkpoint_path: Union[str, Path]) -&gt; None:\n    \"\"\"Resume training from checkpoint.\"\"\"\n    self.fit(ckpt_path=str(checkpoint_path))\n</code></pre>"},{"location":"api/astro_lab.training/#astro_lab.training.AstroTrainer.save_best_models_to_results","title":"save_best_models_to_results","text":"<pre><code>save_best_models_to_results(top_k: int = 3) -&gt; Dict[str, Path]\n</code></pre> <p>Save best models to results directory with organized structure.</p> <p>Parameters:</p> Name Type Description Default <code>top_k</code> <code>int</code> <p>Number of best models to save</p> <code>3</code> <p>Returns:</p> Type Description <code>Dict[str, Path]</code> <p>Dictionary mapping model names to saved paths</p> Source code in <code>src\\astro_lab\\training\\trainer.py</code> <pre><code>def save_best_models_to_results(self, top_k: int = 3) -&gt; Dict[str, Path]:\n    \"\"\"\n    Save best models to results directory with organized structure.\n\n    Args:\n        top_k: Number of best models to save\n\n    Returns:\n        Dictionary mapping model names to saved paths\n    \"\"\"\n    try:\n        # Get survey name for better organization\n        survey = getattr(self, \"survey\", \"gaia\")\n\n        # Create organized results structure\n        results_dir = Path(f\"./results/{survey}\")\n        models_dir = results_dir / \"models\"\n        plots_dir = results_dir / \"plots\"\n\n        models_dir.mkdir(parents=True, exist_ok=True)\n        plots_dir.mkdir(parents=True, exist_ok=True)\n\n        # Get all checkpoints\n        checkpoint_dir = Path(f\"./experiments/{survey}/checkpoints\")\n        if not checkpoint_dir.exists():\n            logger.error(f\"No checkpoint directory found: {checkpoint_dir}\")\n            return {}\n\n        checkpoints = list(checkpoint_dir.glob(\"*.ckpt\"))\n        if not checkpoints:\n            logger.error(f\"No checkpoints found in: {checkpoint_dir}\")\n            return {}\n\n        # Sort by validation loss (best first)\n        def extract_val_loss(checkpoint_path):\n            try:\n                # Extract validation loss from filename\n                filename = checkpoint_path.stem\n                if \"val_loss=\" in filename:\n                    loss_str = filename.split(\"val_loss=\")[1].split(\"_\")[0]\n                    return float(loss_str)\n                return float(\"inf\")  # Put files without loss info at the end\n            except:\n                return float(\"inf\")\n\n        checkpoints.sort(key=extract_val_loss)\n\n        # Save top-k models with descriptive names\n        saved_models = {}\n        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n\n        for i, checkpoint_path in enumerate(checkpoints[:top_k]):\n            # Create descriptive filename\n            val_loss = extract_val_loss(checkpoint_path)\n            if val_loss == float(\"inf\"):\n                model_name = f\"{survey}_model_{i + 1}_{timestamp}\"\n            else:\n                model_name = (\n                    f\"{survey}_model_{i + 1}_val_loss_{val_loss:.4f}_{timestamp}\"\n                )\n\n            # Save to results/models\n            target_path = models_dir / f\"{model_name}.ckpt\"\n\n            try:\n                shutil.copy2(checkpoint_path, target_path)\n                saved_models[model_name] = target_path\n            except Exception as e:\n                logger.error(f\"Failed to save model {i + 1}: {e}\")\n\n        # Create README with model information\n        self._create_models_readme(saved_models, checkpoints[:top_k])\n\n        logger.info(f\"Saved {len(saved_models)} models to: {models_dir}\")\n        return saved_models\n\n    except Exception as e:\n        logger.error(f\"Failed to save models to results: {e}\")\n        return {}\n</code></pre>"},{"location":"api/astro_lab.training/#astro_lab.training.AstroTrainer.save_model","title":"save_model","text":"<pre><code>save_model(path: str) -&gt; None\n</code></pre> <p>Save model to path.</p> Source code in <code>src\\astro_lab\\training\\trainer.py</code> <pre><code>def save_model(self, path: str) -&gt; None:\n    \"\"\"Save model to path.\"\"\"\n    if self.astro_module is not None:\n        torch.save(self.astro_module.state_dict(), path)\n</code></pre>"},{"location":"api/astro_lab.training/#astro_lab.training.AstroTrainer.test","title":"test","text":"<pre><code>test(\n    test_dataloader: Optional[DataLoader] = None, datamodule: Any = None\n) -&gt; List[Mapping[str, float]]\n</code></pre> <p>Test the model with modern Lightning 2.0+ compatibility.</p> <p>Parameters:</p> Name Type Description Default <code>test_dataloader</code> <code>Optional[DataLoader]</code> <p>Test data loader</p> <code>None</code> <code>datamodule</code> <code>Any</code> <p>Lightning DataModule</p> <code>None</code> <p>Returns:</p> Type Description <code>List[Mapping[str, float]]</code> <p>Test results</p> Source code in <code>src\\astro_lab\\training\\trainer.py</code> <pre><code>def test(\n    self,\n    test_dataloader: Optional[DataLoader] = None,\n    datamodule: Any = None,\n) -&gt; List[Mapping[str, float]]:\n    \"\"\"\n    Test the model with modern Lightning 2.0+ compatibility.\n\n    Args:\n        test_dataloader: Test data loader\n        datamodule: Lightning DataModule\n\n    Returns:\n        Test results\n    \"\"\"\n    try:\n        if datamodule is not None:\n            results = super().test(\n                model=self.astro_module,\n                datamodule=datamodule,\n            )\n        else:\n            results = super().test(\n                model=self.astro_module,\n                dataloaders=test_dataloader,\n            )\n\n        # Cleanup after testing\n        self._cleanup_after_training()\n        return results\n\n    except Exception as e:\n        logger.error(f\"Testing failed: {e}\")\n        self._cleanup_after_training()\n        raise\n</code></pre>"},{"location":"api/astro_lab.training/#astro_lab.training.TrainingConfig","title":"TrainingConfig","text":"<p>               Bases: <code>BaseModel</code></p> <p>Complete training configuration integrating model and training settings.</p> <p>Methods:</p> Name Description <code>get_optimizer_config</code> <p>Get optimizer configuration.</p> <code>get_scheduler_config</code> <p>Get scheduler configuration.</p> <code>get_survey_config</code> <p>Get configuration with survey-specific overrides.</p> <code>validate_hardware_compatibility</code> <p>Validate that the configuration is compatible with available hardware.</p> Source code in <code>src\\astro_lab\\training\\config.py</code> <pre><code>class TrainingConfig(BaseModel):\n    \"\"\"Complete training configuration integrating model and training settings.\"\"\"\n\n    # Basic training info\n    name: str = Field(..., min_length=1)\n    version: str = \"1.0.0\"\n    description: Optional[str] = None\n\n    # Model configuration\n    model: ModelConfig\n\n    # Training components\n    scheduler: SchedulerConfig = Field(default_factory=SchedulerConfig)\n    callbacks: CallbackConfig = Field(default_factory=CallbackConfig)\n    hardware: HardwareConfig = Field(default_factory=HardwareConfig)\n    logging: LoggingConfig = Field(default_factory=LoggingConfig)\n    data: DataConfig = Field(default_factory=DataConfig)\n\n    # Override model training config with training-specific settings\n    training: Optional[Dict[str, Any]] = None\n\n    # Survey-specific overrides\n    survey_overrides: Dict[str, Dict[str, Union[str, int, float, bool]]] = Field(\n        default_factory=dict\n    )\n\n    class Config:\n        extra = \"forbid\"\n\n    def __init__(self, **data):\n        super().__init__(**data)\n\n        # Sync training config if provided\n        if self.training is not None:\n            self.scheduler.learning_rate = self.training.get(\n                \"learning_rate\", self.scheduler.learning_rate\n            )\n            self.scheduler.weight_decay = self.training.get(\n                \"weight_decay\", self.scheduler.weight_decay\n            )\n            self.scheduler.max_epochs = self.training.get(\n                \"num_epochs\", self.scheduler.max_epochs\n            )\n            self.data.batch_size = self.training.get(\"batch_size\", self.data.batch_size)\n            self.hardware.gradient_clip_val = self.training.get(\n                \"gradient_clip_val\", self.hardware.gradient_clip_val\n            )\n\n    def get_survey_config(self, survey: str) -&gt; \"TrainingConfig\":\n        \"\"\"Get configuration with survey-specific overrides.\"\"\"\n        if survey not in self.survey_overrides:\n            return self\n\n        # Create a copy with overrides\n        config_dict = self.dict()\n        overrides = self.survey_overrides[survey]\n\n        # Apply overrides recursively\n        for key, value in overrides.items():\n            keys = key.split(\".\")\n            current = config_dict\n            for k in keys[:-1]:\n                current = current[k]\n            current[keys[-1]] = value\n\n        return TrainingConfig(**config_dict)\n\n    def validate_hardware_compatibility(self) -&gt; bool:\n        \"\"\"Validate that the configuration is compatible with available hardware.\"\"\"\n        # Check CUDA availability\n        if self.hardware.accelerator == \"gpu\" and not torch.cuda.is_available():\n            return False\n\n        # Check precision compatibility\n        if (\n            self.hardware.precision in [\"16\", \"16-mixed\"]\n            and not torch.cuda.is_available()\n        ):\n            return False\n\n            # Model compatibility check not needed with simplified config\n        return True\n\n    def get_optimizer_config(self) -&gt; Dict[str, Any]:\n        \"\"\"Get optimizer configuration.\"\"\"\n        return {\n            \"lr\": self.scheduler.learning_rate,\n            \"weight_decay\": self.scheduler.weight_decay,\n        }\n\n    def get_scheduler_config(self) -&gt; Dict[str, Any]:\n        \"\"\"Get scheduler configuration.\"\"\"\n        base_config = {\n            \"scheduler_type\": self.scheduler.scheduler_type,\n            \"max_epochs\": self.scheduler.max_epochs,\n            \"warmup_epochs\": self.scheduler.warmup_epochs,\n        }\n\n        if self.scheduler.scheduler_type == \"cosine\":\n            base_config.update(\n                {\n                    \"eta_min\": self.scheduler.eta_min,\n                }\n            )\n        elif self.scheduler.scheduler_type == \"onecycle\":\n            base_config.update(\n                {\n                    \"div_factor\": self.scheduler.div_factor,\n                    \"final_div_factor\": self.scheduler.final_div_factor,\n                }\n            )\n        elif self.scheduler.scheduler_type == \"plateau\":\n            base_config.update(\n                {\n                    \"patience\": self.scheduler.patience,\n                    \"factor\": self.scheduler.factor,\n                    \"min_lr\": self.scheduler.min_lr,\n                }\n            )\n        elif self.scheduler.scheduler_type == \"step\":\n            base_config.update(\n                {\n                    \"step_size\": self.scheduler.step_size,\n                    \"gamma\": self.scheduler.gamma,\n                }\n            )\n        elif self.scheduler.scheduler_type == \"exponential\":\n            base_config.update(\n                {\n                    \"decay_rate\": self.scheduler.decay_rate,\n                }\n            )\n\n        return base_config\n</code></pre>"},{"location":"api/astro_lab.training/#astro_lab.training.TrainingConfig.get_optimizer_config","title":"get_optimizer_config","text":"<pre><code>get_optimizer_config() -&gt; Dict[str, Any]\n</code></pre> <p>Get optimizer configuration.</p> Source code in <code>src\\astro_lab\\training\\config.py</code> <pre><code>def get_optimizer_config(self) -&gt; Dict[str, Any]:\n    \"\"\"Get optimizer configuration.\"\"\"\n    return {\n        \"lr\": self.scheduler.learning_rate,\n        \"weight_decay\": self.scheduler.weight_decay,\n    }\n</code></pre>"},{"location":"api/astro_lab.training/#astro_lab.training.TrainingConfig.get_scheduler_config","title":"get_scheduler_config","text":"<pre><code>get_scheduler_config() -&gt; Dict[str, Any]\n</code></pre> <p>Get scheduler configuration.</p> Source code in <code>src\\astro_lab\\training\\config.py</code> <pre><code>def get_scheduler_config(self) -&gt; Dict[str, Any]:\n    \"\"\"Get scheduler configuration.\"\"\"\n    base_config = {\n        \"scheduler_type\": self.scheduler.scheduler_type,\n        \"max_epochs\": self.scheduler.max_epochs,\n        \"warmup_epochs\": self.scheduler.warmup_epochs,\n    }\n\n    if self.scheduler.scheduler_type == \"cosine\":\n        base_config.update(\n            {\n                \"eta_min\": self.scheduler.eta_min,\n            }\n        )\n    elif self.scheduler.scheduler_type == \"onecycle\":\n        base_config.update(\n            {\n                \"div_factor\": self.scheduler.div_factor,\n                \"final_div_factor\": self.scheduler.final_div_factor,\n            }\n        )\n    elif self.scheduler.scheduler_type == \"plateau\":\n        base_config.update(\n            {\n                \"patience\": self.scheduler.patience,\n                \"factor\": self.scheduler.factor,\n                \"min_lr\": self.scheduler.min_lr,\n            }\n        )\n    elif self.scheduler.scheduler_type == \"step\":\n        base_config.update(\n            {\n                \"step_size\": self.scheduler.step_size,\n                \"gamma\": self.scheduler.gamma,\n            }\n        )\n    elif self.scheduler.scheduler_type == \"exponential\":\n        base_config.update(\n            {\n                \"decay_rate\": self.scheduler.decay_rate,\n            }\n        )\n\n    return base_config\n</code></pre>"},{"location":"api/astro_lab.training/#astro_lab.training.TrainingConfig.get_survey_config","title":"get_survey_config","text":"<pre><code>get_survey_config(survey: str) -&gt; TrainingConfig\n</code></pre> <p>Get configuration with survey-specific overrides.</p> Source code in <code>src\\astro_lab\\training\\config.py</code> <pre><code>def get_survey_config(self, survey: str) -&gt; \"TrainingConfig\":\n    \"\"\"Get configuration with survey-specific overrides.\"\"\"\n    if survey not in self.survey_overrides:\n        return self\n\n    # Create a copy with overrides\n    config_dict = self.dict()\n    overrides = self.survey_overrides[survey]\n\n    # Apply overrides recursively\n    for key, value in overrides.items():\n        keys = key.split(\".\")\n        current = config_dict\n        for k in keys[:-1]:\n            current = current[k]\n        current[keys[-1]] = value\n\n    return TrainingConfig(**config_dict)\n</code></pre>"},{"location":"api/astro_lab.training/#astro_lab.training.TrainingConfig.validate_hardware_compatibility","title":"validate_hardware_compatibility","text":"<pre><code>validate_hardware_compatibility() -&gt; bool\n</code></pre> <p>Validate that the configuration is compatible with available hardware.</p> Source code in <code>src\\astro_lab\\training\\config.py</code> <pre><code>def validate_hardware_compatibility(self) -&gt; bool:\n    \"\"\"Validate that the configuration is compatible with available hardware.\"\"\"\n    # Check CUDA availability\n    if self.hardware.accelerator == \"gpu\" and not torch.cuda.is_available():\n        return False\n\n    # Check precision compatibility\n    if (\n        self.hardware.precision in [\"16\", \"16-mixed\"]\n        and not torch.cuda.is_available()\n    ):\n        return False\n\n        # Model compatibility check not needed with simplified config\n    return True\n</code></pre>"},{"location":"api/astro_lab.training/#astro_lab.training.optimize_hyperparameters","title":"optimize_hyperparameters","text":"<pre><code>optimize_hyperparameters(\n    config_path: Union[str, Path], n_trials: int = 10, **kwargs\n) -&gt; int\n</code></pre> <p>Run hyperparameter optimization. Args:     config_path: Path to configuration file     n_trials: Number of optimization trials     **kwargs: Additional arguments Returns:     Exit code (0 for success)</p> Source code in <code>src\\astro_lab\\training\\__init__.py</code> <pre><code>def optimize_hyperparameters(\n    config_path: Union[str, Path], n_trials: int = 10, **kwargs\n) -&gt; int:\n    \"\"\"\n    Run hyperparameter optimization.\n    Args:\n        config_path: Path to configuration file\n        n_trials: Number of optimization trials\n        **kwargs: Additional arguments\n    Returns:\n        Exit code (0 for success)\n    \"\"\"\n    from astro_lab.data import create_astro_datamodule\n    from astro_lab.models.config import ModelConfig\n    from astro_lab.utils.config.loader import ConfigLoader\n\n    try:\n        config_loader = ConfigLoader(config_path)\n        config_loader.load_config()\n        training_dict = config_loader.get_training_config()\n        model_dict = config_loader.get_model_config()\n        model_config = ModelConfig(**model_dict)\n        training_config = TrainingConfig(\n            name=training_dict.get(\"name\", \"optimization_training\"),\n            model=model_config,\n            **{k: v for k, v in training_dict.items() if k not in [\"name\", \"model\"]},\n        )\n        survey = model_dict.get(\"name\", \"gaia\")\n        datamodule = create_astro_datamodule(\n            survey=survey,\n            batch_size=training_dict.get(\"data\", {}).get(\"batch_size\", 32),\n            max_samples=training_dict.get(\"data\", {}).get(\"max_samples\", 1000),\n        )\n        datamodule.setup()\n        trainer = AstroTrainer(training_config=training_config)\n        trainer.optimize_hyperparameters(\n            train_dataloader=datamodule.train_dataloader(),\n            val_dataloader=datamodule.val_dataloader(),\n            n_trials=n_trials,\n        )\n        return 0\n    except Exception as e:\n        logger.error(f\"Optimization failed: {e}\")\n        return 1\n</code></pre>"},{"location":"api/astro_lab.training/#astro_lab.training.run_training","title":"run_training","text":"<pre><code>run_training(args)\n</code></pre> <p>Run training based on CLI arguments.</p> Source code in <code>src\\astro_lab\\training\\__init__.py</code> <pre><code>def run_training(args):\n    \"\"\"Run training based on CLI arguments.\"\"\"\n    if args.config:\n        # Load config and create TrainingConfig object\n        config_loader = ConfigLoader(args.config)\n        config_loader.load_config()\n\n        # Get config sections\n        training_dict = config_loader.get_training_config()\n        model_dict = config_loader.get_model_config()\n\n        # Create proper config objects\n        model_config = ModelConfig(**model_dict)\n        training_config = TrainingConfig(\n            name=training_dict.get(\"name\", \"config_training\"),\n            model=model_config,\n            **{k: v for k, v in training_dict.items() if k != \"name\" and k != \"model\"},\n        )\n\n        # Create DataModule with survey from config\n        survey = model_dict.get(\"name\", \"gaia\")  # Default to gaia if not specified\n        datamodule = AstroDataModule(\n            survey=survey,\n            batch_size=args.batch_size,\n            max_samples=args.max_samples,\n        )\n\n        # Setup datamodule to detect classes\n        datamodule.setup()\n\n        # Update model config with detected classes if needed\n        if datamodule.num_classes:\n            model_config.output_dim = datamodule.num_classes\n            logger.info(f\"Detected {datamodule.num_classes} classes from data\")\n\n        trainer = AstroTrainer(training_config=training_config)\n        trainer.fit(datamodule=datamodule)\n\n    elif args.dataset and args.model:\n        # Quick training with minimal config\n        # Create DataModule first to detect classes\n        datamodule = AstroDataModule(\n            survey=args.dataset,\n            batch_size=args.batch_size,\n            max_samples=args.max_samples,\n        )\n\n        # Setup datamodule to detect classes\n        datamodule.setup()\n\n        if not datamodule.num_classes:\n            raise ValueError(\n                f\"Could not detect number of classes from {args.dataset} dataset. \"\n                \"Please check your data.\"\n            )\n\n        logger.info(\n            f\"Detected {datamodule.num_classes} classes from {args.dataset} data\"\n        )\n\n        # IMPORTANT: For factory models like gaia_classifier, we need special handling\n        if args.model in [\"gaia_classifier\", \"lsst_transient\", \"lightcurve_classifier\"]:\n            # These are classification models that need num_classes\n            model_config = ModelConfig(\n                name=args.model,\n                output_dim=datamodule.num_classes,\n                task=\"classification\",\n            )\n        elif args.model in [\"sdss_galaxy\", \"galaxy_modeler\"]:\n            # These are regression models - use default output dims\n            model_config = ModelConfig(\n                name=args.model,\n                task=\"regression\",\n                # output_dim will be set by the factory based on the specific model\n            )\n        else:\n            # Generic model\n            model_config = ModelConfig(\n                name=args.model,\n                output_dim=datamodule.num_classes,\n            )\n\n        training_config = TrainingConfig(\n            name=\"quick_training\",\n            model=model_config,\n            scheduler={\"max_epochs\": args.epochs},\n            hardware={\"devices\": args.devices, \"precision\": args.precision},\n        )\n\n        trainer = AstroTrainer(training_config=training_config)\n\n        # Final validation before training\n        logger.info(\n            f\"Starting training with model={args.model}, num_classes={datamodule.num_classes}\"\n        )\n\n        trainer.fit(datamodule=datamodule)\n    else:\n        print(\"Error: Either --config or both --dataset and --model must be specified\")\n        return 1\n\n    return 0\n</code></pre>"},{"location":"api/astro_lab.training/#astro_lab.training.train_from_config","title":"train_from_config","text":"<pre><code>train_from_config(config_path: Union[str, Path], **kwargs) -&gt; int\n</code></pre> <p>Train a model from configuration file. Args:     config_path: Path to configuration file     **kwargs: Additional arguments to override config Returns:     Exit code (0 for success)</p> Source code in <code>src\\astro_lab\\training\\__init__.py</code> <pre><code>def train_from_config(config_path: Union[str, Path], **kwargs) -&gt; int:\n    \"\"\"\n    Train a model from configuration file.\n    Args:\n        config_path: Path to configuration file\n        **kwargs: Additional arguments to override config\n    Returns:\n        Exit code (0 for success)\n    \"\"\"\n    from astro_lab.data import create_astro_datamodule\n    from astro_lab.models.config import ModelConfig\n    from astro_lab.utils.config.loader import ConfigLoader\n\n    try:\n        config_loader = ConfigLoader(config_path)\n        config_loader.load_config()\n        training_dict = config_loader.get_training_config()\n        model_dict = config_loader.get_model_config()\n        model_config = ModelConfig(**model_dict)\n        training_config = TrainingConfig(\n            name=training_dict.get(\"name\", \"config_training\"),\n            model=model_config,\n            **{k: v for k, v in training_dict.items() if k not in [\"name\", \"model\"]},\n        )\n        # Override with kwargs\n        if \"epochs\" in kwargs and kwargs[\"epochs\"] is not None:\n            training_config.scheduler[\"max_epochs\"] = kwargs[\"epochs\"]\n        if \"learning_rate\" in kwargs and kwargs[\"learning_rate\"] is not None:\n            training_config.optimizer[\"lr\"] = kwargs[\"learning_rate\"]\n        if \"devices\" in kwargs and kwargs[\"devices\"] is not None:\n            training_config.hardware[\"devices\"] = kwargs[\"devices\"]\n        if \"precision\" in kwargs and kwargs[\"precision\"] is not None:\n            training_config.hardware[\"precision\"] = kwargs[\"precision\"]\n        survey = model_dict.get(\"name\", \"gaia\")\n        datamodule = create_astro_datamodule(\n            survey=survey,\n            batch_size=kwargs.get(\"batch_size\", 32),\n            max_samples=kwargs.get(\"max_samples\", 1000),\n        )\n        datamodule.setup()\n        trainer = AstroTrainer(training_config=training_config)\n        trainer.fit(datamodule=datamodule)\n        return 0\n    except FileNotFoundError:\n        logger.error(f\"Configuration file not found: {config_path}\")\n        return 1\n    except Exception as e:\n        logger.error(f\"Training failed: {e}\")\n        return 1\n</code></pre>"},{"location":"api/astro_lab.training/#astro_lab.training.train_quick","title":"train_quick","text":"<pre><code>train_quick(dataset: str, model: str, **kwargs) -&gt; int\n</code></pre> <p>Quick training with dataset and model names. Args:     dataset: Dataset name (e.g., 'gaia', 'sdss')     model: Model preset name     **kwargs: Training parameters Returns:     Exit code (0 for success)</p> Source code in <code>src\\astro_lab\\training\\__init__.py</code> <pre><code>def train_quick(dataset: str, model: str, **kwargs) -&gt; int:\n    \"\"\"\n    Quick training with dataset and model names.\n    Args:\n        dataset: Dataset name (e.g., 'gaia', 'sdss')\n        model: Model preset name\n        **kwargs: Training parameters\n    Returns:\n        Exit code (0 for success)\n    \"\"\"\n    from astro_lab.data import create_astro_datamodule\n    from astro_lab.models.config import ModelConfig\n\n    MODEL_PRESETS = {\n        \"gaia_classifier\": {\"task\": \"classification\"},\n        \"lsst_transient\": {\"task\": \"classification\"},\n        \"lightcurve_classifier\": {\"task\": \"classification\"},\n        \"sdss_galaxy\": {\"task\": \"regression\"},\n        \"galaxy_modeler\": {\"task\": \"regression\"},\n    }\n    try:\n        if model not in MODEL_PRESETS:\n            available = \", \".join(MODEL_PRESETS.keys())\n            logger.error(f\"Unknown model '{model}'. Available models: {available}\")\n            return 1\n        datamodule = create_astro_datamodule(\n            survey=dataset,\n            batch_size=kwargs.get(\"batch_size\", 32),\n            max_samples=kwargs.get(\"max_samples\", 1000),\n        )\n        datamodule.setup()\n        num_classes = getattr(datamodule, \"num_classes\", None)\n        if MODEL_PRESETS[model][\"task\"] == \"classification\" and not num_classes:\n            logger.error(\n                f\"Could not detect number of classes for {dataset} dataset. Please check your data.\"\n            )\n            return 1\n        model_config = ModelConfig(\n            name=model,\n            task=MODEL_PRESETS[model][\"task\"],\n            output_dim=num_classes if num_classes else 1,\n        )\n        training_config = TrainingConfig(\n            name=f\"quick_{dataset}_{model}\",\n            model=model_config,\n            scheduler={\n                \"max_epochs\": kwargs.get(\"epochs\", 10),\n                \"learning_rate\": kwargs.get(\"learning_rate\", 0.001),\n            },\n            hardware={\n                \"devices\": kwargs.get(\"devices\", 1),\n                \"precision\": kwargs.get(\"precision\", \"16-mixed\"),\n                \"strategy\": kwargs.get(\"strategy\", \"auto\"),\n            },\n        )\n        trainer = AstroTrainer(training_config=training_config)\n        trainer.fit(datamodule=datamodule)\n        return 0\n    except Exception as e:\n        logger.error(f\"Training failed: {e}\")\n        return 1\n</code></pre>"},{"location":"api/astro_lab.ui/","title":"astro_lab.ui","text":""},{"location":"api/astro_lab.ui/#astro_lab.ui","title":"ui","text":""},{"location":"api/astro_lab.ui/#astro_lab.ui--astrolab-ui-module","title":"AstroLab UI Module","text":"<p>Clean marimo-based user interface for AstroLab. Integrated with the AstroLab widget system for advanced functionality.</p>"},{"location":"api/astro_lab.ui/#astro_lab.ui--quick-start","title":"Quick Start","text":"<pre><code>import marimo as mo\nfrom astro_lab.ui import create_astrolab_dashboard\n\n# Create full dashboard\ndashboard = create_astrolab_dashboard()\n\n# Or use individual components\nfrom astro_lab.ui import (\n    ui_config_loader,\n    ui_quick_setup,\n    ui_model_selector,\n    ui_graph_controls,\n    handle_component_actions,\n    WIDGETS_AVAILABLE\n)\n\nconfig_loader = ui_config_loader()\nquick_setup = ui_quick_setup()\n\n# Check if advanced widget features are available\nif WIDGETS_AVAILABLE:\n    graph_controls = ui_graph_controls()\n</code></pre>"},{"location":"api/astro_lab.ui/#astro_lab.ui--integration","title":"Integration","text":"<ul> <li>ConfigLoader: Uses real AstroLab configuration system</li> <li>data_config: Integrates with path management</li> <li>Model Configs: Works with actual model configurations</li> <li>Training Configs: Supports predefined training setups</li> <li>AstroLab Widgets: Advanced visualization and analysis (if available)</li> </ul> <p>Modules:</p> Name Description <code>components</code> <p>AstroLab UI Components - Marimo UI Components</p> <code>dashboard</code> <p>AstroLab Dashboard</p> <code>settings</code> <p>AstroLab UI Settings Integration</p> <p>Classes:</p> Name Description <code>AstroLabDashboard</code> <p>Complete AstroLab dashboard with integrated components.</p> <code>UIConfigManager</code> <p>UI interface for AstroLab's configuration system.</p> <p>Functions:</p> Name Description <code>create_analysis_dashboard</code> <p>Create an analysis-focused dashboard with widget integration.</p> <code>create_astrolab_dashboard</code> <p>Create the complete AstroLab dashboard.</p> <code>create_config_dashboard</code> <p>Create a configuration-focused dashboard.</p> <code>create_dashboard</code> <p>Create a dashboard of specified type.</p> <code>create_minimal_dashboard</code> <p>Create a minimal dashboard for quick analysis.</p> <code>create_widget_showcase</code> <p>Create a dashboard showcasing AstroLab widget capabilities.</p> <code>handle_component_actions</code> <p>Handle actions from UI components.</p> <code>handle_config_actions</code> <p>Handle configuration-related actions from UI components.</p> <code>ui_analysis_controls</code> <p>Analysis controls using AstroLab widget functionality.</p> <code>ui_config_loader</code> <p>Configuration loader UI component.</p> <code>ui_config_status</code> <p>Configuration status display.</p> <code>ui_data_controls</code> <p>Data loading and management controls.</p> <code>ui_data_paths</code> <p>Data paths configuration UI component.</p> <code>ui_experiment_manager</code> <p>Experiment management UI component.</p> <code>ui_graph_controls</code> <p>Graph analysis controls using AstroLab widget.</p> <code>ui_model_controls</code> <p>Model training controls.</p> <code>ui_model_selector</code> <p>Model selection UI component.</p> <code>ui_quick_actions</code> <p>Quick action buttons for common tasks.</p> <code>ui_quick_setup</code> <p>Quick setup component for common workflows.</p> <code>ui_survey_selector</code> <p>Survey selection and configuration UI component.</p> <code>ui_system_status</code> <p>System status and information.</p> <code>ui_training_selector</code> <p>Training configuration selector UI component.</p> <code>ui_visualization_controls</code> <p>Visualization controls using AstroLab widget backends.</p>"},{"location":"api/astro_lab.ui/#astro_lab.ui.AstroLabDashboard","title":"AstroLabDashboard","text":"<p>Complete AstroLab dashboard with integrated components.</p> <p>Methods:</p> Name Description <code>create_full_dashboard</code> <p>Create the complete dashboard layout.</p> <code>create_main_tabs</code> <p>Create main dashboard tabs.</p> <code>create_sidebar</code> <p>Create dashboard sidebar.</p> <code>create_welcome_screen</code> <p>Create welcome screen.</p> <code>handle_interactions</code> <p>Handle all dashboard interactions.</p> Source code in <code>src\\astro_lab\\ui\\dashboard.py</code> <pre><code>class AstroLabDashboard:\n    \"\"\"Complete AstroLab dashboard with integrated components.\"\"\"\n\n    def __init__(self):\n        \"\"\"Initialize the dashboard.\"\"\"\n        self.current_data = None\n        self.status_message = \"Ready\"\n\n    def create_main_tabs(self) -&gt; mo.ui.tabs:\n        \"\"\"Create main dashboard tabs.\"\"\"\n\n        # Data &amp; Analysis Tab\n        data_tab = mo.vstack(\n            [\n                mo.md(\"## \ud83d\udcca Data Management\"),\n                ui_data_controls(),\n                mo.md(\"## \ud83d\udd2c Analysis\"),\n                ui_analysis_controls(),\n                mo.md(\"## \ud83d\udcc8 Visualization\"),\n                ui_visualization_controls(),\n            ]\n        )\n\n        #  Analysis Tab (with graph functionality)\n        advanced_tab = mo.vstack(\n            [\n                mo.md(\"## \ud83d\udd78\ufe0f Graph Analysis\"),\n                ui_graph_controls(),\n                mo.md(\"## \ud83e\udd16 Model Training\"),\n                ui_model_controls(),\n                mo.md(\"## \ud83d\udcca System Status\"),\n                ui_system_status(),\n            ]\n        )\n\n        # Configuration Tab\n        config_tab = mo.vstack(\n            [\n                mo.md(\"## \ud83d\ude80 Quick Setup\"),\n                ui_quick_setup(),\n                mo.md(\"## \ud83d\udcc2 Configuration\"),\n                ui_config_loader(),\n                mo.md(\"## \ud83d\udcca Survey Selection\"),\n                ui_survey_selector(),\n                mo.md(\"## \ud83e\udd16 Model Selection\"),\n                ui_model_selector(),\n            ]\n        )\n\n        # Management Tab\n        management_tab = mo.vstack(\n            [\n                mo.md(\"## \ud83e\uddea Experiment Management\"),\n                ui_experiment_manager(),\n                mo.md(\"## \ud83d\udcc1 Data Paths\"),\n                ui_data_paths(),\n                mo.md(\"## \u2139\ufe0f Configuration Status\"),\n                ui_config_status(),\n            ]\n        )\n\n        return mo.ui.tabs(\n            {\n                \"\ud83d\ude80 Main\": data_tab,\n                \"\ud83d\udd2c \": advanced_tab,\n                \"\u2699\ufe0f Config\": config_tab,\n                \"\ud83d\udd27 Manage\": management_tab,\n            }\n        )\n\n    def create_sidebar(self) -&gt; mo.vstack:\n        \"\"\"Create dashboard sidebar.\"\"\"\n        widget_status = \"\ud83d\udfe2 Available\" if WIDGETS_AVAILABLE else \"\ud83d\udd34 Not Available\"\n\n        return mo.vstack(\n            [\n                mo.md(\"# \ud83c\udf1f AstroLab\"),\n                mo.md(\"* Astronomical Data Analysis*\"),\n                mo.md(\"---\"),\n                mo.md(\"### \u26a1 Quick Actions\"),\n                ui_quick_actions(),\n                mo.md(\"### \ud83d\udcca System Status\"),\n                ui_system_status(),\n                mo.md(f\"**AstroLab Widgets:** {widget_status}\"),\n                mo.md(\"---\"),\n                mo.md(\"### \ud83d\udcda Resources\"),\n                mo.md(\"\"\"\n- [Documentation](https://astro-lab.readthedocs.io)\n- [GitHub](https://github.com/astro-lab/astro-lab)\n- [Examples](examples/)\n            \"\"\"),\n            ]\n        )\n\n    def create_welcome_screen(self) -&gt; mo.vstack:\n        \"\"\"Create welcome screen.\"\"\"\n        features_text = \"\"\"\n## Modern Astronomical Data Analysis Platform\n\nAstroLab provides comprehensive tools for astronomical data analysis,\nmachine learning, and interactive visualization.\n\n### \ud83d\ude80 Getting Started\n\n1. **Configure**: Set up your experiment and data paths\n2. **Load Data**: Choose a survey and load astronomical data\n3. **Visualize**: Create interactive plots and visualizations\n4. **Analyze**: Run clustering, classification, and analysis\n5. **Model**: Train machine learning models on your data\n\n### \ud83d\udcca Supported Surveys\n- **Gaia**: European Space Agency's stellar survey\n- **SDSS**: Sloan Digital Sky Survey\n- **NSA**: NASA-Sloan Atlas\n- **LINEAR**: Linear Asteroid Survey\n- **TNG50**: IllustrisTNG simulation data\n\n### \ud83e\uddf0 Available Tools\n- Interactive plotting with Plotly, Matplotlib, Bokeh\"\"\"\n\n        if WIDGETS_AVAILABLE:\n            features_text += \"\"\"\n- ** 3D visualization** with Open3D, PyVista, Blender\n- **GPU-accelerated analysis** and clustering\n- **Graph-based analysis** with PyTorch Geometric\n- **High-performance neighbor finding**\"\"\"\n        else:\n            features_text += \"\"\"\n- Graph-based analysis and clustering\n- Neural networks and machine learning\"\"\"\n\n        features_text += \"\"\"\n- GPU acceleration support\n            \"\"\"\n\n        return mo.vstack(\n            [\n                mo.md(\"# \ud83c\udf1f Welcome to AstroLab\"),\n                mo.md(features_text),\n                mo.md(\"### \ud83c\udfaf Quick Start\"),\n                mo.hstack(\n                    [\n                        mo.ui.button(label=\"\ud83d\udcca Load Sample Data\"),\n                        mo.ui.button(label=\"\ud83c\udfa8 Create Plot\"),\n                        mo.ui.button(label=\"\ud83e\udd16 Train Model\"),\n                    ]\n                ),\n            ]\n        )\n\n    def create_full_dashboard(self) -&gt; mo.vstack:\n        \"\"\"Create the complete dashboard layout.\"\"\"\n        header = mo.md(\"# \ud83c\udf1f AstroLab Dashboard\")\n\n        welcome = self.create_welcome_screen()\n        main_tabs = self.create_main_tabs()\n        sidebar = self.create_sidebar()\n\n        # Main content area\n        main_content = mo.hstack(\n            [\n                mo.vstack([welcome, main_tabs]),\n                sidebar,\n            ]\n        )\n\n        return mo.vstack(\n            [\n                header,\n                mo.md(\"---\"),\n                main_content,\n            ]\n        )\n\n    def handle_interactions(self, components: Dict[str, Any]) -&gt; Optional[str]:\n        \"\"\"Handle all dashboard interactions.\"\"\"\n\n        # Handle component actions\n        component_result = handle_component_actions(components)\n        if component_result:\n            self.status_message = component_result\n            return component_result\n\n        # Handle config actions\n        config_result = handle_config_actions(components)\n        if config_result:\n            self.status_message = \"\u2705 Configuration action completed\"\n            return self.status_message\n\n        return None\n</code></pre>"},{"location":"api/astro_lab.ui/#astro_lab.ui.AstroLabDashboard.create_full_dashboard","title":"create_full_dashboard","text":"<pre><code>create_full_dashboard() -&gt; vstack\n</code></pre> <p>Create the complete dashboard layout.</p> Source code in <code>src\\astro_lab\\ui\\dashboard.py</code> <pre><code>def create_full_dashboard(self) -&gt; mo.vstack:\n    \"\"\"Create the complete dashboard layout.\"\"\"\n    header = mo.md(\"# \ud83c\udf1f AstroLab Dashboard\")\n\n    welcome = self.create_welcome_screen()\n    main_tabs = self.create_main_tabs()\n    sidebar = self.create_sidebar()\n\n    # Main content area\n    main_content = mo.hstack(\n        [\n            mo.vstack([welcome, main_tabs]),\n            sidebar,\n        ]\n    )\n\n    return mo.vstack(\n        [\n            header,\n            mo.md(\"---\"),\n            main_content,\n        ]\n    )\n</code></pre>"},{"location":"api/astro_lab.ui/#astro_lab.ui.AstroLabDashboard.create_main_tabs","title":"create_main_tabs","text":"<pre><code>create_main_tabs() -&gt; tabs\n</code></pre> <p>Create main dashboard tabs.</p> Source code in <code>src\\astro_lab\\ui\\dashboard.py</code> <pre><code>def create_main_tabs(self) -&gt; mo.ui.tabs:\n    \"\"\"Create main dashboard tabs.\"\"\"\n\n    # Data &amp; Analysis Tab\n    data_tab = mo.vstack(\n        [\n            mo.md(\"## \ud83d\udcca Data Management\"),\n            ui_data_controls(),\n            mo.md(\"## \ud83d\udd2c Analysis\"),\n            ui_analysis_controls(),\n            mo.md(\"## \ud83d\udcc8 Visualization\"),\n            ui_visualization_controls(),\n        ]\n    )\n\n    #  Analysis Tab (with graph functionality)\n    advanced_tab = mo.vstack(\n        [\n            mo.md(\"## \ud83d\udd78\ufe0f Graph Analysis\"),\n            ui_graph_controls(),\n            mo.md(\"## \ud83e\udd16 Model Training\"),\n            ui_model_controls(),\n            mo.md(\"## \ud83d\udcca System Status\"),\n            ui_system_status(),\n        ]\n    )\n\n    # Configuration Tab\n    config_tab = mo.vstack(\n        [\n            mo.md(\"## \ud83d\ude80 Quick Setup\"),\n            ui_quick_setup(),\n            mo.md(\"## \ud83d\udcc2 Configuration\"),\n            ui_config_loader(),\n            mo.md(\"## \ud83d\udcca Survey Selection\"),\n            ui_survey_selector(),\n            mo.md(\"## \ud83e\udd16 Model Selection\"),\n            ui_model_selector(),\n        ]\n    )\n\n    # Management Tab\n    management_tab = mo.vstack(\n        [\n            mo.md(\"## \ud83e\uddea Experiment Management\"),\n            ui_experiment_manager(),\n            mo.md(\"## \ud83d\udcc1 Data Paths\"),\n            ui_data_paths(),\n            mo.md(\"## \u2139\ufe0f Configuration Status\"),\n            ui_config_status(),\n        ]\n    )\n\n    return mo.ui.tabs(\n        {\n            \"\ud83d\ude80 Main\": data_tab,\n            \"\ud83d\udd2c \": advanced_tab,\n            \"\u2699\ufe0f Config\": config_tab,\n            \"\ud83d\udd27 Manage\": management_tab,\n        }\n    )\n</code></pre>"},{"location":"api/astro_lab.ui/#astro_lab.ui.AstroLabDashboard.create_sidebar","title":"create_sidebar","text":"<pre><code>create_sidebar() -&gt; vstack\n</code></pre> <p>Create dashboard sidebar.</p> Source code in <code>src\\astro_lab\\ui\\dashboard.py</code> <pre><code>    def create_sidebar(self) -&gt; mo.vstack:\n        \"\"\"Create dashboard sidebar.\"\"\"\n        widget_status = \"\ud83d\udfe2 Available\" if WIDGETS_AVAILABLE else \"\ud83d\udd34 Not Available\"\n\n        return mo.vstack(\n            [\n                mo.md(\"# \ud83c\udf1f AstroLab\"),\n                mo.md(\"* Astronomical Data Analysis*\"),\n                mo.md(\"---\"),\n                mo.md(\"### \u26a1 Quick Actions\"),\n                ui_quick_actions(),\n                mo.md(\"### \ud83d\udcca System Status\"),\n                ui_system_status(),\n                mo.md(f\"**AstroLab Widgets:** {widget_status}\"),\n                mo.md(\"---\"),\n                mo.md(\"### \ud83d\udcda Resources\"),\n                mo.md(\"\"\"\n- [Documentation](https://astro-lab.readthedocs.io)\n- [GitHub](https://github.com/astro-lab/astro-lab)\n- [Examples](examples/)\n            \"\"\"),\n            ]\n        )\n</code></pre>"},{"location":"api/astro_lab.ui/#astro_lab.ui.AstroLabDashboard.create_welcome_screen","title":"create_welcome_screen","text":"<pre><code>create_welcome_screen() -&gt; vstack\n</code></pre> <p>Create welcome screen.</p> Source code in <code>src\\astro_lab\\ui\\dashboard.py</code> <pre><code>    def create_welcome_screen(self) -&gt; mo.vstack:\n        \"\"\"Create welcome screen.\"\"\"\n        features_text = \"\"\"\n## Modern Astronomical Data Analysis Platform\n\nAstroLab provides comprehensive tools for astronomical data analysis,\nmachine learning, and interactive visualization.\n\n### \ud83d\ude80 Getting Started\n\n1. **Configure**: Set up your experiment and data paths\n2. **Load Data**: Choose a survey and load astronomical data\n3. **Visualize**: Create interactive plots and visualizations\n4. **Analyze**: Run clustering, classification, and analysis\n5. **Model**: Train machine learning models on your data\n\n### \ud83d\udcca Supported Surveys\n- **Gaia**: European Space Agency's stellar survey\n- **SDSS**: Sloan Digital Sky Survey\n- **NSA**: NASA-Sloan Atlas\n- **LINEAR**: Linear Asteroid Survey\n- **TNG50**: IllustrisTNG simulation data\n\n### \ud83e\uddf0 Available Tools\n- Interactive plotting with Plotly, Matplotlib, Bokeh\"\"\"\n\n        if WIDGETS_AVAILABLE:\n            features_text += \"\"\"\n- ** 3D visualization** with Open3D, PyVista, Blender\n- **GPU-accelerated analysis** and clustering\n- **Graph-based analysis** with PyTorch Geometric\n- **High-performance neighbor finding**\"\"\"\n        else:\n            features_text += \"\"\"\n- Graph-based analysis and clustering\n- Neural networks and machine learning\"\"\"\n\n        features_text += \"\"\"\n- GPU acceleration support\n            \"\"\"\n\n        return mo.vstack(\n            [\n                mo.md(\"# \ud83c\udf1f Welcome to AstroLab\"),\n                mo.md(features_text),\n                mo.md(\"### \ud83c\udfaf Quick Start\"),\n                mo.hstack(\n                    [\n                        mo.ui.button(label=\"\ud83d\udcca Load Sample Data\"),\n                        mo.ui.button(label=\"\ud83c\udfa8 Create Plot\"),\n                        mo.ui.button(label=\"\ud83e\udd16 Train Model\"),\n                    ]\n                ),\n            ]\n        )\n</code></pre>"},{"location":"api/astro_lab.ui/#astro_lab.ui.AstroLabDashboard.handle_interactions","title":"handle_interactions","text":"<pre><code>handle_interactions(components: Dict[str, Any]) -&gt; Optional[str]\n</code></pre> <p>Handle all dashboard interactions.</p> Source code in <code>src\\astro_lab\\ui\\dashboard.py</code> <pre><code>def handle_interactions(self, components: Dict[str, Any]) -&gt; Optional[str]:\n    \"\"\"Handle all dashboard interactions.\"\"\"\n\n    # Handle component actions\n    component_result = handle_component_actions(components)\n    if component_result:\n        self.status_message = component_result\n        return component_result\n\n    # Handle config actions\n    config_result = handle_config_actions(components)\n    if config_result:\n        self.status_message = \"\u2705 Configuration action completed\"\n        return self.status_message\n\n    return None\n</code></pre>"},{"location":"api/astro_lab.ui/#astro_lab.ui.UIConfigManager","title":"UIConfigManager","text":"<p>UI interface for AstroLab's configuration system.</p> <p>Methods:</p> Name Description <code>get_current_config</code> <p>Get currently loaded configuration.</p> <code>get_data_config</code> <p>Get data configuration from data_config.</p> <code>get_model_configs</code> <p>Get available model configurations.</p> <code>get_survey_info</code> <p>Get survey configuration and paths.</p> <code>get_training_configs</code> <p>Get available training configurations.</p> <code>load_config</code> <p>Load configuration using ConfigLoader.</p> <code>setup_experiment</code> <p>Setup experiment directories.</p> Source code in <code>src\\astro_lab\\ui\\settings.py</code> <pre><code>class UIConfigManager:\n    \"\"\"UI interface for AstroLab's configuration system.\"\"\"\n\n    def __init__(self):\n        \"\"\"Initialize the UI config manager.\"\"\"\n        self.config_loader = ConfigLoader()\n        self.current_config: Optional[Dict[str, Any]] = None\n        self.available_configs = self._discover_configs()\n\n    def _discover_configs(self) -&gt; Dict[str, List[str]]:\n        \"\"\"Discover available configuration files.\"\"\"\n        configs_dir = Path(\"configs\")\n\n        available = {\n            \"experiments\": [],\n            \"surveys\": [],\n            \"models\": list(MODEL_CONFIGS.keys()),\n            \"training\": list(PREDEFINED_TRAINING_CONFIGS.keys()),\n        }\n\n        if configs_dir.exists():\n            # Find experiment configs\n            for config_file in configs_dir.glob(\"*.yaml\"):\n                if config_file.name != \"default.yaml\":\n                    available[\"experiments\"].append(config_file.stem)\n\n            # Find survey configs\n            surveys_dir = configs_dir / \"surveys\"\n            if surveys_dir.exists():\n                for survey_file in surveys_dir.glob(\"*.yaml\"):\n                    available[\"surveys\"].append(survey_file.stem)\n\n        return available\n\n    def load_config(\n        self,\n        config_path: str = \"configs/default.yaml\",\n        experiment_name: Optional[str] = None,\n    ) -&gt; Dict[str, Any]:\n        \"\"\"Load configuration using ConfigLoader.\"\"\"\n        self.config_loader = ConfigLoader(config_path)\n        self.current_config = self.config_loader.load_config(experiment_name)\n        return self.current_config\n\n    def get_current_config(self) -&gt; Optional[Dict[str, Any]]:\n        \"\"\"Get currently loaded configuration.\"\"\"\n        return self.current_config\n\n    def get_data_config(self) -&gt; Dict[str, Any]:\n        \"\"\"Get data configuration from data_config.\"\"\"\n        return {\n            \"base_dir\": str(data_config.base_dir),\n            \"raw_dir\": str(data_config.raw_dir),\n            \"processed_dir\": str(data_config.processed_dir),\n            \"cache_dir\": str(data_config.cache_dir),\n            \"results_dir\": str(data_config.results_dir),\n            \"experiments_dir\": str(data_config.experiments_dir),\n        }\n\n    def get_model_configs(self) -&gt; Dict[str, Any]:\n        \"\"\"Get available model configurations.\"\"\"\n        return {name: config.to_dict() for name, config in MODEL_CONFIGS.items()}\n\n    def get_training_configs(self) -&gt; Dict[str, str]:\n        \"\"\"Get available training configurations.\"\"\"\n        return {\n            name: config.description or \"No description\"\n            for name, config in PREDEFINED_TRAINING_CONFIGS.items()\n        }\n\n    def setup_experiment(self, experiment_name: str) -&gt; Dict[str, Path]:\n        \"\"\"Setup experiment directories.\"\"\"\n        data_config.ensure_experiment_directories(experiment_name)\n        return data_config.get_experiment_paths(experiment_name)\n\n    def get_survey_info(self, survey: str) -&gt; Dict[str, Any]:\n        \"\"\"Get survey configuration and paths.\"\"\"\n        try:\n            survey_config = load_survey_config(survey)\n            survey_paths = get_survey_paths(survey)\n            return {\n                \"config\": survey_config,\n                \"paths\": {k: str(v) for k, v in survey_paths.items()},\n            }\n        except Exception as e:\n            return {\"error\": str(e)}\n</code></pre>"},{"location":"api/astro_lab.ui/#astro_lab.ui.UIConfigManager.get_current_config","title":"get_current_config","text":"<pre><code>get_current_config() -&gt; Optional[Dict[str, Any]]\n</code></pre> <p>Get currently loaded configuration.</p> Source code in <code>src\\astro_lab\\ui\\settings.py</code> <pre><code>def get_current_config(self) -&gt; Optional[Dict[str, Any]]:\n    \"\"\"Get currently loaded configuration.\"\"\"\n    return self.current_config\n</code></pre>"},{"location":"api/astro_lab.ui/#astro_lab.ui.UIConfigManager.get_data_config","title":"get_data_config","text":"<pre><code>get_data_config() -&gt; Dict[str, Any]\n</code></pre> <p>Get data configuration from data_config.</p> Source code in <code>src\\astro_lab\\ui\\settings.py</code> <pre><code>def get_data_config(self) -&gt; Dict[str, Any]:\n    \"\"\"Get data configuration from data_config.\"\"\"\n    return {\n        \"base_dir\": str(data_config.base_dir),\n        \"raw_dir\": str(data_config.raw_dir),\n        \"processed_dir\": str(data_config.processed_dir),\n        \"cache_dir\": str(data_config.cache_dir),\n        \"results_dir\": str(data_config.results_dir),\n        \"experiments_dir\": str(data_config.experiments_dir),\n    }\n</code></pre>"},{"location":"api/astro_lab.ui/#astro_lab.ui.UIConfigManager.get_model_configs","title":"get_model_configs","text":"<pre><code>get_model_configs() -&gt; Dict[str, Any]\n</code></pre> <p>Get available model configurations.</p> Source code in <code>src\\astro_lab\\ui\\settings.py</code> <pre><code>def get_model_configs(self) -&gt; Dict[str, Any]:\n    \"\"\"Get available model configurations.\"\"\"\n    return {name: config.to_dict() for name, config in MODEL_CONFIGS.items()}\n</code></pre>"},{"location":"api/astro_lab.ui/#astro_lab.ui.UIConfigManager.get_survey_info","title":"get_survey_info","text":"<pre><code>get_survey_info(survey: str) -&gt; Dict[str, Any]\n</code></pre> <p>Get survey configuration and paths.</p> Source code in <code>src\\astro_lab\\ui\\settings.py</code> <pre><code>def get_survey_info(self, survey: str) -&gt; Dict[str, Any]:\n    \"\"\"Get survey configuration and paths.\"\"\"\n    try:\n        survey_config = load_survey_config(survey)\n        survey_paths = get_survey_paths(survey)\n        return {\n            \"config\": survey_config,\n            \"paths\": {k: str(v) for k, v in survey_paths.items()},\n        }\n    except Exception as e:\n        return {\"error\": str(e)}\n</code></pre>"},{"location":"api/astro_lab.ui/#astro_lab.ui.UIConfigManager.get_training_configs","title":"get_training_configs","text":"<pre><code>get_training_configs() -&gt; Dict[str, str]\n</code></pre> <p>Get available training configurations.</p> Source code in <code>src\\astro_lab\\ui\\settings.py</code> <pre><code>def get_training_configs(self) -&gt; Dict[str, str]:\n    \"\"\"Get available training configurations.\"\"\"\n    return {\n        name: config.description or \"No description\"\n        for name, config in PREDEFINED_TRAINING_CONFIGS.items()\n    }\n</code></pre>"},{"location":"api/astro_lab.ui/#astro_lab.ui.UIConfigManager.load_config","title":"load_config","text":"<pre><code>load_config(\n    config_path: str = \"configs/default.yaml\", experiment_name: Optional[str] = None\n) -&gt; Dict[str, Any]\n</code></pre> <p>Load configuration using ConfigLoader.</p> Source code in <code>src\\astro_lab\\ui\\settings.py</code> <pre><code>def load_config(\n    self,\n    config_path: str = \"configs/default.yaml\",\n    experiment_name: Optional[str] = None,\n) -&gt; Dict[str, Any]:\n    \"\"\"Load configuration using ConfigLoader.\"\"\"\n    self.config_loader = ConfigLoader(config_path)\n    self.current_config = self.config_loader.load_config(experiment_name)\n    return self.current_config\n</code></pre>"},{"location":"api/astro_lab.ui/#astro_lab.ui.UIConfigManager.setup_experiment","title":"setup_experiment","text":"<pre><code>setup_experiment(experiment_name: str) -&gt; Dict[str, Path]\n</code></pre> <p>Setup experiment directories.</p> Source code in <code>src\\astro_lab\\ui\\settings.py</code> <pre><code>def setup_experiment(self, experiment_name: str) -&gt; Dict[str, Path]:\n    \"\"\"Setup experiment directories.\"\"\"\n    data_config.ensure_experiment_directories(experiment_name)\n    return data_config.get_experiment_paths(experiment_name)\n</code></pre>"},{"location":"api/astro_lab.ui/#astro_lab.ui.create_analysis_dashboard","title":"create_analysis_dashboard","text":"<pre><code>create_analysis_dashboard() -&gt; vstack\n</code></pre> <p>Create an analysis-focused dashboard with widget integration.</p> Source code in <code>src\\astro_lab\\ui\\dashboard.py</code> <pre><code>def create_analysis_dashboard() -&gt; mo.vstack:\n    \"\"\"Create an analysis-focused dashboard with widget integration.\"\"\"\n    return mo.vstack(\n        [\n            mo.md(\"# \ud83d\udd2c AstroLab Analysis Dashboard\"),\n            mo.hstack(\n                [\n                    mo.vstack(\n                        [\n                            mo.md(\"### \ud83d\udcca Data\"),\n                            ui_data_controls(),\n                            mo.md(\"### \ud83d\udd2c Analysis\"),\n                            ui_analysis_controls(),\n                        ]\n                    ),\n                    mo.vstack(\n                        [\n                            mo.md(\"### \ud83d\udcc8 Visualization\"),\n                            ui_visualization_controls(),\n                            mo.md(\"### \ud83d\udd78\ufe0f Graph Analysis\"),\n                            ui_graph_controls(),\n                        ]\n                    ),\n                ]\n            ),\n            mo.md(\"## \ud83e\udd16 Model Training\"),\n            ui_model_controls(),\n            mo.md(\"## \ud83d\udcca Status &amp; Actions\"),\n            mo.hstack(\n                [\n                    ui_system_status(),\n                    ui_quick_actions(),\n                ]\n            ),\n        ]\n    )\n</code></pre>"},{"location":"api/astro_lab.ui/#astro_lab.ui.create_astrolab_dashboard","title":"create_astrolab_dashboard","text":"<pre><code>create_astrolab_dashboard() -&gt; vstack\n</code></pre> <p>Create the complete AstroLab dashboard.</p> Source code in <code>src\\astro_lab\\ui\\dashboard.py</code> <pre><code>def create_astrolab_dashboard() -&gt; mo.vstack:\n    \"\"\"Create the complete AstroLab dashboard.\"\"\"\n    return dashboard.create_full_dashboard()\n</code></pre>"},{"location":"api/astro_lab.ui/#astro_lab.ui.create_config_dashboard","title":"create_config_dashboard","text":"<pre><code>create_config_dashboard() -&gt; vstack\n</code></pre> <p>Create a configuration-focused dashboard.</p> Source code in <code>src\\astro_lab\\ui\\dashboard.py</code> <pre><code>def create_config_dashboard() -&gt; mo.vstack:\n    \"\"\"Create a configuration-focused dashboard.\"\"\"\n    return mo.vstack(\n        [\n            mo.md(\"# \u2699\ufe0f AstroLab Configuration\"),\n            mo.ui.tabs(\n                {\n                    \"\ud83d\ude80 Quick\": ui_quick_setup(),\n                    \"\ud83d\udcc2 Config\": ui_config_loader(),\n                    \"\ud83d\udcca Survey\": ui_survey_selector(),\n                    \"\ud83e\udd16 Model\": ui_model_selector(),\n                    \"\ud83c\udfcb\ufe0f Training\": ui_training_selector(),\n                    \"\ud83e\uddea Experiment\": ui_experiment_manager(),\n                    \"\ud83d\udcc1 Paths\": ui_data_paths(),\n                    \"\u2139\ufe0f Status\": ui_config_status(),\n                }\n            ),\n        ]\n    )\n</code></pre>"},{"location":"api/astro_lab.ui/#astro_lab.ui.create_dashboard","title":"create_dashboard","text":"<pre><code>create_dashboard(dashboard_type: str = 'full') -&gt; Any\n</code></pre> <p>Create a dashboard of specified type.</p> <p>Parameters:</p> Name Type Description Default <code>dashboard_type</code> <code>str</code> <p>One of \"full\", \"minimal\", \"config\", \"analysis\", \"widgets\"</p> <code>'full'</code> <p>Returns:</p> Type Description <code>Any</code> <p>Marimo UI component</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If dashboard_type is not supported</p> Source code in <code>src\\astro_lab\\ui\\__init__.py</code> <pre><code>def create_dashboard(dashboard_type: str = \"full\") -&gt; Any:\n    \"\"\"\n    Create a dashboard of specified type.\n\n    Args:\n        dashboard_type: One of \"full\", \"minimal\", \"config\", \"analysis\", \"widgets\"\n\n    Returns:\n        Marimo UI component\n\n    Raises:\n        ValueError: If dashboard_type is not supported\n    \"\"\"\n    dashboard_factories = {\n        \"full\": create_astrolab_dashboard,\n        \"minimal\": create_minimal_dashboard,\n        \"config\": create_config_dashboard,\n        \"analysis\": create_analysis_dashboard,\n        \"widgets\": create_widget_showcase,\n    }\n\n    if dashboard_type not in dashboard_factories:\n        raise ValueError(\n            f\"Unknown dashboard type: {dashboard_type}. Supported: {list(dashboard_factories.keys())}\"\n        )\n\n    return dashboard_factories[dashboard_type]()\n</code></pre>"},{"location":"api/astro_lab.ui/#astro_lab.ui.create_minimal_dashboard","title":"create_minimal_dashboard","text":"<pre><code>create_minimal_dashboard() -&gt; vstack\n</code></pre> <p>Create a minimal dashboard for quick analysis.</p> Source code in <code>src\\astro_lab\\ui\\dashboard.py</code> <pre><code>def create_minimal_dashboard() -&gt; mo.vstack:\n    \"\"\"Create a minimal dashboard for quick analysis.\"\"\"\n    return mo.vstack(\n        [\n            mo.md(\"# \ud83c\udf1f AstroLab - Quick Analysis\"),\n            mo.md(\"## \ud83d\udcca Data Loading\"),\n            ui_data_controls(),\n            mo.md(\"## \ud83d\udcc8 Visualization\"),\n            ui_visualization_controls(),\n            mo.md(\"## \ud83d\udd2c Analysis\"),\n            ui_analysis_controls(),\n            mo.md(\"## \u26a1 Quick Actions\"),\n            ui_quick_actions(),\n        ]\n    )\n</code></pre>"},{"location":"api/astro_lab.ui/#astro_lab.ui.create_widget_showcase","title":"create_widget_showcase","text":"<pre><code>create_widget_showcase() -&gt; vstack\n</code></pre> <p>Create a dashboard showcasing AstroLab widget capabilities.</p> Source code in <code>src\\astro_lab\\ui\\dashboard.py</code> <pre><code>def create_widget_showcase() -&gt; mo.vstack:\n    \"\"\"Create a dashboard showcasing AstroLab widget capabilities.\"\"\"\n    if not WIDGETS_AVAILABLE:\n        return mo.vstack(\n            [\n                mo.md(\"# \u274c AstroLab Widgets Not Available\"),\n                mo.md(\n                    \"Please install the required dependencies for widget functionality.\"\n                ),\n            ]\n        )\n\n    return mo.vstack(\n        [\n            mo.md(\"# \ud83c\udf1f AstroLab Widget Showcase\"),\n            mo.md(\"*Demonstrating advanced visualization and analysis capabilities*\"),\n            mo.md(\"## \ud83c\udfa8  Visualization\"),\n            mo.md(\"**Backends:** Open3D, PyVista, Blender integration\"),\n            ui_visualization_controls(),\n            mo.md(\"## \ud83d\udd78\ufe0f Graph Analysis\"),\n            mo.md(\n                \"**Features:** GPU-accelerated neighbor finding, PyTorch Geometric integration\"\n            ),\n            ui_graph_controls(),\n            mo.md(\"## \ud83d\udd2c  Analysis\"),\n            mo.md(\n                \"**Capabilities:** GPU clustering, density analysis, structure analysis\"\n            ),\n            ui_analysis_controls(),\n            mo.md(\"## \ud83d\udcca System Information\"),\n            ui_system_status(),\n        ]\n    )\n</code></pre>"},{"location":"api/astro_lab.ui/#astro_lab.ui.handle_component_actions","title":"handle_component_actions","text":"<pre><code>handle_component_actions(components: Dict[str, dictionary]) -&gt; Optional[str]\n</code></pre> <p>Handle actions from UI components.</p> Source code in <code>src\\astro_lab\\ui\\components.py</code> <pre><code>def handle_component_actions(components: Dict[str, mo.ui.dictionary]) -&gt; Optional[str]:\n    \"\"\"Handle actions from UI components.\"\"\"\n    # This function would handle the actual logic for component actions\n    # For now, it's a placeholder\n    return \"Action handled\"\n</code></pre>"},{"location":"api/astro_lab.ui/#astro_lab.ui.handle_config_actions","title":"handle_config_actions","text":"<pre><code>handle_config_actions(components: Dict[str, dictionary]) -&gt; Optional[Dict[str, Any]]\n</code></pre> <p>Handle configuration-related actions from UI components.</p> Source code in <code>src\\astro_lab\\ui\\settings.py</code> <pre><code>def handle_config_actions(\n    components: Dict[str, mo.ui.dictionary],\n) -&gt; Optional[Dict[str, Any]]:\n    \"\"\"Handle configuration-related actions from UI components.\"\"\"\n\n    # Handle config loading\n    if \"config_loader\" in components:\n        loader_values = components[\"config_loader\"].value\n\n        if loader_values.get(\"load_button\"):\n            config_file = f\"configs/{loader_values.get('config_file', 'default.yaml')}\"\n            experiment_name = loader_values.get(\"experiment_name\")\n            experiment_name = str(experiment_name) if experiment_name else None\n\n            try:\n                config = ui_config.load_config(config_file, experiment_name)\n                print(f\"\u2705 Configuration loaded: {config_file}\")\n                if experiment_name:\n                    print(f\"\ud83e\uddea Experiment: {experiment_name}\")\n                return config\n            except Exception as e:\n                print(f\"\u274c Failed to load configuration: {e}\")\n                return None\n\n        if loader_values.get(\"create_experiment\"):\n            experiment_name = loader_values.get(\"experiment_name\")\n            if experiment_name:\n                experiment_name = str(experiment_name)\n                try:\n                    paths = ui_config.setup_experiment(experiment_name)\n                    print(f\"\u2705 Experiment created: {experiment_name}\")\n                    print(f\"\ud83d\udcc1 Paths: {paths}\")\n                except Exception as e:\n                    print(f\"\u274c Failed to create experiment: {e}\")\n\n    # Handle survey configuration\n    if \"survey_selector\" in components:\n        survey_values = components[\"survey_selector\"].value\n\n        if survey_values.get(\"load_survey_config\"):\n            survey = survey_values.get(\"survey\", \"gaia\")\n            survey = str(survey)\n            try:\n                survey_info = ui_config.get_survey_info(survey)\n                print(f\"\u2705 Survey config loaded: {survey}\")\n                return survey_info\n            except Exception as e:\n                print(f\"\u274c Failed to load survey config: {e}\")\n\n        if survey_values.get(\"setup_survey_dirs\"):\n            survey = survey_values.get(\"survey\", \"gaia\")\n            survey = str(survey)\n            try:\n                data_config.ensure_survey_directories(survey)\n                print(f\"\u2705 Survey directories created: {survey}\")\n            except Exception as e:\n                print(f\"\u274c Failed to create survey directories: {e}\")\n\n    # Handle model configuration\n    if \"model_selector\" in components:\n        model_values = components[\"model_selector\"].value\n\n        if model_values.get(\"load_model_config\"):\n            model_name = model_values.get(\"model_name\", \"gaia_classifier\")\n            model_name = str(model_name)\n            try:\n                model_config = get_predefined_config(model_name)\n                print(f\"\u2705 Model config loaded: {model_name}\")\n                print(f\"\ud83e\udd16 Details: {model_config.to_dict()}\")\n                return model_config.to_dict()\n            except Exception as e:\n                print(f\"\u274c Failed to load model config: {e}\")\n\n    # Handle data paths\n    if \"data_paths\" in components:\n        paths_values = components[\"data_paths\"].value\n\n        if paths_values.get(\"setup_dirs\"):\n            try:\n                data_config.setup_directories()\n                print(\"\u2705 Data directories created\")\n            except Exception as e:\n                print(f\"\u274c Failed to create data directories: {e}\")\n\n    return None\n</code></pre>"},{"location":"api/astro_lab.ui/#astro_lab.ui.ui_analysis_controls","title":"ui_analysis_controls","text":"<pre><code>ui_analysis_controls() -&gt; dictionary\n</code></pre> <p>Analysis controls using AstroLab widget functionality.</p> Source code in <code>src\\astro_lab\\ui\\components.py</code> <pre><code>def ui_analysis_controls() -&gt; mo.ui.dictionary:\n    \"\"\"Analysis controls using AstroLab widget functionality.\"\"\"\n    return mo.ui.dictionary(\n        {\n            \"method\": mo.ui.dropdown(\n                label=\"Analysis Method\",\n                options=[\"clustering\", \"density\", \"structure\", \"neighbors\"],\n                value=\"clustering\",\n            ),\n            \"clustering_algorithm\": mo.ui.dropdown(\n                label=\"Clustering Algorithm\",\n                options=[\"dbscan\", \"kmeans\", \"agglomerative\"],\n                value=\"dbscan\",\n            ),\n            \"k_neighbors\": mo.ui.slider(\n                label=\"K-Neighbors\",\n                start=3,\n                stop=50,\n                step=1,\n                value=10,\n            ),\n            \"eps\": mo.ui.number(\n                label=\"DBSCAN Eps\",\n                value=10.0,\n                start=0.1,\n                stop=100.0,\n                step=0.1,\n            ),\n            \"min_samples\": mo.ui.slider(\n                label=\"Min Samples\",\n                start=3,\n                stop=20,\n                step=1,\n                value=5,\n            ),\n            \"use_gpu\": mo.ui.checkbox(\n                label=\"Use GPU\",\n                value=torch.cuda.is_available(),\n            ),\n            \"run_analysis\": mo.ui.button(label=\"\ud83d\udd2c Run Analysis\"),\n        },\n        label=\"\ud83d\udd2c Analysis\",\n    )\n</code></pre>"},{"location":"api/astro_lab.ui/#astro_lab.ui.ui_config_loader","title":"ui_config_loader","text":"<pre><code>ui_config_loader() -&gt; dictionary\n</code></pre> <p>Configuration loader UI component.</p> Source code in <code>src\\astro_lab\\ui\\settings.py</code> <pre><code>def ui_config_loader() -&gt; mo.ui.dictionary:\n    \"\"\"Configuration loader UI component.\"\"\"\n    configs_dir = Path(\"configs\")\n    config_files = []\n\n    if configs_dir.exists():\n        config_files = [f.name for f in configs_dir.glob(\"*.yaml\")]\n\n    if not config_files:\n        config_files = [\"default.yaml\"]\n\n    return mo.ui.dictionary(\n        {\n            \"config_file\": mo.ui.dropdown(\n                label=\"Configuration File\",\n                options=config_files,\n                value=\"default.yaml\",\n            ),\n            \"experiment_name\": mo.ui.text(\n                label=\"Experiment Name\",\n                placeholder=\"e.g., gaia_stellar_classification\",\n            ),\n            \"load_button\": mo.ui.button(label=\"\ud83d\udcc2 Load Configuration\"),\n            \"create_experiment\": mo.ui.button(label=\"\ud83e\uddea Create Experiment\"),\n        },\n        label=\"\ud83d\udcc2 Configuration Loader\",\n    )\n</code></pre>"},{"location":"api/astro_lab.ui/#astro_lab.ui.ui_config_status","title":"ui_config_status","text":"<pre><code>ui_config_status() -&gt; dictionary\n</code></pre> <p>Configuration status display.</p> Source code in <code>src\\astro_lab\\ui\\settings.py</code> <pre><code>def ui_config_status() -&gt; mo.ui.dictionary:\n    \"\"\"Configuration status display.\"\"\"\n    current_config = ui_config.get_current_config()\n\n    status_text = \"No configuration loaded\"\n    if current_config:\n        exp_name = current_config.get(\"mlflow\", {}).get(\"experiment_name\", \"Unknown\")\n        status_text = f\"Loaded: {exp_name}\"\n\n    return mo.ui.dictionary(\n        {\n            \"status\": mo.ui.text(\n                label=\"Configuration Status\",\n                value=status_text,\n                disabled=True,\n            ),\n            \"config_details\": mo.ui.text_area(\n                label=\"Configuration Details\",\n                value=str(current_config) if current_config else \"No config loaded\",\n                disabled=True,\n            ),\n            \"refresh\": mo.ui.button(label=\"\ud83d\udd04 Refresh Status\"),\n        },\n        label=\"\u2139\ufe0f Configuration Status\",\n    )\n</code></pre>"},{"location":"api/astro_lab.ui/#astro_lab.ui.ui_data_controls","title":"ui_data_controls","text":"<pre><code>ui_data_controls() -&gt; dictionary\n</code></pre> <p>Data loading and management controls.</p> Source code in <code>src\\astro_lab\\ui\\components.py</code> <pre><code>def ui_data_controls() -&gt; mo.ui.dictionary:\n    \"\"\"Data loading and management controls.\"\"\"\n    return mo.ui.dictionary(\n        {\n            \"survey\": mo.ui.dropdown(\n                label=\"Survey\",\n                options=[\"gaia\", \"sdss\", \"nsa\", \"linear\", \"tng50\"],\n                value=\"gaia\",\n            ),\n            \"max_samples\": mo.ui.slider(\n                label=\"Max Samples\",\n                start=1000,\n                stop=100000,\n                step=1000,\n                value=25000,\n            ),\n            \"use_cache\": mo.ui.checkbox(\n                label=\"Use Cache\",\n                value=True,\n            ),\n            \"load_data\": mo.ui.button(label=\"\ud83d\udcca Load Data\"),\n            \"preview_data\": mo.ui.button(label=\"\ud83d\udc41\ufe0f Preview Data\"),\n        },\n        label=\"\ud83d\udcca Data Controls\",\n    )\n</code></pre>"},{"location":"api/astro_lab.ui/#astro_lab.ui.ui_data_paths","title":"ui_data_paths","text":"<pre><code>ui_data_paths() -&gt; dictionary\n</code></pre> <p>Data paths configuration UI component.</p> Source code in <code>src\\astro_lab\\ui\\settings.py</code> <pre><code>def ui_data_paths() -&gt; mo.ui.dictionary:\n    \"\"\"Data paths configuration UI component.\"\"\"\n    data_info = ui_config.get_data_config()\n\n    return mo.ui.dictionary(\n        {\n            \"base_dir\": mo.ui.text(\n                label=\"Base Data Directory\",\n                value=data_info[\"base_dir\"],\n                disabled=True,\n            ),\n            \"raw_dir\": mo.ui.text(\n                label=\"Raw Data Directory\",\n                value=data_info[\"raw_dir\"],\n                disabled=True,\n            ),\n            \"processed_dir\": mo.ui.text(\n                label=\"Processed Data Directory\",\n                value=data_info[\"processed_dir\"],\n                disabled=True,\n            ),\n            \"cache_dir\": mo.ui.text(\n                label=\"Cache Directory\",\n                value=data_info[\"cache_dir\"],\n                disabled=True,\n            ),\n            \"setup_dirs\": mo.ui.button(label=\"\ud83d\udcc1 Setup Directories\"),\n        },\n        label=\"\ud83d\udcc1 Data Paths\",\n    )\n</code></pre>"},{"location":"api/astro_lab.ui/#astro_lab.ui.ui_experiment_manager","title":"ui_experiment_manager","text":"<pre><code>ui_experiment_manager() -&gt; dictionary\n</code></pre> <p>Experiment management UI component.</p> Source code in <code>src\\astro_lab\\ui\\settings.py</code> <pre><code>def ui_experiment_manager() -&gt; mo.ui.dictionary:\n    \"\"\"Experiment management UI component.\"\"\"\n    return mo.ui.dictionary(\n        {\n            \"experiment_name\": mo.ui.text(\n                label=\"Experiment Name\",\n                placeholder=\"e.g., gaia_stellar_v2\",\n            ),\n            \"description\": mo.ui.text_area(\n                label=\"Description\",\n                placeholder=\"Experiment description...\",\n            ),\n            \"create_experiment\": mo.ui.button(label=\"\ud83e\uddea Create Experiment\"),\n            \"list_experiments\": mo.ui.button(label=\"\ud83d\udccb List Experiments\"),\n        },\n        label=\"\ud83e\uddea Experiment Management\",\n    )\n</code></pre>"},{"location":"api/astro_lab.ui/#astro_lab.ui.ui_graph_controls","title":"ui_graph_controls","text":"<pre><code>ui_graph_controls() -&gt; dictionary\n</code></pre> <p>Graph analysis controls using AstroLab widget.</p> Source code in <code>src\\astro_lab\\ui\\components.py</code> <pre><code>def ui_graph_controls() -&gt; mo.ui.dictionary:\n    \"\"\"Graph analysis controls using AstroLab widget.\"\"\"\n    return mo.ui.dictionary(\n        {\n            \"graph_type\": mo.ui.dropdown(\n                label=\"Graph Type\",\n                options=[\"spatial\", \"knn\", \"radius\", \"delaunay\"],\n                value=\"spatial\",\n            ),\n            \"k_neighbors\": mo.ui.slider(\n                label=\"K-Neighbors\",\n                start=3,\n                stop=50,\n                step=1,\n                value=10,\n            ),\n            \"radius\": mo.ui.number(\n                label=\"Radius\",\n                value=1.0,\n                start=0.1,\n                stop=10.0,\n                step=0.1,\n            ),\n            \"edge_weight\": mo.ui.dropdown(\n                label=\"Edge Weight\",\n                options=[\"distance\", \"inverse_distance\", \"none\"],\n                value=\"distance\",\n            ),\n            \"use_gpu\": mo.ui.checkbox(\n                label=\"Use GPU\",\n                value=torch.cuda.is_available(),\n            ),\n            \"create_graph\": mo.ui.button(label=\"\ud83d\udd78\ufe0f Create Graph\"),\n            \"analyze_graph\": mo.ui.button(label=\"\ud83d\udcca Analyze Graph\"),\n        },\n        label=\"\ud83d\udd78\ufe0f Graph Analysis\",\n    )\n</code></pre>"},{"location":"api/astro_lab.ui/#astro_lab.ui.ui_model_controls","title":"ui_model_controls","text":"<pre><code>ui_model_controls() -&gt; dictionary\n</code></pre> <p>Model training controls.</p> Source code in <code>src\\astro_lab\\ui\\components.py</code> <pre><code>def ui_model_controls() -&gt; mo.ui.dictionary:\n    \"\"\"Model training controls.\"\"\"\n    return mo.ui.dictionary(\n        {\n            \"model_type\": mo.ui.dropdown(\n                label=\"Model Type\",\n                options=[\"gaia_classifier\", \"survey_gnn\", \"point_cloud_gnn\"],\n                value=\"gaia_classifier\",\n            ),\n            \"task\": mo.ui.dropdown(\n                label=\"Task\",\n                options=[\"classification\", \"regression\", \"clustering\"],\n                value=\"classification\",\n            ),\n            \"batch_size\": mo.ui.slider(\n                label=\"Batch Size\",\n                start=8,\n                stop=128,\n                step=8,\n                value=32,\n            ),\n            \"epochs\": mo.ui.slider(\n                label=\"Epochs\",\n                start=1,\n                stop=100,\n                step=1,\n                value=10,\n            ),\n            \"learning_rate\": mo.ui.number(\n                label=\"Learning Rate\",\n                value=0.001,\n                start=0.0001,\n                stop=0.1,\n                step=0.0001,\n            ),\n            \"prepare_data\": mo.ui.button(label=\"\ud83d\udd27 Prepare Data for Model\"),\n            \"train_model\": mo.ui.button(label=\"\ud83c\udfcb\ufe0f Train Model\"),\n        },\n        label=\"\ud83e\udd16 Model Training\",\n    )\n</code></pre>"},{"location":"api/astro_lab.ui/#astro_lab.ui.ui_model_selector","title":"ui_model_selector","text":"<pre><code>ui_model_selector() -&gt; dictionary\n</code></pre> <p>Model selection UI component.</p> Source code in <code>src\\astro_lab\\ui\\settings.py</code> <pre><code>def ui_model_selector() -&gt; mo.ui.dictionary:\n    \"\"\"Model selection UI component.\"\"\"\n    available_models = ui_config.available_configs[\"models\"]\n\n    return mo.ui.dictionary(\n        {\n            \"model_name\": mo.ui.dropdown(\n                label=\"Model Configuration\",\n                options=available_models,\n                value=available_models[0] if available_models else \"gaia_classifier\",\n            ),\n            \"load_model_config\": mo.ui.button(label=\"\ud83e\udd16 Load Model Config\"),\n            \"show_model_details\": mo.ui.button(label=\"\u2139\ufe0f Show Details\"),\n        },\n        label=\"\ud83e\udd16 Model Configuration\",\n    )\n</code></pre>"},{"location":"api/astro_lab.ui/#astro_lab.ui.ui_quick_actions","title":"ui_quick_actions","text":"<pre><code>ui_quick_actions() -&gt; dictionary\n</code></pre> <p>Quick action buttons for common tasks.</p> Source code in <code>src\\astro_lab\\ui\\components.py</code> <pre><code>def ui_quick_actions() -&gt; mo.ui.dictionary:\n    \"\"\"Quick action buttons for common tasks.\"\"\"\n    return mo.ui.dictionary(\n        {\n            \"load_gaia\": mo.ui.button(label=\"\ud83c\udf1f Load Gaia\"),\n            \"load_sdss\": mo.ui.button(label=\"\ud83d\udd2d Load SDSS\"),\n            \"load_tng50\": mo.ui.button(label=\"\ud83c\udf0c Load TNG50\"),\n            \"create_3d_plot\": mo.ui.button(label=\"\ud83c\udfa8 Create 3D Plot\"),\n            \"run_clustering\": mo.ui.button(label=\"\ud83d\udd2c Run Clustering\"),\n            \"export_results\": mo.ui.button(label=\"\ud83d\udcbe Export Results\"),\n        },\n        label=\"\u26a1 Quick Actions\",\n    )\n</code></pre>"},{"location":"api/astro_lab.ui/#astro_lab.ui.ui_quick_setup","title":"ui_quick_setup","text":"<pre><code>ui_quick_setup() -&gt; dictionary\n</code></pre> <p>Quick setup component for common workflows.</p> Source code in <code>src\\astro_lab\\ui\\components.py</code> <pre><code>def ui_quick_setup() -&gt; mo.ui.dictionary:\n    \"\"\"Quick setup component for common workflows.\"\"\"\n    return mo.ui.dictionary(\n        {\n            \"experiment_name\": mo.ui.text(\n                label=\"Experiment Name\",\n                placeholder=\"e.g., gaia_stellar_v1\",\n            ),\n            \"survey\": mo.ui.dropdown(\n                label=\"Survey\",\n                options=[\"gaia\", \"sdss\", \"nsa\", \"linear\", \"tng50\"],\n                value=\"gaia\",\n            ),\n            \"model\": mo.ui.dropdown(\n                label=\"Model\",\n                options=[\"gaia_classifier\", \"survey_gnn\", \"point_cloud_gnn\"],\n                value=\"gaia_classifier\",\n            ),\n            \"quick_setup\": mo.ui.button(label=\"\ud83d\ude80 Quick Setup\"),\n        },\n        label=\"\ud83d\ude80 Quick Setup\",\n    )\n</code></pre>"},{"location":"api/astro_lab.ui/#astro_lab.ui.ui_survey_selector","title":"ui_survey_selector","text":"<pre><code>ui_survey_selector() -&gt; dictionary\n</code></pre> <p>Survey selection and configuration UI component.</p> Source code in <code>src\\astro_lab\\ui\\settings.py</code> <pre><code>def ui_survey_selector() -&gt; mo.ui.dictionary:\n    \"\"\"Survey selection and configuration UI component.\"\"\"\n    available_surveys = ui_config.available_configs[\"surveys\"]\n\n    if not available_surveys:\n        available_surveys = [\"gaia\", \"sdss\", \"nsa\", \"linear\", \"tng50\"]\n\n    return mo.ui.dictionary(\n        {\n            \"survey\": mo.ui.dropdown(\n                label=\"Survey\",\n                options=available_surveys,\n                value=available_surveys[0] if available_surveys else \"gaia\",\n            ),\n            \"load_survey_config\": mo.ui.button(label=\"\ud83d\udcca Load Survey Config\"),\n            \"setup_survey_dirs\": mo.ui.button(label=\"\ud83d\udcc1 Setup Survey Directories\"),\n        },\n        label=\"\ud83d\udcca Survey Configuration\",\n    )\n</code></pre>"},{"location":"api/astro_lab.ui/#astro_lab.ui.ui_system_status","title":"ui_system_status","text":"<pre><code>ui_system_status() -&gt; dictionary\n</code></pre> <p>System status and information.</p> Source code in <code>src\\astro_lab\\ui\\components.py</code> <pre><code>def ui_system_status() -&gt; mo.ui.dictionary:\n    \"\"\"System status and information.\"\"\"\n    # Get system information\n    device_info = \"CUDA\" if torch.cuda.is_available() else \"CPU\"\n    if torch.cuda.is_available():\n        device_info += f\" ({torch.cuda.get_device_name()})\"\n\n    return mo.ui.dictionary(\n        {\n            \"device\": mo.ui.text(\n                label=\"Device\",\n                value=device_info,\n                disabled=True,\n            ),\n            \"widgets_available\": mo.ui.text(\n                label=\"Widgets Available\",\n                value=\"Yes\" if WIDGETS_AVAILABLE else \"No\",\n                disabled=True,\n            ),\n            \"memory_usage\": mo.ui.text(\n                label=\"Memory Usage\",\n                value=\"Checking...\",\n                disabled=True,\n            ),\n            \"refresh_status\": mo.ui.button(label=\"\ud83d\udd04 Refresh Status\"),\n        },\n        label=\"\ud83d\udcbb System Status\",\n    )\n</code></pre>"},{"location":"api/astro_lab.ui/#astro_lab.ui.ui_training_selector","title":"ui_training_selector","text":"<pre><code>ui_training_selector() -&gt; dictionary\n</code></pre> <p>Training configuration selector UI component.</p> Source code in <code>src\\astro_lab\\ui\\settings.py</code> <pre><code>def ui_training_selector() -&gt; mo.ui.dictionary:\n    \"\"\"Training configuration selector UI component.\"\"\"\n    available_training = ui_config.available_configs[\"training\"]\n\n    return mo.ui.dictionary(\n        {\n            \"training_config\": mo.ui.dropdown(\n                label=\"Training Configuration\",\n                options=available_training,\n                value=available_training[0]\n                if available_training\n                else \"gaia_stellar_training\",\n            ),\n            \"load_training_config\": mo.ui.button(label=\"\ud83c\udfcb\ufe0f Load Training Config\"),\n            \"show_training_details\": mo.ui.button(label=\"\u2139\ufe0f Show Details\"),\n        },\n        label=\"\ud83c\udfcb\ufe0f Training Configuration\",\n    )\n</code></pre>"},{"location":"api/astro_lab.ui/#astro_lab.ui.ui_visualization_controls","title":"ui_visualization_controls","text":"<pre><code>ui_visualization_controls() -&gt; dictionary\n</code></pre> <p>Visualization controls using AstroLab widget backends.</p> Source code in <code>src\\astro_lab\\ui\\components.py</code> <pre><code>def ui_visualization_controls() -&gt; mo.ui.dictionary:\n    \"\"\"Visualization controls using AstroLab widget backends.\"\"\"\n    backend_options = [\"plotly\", \"matplotlib\", \"bokeh\"]\n\n    # Add widget backends if available\n    if WIDGETS_AVAILABLE:\n        backend_options.extend([\"open3d\", \"pyvista\", \"blender\"])\n\n    return mo.ui.dictionary(\n        {\n            \"backend\": mo.ui.dropdown(\n                label=\"Backend\",\n                options=backend_options,\n                value=\"plotly\",\n            ),\n            \"plot_type\": mo.ui.dropdown(\n                label=\"Plot Type\",\n                options=[\"scatter\", \"scatter_3d\", \"histogram\", \"heatmap\", \"density\"],\n                value=\"scatter\",\n            ),\n            \"max_points\": mo.ui.slider(\n                label=\"Max Points\",\n                start=1000,\n                stop=100000,\n                step=5000,\n                value=25000,\n            ),\n            \"interactive\": mo.ui.checkbox(\n                label=\"Interactive\",\n                value=True,\n            ),\n            \"enable_3d\": mo.ui.checkbox(\n                label=\"Enable 3D\",\n                value=True,\n            ),\n            \"create_plot\": mo.ui.button(label=\"\ud83d\udcc8 Create Plot\"),\n        },\n        label=\"\ud83c\udfa8 Visualization\",\n    )\n</code></pre>"},{"location":"api/astro_lab.utils/","title":"astro_lab.utils","text":""},{"location":"api/astro_lab.utils/#astro_lab.utils","title":"utils","text":""},{"location":"api/astro_lab.utils/#astro_lab.utils--astrolab-utilities-core-utility-functions","title":"AstroLab Utilities - Core Utility Functions","text":"<p>Provides core utility functions for configuration, visualization, and data processing in the AstroLab framework.</p> <p>Modules:</p> Name Description <code>bpy</code> <p>AstroLab BPY (Blender Python API) Integration</p> <code>config</code> <p>AstroLab Configuration Management</p> <code>viz</code> <p>AstroLab Visualization Utilities</p> <p>Functions:</p> Name Description <code>calculate_graph_metrics</code> <p>Calculate basic graph metrics.</p> <code>calculate_mean_density</code> <p>Calculates the mean density of objects in the volume.</p> <code>calculate_volume</code> <p>Calculates the volume of the 3D cuboid enclosing all points.</p> <code>create_spatial_graph</code> <p>Create a spatial graph from Spatial3DTensor.</p> <code>get_device</code> <p>Get the best available device (CUDA if available, else CPU).</p> <code>get_utils_info</code> <p>Get information about available utilities.</p> <code>set_random_seed</code> <p>Set random seed for reproducibility.</p> <code>setup_logging</code> <p>Setup logging configuration.</p> <code>spatial_distance_matrix</code> <p>Compute pairwise distance matrix.</p>"},{"location":"api/astro_lab.utils/#astro_lab.utils.calculate_graph_metrics","title":"calculate_graph_metrics","text":"<pre><code>calculate_graph_metrics(data: Data) -&gt; dict\n</code></pre> <p>Calculate basic graph metrics.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Data</code> <p>PyTorch Geometric Data object</p> required <p>Returns:</p> Type Description <code>dict</code> <p>Dictionary of graph metrics</p> Source code in <code>src\\astro_lab\\utils\\viz\\graph.py</code> <pre><code>def calculate_graph_metrics(data: Data) -&gt; dict:\n    \"\"\"\n    Calculate basic graph metrics.\n\n    Args:\n        data: PyTorch Geometric Data object\n\n    Returns:\n        Dictionary of graph metrics\n    \"\"\"\n    # Use data.x if available, otherwise fall back to data.pos\n    if hasattr(data, \"x\") and data.x is not None:\n        num_nodes = data.x.size(0)\n    elif hasattr(data, \"pos\") and data.pos is not None:\n        num_nodes = data.pos.size(0)\n    else:\n        raise ValueError(\"Data object must have either 'x' or 'pos' attribute\")\n\n    num_edges = data.edge_index.size(1)\n\n    # Calculate degree statistics\n    degrees = torch.bincount(data.edge_index[0], minlength=num_nodes)\n\n    # Calculate clustering coefficient (simplified version)\n    clustering_coeff = _calculate_clustering_coefficient(data.edge_index, num_nodes)\n\n    metrics = {\n        \"num_nodes\": num_nodes,\n        \"num_edges\": num_edges,\n        \"avg_degree\": float(degrees.float().mean()),\n        \"max_degree\": int(degrees.max()),\n        \"min_degree\": int(degrees.min()),\n        \"density\": 2 * num_edges / (num_nodes * (num_nodes - 1))\n        if num_nodes &gt; 1\n        else 0.0,\n        \"clustering_coefficient\": clustering_coeff,\n    }\n\n    return metrics\n</code></pre>"},{"location":"api/astro_lab.utils/#astro_lab.utils.calculate_mean_density","title":"calculate_mean_density","text":"<pre><code>calculate_mean_density(coords: ndarray | Tensor) -&gt; float\n</code></pre> <p>Calculates the mean density of objects in the volume.</p> Source code in <code>src\\astro_lab\\utils\\__init__.py</code> <pre><code>def calculate_mean_density(coords: \"np.ndarray | torch.Tensor\") -&gt; float:\n    \"\"\"Calculates the mean density of objects in the volume.\"\"\"\n    n = coords.shape[0]\n    vol = calculate_volume(coords)\n    return n / vol if vol &gt; 0 else float(\"nan\")\n</code></pre>"},{"location":"api/astro_lab.utils/#astro_lab.utils.calculate_volume","title":"calculate_volume","text":"<pre><code>calculate_volume(coords: ndarray | Tensor) -&gt; float\n</code></pre> <p>Calculates the volume of the 3D cuboid enclosing all points.</p> Source code in <code>src\\astro_lab\\utils\\__init__.py</code> <pre><code>def calculate_volume(coords: \"np.ndarray | torch.Tensor\") -&gt; float:\n    \"\"\"Calculates the volume of the 3D cuboid enclosing all points.\"\"\"\n    if hasattr(coords, \"detach\"):\n        coords = coords.detach().cpu().numpy()\n    x_min, x_max = coords[:, 0].min(), coords[:, 0].max()\n    y_min, y_max = coords[:, 1].min(), coords[:, 1].max()\n    z_min, z_max = coords[:, 2].min(), coords[:, 2].max()\n    return float((x_max - x_min) * (y_max - y_min) * (z_max - z_min))\n</code></pre>"},{"location":"api/astro_lab.utils/#astro_lab.utils.create_spatial_graph","title":"create_spatial_graph","text":"<pre><code>create_spatial_graph(\n    spatial_tensor: Spatial3DTensor,\n    method: str = \"knn\",\n    k: int = 5,\n    radius: float = 1.0,\n    **kwargs: Any\n) -&gt; Data\n</code></pre> <p>Create a spatial graph from Spatial3DTensor.</p> <p>Parameters:</p> Name Type Description Default <code>spatial_tensor</code> <code>Spatial3DTensor</code> <p>Input spatial tensor</p> required <code>method</code> <code>str</code> <p>Graph construction method ('knn', 'radius')</p> <code>'knn'</code> <code>k</code> <code>int</code> <p>Number of neighbors for KNN</p> <code>5</code> <code>radius</code> <code>float</code> <p>Radius for radius graph</p> <code>1.0</code> <code>**kwargs</code> <code>Any</code> <p>Additional arguments</p> <code>{}</code> <p>Returns:</p> Type Description <code>Data</code> <p>PyTorch Geometric Data object</p> Source code in <code>src\\astro_lab\\utils\\viz\\graph.py</code> <pre><code>def create_spatial_graph(\n    spatial_tensor: \"Spatial3DTensor\",\n    method: str = \"knn\",\n    k: int = 5,\n    radius: float = 1.0,\n    **kwargs: Any,\n) -&gt; Data:\n    \"\"\"\n    Create a spatial graph from Spatial3DTensor.\n\n    Args:\n        spatial_tensor: Input spatial tensor\n        method: Graph construction method ('knn', 'radius')\n        k: Number of neighbors for KNN\n        radius: Radius for radius graph\n        **kwargs: Additional arguments\n\n    Returns:\n        PyTorch Geometric Data object\n    \"\"\"\n\n    # Get Cartesian coordinates\n    coords = spatial_tensor.cartesian\n\n    # Create edges\n    if method == \"knn\":\n        # Use GPU-accelerated KNN by default\n        edge_index = _gpu_knn_graph(coords, k=k, **kwargs)\n    elif method == \"radius\":\n        edge_index = radius_graph(coords, r=radius, **kwargs)\n    else:\n        raise ValueError(f\"Unknown method: {method}\")\n\n    # Create data object\n    data = Data(\n        x=coords,\n        edge_index=edge_index,\n        pos=coords,\n    )\n\n    # Add metadata\n    if hasattr(spatial_tensor, \"_metadata\"):\n        for key, value in spatial_tensor._metadata.items():\n            if isinstance(value, torch.Tensor):\n                setattr(data, key, value)\n\n    return data\n</code></pre>"},{"location":"api/astro_lab.utils/#astro_lab.utils.get_device","title":"get_device","text":"<pre><code>get_device() -&gt; device\n</code></pre> <p>Get the best available device (CUDA if available, else CPU).</p> Source code in <code>src\\astro_lab\\utils\\__init__.py</code> <pre><code>def get_device() -&gt; torch.device:\n    \"\"\"Get the best available device (CUDA if available, else CPU).\"\"\"\n    return torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n</code></pre>"},{"location":"api/astro_lab.utils/#astro_lab.utils.get_utils_info","title":"get_utils_info","text":"<pre><code>get_utils_info() -&gt; Dict[str, Any]\n</code></pre> <p>Get information about available utilities.</p> Source code in <code>src\\astro_lab\\utils\\__init__.py</code> <pre><code>def get_utils_info() -&gt; Dict[str, Any]:\n    \"\"\"Get information about available utilities.\"\"\"\n    # Check blender availability WITHOUT importing our blender module\n    blender_available = False\n    try:\n        import bpy  # Direct check without loading our blender module\n\n        blender_available = True\n    except ImportError:\n        pass\n\n    return {\n        \"config_available\": True,\n        \"viz_available\": True,\n        \"blender_available\": blender_available,\n        \"torch_geometric_available\": True,\n        \"graph_available\": True,\n    }\n</code></pre>"},{"location":"api/astro_lab.utils/#astro_lab.utils.set_random_seed","title":"set_random_seed","text":"<pre><code>set_random_seed(seed: int = 42) -&gt; None\n</code></pre> <p>Set random seed for reproducibility.</p> Source code in <code>src\\astro_lab\\utils\\__init__.py</code> <pre><code>def set_random_seed(seed: int = 42) -&gt; None:\n    \"\"\"Set random seed for reproducibility.\"\"\"\n    import random\n\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed(seed)\n        torch.cuda.manual_seed_all(seed)\n</code></pre>"},{"location":"api/astro_lab.utils/#astro_lab.utils.setup_logging","title":"setup_logging","text":"<pre><code>setup_logging(level: int = INFO) -&gt; None\n</code></pre> <p>Setup logging configuration.</p> Source code in <code>src\\astro_lab\\utils\\__init__.py</code> <pre><code>def setup_logging(level: int = logging.INFO) -&gt; None:\n    \"\"\"Setup logging configuration.\"\"\"\n    logging.basicConfig(\n        level=level, format=\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\"\n    )\n</code></pre>"},{"location":"api/astro_lab.utils/#astro_lab.utils.spatial_distance_matrix","title":"spatial_distance_matrix","text":"<pre><code>spatial_distance_matrix(coords: Tensor, metric: str = 'euclidean') -&gt; Tensor\n</code></pre> <p>Compute pairwise distance matrix.</p> <p>Parameters:</p> Name Type Description Default <code>coords</code> <code>Tensor</code> <p>Coordinate tensor [N, 3]</p> required <code>metric</code> <code>str</code> <p>Distance metric ('euclidean', 'angular')</p> <code>'euclidean'</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>Distance matrix [N, N]</p> Source code in <code>src\\astro_lab\\utils\\viz\\graph.py</code> <pre><code>def spatial_distance_matrix(\n    coords: torch.Tensor, metric: str = \"euclidean\"\n) -&gt; torch.Tensor:\n    \"\"\"\n    Compute pairwise distance matrix.\n\n    Args:\n        coords: Coordinate tensor [N, 3]\n        metric: Distance metric ('euclidean', 'angular')\n\n    Returns:\n        Distance matrix [N, N]\n    \"\"\"\n    if metric == \"euclidean\":\n        diff = coords.unsqueeze(1) - coords.unsqueeze(0)\n        distances = torch.norm(diff, dim=-1)\n    elif metric == \"angular\":\n        # Normalize to unit vectors\n        unit_coords = coords / torch.norm(coords, dim=-1, keepdim=True)\n        # Compute dot product\n        dot_products = torch.mm(unit_coords, unit_coords.t())\n        # Clamp to avoid numerical issues\n        dot_products = torch.clamp(dot_products, -1.0, 1.0)\n        # Convert to angular distance\n        distances = torch.acos(dot_products)\n    else:\n        raise ValueError(f\"Unknown metric: {metric}\")\n\n    return distances\n</code></pre>"},{"location":"api/astro_lab.widgets/","title":"astro_lab.widgets","text":""},{"location":"api/astro_lab.widgets/#astro_lab.widgets","title":"widgets","text":"<p>AstroLab Widgets - Interactive astronomical visualization and analysis.</p> <p>Modules:</p> Name Description <code>analysis</code> <p>Analysis Module - GPU-accelerated data analysis</p> <code>astro_lab</code> <p>AstroLab Widget - Main interface for astronomical data visualization and analysis</p> <code>clustering</code> <p>Clustering Module - GPU-accelerated clustering analysis</p> <code>graph</code> <p>Graph Module - PyTorch Geometric integration and model preparation</p> <code>visualization</code> <p>Visualization Module - Multi-backend astronomical data visualization</p> <p>Classes:</p> Name Description <code>AnalysisModule</code> <p>GPU-accelerated data analysis.</p> <code>AstroLabWidget</code> <p>Main widget for astronomical data visualization and analysis.</p> <code>GraphModule</code> <p>PyTorch Geometric integration and model preparation.</p> <code>VisualizationModule</code> <p>Multi-backend visualization for astronomical data.</p>"},{"location":"api/astro_lab.widgets/#astro_lab.widgets.AnalysisModule","title":"AnalysisModule","text":"<p>GPU-accelerated data analysis.</p> <p>Methods:</p> Name Description <code>analyze_density</code> <p>GPU-accelerated local density analysis.</p> <code>analyze_structure</code> <p>GPU-accelerated structure analysis.</p> <code>cluster_data</code> <p>GPU-accelerated clustering analysis.</p> <code>find_neighbors</code> <p>GPU-accelerated neighbor finding using torch_cluster and torch_geometric.</p> Source code in <code>src\\astro_lab\\widgets\\analysis.py</code> <pre><code>class AnalysisModule:\n    \"\"\"\n    GPU-accelerated data analysis.\n    \"\"\"\n\n    def find_neighbors(\n        self,\n        survey_tensor: SurveyTensorDict,\n        k: int = 10,\n        radius: Optional[float] = None,\n        use_gpu: bool = True,\n    ) -&gt; Dict[str, torch.Tensor]:\n        \"\"\"\n        GPU-accelerated neighbor finding using torch_cluster and torch_geometric.\n\n        Args:\n            survey_tensor: SurveyTensor with spatial data\n            k: Number of nearest neighbors (if radius is None)\n            radius: Radius for neighbor search (if provided, overrides k)\n            use_gpu: Whether to use GPU acceleration\n\n        Returns:\n            Dictionary with 'edge_index' and 'distances'\n        \"\"\"\n        spatial_tensor = survey_tensor.get_spatial_tensor()\n        coords = spatial_tensor.data\n\n        device = torch.device(\n            \"cuda\" if torch.cuda.is_available() and use_gpu else \"cpu\"\n        )\n        coords_device = coords.to(device)\n\n        logger.info(f\"Finding neighbors on {device} for {len(coords)} points...\")\n\n        if radius is not None:\n            # Radius-based search\n            edge_index = radius_graph(\n                x=coords_device, r=radius, loop=False, flow=\"source_to_target\"\n            )\n        else:\n            # k-NN search\n            edge_index = torch_cluster.knn_graph(\n                x=coords_device, k=k, loop=False, flow=\"source_to_target\"\n            )\n\n        # Calculate distances\n        distances = torch.norm(\n            coords_device[edge_index[0]] - coords_device[edge_index[1]], dim=1\n        )\n\n        return {\"edge_index\": edge_index.cpu(), \"distances\": distances.cpu()}\n\n    def cluster_data(\n        self,\n        survey_tensor: SurveyTensorDict,\n        eps: float = 10.0,\n        min_samples: int = 5,\n        algorithm: str = \"dbscan\",\n    ) -&gt; Dict[str, Any]:\n        \"\"\"\n        GPU-accelerated clustering analysis.\n\n        Args:\n            survey_tensor: SurveyTensor with spatial data\n            eps: Clustering radius\n            min_samples: Minimum samples for core points\n            algorithm: 'dbscan' or 'hierarchical'\n\n        Returns:\n            Dictionary with cluster labels, statistics, and analysis results\n        \"\"\"\n        spatial_tensor = survey_tensor.get_spatial_tensor()\n        coords = spatial_tensor.data.cpu().numpy()\n\n        logger.info(f\"Clustering {len(coords)} points with {algorithm}...\")\n\n        if algorithm == \"dbscan\":\n            clusterer = DBSCAN(eps=eps, min_samples=min_samples)\n        else:\n            from sklearn.cluster import AgglomerativeClustering\n\n            clusterer = AgglomerativeClustering(distance_threshold=eps, linkage=\"ward\")\n\n        labels = clusterer.fit_predict(coords)\n        labels_tensor = torch.from_numpy(labels)\n\n        # Analyze results\n        unique_labels = set(labels)\n        n_clusters = len(unique_labels) - (1 if -1 in unique_labels else 0)\n        n_noise = sum(1 for label in labels if label == -1)\n\n        # Calculate cluster properties\n        cluster_stats = {}\n        if n_clusters &gt; 0:\n            for cluster_id in unique_labels:\n                if cluster_id == -1:  # Skip noise\n                    continue\n\n                cluster_mask = labels == cluster_id\n                cluster_coords = coords[cluster_mask]\n\n                # Cluster center and size\n                center = cluster_coords.mean(axis=0)\n                distances = np.linalg.norm(cluster_coords - center, axis=1)\n\n                cluster_stats[cluster_id] = {\n                    \"n_points\": int(cluster_mask.sum()),\n                    \"center\": center,\n                    \"radius\": float(distances.max()),\n                    \"density\": float(\n                        cluster_mask.sum()\n                        / (4 / 3 * np.pi * max(distances.max(), 1e-6) ** 3)\n                    ),\n                }\n\n        logger.info(f\"Found {n_clusters} clusters, {n_noise} noise points\")\n\n        return {\n            \"cluster_labels\": labels_tensor,\n            \"n_clusters\": n_clusters,\n            \"n_noise\": n_noise,\n            \"cluster_stats\": cluster_stats,\n            \"coords\": torch.from_numpy(coords),\n        }\n\n    def analyze_density(\n        self, survey_tensor: SurveyTensorDict, radius: float = 5.0, use_gpu: bool = True\n    ) -&gt; torch.Tensor:\n        \"\"\"\n        GPU-accelerated local density analysis.\n\n        Args:\n            survey_tensor: SurveyTensor with spatial data\n            radius: Radius for density calculation\n            use_gpu: Whether to use GPU acceleration\n\n        Returns:\n            Local density for each point\n        \"\"\"\n        if not use_gpu:\n            logger.info(\"Using CPU density analysis...\")\n            return self._analyze_density_cpu(survey_tensor, radius)\n\n        logger.info(\"Using GPU-accelerated density analysis...\")\n        return self._analyze_density_gpu(survey_tensor, radius)\n\n    def analyze_structure(\n        self, survey_tensor: SurveyTensorDict, k: int = 10, use_gpu: bool = True\n    ) -&gt; Dict[str, Any]:\n        \"\"\"\n        GPU-accelerated structure analysis.\n\n        Args:\n            survey_tensor: SurveyTensor with spatial data\n            k: Number of neighbors for analysis\n            use_gpu: Whether to use GPU acceleration\n\n        Returns:\n            Dictionary with structure analysis results\n        \"\"\"\n        if not use_gpu:\n            logger.info(\"Using CPU structure analysis...\")\n            return self._analyze_structure_cpu(survey_tensor, k)\n\n        logger.info(\"Using GPU-accelerated structure analysis...\")\n        return self._analyze_structure_gpu(survey_tensor, k)\n\n    def _analyze_density_gpu(\n        self, survey_tensor: SurveyTensorDict, radius: float\n    ) -&gt; torch.Tensor:\n        \"\"\"GPU-accelerated density analysis using PyTorch Geometric.\"\"\"\n        spatial_tensor = survey_tensor.get_spatial_tensor()\n        coords = spatial_tensor.data\n\n        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n        coords_device = coords.to(device)\n\n        logger.info(f\"GPU density analysis on {device}...\")\n\n        # Create radius graph using PyTorch Geometric\n        edge_index = radius_graph(\n            x=coords_device, r=radius, loop=False, flow=\"source_to_target\"\n        )\n\n        # Count neighbors for each point\n        densities = []\n        for i in range(len(coords)):\n            n_neighbors = (edge_index[0] == i).sum().item()\n            volume = (4 / 3) * np.pi * radius**3\n            density = n_neighbors / volume\n            densities.append(density)\n\n        return torch.tensor(densities, dtype=torch.float32)\n\n    def _analyze_density_cpu(\n        self, survey_tensor: SurveyTensorDict, radius: float\n    ) -&gt; torch.Tensor:\n        \"\"\"CPU fallback for density analysis.\"\"\"\n        spatial_tensor = survey_tensor.get_spatial_tensor()\n        coords = spatial_tensor.data.cpu().numpy()\n\n        tree = BallTree(coords)\n        densities = []\n\n        for i in range(len(coords)):\n            neighbors = tree.query_radius([coords[i]], r=radius)[0]\n            n_neighbors = len(neighbors) - 1  # Exclude self\n            volume = (4 / 3) * np.pi * radius**3\n            density = n_neighbors / volume\n            densities.append(density)\n\n        return torch.tensor(densities, dtype=torch.float32)\n\n    def _analyze_structure_gpu(\n        self, survey_tensor: SurveyTensorDict, k: int\n    ) -&gt; Dict[str, Any]:\n        \"\"\"GPU-accelerated structure analysis.\"\"\"\n        spatial_tensor = survey_tensor.get_spatial_tensor()\n        coords = spatial_tensor.data\n\n        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n        coords_device = coords.to(device)\n\n        logger.info(f\"GPU structure analysis on {device}...\")\n\n        # Create k-NN graph using torch_cluster\n        edge_index = torch_cluster.knn_graph(\n            x=coords_device, k=k, loop=False, flow=\"source_to_target\"\n        )\n\n        # Analyze structure\n        num_nodes = len(coords)\n        num_edges = edge_index.shape[1]\n\n        # Calculate degrees\n        degrees = torch.bincount(edge_index[0], minlength=num_nodes)\n\n        # Calculate average degree\n        avg_degree = degrees.float().mean().item()\n\n        # Calculate graph density\n        max_edges = num_nodes * (num_nodes - 1) / 2\n        graph_density = num_edges / max_edges if max_edges &gt; 0 else 0\n\n        return {\n            \"num_nodes\": num_nodes,\n            \"num_edges\": num_edges,\n            \"avg_degree\": avg_degree,\n            \"graph_density\": graph_density,\n            \"degrees\": degrees.cpu(),\n        }\n\n    def _analyze_structure_cpu(\n        self, survey_tensor: SurveyTensorDict, k: int\n    ) -&gt; Dict[str, Any]:\n        \"\"\"CPU fallback for structure analysis.\"\"\"\n        spatial_tensor = survey_tensor.get_spatial_tensor()\n        coords = spatial_tensor.data.cpu().numpy()\n\n        tree = BallTree(coords)\n        distances, indices = tree.query(coords, k=k + 1)  # +1 to exclude self\n\n        # Create edge list\n        edge_list = []\n        for i, neighbors in enumerate(indices):\n            for j in neighbors[1:]:  # Skip first (self)\n                edge_list.append([i, j])\n\n        edge_index = torch.tensor(edge_list, dtype=torch.long).t()\n\n        # Analyze structure\n        num_nodes = len(coords)\n        num_edges = edge_index.shape[1]\n\n        # Calculate degrees\n        degrees = torch.bincount(edge_index[0], minlength=num_nodes)\n\n        # Calculate average degree\n        avg_degree = degrees.float().mean().item()\n\n        # Calculate graph density\n        max_edges = num_nodes * (num_nodes - 1) / 2\n        graph_density = num_edges / max_edges if max_edges &gt; 0 else 0\n\n        return {\n            \"num_nodes\": num_nodes,\n            \"num_edges\": num_edges,\n            \"avg_degree\": avg_degree,\n            \"graph_density\": graph_density,\n            \"degrees\": degrees,\n        }\n</code></pre>"},{"location":"api/astro_lab.widgets/#astro_lab.widgets.AnalysisModule.analyze_density","title":"analyze_density","text":"<pre><code>analyze_density(\n    survey_tensor: SurveyTensorDict, radius: float = 5.0, use_gpu: bool = True\n) -&gt; Tensor\n</code></pre> <p>GPU-accelerated local density analysis.</p> <p>Parameters:</p> Name Type Description Default <code>survey_tensor</code> <code>SurveyTensorDict</code> <p>SurveyTensor with spatial data</p> required <code>radius</code> <code>float</code> <p>Radius for density calculation</p> <code>5.0</code> <code>use_gpu</code> <code>bool</code> <p>Whether to use GPU acceleration</p> <code>True</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>Local density for each point</p> Source code in <code>src\\astro_lab\\widgets\\analysis.py</code> <pre><code>def analyze_density(\n    self, survey_tensor: SurveyTensorDict, radius: float = 5.0, use_gpu: bool = True\n) -&gt; torch.Tensor:\n    \"\"\"\n    GPU-accelerated local density analysis.\n\n    Args:\n        survey_tensor: SurveyTensor with spatial data\n        radius: Radius for density calculation\n        use_gpu: Whether to use GPU acceleration\n\n    Returns:\n        Local density for each point\n    \"\"\"\n    if not use_gpu:\n        logger.info(\"Using CPU density analysis...\")\n        return self._analyze_density_cpu(survey_tensor, radius)\n\n    logger.info(\"Using GPU-accelerated density analysis...\")\n    return self._analyze_density_gpu(survey_tensor, radius)\n</code></pre>"},{"location":"api/astro_lab.widgets/#astro_lab.widgets.AnalysisModule.analyze_structure","title":"analyze_structure","text":"<pre><code>analyze_structure(\n    survey_tensor: SurveyTensorDict, k: int = 10, use_gpu: bool = True\n) -&gt; Dict[str, Any]\n</code></pre> <p>GPU-accelerated structure analysis.</p> <p>Parameters:</p> Name Type Description Default <code>survey_tensor</code> <code>SurveyTensorDict</code> <p>SurveyTensor with spatial data</p> required <code>k</code> <code>int</code> <p>Number of neighbors for analysis</p> <code>10</code> <code>use_gpu</code> <code>bool</code> <p>Whether to use GPU acceleration</p> <code>True</code> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dictionary with structure analysis results</p> Source code in <code>src\\astro_lab\\widgets\\analysis.py</code> <pre><code>def analyze_structure(\n    self, survey_tensor: SurveyTensorDict, k: int = 10, use_gpu: bool = True\n) -&gt; Dict[str, Any]:\n    \"\"\"\n    GPU-accelerated structure analysis.\n\n    Args:\n        survey_tensor: SurveyTensor with spatial data\n        k: Number of neighbors for analysis\n        use_gpu: Whether to use GPU acceleration\n\n    Returns:\n        Dictionary with structure analysis results\n    \"\"\"\n    if not use_gpu:\n        logger.info(\"Using CPU structure analysis...\")\n        return self._analyze_structure_cpu(survey_tensor, k)\n\n    logger.info(\"Using GPU-accelerated structure analysis...\")\n    return self._analyze_structure_gpu(survey_tensor, k)\n</code></pre>"},{"location":"api/astro_lab.widgets/#astro_lab.widgets.AnalysisModule.cluster_data","title":"cluster_data","text":"<pre><code>cluster_data(\n    survey_tensor: SurveyTensorDict,\n    eps: float = 10.0,\n    min_samples: int = 5,\n    algorithm: str = \"dbscan\",\n) -&gt; Dict[str, Any]\n</code></pre> <p>GPU-accelerated clustering analysis.</p> <p>Parameters:</p> Name Type Description Default <code>survey_tensor</code> <code>SurveyTensorDict</code> <p>SurveyTensor with spatial data</p> required <code>eps</code> <code>float</code> <p>Clustering radius</p> <code>10.0</code> <code>min_samples</code> <code>int</code> <p>Minimum samples for core points</p> <code>5</code> <code>algorithm</code> <code>str</code> <p>'dbscan' or 'hierarchical'</p> <code>'dbscan'</code> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dictionary with cluster labels, statistics, and analysis results</p> Source code in <code>src\\astro_lab\\widgets\\analysis.py</code> <pre><code>def cluster_data(\n    self,\n    survey_tensor: SurveyTensorDict,\n    eps: float = 10.0,\n    min_samples: int = 5,\n    algorithm: str = \"dbscan\",\n) -&gt; Dict[str, Any]:\n    \"\"\"\n    GPU-accelerated clustering analysis.\n\n    Args:\n        survey_tensor: SurveyTensor with spatial data\n        eps: Clustering radius\n        min_samples: Minimum samples for core points\n        algorithm: 'dbscan' or 'hierarchical'\n\n    Returns:\n        Dictionary with cluster labels, statistics, and analysis results\n    \"\"\"\n    spatial_tensor = survey_tensor.get_spatial_tensor()\n    coords = spatial_tensor.data.cpu().numpy()\n\n    logger.info(f\"Clustering {len(coords)} points with {algorithm}...\")\n\n    if algorithm == \"dbscan\":\n        clusterer = DBSCAN(eps=eps, min_samples=min_samples)\n    else:\n        from sklearn.cluster import AgglomerativeClustering\n\n        clusterer = AgglomerativeClustering(distance_threshold=eps, linkage=\"ward\")\n\n    labels = clusterer.fit_predict(coords)\n    labels_tensor = torch.from_numpy(labels)\n\n    # Analyze results\n    unique_labels = set(labels)\n    n_clusters = len(unique_labels) - (1 if -1 in unique_labels else 0)\n    n_noise = sum(1 for label in labels if label == -1)\n\n    # Calculate cluster properties\n    cluster_stats = {}\n    if n_clusters &gt; 0:\n        for cluster_id in unique_labels:\n            if cluster_id == -1:  # Skip noise\n                continue\n\n            cluster_mask = labels == cluster_id\n            cluster_coords = coords[cluster_mask]\n\n            # Cluster center and size\n            center = cluster_coords.mean(axis=0)\n            distances = np.linalg.norm(cluster_coords - center, axis=1)\n\n            cluster_stats[cluster_id] = {\n                \"n_points\": int(cluster_mask.sum()),\n                \"center\": center,\n                \"radius\": float(distances.max()),\n                \"density\": float(\n                    cluster_mask.sum()\n                    / (4 / 3 * np.pi * max(distances.max(), 1e-6) ** 3)\n                ),\n            }\n\n    logger.info(f\"Found {n_clusters} clusters, {n_noise} noise points\")\n\n    return {\n        \"cluster_labels\": labels_tensor,\n        \"n_clusters\": n_clusters,\n        \"n_noise\": n_noise,\n        \"cluster_stats\": cluster_stats,\n        \"coords\": torch.from_numpy(coords),\n    }\n</code></pre>"},{"location":"api/astro_lab.widgets/#astro_lab.widgets.AnalysisModule.find_neighbors","title":"find_neighbors","text":"<pre><code>find_neighbors(\n    survey_tensor: SurveyTensorDict,\n    k: int = 10,\n    radius: Optional[float] = None,\n    use_gpu: bool = True,\n) -&gt; Dict[str, Tensor]\n</code></pre> <p>GPU-accelerated neighbor finding using torch_cluster and torch_geometric.</p> <p>Parameters:</p> Name Type Description Default <code>survey_tensor</code> <code>SurveyTensorDict</code> <p>SurveyTensor with spatial data</p> required <code>k</code> <code>int</code> <p>Number of nearest neighbors (if radius is None)</p> <code>10</code> <code>radius</code> <code>Optional[float]</code> <p>Radius for neighbor search (if provided, overrides k)</p> <code>None</code> <code>use_gpu</code> <code>bool</code> <p>Whether to use GPU acceleration</p> <code>True</code> <p>Returns:</p> Type Description <code>Dict[str, Tensor]</code> <p>Dictionary with 'edge_index' and 'distances'</p> Source code in <code>src\\astro_lab\\widgets\\analysis.py</code> <pre><code>def find_neighbors(\n    self,\n    survey_tensor: SurveyTensorDict,\n    k: int = 10,\n    radius: Optional[float] = None,\n    use_gpu: bool = True,\n) -&gt; Dict[str, torch.Tensor]:\n    \"\"\"\n    GPU-accelerated neighbor finding using torch_cluster and torch_geometric.\n\n    Args:\n        survey_tensor: SurveyTensor with spatial data\n        k: Number of nearest neighbors (if radius is None)\n        radius: Radius for neighbor search (if provided, overrides k)\n        use_gpu: Whether to use GPU acceleration\n\n    Returns:\n        Dictionary with 'edge_index' and 'distances'\n    \"\"\"\n    spatial_tensor = survey_tensor.get_spatial_tensor()\n    coords = spatial_tensor.data\n\n    device = torch.device(\n        \"cuda\" if torch.cuda.is_available() and use_gpu else \"cpu\"\n    )\n    coords_device = coords.to(device)\n\n    logger.info(f\"Finding neighbors on {device} for {len(coords)} points...\")\n\n    if radius is not None:\n        # Radius-based search\n        edge_index = radius_graph(\n            x=coords_device, r=radius, loop=False, flow=\"source_to_target\"\n        )\n    else:\n        # k-NN search\n        edge_index = torch_cluster.knn_graph(\n            x=coords_device, k=k, loop=False, flow=\"source_to_target\"\n        )\n\n    # Calculate distances\n    distances = torch.norm(\n        coords_device[edge_index[0]] - coords_device[edge_index[1]], dim=1\n    )\n\n    return {\"edge_index\": edge_index.cpu(), \"distances\": distances.cpu()}\n</code></pre>"},{"location":"api/astro_lab.widgets/#astro_lab.widgets.AstroLabWidget","title":"AstroLabWidget","text":"<p>Main widget for astronomical data visualization and analysis.</p> <p>Provides a unified interface to all AstroLab functionality including visualization, graph analysis, clustering, and model preparation. Delegates to specialized modules for specific functionality.</p> <p>Methods:</p> Name Description <code>analyze_density</code> <p>GPU-accelerated local density analysis.</p> <code>analyze_structure</code> <p>GPU-accelerated structure analysis.</p> <code>cluster_data</code> <p>GPU-accelerated clustering analysis.</p> <code>create_graph</code> <p>Create PyTorch Geometric Data object for model training.</p> <code>find_neighbors</code> <p>GPU-accelerated neighbor finding.</p> <code>get_model_input_features</code> <p>Extract all available features for model input.</p> <code>plot</code> <p>Visualize astronomical data using the optimal backend.</p> <code>prepare_for_model</code> <p>Prepare data for specific model types.</p> <code>show</code> <p>Show the last created interactive visualization.</p> Source code in <code>src\\astro_lab\\widgets\\astro_lab.py</code> <pre><code>class AstroLabWidget:\n    \"\"\"\n    Main widget for astronomical data visualization and analysis.\n\n    Provides a unified interface to all AstroLab functionality including\n    visualization, graph analysis, clustering, and model preparation.\n    Delegates to specialized modules for specific functionality.\n    \"\"\"\n\n    def __init__(self, **kwargs: Any) -&gt; None:\n        \"\"\"\n        Initialize the widget and set up Blender API attributes if available.\n\n        Args:\n            **kwargs: Additional configuration parameters\n        \"\"\"\n        self._setup_blender_api()\n\n        # Initialize specialized modules\n        self.graph = GraphModule()\n        self.clustering = ClusteringModule()\n        self.analysis = AnalysisModule()\n        self.visualization = VisualizationModule()\n\n        # Set default backend\n        self.backend = kwargs.get(\"backend\", \"auto\")\n\n        logger.info(\"AstroLabWidget initialized with all modules.\")\n\n    def _setup_blender_api(self) -&gt; None:\n        \"\"\"\n        Set up direct API access to Blender (al, ops, data, context, scene).\n\n        If Blender is not available, attributes are set to None.\n        \"\"\"\n        if bpy is None or AstroLabApi is None:\n            self.al = None\n            self.ops = None\n            self.data = None\n            self.context = None\n            self.scene = None\n            logger.warning(\"Blender not available - API access disabled.\")\n            return\n\n        try:\n            self.al = AstroLabApi()\n            self.ops = getattr(bpy, \"ops\", None)\n            self.data = getattr(bpy, \"data\", None)\n            self.context = getattr(bpy, \"context\", None)\n            self.scene = getattr(self.context, \"scene\", None) if self.context else None\n            logger.info(\"Blender API connected. Access via widget.al, widget.ops, ...\")\n        except Exception as e:\n            self.al = None\n            self.ops = None\n            self.data = None\n            self.context = None\n            self.scene = None\n            logger.error(f\"Failed to connect Blender API: {e}\")\n\n    def plot(\n        self,\n        data: Any,\n        plot_type: str = \"scatter_3d\",\n        backend: str = \"auto\",\n        max_points: int = 100_000,\n        **config: Any,\n    ) -&gt; Any:\n        \"\"\"\n        Visualize astronomical data using the optimal backend.\n\n        Args:\n            data: AstroDataset or SurveyTensor\n            plot_type: Type of plot (default: 'scatter_3d')\n            backend: 'auto', 'open3d', 'pyvista', or 'blender'\n            max_points: Maximum number of points to visualize\n            **config: Additional backend-specific configuration\n\n        Returns:\n            The visualization object from the chosen backend\n\n        Raises:\n            ValueError: If backend is not supported\n        \"\"\"\n        logger.info(f\"Plotting with backend: {backend}\")\n\n        # Extract SurveyTensor\n        survey_tensor = self._extract_survey_tensor(data)\n\n        # Subsample if needed\n        if len(survey_tensor.data) &gt; max_points:\n            logger.warning(\n                f\"Subsampling {len(survey_tensor.data)} to {max_points} points for performance.\"\n            )\n            indices = torch.randperm(len(survey_tensor.data))[:max_points]\n            survey_tensor = survey_tensor.apply_mask(indices)\n\n        # Select backend\n        if backend == \"auto\":\n            backend = self.visualization.select_backend(survey_tensor)\n\n        # Delegate to visualization module\n        if backend == \"open3d\":\n            return self.visualization.plot_to_open3d(survey_tensor, plot_type, **config)\n        elif backend == \"pyvista\":\n            return self.visualization.plot_to_pyvista(\n                survey_tensor, plot_type, **config\n            )\n        elif backend == \"blender\":\n            return self.visualization.plot_to_blender(\n                survey_tensor, plot_type, **config\n            )\n        else:\n            raise ValueError(f\"Unsupported backend: {backend}\")\n\n    def show(self, *args: Any, **kwargs: Any) -&gt; None:\n        \"\"\"\n        Show the last created interactive visualization.\n\n        Args:\n            *args: Positional arguments passed to visualization backend\n            **kwargs: Keyword arguments passed to visualization backend\n        \"\"\"\n        self.visualization.show(*args, **kwargs)\n\n    # Delegate to specialized modules\n    def create_graph(\n        self,\n        data: Any,\n        k: int = 10,\n        radius: Optional[float] = None,\n        use_gpu: bool = True,\n    ) -&gt; Any:\n        \"\"\"\n        Create PyTorch Geometric Data object for model training.\n\n        Args:\n            data: AstroDataset or SurveyTensor\n            k: Number of nearest neighbors\n            radius: Radius for neighbor search\n            use_gpu: Whether to use GPU acceleration\n\n        Returns:\n            PyTorch Geometric Data object\n        \"\"\"\n        survey_tensor = self._extract_survey_tensor(data)\n        return self.graph.create_graph(survey_tensor, k, radius, use_gpu)\n\n    def find_neighbors(\n        self,\n        data: Any,\n        k: int = 10,\n        radius: Optional[float] = None,\n        use_gpu: bool = True,\n    ) -&gt; Any:\n        \"\"\"\n        GPU-accelerated neighbor finding.\n\n        Args:\n            data: AstroDataset or SurveyTensor\n            k: Number of nearest neighbors\n            radius: Radius for neighbor search\n            use_gpu: Whether to use GPU acceleration\n\n        Returns:\n            Dictionary with edge_index and distances\n        \"\"\"\n        survey_tensor = self._extract_survey_tensor(data)\n        return self.graph.find_neighbors(survey_tensor, k, radius, use_gpu)\n\n    def prepare_for_model(\n        self, data: Any, model_type: str = \"gnn\", **kwargs: Any\n    ) -&gt; Any:\n        \"\"\"\n        Prepare data for specific model types.\n\n        Args:\n            data: AstroDataset or SurveyTensor\n            model_type: Type of model ('gnn', 'point_cloud', 'spatial')\n            **kwargs: Additional model-specific parameters\n\n        Returns:\n            Prepared data for the specified model type\n        \"\"\"\n        survey_tensor = self._extract_survey_tensor(data)\n        return self.graph.prepare_for_model(survey_tensor, model_type, **kwargs)\n\n    def get_model_input_features(self, data: Any) -&gt; Any:\n        \"\"\"\n        Extract all available features for model input.\n\n        Args:\n            data: AstroDataset or SurveyTensor\n\n        Returns:\n            Dictionary with different feature types\n        \"\"\"\n        survey_tensor = self._extract_survey_tensor(data)\n        return self.graph.get_model_input_features(survey_tensor)\n\n    def cluster_data(\n        self,\n        data: Any,\n        eps: float = 10.0,\n        min_samples: int = 5,\n        algorithm: str = \"dbscan\",\n        use_gpu: bool = True,\n    ) -&gt; Any:\n        \"\"\"\n        GPU-accelerated clustering analysis.\n\n        Args:\n            data: AstroDataset or SurveyTensor\n            eps: Epsilon parameter for DBSCAN\n            min_samples: Minimum samples for DBSCAN\n            algorithm: Clustering algorithm ('dbscan', 'kmeans')\n            use_gpu: Whether to use GPU acceleration\n\n        Returns:\n            Clustering results dictionary\n        \"\"\"\n        survey_tensor = self._extract_survey_tensor(data)\n        return self.clustering.cluster_data(\n            survey_tensor, eps, min_samples, algorithm, use_gpu\n        )\n\n    def analyze_density(\n        self, data: Any, radius: float = 5.0, use_gpu: bool = True\n    ) -&gt; Any:\n        \"\"\"\n        GPU-accelerated local density analysis.\n\n        Args:\n            data: AstroDataset or SurveyTensor\n            radius: Radius for density calculation\n            use_gpu: Whether to use GPU acceleration\n\n        Returns:\n            Density analysis results\n        \"\"\"\n        survey_tensor = self._extract_survey_tensor(data)\n        return self.analysis.analyze_density(survey_tensor, radius, use_gpu)\n\n    def analyze_structure(self, data: Any, k: int = 10, use_gpu: bool = True) -&gt; Any:\n        \"\"\"\n        GPU-accelerated structure analysis.\n\n        Args:\n            data: AstroDataset or SurveyTensor\n            k: Number of neighbors for structure analysis\n            use_gpu: Whether to use GPU acceleration\n\n        Returns:\n            Structure analysis results\n        \"\"\"\n        survey_tensor = self._extract_survey_tensor(data)\n        return self.analysis.analyze_structure(survey_tensor, k, use_gpu)\n\n    def _extract_survey_tensor(self, data: Any) -&gt; Any:\n        \"\"\"\n        Extract or create a SurveyTensorDict from various input data types.\n\n        Args:\n            data: Input data (AstroDataset, SurveyTensorDict, etc.)\n\n        Returns:\n            SurveyTensorDict object\n\n        Raises:\n            ValueError: If data is empty or invalid\n            TypeError: If data type is not supported\n        \"\"\"\n        from ..data.core import AstroDataset\n\n        if isinstance(data, SurveyTensorDict):\n            return data\n\n        if isinstance(data, AstroDataset):\n            if not data:\n                raise ValueError(\"Cannot process an empty AstroDataset.\")\n\n            pyg_data = data[0]\n            survey_name = getattr(data, \"survey\", \"unknown\")\n\n            # Check if pyg_data has attribute x\n            if hasattr(pyg_data, \"x\"):\n                logger.info(\n                    f\"Creating SurveyTensorDict for '{survey_name}' from AstroDataset.\"\n                )\n                return SurveyTensorDict(\n                    data={\"features\": getattr(pyg_data, \"x\")}, survey_name=survey_name\n                )\n            else:\n                raise AttributeError(\"AstroDataset[0] has no attribute 'x'.\")\n\n        raise TypeError(\n            f\"Unsupported data type: {type(data).__name__}. Please provide an AstroDataset or SurveyTensorDict.\"\n        )\n</code></pre>"},{"location":"api/astro_lab.widgets/#astro_lab.widgets.AstroLabWidget.analyze_density","title":"analyze_density","text":"<pre><code>analyze_density(data: Any, radius: float = 5.0, use_gpu: bool = True) -&gt; Any\n</code></pre> <p>GPU-accelerated local density analysis.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Any</code> <p>AstroDataset or SurveyTensor</p> required <code>radius</code> <code>float</code> <p>Radius for density calculation</p> <code>5.0</code> <code>use_gpu</code> <code>bool</code> <p>Whether to use GPU acceleration</p> <code>True</code> <p>Returns:</p> Type Description <code>Any</code> <p>Density analysis results</p> Source code in <code>src\\astro_lab\\widgets\\astro_lab.py</code> <pre><code>def analyze_density(\n    self, data: Any, radius: float = 5.0, use_gpu: bool = True\n) -&gt; Any:\n    \"\"\"\n    GPU-accelerated local density analysis.\n\n    Args:\n        data: AstroDataset or SurveyTensor\n        radius: Radius for density calculation\n        use_gpu: Whether to use GPU acceleration\n\n    Returns:\n        Density analysis results\n    \"\"\"\n    survey_tensor = self._extract_survey_tensor(data)\n    return self.analysis.analyze_density(survey_tensor, radius, use_gpu)\n</code></pre>"},{"location":"api/astro_lab.widgets/#astro_lab.widgets.AstroLabWidget.analyze_structure","title":"analyze_structure","text":"<pre><code>analyze_structure(data: Any, k: int = 10, use_gpu: bool = True) -&gt; Any\n</code></pre> <p>GPU-accelerated structure analysis.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Any</code> <p>AstroDataset or SurveyTensor</p> required <code>k</code> <code>int</code> <p>Number of neighbors for structure analysis</p> <code>10</code> <code>use_gpu</code> <code>bool</code> <p>Whether to use GPU acceleration</p> <code>True</code> <p>Returns:</p> Type Description <code>Any</code> <p>Structure analysis results</p> Source code in <code>src\\astro_lab\\widgets\\astro_lab.py</code> <pre><code>def analyze_structure(self, data: Any, k: int = 10, use_gpu: bool = True) -&gt; Any:\n    \"\"\"\n    GPU-accelerated structure analysis.\n\n    Args:\n        data: AstroDataset or SurveyTensor\n        k: Number of neighbors for structure analysis\n        use_gpu: Whether to use GPU acceleration\n\n    Returns:\n        Structure analysis results\n    \"\"\"\n    survey_tensor = self._extract_survey_tensor(data)\n    return self.analysis.analyze_structure(survey_tensor, k, use_gpu)\n</code></pre>"},{"location":"api/astro_lab.widgets/#astro_lab.widgets.AstroLabWidget.cluster_data","title":"cluster_data","text":"<pre><code>cluster_data(\n    data: Any,\n    eps: float = 10.0,\n    min_samples: int = 5,\n    algorithm: str = \"dbscan\",\n    use_gpu: bool = True,\n) -&gt; Any\n</code></pre> <p>GPU-accelerated clustering analysis.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Any</code> <p>AstroDataset or SurveyTensor</p> required <code>eps</code> <code>float</code> <p>Epsilon parameter for DBSCAN</p> <code>10.0</code> <code>min_samples</code> <code>int</code> <p>Minimum samples for DBSCAN</p> <code>5</code> <code>algorithm</code> <code>str</code> <p>Clustering algorithm ('dbscan', 'kmeans')</p> <code>'dbscan'</code> <code>use_gpu</code> <code>bool</code> <p>Whether to use GPU acceleration</p> <code>True</code> <p>Returns:</p> Type Description <code>Any</code> <p>Clustering results dictionary</p> Source code in <code>src\\astro_lab\\widgets\\astro_lab.py</code> <pre><code>def cluster_data(\n    self,\n    data: Any,\n    eps: float = 10.0,\n    min_samples: int = 5,\n    algorithm: str = \"dbscan\",\n    use_gpu: bool = True,\n) -&gt; Any:\n    \"\"\"\n    GPU-accelerated clustering analysis.\n\n    Args:\n        data: AstroDataset or SurveyTensor\n        eps: Epsilon parameter for DBSCAN\n        min_samples: Minimum samples for DBSCAN\n        algorithm: Clustering algorithm ('dbscan', 'kmeans')\n        use_gpu: Whether to use GPU acceleration\n\n    Returns:\n        Clustering results dictionary\n    \"\"\"\n    survey_tensor = self._extract_survey_tensor(data)\n    return self.clustering.cluster_data(\n        survey_tensor, eps, min_samples, algorithm, use_gpu\n    )\n</code></pre>"},{"location":"api/astro_lab.widgets/#astro_lab.widgets.AstroLabWidget.create_graph","title":"create_graph","text":"<pre><code>create_graph(\n    data: Any, k: int = 10, radius: Optional[float] = None, use_gpu: bool = True\n) -&gt; Any\n</code></pre> <p>Create PyTorch Geometric Data object for model training.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Any</code> <p>AstroDataset or SurveyTensor</p> required <code>k</code> <code>int</code> <p>Number of nearest neighbors</p> <code>10</code> <code>radius</code> <code>Optional[float]</code> <p>Radius for neighbor search</p> <code>None</code> <code>use_gpu</code> <code>bool</code> <p>Whether to use GPU acceleration</p> <code>True</code> <p>Returns:</p> Type Description <code>Any</code> <p>PyTorch Geometric Data object</p> Source code in <code>src\\astro_lab\\widgets\\astro_lab.py</code> <pre><code>def create_graph(\n    self,\n    data: Any,\n    k: int = 10,\n    radius: Optional[float] = None,\n    use_gpu: bool = True,\n) -&gt; Any:\n    \"\"\"\n    Create PyTorch Geometric Data object for model training.\n\n    Args:\n        data: AstroDataset or SurveyTensor\n        k: Number of nearest neighbors\n        radius: Radius for neighbor search\n        use_gpu: Whether to use GPU acceleration\n\n    Returns:\n        PyTorch Geometric Data object\n    \"\"\"\n    survey_tensor = self._extract_survey_tensor(data)\n    return self.graph.create_graph(survey_tensor, k, radius, use_gpu)\n</code></pre>"},{"location":"api/astro_lab.widgets/#astro_lab.widgets.AstroLabWidget.find_neighbors","title":"find_neighbors","text":"<pre><code>find_neighbors(\n    data: Any, k: int = 10, radius: Optional[float] = None, use_gpu: bool = True\n) -&gt; Any\n</code></pre> <p>GPU-accelerated neighbor finding.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Any</code> <p>AstroDataset or SurveyTensor</p> required <code>k</code> <code>int</code> <p>Number of nearest neighbors</p> <code>10</code> <code>radius</code> <code>Optional[float]</code> <p>Radius for neighbor search</p> <code>None</code> <code>use_gpu</code> <code>bool</code> <p>Whether to use GPU acceleration</p> <code>True</code> <p>Returns:</p> Type Description <code>Any</code> <p>Dictionary with edge_index and distances</p> Source code in <code>src\\astro_lab\\widgets\\astro_lab.py</code> <pre><code>def find_neighbors(\n    self,\n    data: Any,\n    k: int = 10,\n    radius: Optional[float] = None,\n    use_gpu: bool = True,\n) -&gt; Any:\n    \"\"\"\n    GPU-accelerated neighbor finding.\n\n    Args:\n        data: AstroDataset or SurveyTensor\n        k: Number of nearest neighbors\n        radius: Radius for neighbor search\n        use_gpu: Whether to use GPU acceleration\n\n    Returns:\n        Dictionary with edge_index and distances\n    \"\"\"\n    survey_tensor = self._extract_survey_tensor(data)\n    return self.graph.find_neighbors(survey_tensor, k, radius, use_gpu)\n</code></pre>"},{"location":"api/astro_lab.widgets/#astro_lab.widgets.AstroLabWidget.get_model_input_features","title":"get_model_input_features","text":"<pre><code>get_model_input_features(data: Any) -&gt; Any\n</code></pre> <p>Extract all available features for model input.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Any</code> <p>AstroDataset or SurveyTensor</p> required <p>Returns:</p> Type Description <code>Any</code> <p>Dictionary with different feature types</p> Source code in <code>src\\astro_lab\\widgets\\astro_lab.py</code> <pre><code>def get_model_input_features(self, data: Any) -&gt; Any:\n    \"\"\"\n    Extract all available features for model input.\n\n    Args:\n        data: AstroDataset or SurveyTensor\n\n    Returns:\n        Dictionary with different feature types\n    \"\"\"\n    survey_tensor = self._extract_survey_tensor(data)\n    return self.graph.get_model_input_features(survey_tensor)\n</code></pre>"},{"location":"api/astro_lab.widgets/#astro_lab.widgets.AstroLabWidget.plot","title":"plot","text":"<pre><code>plot(\n    data: Any,\n    plot_type: str = \"scatter_3d\",\n    backend: str = \"auto\",\n    max_points: int = 100000,\n    **config: Any\n) -&gt; Any\n</code></pre> <p>Visualize astronomical data using the optimal backend.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Any</code> <p>AstroDataset or SurveyTensor</p> required <code>plot_type</code> <code>str</code> <p>Type of plot (default: 'scatter_3d')</p> <code>'scatter_3d'</code> <code>backend</code> <code>str</code> <p>'auto', 'open3d', 'pyvista', or 'blender'</p> <code>'auto'</code> <code>max_points</code> <code>int</code> <p>Maximum number of points to visualize</p> <code>100000</code> <code>**config</code> <code>Any</code> <p>Additional backend-specific configuration</p> <code>{}</code> <p>Returns:</p> Type Description <code>Any</code> <p>The visualization object from the chosen backend</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If backend is not supported</p> Source code in <code>src\\astro_lab\\widgets\\astro_lab.py</code> <pre><code>def plot(\n    self,\n    data: Any,\n    plot_type: str = \"scatter_3d\",\n    backend: str = \"auto\",\n    max_points: int = 100_000,\n    **config: Any,\n) -&gt; Any:\n    \"\"\"\n    Visualize astronomical data using the optimal backend.\n\n    Args:\n        data: AstroDataset or SurveyTensor\n        plot_type: Type of plot (default: 'scatter_3d')\n        backend: 'auto', 'open3d', 'pyvista', or 'blender'\n        max_points: Maximum number of points to visualize\n        **config: Additional backend-specific configuration\n\n    Returns:\n        The visualization object from the chosen backend\n\n    Raises:\n        ValueError: If backend is not supported\n    \"\"\"\n    logger.info(f\"Plotting with backend: {backend}\")\n\n    # Extract SurveyTensor\n    survey_tensor = self._extract_survey_tensor(data)\n\n    # Subsample if needed\n    if len(survey_tensor.data) &gt; max_points:\n        logger.warning(\n            f\"Subsampling {len(survey_tensor.data)} to {max_points} points for performance.\"\n        )\n        indices = torch.randperm(len(survey_tensor.data))[:max_points]\n        survey_tensor = survey_tensor.apply_mask(indices)\n\n    # Select backend\n    if backend == \"auto\":\n        backend = self.visualization.select_backend(survey_tensor)\n\n    # Delegate to visualization module\n    if backend == \"open3d\":\n        return self.visualization.plot_to_open3d(survey_tensor, plot_type, **config)\n    elif backend == \"pyvista\":\n        return self.visualization.plot_to_pyvista(\n            survey_tensor, plot_type, **config\n        )\n    elif backend == \"blender\":\n        return self.visualization.plot_to_blender(\n            survey_tensor, plot_type, **config\n        )\n    else:\n        raise ValueError(f\"Unsupported backend: {backend}\")\n</code></pre>"},{"location":"api/astro_lab.widgets/#astro_lab.widgets.AstroLabWidget.prepare_for_model","title":"prepare_for_model","text":"<pre><code>prepare_for_model(data: Any, model_type: str = 'gnn', **kwargs: Any) -&gt; Any\n</code></pre> <p>Prepare data for specific model types.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Any</code> <p>AstroDataset or SurveyTensor</p> required <code>model_type</code> <code>str</code> <p>Type of model ('gnn', 'point_cloud', 'spatial')</p> <code>'gnn'</code> <code>**kwargs</code> <code>Any</code> <p>Additional model-specific parameters</p> <code>{}</code> <p>Returns:</p> Type Description <code>Any</code> <p>Prepared data for the specified model type</p> Source code in <code>src\\astro_lab\\widgets\\astro_lab.py</code> <pre><code>def prepare_for_model(\n    self, data: Any, model_type: str = \"gnn\", **kwargs: Any\n) -&gt; Any:\n    \"\"\"\n    Prepare data for specific model types.\n\n    Args:\n        data: AstroDataset or SurveyTensor\n        model_type: Type of model ('gnn', 'point_cloud', 'spatial')\n        **kwargs: Additional model-specific parameters\n\n    Returns:\n        Prepared data for the specified model type\n    \"\"\"\n    survey_tensor = self._extract_survey_tensor(data)\n    return self.graph.prepare_for_model(survey_tensor, model_type, **kwargs)\n</code></pre>"},{"location":"api/astro_lab.widgets/#astro_lab.widgets.AstroLabWidget.show","title":"show","text":"<pre><code>show(*args: Any, **kwargs: Any) -&gt; None\n</code></pre> <p>Show the last created interactive visualization.</p> <p>Parameters:</p> Name Type Description Default <code>*args</code> <code>Any</code> <p>Positional arguments passed to visualization backend</p> <code>()</code> <code>**kwargs</code> <code>Any</code> <p>Keyword arguments passed to visualization backend</p> <code>{}</code> Source code in <code>src\\astro_lab\\widgets\\astro_lab.py</code> <pre><code>def show(self, *args: Any, **kwargs: Any) -&gt; None:\n    \"\"\"\n    Show the last created interactive visualization.\n\n    Args:\n        *args: Positional arguments passed to visualization backend\n        **kwargs: Keyword arguments passed to visualization backend\n    \"\"\"\n    self.visualization.show(*args, **kwargs)\n</code></pre>"},{"location":"api/astro_lab.widgets/#astro_lab.widgets.GraphModule","title":"GraphModule","text":"<p>PyTorch Geometric integration and model preparation.</p> <p>Provides methods for creating graph representations of astronomical data and preparing data for machine learning models.</p> <p>Methods:</p> Name Description <code>create_graph</code> <p>Create PyTorch Geometric Data object for model training.</p> <code>find_neighbors</code> <p>GPU-accelerated neighbor finding using torch_cluster and torch_geometric.</p> <code>get_model_input_features</code> <p>Extract all available features for model input.</p> <code>prepare_for_model</code> <p>Prepare data for specific model types from the models module.</p> Source code in <code>src\\astro_lab\\widgets\\graph.py</code> <pre><code>class GraphModule:\n    \"\"\"\n    PyTorch Geometric integration and model preparation.\n\n    Provides methods for creating graph representations of astronomical data\n    and preparing data for machine learning models.\n    \"\"\"\n\n    def create_graph(\n        self,\n        survey_tensor: SurveyTensorDict,\n        k: int = 10,\n        radius: Optional[float] = None,\n        use_gpu: bool = True,\n        **kwargs: Any,\n    ) -&gt; Data:\n        \"\"\"\n        Create PyTorch Geometric Data object for model training.\n\n        Args:\n            survey_tensor: SurveyTensorDict with spatial and photometric data\n            k: Number of nearest neighbors for k-NN graph\n            radius: Radius for neighbor search (overrides k if provided)\n            use_gpu: Whether to use GPU acceleration\n            **kwargs: Additional parameters\n\n        Returns:\n            PyTorch Geometric Data object with node features and edge indices\n        \"\"\"\n        spatial_tensor = survey_tensor.get_spatial_tensor()\n\n        # Get node features (coordinates + photometric data)\n        node_features = []\n        node_features.append(spatial_tensor.data)  # 3D coordinates\n\n        try:\n            phot_tensor = survey_tensor.get_photometric_tensor()\n            node_features.append(phot_tensor.data)\n        except Exception as e:\n            logger.warning(f\"Could not add photometric features: {e}\")\n\n        # Combine features\n        x = torch.cat(node_features, dim=1)\n\n        # Get edge index from neighbor finding\n        neighbor_result = self.find_neighbors(\n            survey_tensor, k=k, radius=radius, use_gpu=use_gpu\n        )\n        edge_index = neighbor_result[\"edge_index\"]\n\n        # Create PyG Data object\n        graph_data = Data(\n            x=x,\n            edge_index=edge_index,\n            pos=spatial_tensor.data,  # Node positions for spatial models\n        )\n\n        logger.info(\n            f\"Created graph with {len(x)} nodes and {edge_index.shape[1]} edges\"\n        )\n        return graph_data\n\n    def find_neighbors(\n        self,\n        survey_tensor: SurveyTensorDict,\n        k: int = 10,\n        radius: Optional[float] = None,\n        use_gpu: bool = True,\n    ) -&gt; Dict[str, torch.Tensor]:\n        \"\"\"\n        GPU-accelerated neighbor finding using torch_cluster and torch_geometric.\n\n        Args:\n            survey_tensor: SurveyTensorDict with spatial data\n            k: Number of nearest neighbors (if radius is None)\n            radius: Radius for neighbor search (if provided, overrides k)\n            use_gpu: Whether to use GPU acceleration\n\n        Returns:\n            Dictionary with 'edge_index' and 'distances' tensors\n        \"\"\"\n        spatial_tensor = survey_tensor.get_spatial_tensor()\n        coords = spatial_tensor.data\n\n        device = torch.device(\n            \"cuda\" if torch.cuda.is_available() and use_gpu else \"cpu\"\n        )\n        coords_device = coords.to(device)\n\n        logger.info(f\"Finding neighbors on {device} for {len(coords)} points...\")\n\n        if radius is not None:\n            # Radius-based search using PyTorch Geometric\n            edge_index = radius_graph(\n                x=coords_device, r=radius, loop=False, flow=\"source_to_target\"\n            )\n        else:\n            # k-NN search using torch_cluster\n            edge_index = torch_cluster.knn_graph(\n                x=coords_device, k=k, loop=False, flow=\"source_to_target\"\n            )\n\n        # Calculate distances\n        distances = torch.norm(\n            coords_device[edge_index[0]] - coords_device[edge_index[1]], dim=1\n        )\n\n        return {\"edge_index\": edge_index.cpu(), \"distances\": distances.cpu()}\n\n    def prepare_for_model(\n        self, survey_tensor: SurveyTensorDict, model_type: str = \"gnn\", **kwargs: Any\n    ) -&gt; Any:\n        \"\"\"\n        Prepare data for specific model types from the models module.\n\n        Args:\n            survey_tensor: SurveyTensorDict with data\n            model_type: Type of model ('gnn', 'point_cloud', 'spatial')\n            **kwargs: Additional model-specific parameters\n\n        Returns:\n            Prepared data for the specified model type\n\n        Raises:\n            ValueError: If model_type is not supported\n        \"\"\"\n        if model_type == \"gnn\":\n            return self.create_graph(survey_tensor, **kwargs)\n        elif model_type == \"point_cloud\":\n            spatial_tensor = survey_tensor.get_spatial_tensor()\n            return spatial_tensor.data\n        elif model_type == \"spatial\":\n            return survey_tensor\n        else:\n            raise ValueError(f\"Unknown model type: {model_type}\")\n\n    def get_model_input_features(\n        self, survey_tensor: SurveyTensorDict\n    ) -&gt; Dict[str, torch.Tensor]:\n        \"\"\"\n        Extract all available features for model input.\n\n        Args:\n            survey_tensor: SurveyTensorDict with data\n\n        Returns:\n            Dictionary with different feature types (spatial, photometric, raw)\n        \"\"\"\n        features = {}\n\n        # Spatial features\n        spatial_tensor = survey_tensor.get_spatial_tensor()\n        features[\"spatial\"] = spatial_tensor.data\n        features[\"x\"] = spatial_tensor.x\n        features[\"y\"] = spatial_tensor.y\n        features[\"z\"] = spatial_tensor.z\n\n        # Photometric features\n        try:\n            phot_tensor = survey_tensor.get_photometric_tensor()\n            features[\"photometric\"] = phot_tensor.data\n            features[\"bands\"] = phot_tensor.bands\n        except Exception as e:\n            logger.warning(f\"Could not extract photometric features: {e}\")\n\n        # Raw survey data\n        features[\"raw\"] = survey_tensor.data\n\n        return features\n</code></pre>"},{"location":"api/astro_lab.widgets/#astro_lab.widgets.GraphModule.create_graph","title":"create_graph","text":"<pre><code>create_graph(\n    survey_tensor: SurveyTensorDict,\n    k: int = 10,\n    radius: Optional[float] = None,\n    use_gpu: bool = True,\n    **kwargs: Any\n) -&gt; Data\n</code></pre> <p>Create PyTorch Geometric Data object for model training.</p> <p>Parameters:</p> Name Type Description Default <code>survey_tensor</code> <code>SurveyTensorDict</code> <p>SurveyTensorDict with spatial and photometric data</p> required <code>k</code> <code>int</code> <p>Number of nearest neighbors for k-NN graph</p> <code>10</code> <code>radius</code> <code>Optional[float]</code> <p>Radius for neighbor search (overrides k if provided)</p> <code>None</code> <code>use_gpu</code> <code>bool</code> <p>Whether to use GPU acceleration</p> <code>True</code> <code>**kwargs</code> <code>Any</code> <p>Additional parameters</p> <code>{}</code> <p>Returns:</p> Type Description <code>Data</code> <p>PyTorch Geometric Data object with node features and edge indices</p> Source code in <code>src\\astro_lab\\widgets\\graph.py</code> <pre><code>def create_graph(\n    self,\n    survey_tensor: SurveyTensorDict,\n    k: int = 10,\n    radius: Optional[float] = None,\n    use_gpu: bool = True,\n    **kwargs: Any,\n) -&gt; Data:\n    \"\"\"\n    Create PyTorch Geometric Data object for model training.\n\n    Args:\n        survey_tensor: SurveyTensorDict with spatial and photometric data\n        k: Number of nearest neighbors for k-NN graph\n        radius: Radius for neighbor search (overrides k if provided)\n        use_gpu: Whether to use GPU acceleration\n        **kwargs: Additional parameters\n\n    Returns:\n        PyTorch Geometric Data object with node features and edge indices\n    \"\"\"\n    spatial_tensor = survey_tensor.get_spatial_tensor()\n\n    # Get node features (coordinates + photometric data)\n    node_features = []\n    node_features.append(spatial_tensor.data)  # 3D coordinates\n\n    try:\n        phot_tensor = survey_tensor.get_photometric_tensor()\n        node_features.append(phot_tensor.data)\n    except Exception as e:\n        logger.warning(f\"Could not add photometric features: {e}\")\n\n    # Combine features\n    x = torch.cat(node_features, dim=1)\n\n    # Get edge index from neighbor finding\n    neighbor_result = self.find_neighbors(\n        survey_tensor, k=k, radius=radius, use_gpu=use_gpu\n    )\n    edge_index = neighbor_result[\"edge_index\"]\n\n    # Create PyG Data object\n    graph_data = Data(\n        x=x,\n        edge_index=edge_index,\n        pos=spatial_tensor.data,  # Node positions for spatial models\n    )\n\n    logger.info(\n        f\"Created graph with {len(x)} nodes and {edge_index.shape[1]} edges\"\n    )\n    return graph_data\n</code></pre>"},{"location":"api/astro_lab.widgets/#astro_lab.widgets.GraphModule.find_neighbors","title":"find_neighbors","text":"<pre><code>find_neighbors(\n    survey_tensor: SurveyTensorDict,\n    k: int = 10,\n    radius: Optional[float] = None,\n    use_gpu: bool = True,\n) -&gt; Dict[str, Tensor]\n</code></pre> <p>GPU-accelerated neighbor finding using torch_cluster and torch_geometric.</p> <p>Parameters:</p> Name Type Description Default <code>survey_tensor</code> <code>SurveyTensorDict</code> <p>SurveyTensorDict with spatial data</p> required <code>k</code> <code>int</code> <p>Number of nearest neighbors (if radius is None)</p> <code>10</code> <code>radius</code> <code>Optional[float]</code> <p>Radius for neighbor search (if provided, overrides k)</p> <code>None</code> <code>use_gpu</code> <code>bool</code> <p>Whether to use GPU acceleration</p> <code>True</code> <p>Returns:</p> Type Description <code>Dict[str, Tensor]</code> <p>Dictionary with 'edge_index' and 'distances' tensors</p> Source code in <code>src\\astro_lab\\widgets\\graph.py</code> <pre><code>def find_neighbors(\n    self,\n    survey_tensor: SurveyTensorDict,\n    k: int = 10,\n    radius: Optional[float] = None,\n    use_gpu: bool = True,\n) -&gt; Dict[str, torch.Tensor]:\n    \"\"\"\n    GPU-accelerated neighbor finding using torch_cluster and torch_geometric.\n\n    Args:\n        survey_tensor: SurveyTensorDict with spatial data\n        k: Number of nearest neighbors (if radius is None)\n        radius: Radius for neighbor search (if provided, overrides k)\n        use_gpu: Whether to use GPU acceleration\n\n    Returns:\n        Dictionary with 'edge_index' and 'distances' tensors\n    \"\"\"\n    spatial_tensor = survey_tensor.get_spatial_tensor()\n    coords = spatial_tensor.data\n\n    device = torch.device(\n        \"cuda\" if torch.cuda.is_available() and use_gpu else \"cpu\"\n    )\n    coords_device = coords.to(device)\n\n    logger.info(f\"Finding neighbors on {device} for {len(coords)} points...\")\n\n    if radius is not None:\n        # Radius-based search using PyTorch Geometric\n        edge_index = radius_graph(\n            x=coords_device, r=radius, loop=False, flow=\"source_to_target\"\n        )\n    else:\n        # k-NN search using torch_cluster\n        edge_index = torch_cluster.knn_graph(\n            x=coords_device, k=k, loop=False, flow=\"source_to_target\"\n        )\n\n    # Calculate distances\n    distances = torch.norm(\n        coords_device[edge_index[0]] - coords_device[edge_index[1]], dim=1\n    )\n\n    return {\"edge_index\": edge_index.cpu(), \"distances\": distances.cpu()}\n</code></pre>"},{"location":"api/astro_lab.widgets/#astro_lab.widgets.GraphModule.get_model_input_features","title":"get_model_input_features","text":"<pre><code>get_model_input_features(survey_tensor: SurveyTensorDict) -&gt; Dict[str, Tensor]\n</code></pre> <p>Extract all available features for model input.</p> <p>Parameters:</p> Name Type Description Default <code>survey_tensor</code> <code>SurveyTensorDict</code> <p>SurveyTensorDict with data</p> required <p>Returns:</p> Type Description <code>Dict[str, Tensor]</code> <p>Dictionary with different feature types (spatial, photometric, raw)</p> Source code in <code>src\\astro_lab\\widgets\\graph.py</code> <pre><code>def get_model_input_features(\n    self, survey_tensor: SurveyTensorDict\n) -&gt; Dict[str, torch.Tensor]:\n    \"\"\"\n    Extract all available features for model input.\n\n    Args:\n        survey_tensor: SurveyTensorDict with data\n\n    Returns:\n        Dictionary with different feature types (spatial, photometric, raw)\n    \"\"\"\n    features = {}\n\n    # Spatial features\n    spatial_tensor = survey_tensor.get_spatial_tensor()\n    features[\"spatial\"] = spatial_tensor.data\n    features[\"x\"] = spatial_tensor.x\n    features[\"y\"] = spatial_tensor.y\n    features[\"z\"] = spatial_tensor.z\n\n    # Photometric features\n    try:\n        phot_tensor = survey_tensor.get_photometric_tensor()\n        features[\"photometric\"] = phot_tensor.data\n        features[\"bands\"] = phot_tensor.bands\n    except Exception as e:\n        logger.warning(f\"Could not extract photometric features: {e}\")\n\n    # Raw survey data\n    features[\"raw\"] = survey_tensor.data\n\n    return features\n</code></pre>"},{"location":"api/astro_lab.widgets/#astro_lab.widgets.GraphModule.prepare_for_model","title":"prepare_for_model","text":"<pre><code>prepare_for_model(\n    survey_tensor: SurveyTensorDict, model_type: str = \"gnn\", **kwargs: Any\n) -&gt; Any\n</code></pre> <p>Prepare data for specific model types from the models module.</p> <p>Parameters:</p> Name Type Description Default <code>survey_tensor</code> <code>SurveyTensorDict</code> <p>SurveyTensorDict with data</p> required <code>model_type</code> <code>str</code> <p>Type of model ('gnn', 'point_cloud', 'spatial')</p> <code>'gnn'</code> <code>**kwargs</code> <code>Any</code> <p>Additional model-specific parameters</p> <code>{}</code> <p>Returns:</p> Type Description <code>Any</code> <p>Prepared data for the specified model type</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If model_type is not supported</p> Source code in <code>src\\astro_lab\\widgets\\graph.py</code> <pre><code>def prepare_for_model(\n    self, survey_tensor: SurveyTensorDict, model_type: str = \"gnn\", **kwargs: Any\n) -&gt; Any:\n    \"\"\"\n    Prepare data for specific model types from the models module.\n\n    Args:\n        survey_tensor: SurveyTensorDict with data\n        model_type: Type of model ('gnn', 'point_cloud', 'spatial')\n        **kwargs: Additional model-specific parameters\n\n    Returns:\n        Prepared data for the specified model type\n\n    Raises:\n        ValueError: If model_type is not supported\n    \"\"\"\n    if model_type == \"gnn\":\n        return self.create_graph(survey_tensor, **kwargs)\n    elif model_type == \"point_cloud\":\n        spatial_tensor = survey_tensor.get_spatial_tensor()\n        return spatial_tensor.data\n    elif model_type == \"spatial\":\n        return survey_tensor\n    else:\n        raise ValueError(f\"Unknown model type: {model_type}\")\n</code></pre>"},{"location":"api/astro_lab.widgets/#astro_lab.widgets.VisualizationModule","title":"VisualizationModule","text":"<p>Multi-backend visualization for astronomical data.</p> <p>Supports Open3D, PyVista, and Blender backends for interactive visualization of astronomical datasets.</p> <p>Methods:</p> Name Description <code>plot_to_blender</code> <p>Visualize data using Blender backend.</p> <code>plot_to_open3d</code> <p>Visualize data using Open3D backend.</p> <code>plot_to_pyvista</code> <p>Visualize data using PyVista backend.</p> <code>select_backend</code> <p>Automatically select the best visualization backend.</p> <code>show</code> <p>Show the last created interactive visualization.</p> Source code in <code>src\\astro_lab\\widgets\\visualization.py</code> <pre><code>class VisualizationModule:\n    \"\"\"\n    Multi-backend visualization for astronomical data.\n\n    Supports Open3D, PyVista, and Blender backends for interactive\n    visualization of astronomical datasets.\n    \"\"\"\n\n    def __init__(self) -&gt; None:\n        \"\"\"Initialize the visualization module.\"\"\"\n        self.plotter = None\n\n    def plot_to_open3d(\n        self,\n        survey_tensor: SurveyTensorDict,\n        plot_type: str = \"scatter_3d\",\n        **config: Any,\n    ) -&gt; Any:\n        \"\"\"\n        Visualize data using Open3D backend.\n\n        Args:\n            survey_tensor: SurveyTensorDict with spatial and photometric data\n            plot_type: Type of plot ('scatter_3d', 'point_cloud')\n            **config: Additional Open3D-specific configuration\n\n        Returns:\n            Open3D visualization object\n        \"\"\"\n        try:\n            import open3d as o3d\n\n            logger.info(\"Visualizing with Open3D...\")\n\n            # Extract coordinates\n            spatial_tensor = survey_tensor.get_spatial_tensor()\n            coords_3d = spatial_tensor.data.numpy()\n\n            # Create point cloud\n            pcd = o3d.geometry.PointCloud()\n            pcd.points = o3d.utility.Vector3dVector(coords_3d)\n\n            # Add colors if photometric data available\n            try:\n                phot_tensor = survey_tensor.get_photometric_tensor()\n                colors = self._create_colors_from_photometry(phot_tensor)\n                pcd.colors = o3d.utility.Vector3dVector(colors)\n            except Exception as e:\n                logger.warning(\n                    f\"Could not add photometric colors: {e}. Using default colors.\"\n                )\n                # Use default colors based on position\n                colors = self._create_position_colors(coords_3d)\n                pcd.colors = o3d.utility.Vector3dVector(colors)\n\n            return pcd\n\n        except ImportError:\n            logger.warning(\"Open3D not available. Install with: pip install open3d\")\n            return None\n\n    def plot_to_pyvista(\n        self,\n        survey_tensor: SurveyTensorDict,\n        plot_type: str = \"scatter_3d\",\n        **config: Any,\n    ) -&gt; Any:\n        \"\"\"\n        Visualize data using PyVista backend.\n\n        Args:\n            survey_tensor: SurveyTensorDict with spatial and photometric data\n            plot_type: Type of plot ('scatter_3d', 'point_cloud')\n            **config: Additional PyVista-specific configuration\n\n        Returns:\n            PyVista plotter object\n        \"\"\"\n        try:\n            import pyvista as pv\n\n            logger.info(\"Visualizing with PyVista...\")\n\n            # Extract coordinates\n            spatial_tensor = survey_tensor.get_spatial_tensor()\n            coords_3d = spatial_tensor.data.numpy()\n\n            # Create point cloud\n            cloud = pv.PolyData(coords_3d)\n\n            # Add colors if photometric data available\n            try:\n                phot_tensor = survey_tensor.get_photometric_tensor()\n                colors = self._create_colors_from_photometry(phot_tensor)\n                cloud.point_data[\"colors\"] = colors\n            except Exception as e:\n                logger.warning(f\"Could not get photometric data for coloring: {e}\")\n                # Use default colors based on position\n                colors = self._create_position_colors(coords_3d)\n                cloud.point_data[\"colors\"] = colors\n\n            # Create plotter\n            plotter = pv.Plotter()\n            plotter.add_points(cloud, scalars=\"colors\", rgb=True, point_size=5)\n\n            self.plotter = plotter\n            return plotter\n\n        except ImportError:\n            logger.warning(\"PyVista not available. Install with: pip install pyvista\")\n            return None\n\n    def plot_to_blender(\n        self,\n        survey_tensor: SurveyTensorDict,\n        plot_type: str = \"scatter_3d\",\n        **config: Any,\n    ) -&gt; Any:\n        \"\"\"\n        Visualize data using Blender backend.\n\n        Args:\n            survey_tensor: SurveyTensorDict with spatial and photometric data\n            plot_type: Type of plot ('scatter_3d', 'point_cloud')\n            **config: Additional Blender-specific configuration\n\n        Returns:\n            Blender scene object or None if Blender not available\n        \"\"\"\n        try:\n            from ..utils.bpy import AstroLabApi\n\n            logger.info(\"Visualizing with Blender...\")\n\n            # Extract coordinates\n            spatial_tensor = survey_tensor.get_spatial_tensor()\n            coords_3d = spatial_tensor.data.numpy()\n\n            logger.info(f\"Would send {coords_3d.shape[0]} points to Blender.\")\n\n            # Create Blender API instance\n            api = AstroLabApi()\n\n            # Use the advanced visualization suite to create a scene\n            scene = api.advanced.create_galaxy_showcase()\n\n            # For now, just return the scene since adding individual points\n            # requires more complex integration with the 3D plotter\n            return scene\n\n        except ImportError:\n            logger.warning(\"Blender not available. Install Blender and astro_lab[bpy]\")\n            return None\n\n    def show(self, *args: Any, **kwargs: Any) -&gt; None:\n        \"\"\"\n        Show the last created interactive visualization.\n\n        Args:\n            *args: Positional arguments passed to visualization backend\n            **kwargs: Keyword arguments passed to visualization backend\n        \"\"\"\n        if self.plotter is not None:\n            self.plotter.show(*args, **kwargs)\n        else:\n            logger.warning(\"No plotter available to show.\")\n\n    def select_backend(self, survey_tensor: SurveyTensorDict) -&gt; str:\n        \"\"\"\n        Automatically select the best visualization backend.\n\n        Args:\n            survey_tensor: SurveyTensorDict with data\n\n        Returns:\n            Recommended backend name ('open3d', 'pyvista', 'blender')\n        \"\"\"\n        n_points = len(survey_tensor.data)\n\n        # Simple heuristic based on data size\n        if n_points &lt; 10000:\n            return \"open3d\"  # Good for small datasets\n        elif n_points &lt; 100000:\n            return \"pyvista\"  # Good for medium datasets\n        else:\n            return \"blender\"  # Good for large datasets\n\n    def _create_colors_from_photometry(self, phot_tensor: Any) -&gt; np.ndarray:\n        \"\"\"\n        Create colors from photometric data.\n\n        Args:\n            phot_tensor: PhotometricTensorDict with magnitude data\n\n        Returns:\n            RGB colors array\n        \"\"\"\n        # Simple color mapping based on magnitude\n        magnitudes = phot_tensor.data.mean(dim=1).numpy()\n\n        # Normalize to [0, 1]\n        mag_norm = (magnitudes - magnitudes.min()) / (\n            magnitudes.max() - magnitudes.min()\n        )\n\n        # Create RGB colors (blue to red)\n        colors = np.zeros((len(magnitudes), 3))\n        colors[:, 0] = mag_norm  # Red\n        colors[:, 2] = 1 - mag_norm  # Blue\n\n        return colors\n\n    def _create_position_colors(self, coords: np.ndarray) -&gt; np.ndarray:\n        \"\"\"\n        Create colors based on spatial position.\n\n        Args:\n            coords: 3D coordinates array\n\n        Returns:\n            RGB colors array\n        \"\"\"\n        # Normalize coordinates to [0, 1]\n        coords_norm = (coords - coords.min(axis=0)) / (\n            coords.max(axis=0) - coords.min(axis=0)\n        )\n\n        # Use coordinates as RGB\n        colors = np.clip(coords_norm, 0, 1)\n\n        return colors\n</code></pre>"},{"location":"api/astro_lab.widgets/#astro_lab.widgets.VisualizationModule.plot_to_blender","title":"plot_to_blender","text":"<pre><code>plot_to_blender(\n    survey_tensor: SurveyTensorDict, plot_type: str = \"scatter_3d\", **config: Any\n) -&gt; Any\n</code></pre> <p>Visualize data using Blender backend.</p> <p>Parameters:</p> Name Type Description Default <code>survey_tensor</code> <code>SurveyTensorDict</code> <p>SurveyTensorDict with spatial and photometric data</p> required <code>plot_type</code> <code>str</code> <p>Type of plot ('scatter_3d', 'point_cloud')</p> <code>'scatter_3d'</code> <code>**config</code> <code>Any</code> <p>Additional Blender-specific configuration</p> <code>{}</code> <p>Returns:</p> Type Description <code>Any</code> <p>Blender scene object or None if Blender not available</p> Source code in <code>src\\astro_lab\\widgets\\visualization.py</code> <pre><code>def plot_to_blender(\n    self,\n    survey_tensor: SurveyTensorDict,\n    plot_type: str = \"scatter_3d\",\n    **config: Any,\n) -&gt; Any:\n    \"\"\"\n    Visualize data using Blender backend.\n\n    Args:\n        survey_tensor: SurveyTensorDict with spatial and photometric data\n        plot_type: Type of plot ('scatter_3d', 'point_cloud')\n        **config: Additional Blender-specific configuration\n\n    Returns:\n        Blender scene object or None if Blender not available\n    \"\"\"\n    try:\n        from ..utils.bpy import AstroLabApi\n\n        logger.info(\"Visualizing with Blender...\")\n\n        # Extract coordinates\n        spatial_tensor = survey_tensor.get_spatial_tensor()\n        coords_3d = spatial_tensor.data.numpy()\n\n        logger.info(f\"Would send {coords_3d.shape[0]} points to Blender.\")\n\n        # Create Blender API instance\n        api = AstroLabApi()\n\n        # Use the advanced visualization suite to create a scene\n        scene = api.advanced.create_galaxy_showcase()\n\n        # For now, just return the scene since adding individual points\n        # requires more complex integration with the 3D plotter\n        return scene\n\n    except ImportError:\n        logger.warning(\"Blender not available. Install Blender and astro_lab[bpy]\")\n        return None\n</code></pre>"},{"location":"api/astro_lab.widgets/#astro_lab.widgets.VisualizationModule.plot_to_open3d","title":"plot_to_open3d","text":"<pre><code>plot_to_open3d(\n    survey_tensor: SurveyTensorDict, plot_type: str = \"scatter_3d\", **config: Any\n) -&gt; Any\n</code></pre> <p>Visualize data using Open3D backend.</p> <p>Parameters:</p> Name Type Description Default <code>survey_tensor</code> <code>SurveyTensorDict</code> <p>SurveyTensorDict with spatial and photometric data</p> required <code>plot_type</code> <code>str</code> <p>Type of plot ('scatter_3d', 'point_cloud')</p> <code>'scatter_3d'</code> <code>**config</code> <code>Any</code> <p>Additional Open3D-specific configuration</p> <code>{}</code> <p>Returns:</p> Type Description <code>Any</code> <p>Open3D visualization object</p> Source code in <code>src\\astro_lab\\widgets\\visualization.py</code> <pre><code>def plot_to_open3d(\n    self,\n    survey_tensor: SurveyTensorDict,\n    plot_type: str = \"scatter_3d\",\n    **config: Any,\n) -&gt; Any:\n    \"\"\"\n    Visualize data using Open3D backend.\n\n    Args:\n        survey_tensor: SurveyTensorDict with spatial and photometric data\n        plot_type: Type of plot ('scatter_3d', 'point_cloud')\n        **config: Additional Open3D-specific configuration\n\n    Returns:\n        Open3D visualization object\n    \"\"\"\n    try:\n        import open3d as o3d\n\n        logger.info(\"Visualizing with Open3D...\")\n\n        # Extract coordinates\n        spatial_tensor = survey_tensor.get_spatial_tensor()\n        coords_3d = spatial_tensor.data.numpy()\n\n        # Create point cloud\n        pcd = o3d.geometry.PointCloud()\n        pcd.points = o3d.utility.Vector3dVector(coords_3d)\n\n        # Add colors if photometric data available\n        try:\n            phot_tensor = survey_tensor.get_photometric_tensor()\n            colors = self._create_colors_from_photometry(phot_tensor)\n            pcd.colors = o3d.utility.Vector3dVector(colors)\n        except Exception as e:\n            logger.warning(\n                f\"Could not add photometric colors: {e}. Using default colors.\"\n            )\n            # Use default colors based on position\n            colors = self._create_position_colors(coords_3d)\n            pcd.colors = o3d.utility.Vector3dVector(colors)\n\n        return pcd\n\n    except ImportError:\n        logger.warning(\"Open3D not available. Install with: pip install open3d\")\n        return None\n</code></pre>"},{"location":"api/astro_lab.widgets/#astro_lab.widgets.VisualizationModule.plot_to_pyvista","title":"plot_to_pyvista","text":"<pre><code>plot_to_pyvista(\n    survey_tensor: SurveyTensorDict, plot_type: str = \"scatter_3d\", **config: Any\n) -&gt; Any\n</code></pre> <p>Visualize data using PyVista backend.</p> <p>Parameters:</p> Name Type Description Default <code>survey_tensor</code> <code>SurveyTensorDict</code> <p>SurveyTensorDict with spatial and photometric data</p> required <code>plot_type</code> <code>str</code> <p>Type of plot ('scatter_3d', 'point_cloud')</p> <code>'scatter_3d'</code> <code>**config</code> <code>Any</code> <p>Additional PyVista-specific configuration</p> <code>{}</code> <p>Returns:</p> Type Description <code>Any</code> <p>PyVista plotter object</p> Source code in <code>src\\astro_lab\\widgets\\visualization.py</code> <pre><code>def plot_to_pyvista(\n    self,\n    survey_tensor: SurveyTensorDict,\n    plot_type: str = \"scatter_3d\",\n    **config: Any,\n) -&gt; Any:\n    \"\"\"\n    Visualize data using PyVista backend.\n\n    Args:\n        survey_tensor: SurveyTensorDict with spatial and photometric data\n        plot_type: Type of plot ('scatter_3d', 'point_cloud')\n        **config: Additional PyVista-specific configuration\n\n    Returns:\n        PyVista plotter object\n    \"\"\"\n    try:\n        import pyvista as pv\n\n        logger.info(\"Visualizing with PyVista...\")\n\n        # Extract coordinates\n        spatial_tensor = survey_tensor.get_spatial_tensor()\n        coords_3d = spatial_tensor.data.numpy()\n\n        # Create point cloud\n        cloud = pv.PolyData(coords_3d)\n\n        # Add colors if photometric data available\n        try:\n            phot_tensor = survey_tensor.get_photometric_tensor()\n            colors = self._create_colors_from_photometry(phot_tensor)\n            cloud.point_data[\"colors\"] = colors\n        except Exception as e:\n            logger.warning(f\"Could not get photometric data for coloring: {e}\")\n            # Use default colors based on position\n            colors = self._create_position_colors(coords_3d)\n            cloud.point_data[\"colors\"] = colors\n\n        # Create plotter\n        plotter = pv.Plotter()\n        plotter.add_points(cloud, scalars=\"colors\", rgb=True, point_size=5)\n\n        self.plotter = plotter\n        return plotter\n\n    except ImportError:\n        logger.warning(\"PyVista not available. Install with: pip install pyvista\")\n        return None\n</code></pre>"},{"location":"api/astro_lab.widgets/#astro_lab.widgets.VisualizationModule.select_backend","title":"select_backend","text":"<pre><code>select_backend(survey_tensor: SurveyTensorDict) -&gt; str\n</code></pre> <p>Automatically select the best visualization backend.</p> <p>Parameters:</p> Name Type Description Default <code>survey_tensor</code> <code>SurveyTensorDict</code> <p>SurveyTensorDict with data</p> required <p>Returns:</p> Type Description <code>str</code> <p>Recommended backend name ('open3d', 'pyvista', 'blender')</p> Source code in <code>src\\astro_lab\\widgets\\visualization.py</code> <pre><code>def select_backend(self, survey_tensor: SurveyTensorDict) -&gt; str:\n    \"\"\"\n    Automatically select the best visualization backend.\n\n    Args:\n        survey_tensor: SurveyTensorDict with data\n\n    Returns:\n        Recommended backend name ('open3d', 'pyvista', 'blender')\n    \"\"\"\n    n_points = len(survey_tensor.data)\n\n    # Simple heuristic based on data size\n    if n_points &lt; 10000:\n        return \"open3d\"  # Good for small datasets\n    elif n_points &lt; 100000:\n        return \"pyvista\"  # Good for medium datasets\n    else:\n        return \"blender\"  # Good for large datasets\n</code></pre>"},{"location":"api/astro_lab.widgets/#astro_lab.widgets.VisualizationModule.show","title":"show","text":"<pre><code>show(*args: Any, **kwargs: Any) -&gt; None\n</code></pre> <p>Show the last created interactive visualization.</p> <p>Parameters:</p> Name Type Description Default <code>*args</code> <code>Any</code> <p>Positional arguments passed to visualization backend</p> <code>()</code> <code>**kwargs</code> <code>Any</code> <p>Keyword arguments passed to visualization backend</p> <code>{}</code> Source code in <code>src\\astro_lab\\widgets\\visualization.py</code> <pre><code>def show(self, *args: Any, **kwargs: Any) -&gt; None:\n    \"\"\"\n    Show the last created interactive visualization.\n\n    Args:\n        *args: Positional arguments passed to visualization backend\n        **kwargs: Keyword arguments passed to visualization backend\n    \"\"\"\n    if self.plotter is not None:\n        self.plotter.show(*args, **kwargs)\n    else:\n        logger.warning(\"No plotter available to show.\")\n</code></pre>"}]}