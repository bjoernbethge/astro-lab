{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"AstroLab","text":"<p>AstroLab is a modern framework for astronomical machine learning workflows, built on PyTorch Lightning, Torch Geometric, and Pydantic.</p>"},{"location":"#features","title":"Features","text":"<ul> <li>State-of-the-art ML for astronomy</li> <li>PyTorch Lightning + Torch Geometric integration</li> <li>Pydantic-based configuration</li> <li>MLflow and Optuna integration</li> <li>Automatic API reference (API Reference)</li> <li>Modular, extensible, and production-ready</li> </ul>"},{"location":"#documentation","title":"Documentation","text":"<ul> <li>API Reference: See API Reference for all classes, functions, and configuration models.</li> <li>Configuration Examples: See the <code>configs/</code> directory.</li> <li>Example Workflows: See the <code>examples/</code> directory.</li> </ul> <p>For more information, visit the GitHub repository.</p>"},{"location":"api/","title":"API Reference","text":"<p>The API is organized by module. Each main module has its own page:</p> <ul> <li>astro_lab</li> <li>astro_lab.cli</li> <li>astro_lab.config</li> <li>astro_lab.data</li> <li>astro_lab.data.datasets</li> <li>astro_lab.data.graphs</li> <li>astro_lab.models</li> <li>astro_lab.models.components</li> <li>astro_lab.models.core</li> <li>astro_lab.models.lightning</li> <li>astro_lab.tensors</li> <li>astro_lab.training</li> <li>astro_lab.ui</li> <li>astro_lab.widgets</li> <li>astro_lab.widgets.bpy</li> <li>astro_lab.widgets.bpy.advanced</li> </ul> <p>Each page documents the respective module and all contained classes, functions, and submodules.</p>"},{"location":"api/astro_lab.cli/","title":"astro_lab.cli","text":""},{"location":"api/astro_lab.cli/#astro_lab.cli","title":"cli","text":""},{"location":"api/astro_lab.cli/#astro_lab.cli--astrolab-cli-command-line-interface","title":"AstroLab CLI - Command Line Interface","text":"<p>Provides command-line tools for training, evaluation, and visualization.</p> <p>Modules:</p> Name Description <code>config</code> <p>AstroLab Config CLI</p> <code>download</code> <p>Download CLI for AstroLab - Thin wrapper around data download functions.</p> <code>optimize</code> <p>AstroLab Hyperparameter Optimization CLI</p> <code>preprocess</code> <p>AstroLab Preprocess CLI</p> <code>process</code> <p>AstroLab Process CLI</p> <code>train</code> <p>AstroLab Training CLI (Lightning Edition)</p> <p>Functions:</p> Name Description <code>main</code> <p>Main entry point for the AstroLab CLI.</p>"},{"location":"api/astro_lab.cli/#astro_lab.cli.main","title":"main","text":"<pre><code>main(argv: Optional[List[str]] = None) -&gt; int\n</code></pre> <p>Main entry point for the AstroLab CLI.</p> <p>Parameters:</p> Name Type Description Default <code>argv</code> <code>Optional[List[str]]</code> <p>Command line arguments (defaults to sys.argv)</p> <code>None</code> <p>Returns:</p> Type Description <code>int</code> <p>Exit code (0 for success, non-zero for error)</p> Source code in <code>src\\astro_lab\\cli\\__init__.py</code> <pre><code>def main(argv: Optional[List[str]] = None) -&gt; int:\n    \"\"\"\n    Main entry point for the AstroLab CLI.\n\n    Args:\n        argv: Command line arguments (defaults to sys.argv)\n\n    Returns:\n        Exit code (0 for success, non-zero for error)\n    \"\"\"\n    parser = argparse.ArgumentParser(\n        prog=\"astro-lab\",\n        description=\"AstroLab: Modern Astronomical Machine Learning\",\n        formatter_class=argparse.RawDescriptionHelpFormatter,\n        epilog=\"\"\"\nAvailable commands:\n  train       Train models on astronomical data\n  download    Download survey data\n  optimize    Optimize model hyperparameters\n  preprocess  Preprocess survey data\n        \"\"\",\n    )\n\n    parser.add_argument(\n        \"command\",\n        choices=COMMANDS.keys(),\n        help=\"Command to run\",\n    )\n\n    parser.add_argument(\n        \"args\",\n        nargs=argparse.REMAINDER,\n        help=\"Arguments for the command\",\n    )\n\n    # Parse only the command\n    args = parser.parse_args(argv or sys.argv[1:2])\n\n    # Import and run the command module lazily\n    try:\n        import importlib\n        module = importlib.import_module(COMMANDS[args.command])\n\n        # Set argv for the subcommand\n        sys.argv = [f\"astro-lab {args.command}\"] + (argv or sys.argv)[2:]\n\n        # Run the command's main function\n        return module.main()\n\n    except KeyboardInterrupt:\n        print(\"\\nInterrupted by user\", file=sys.stderr)\n        return 130\n    except Exception as e:\n        print(f\"Error: {e}\", file=sys.stderr)\n        return 1\n</code></pre>"},{"location":"api/astro_lab.config/","title":"astro_lab.config","text":""},{"location":"api/astro_lab.config/#astro_lab.config","title":"config","text":""},{"location":"api/astro_lab.config/#astro_lab.config--astrolab-configuration-management","title":"AstroLab Configuration Management","text":"<p>Centralized configuration management with automatic path setup and experiment organization.</p> <p>Modules:</p> Name Description <code>loader</code> <p>AstroLab Configuration Loader</p> <code>params</code> <p>Centralized Parameter Distribution for AstroLab</p> <code>surveys</code> <p>Survey Configurations</p> <p>Classes:</p> Name Description <code>ConfigLoader</code> <p>Load and manage AstroLab configurations with automatic path setup.</p> <p>Functions:</p> Name Description <code>distribute_config_parameters</code> <p>Distributes configuration parameters to the appropriate components.</p> <code>get_data_params</code> <p>Extracts only Data parameters.</p> <code>get_lightning_params</code> <p>Extracts only Lightning parameters.</p> <code>get_mlflow_params</code> <p>Extracts only MLflow parameters.</p> <code>get_optuna_params</code> <p>Extracts only Optuna parameters.</p> <code>get_survey_config</code> <p>Get configuration for a specific survey.</p> <code>get_survey_coordinates</code> <p>Get coordinate columns for a survey.</p> <code>get_survey_features</code> <p>Get all feature columns for a survey.</p> <code>get_survey_magnitudes</code> <p>Get magnitude columns for a survey.</p> <code>get_trainer_params</code> <p>Extracts only Trainer parameters.</p> <code>load_experiment_config</code> <p>Convenience function to load experiment configuration.</p> <code>load_survey_config</code> <p>Convenience function to load survey configuration.</p> <code>print_parameter_distribution</code> <p>Debug function: Shows parameter distribution.</p> <code>register_survey</code> <p>Register a new survey configuration.</p> <code>setup_experiment_from_config</code> <p>Set up experiment directories and environment from config file.</p> <code>validate_parameter_conflicts</code> <p>Validates configuration for parameter conflicts.</p>"},{"location":"api/astro_lab.config/#astro_lab.config.ConfigLoader","title":"ConfigLoader","text":"<p>Load and manage AstroLab configurations with automatic path setup.</p> <p>Methods:</p> Name Description <code>load_config</code> <p>Load configuration and set up experiment paths.</p> <code>save_config</code> <p>Save current configuration to file.</p> Source code in <code>src\\astro_lab\\config\\loader.py</code> <pre><code>class ConfigLoader:\n    \"\"\"Load and manage AstroLab configurations with automatic path setup.\"\"\"\n\n    def __init__(self, config_file: Optional[str] = None):\n        \"\"\"\n        Initialize config loader.\n\n        Args:\n            config_file: Path to config file. If None, uses default.yaml\n        \"\"\"\n        self.config_file = config_file or \"configs/default.yaml\"\n        self.config: Optional[Dict[str, Any]] = None\n\n    def load_config(self, experiment_name: Optional[str] = None) -&gt; Dict[str, Any]:\n        \"\"\"\n        Load configuration and set up experiment paths.\n\n        Args:\n            experiment_name: Override experiment name from config\n\n        Returns:\n            Loaded configuration dictionary\n        \"\"\"\n        config_path = Path(self.config_file)\n        if not config_path.exists():\n            raise FileNotFoundError(f\"Config file not found: {config_path}\")\n        with open(config_path, \"r\", encoding=\"utf-8\") as f:\n            self.config = yaml.safe_load(f)\n        if experiment_name and self.config:\n            self.config[\"experiment_name\"] = experiment_name\n        if self.config:\n            exp_name = self.config[\"experiment_name\"]\n            data_config.ensure_experiment_directories(exp_name)\n            self._update_paths()\n        return self.config or {}\n\n    def _update_paths(self):\n        \"\"\"Update configuration paths using data_config.\"\"\"\n        if not self.config:\n            return\n        exp_name = self.config[\"experiment_name\"]\n        exp_paths = data_config.get_experiment_paths(exp_name)\n        if \"checkpoints\" in self.config:\n            self.config[\"checkpoints\"][\"dir\"] = str(exp_paths[\"checkpoints\"])\n        if \"data\" in self.config:\n            self.config[\"data\"][\"base_dir\"] = str(data_config.base_dir)\n\n    def save_config(self, output_path: Optional[str] = None):\n        \"\"\"\n        Save current configuration to file.\n\n        Args:\n            output_path: Path to save config. If None, saves to experiment config path.\n        \"\"\"\n        if not self.config:\n            raise ValueError(\"Config not loaded. Call load_config() first.\")\n        if output_path is None:\n            exp_name = self.config[\"experiment_name\"]\n            output_file = data_config.configs_dir / f\"{exp_name}.yaml\"\n        else:\n            output_file = Path(output_path)\n        output_file.parent.mkdir(parents=True, exist_ok=True)\n        with open(output_file, \"w\", encoding=\"utf-8\") as f:\n            yaml.dump(self.config, f, default_flow_style=False, indent=2)\n        print(f\"\ud83d\udcbe Config saved to: {output_file}\")\n</code></pre>"},{"location":"api/astro_lab.config/#astro_lab.config.ConfigLoader.load_config","title":"load_config","text":"<pre><code>load_config(experiment_name: Optional[str] = None) -&gt; Dict[str, Any]\n</code></pre> <p>Load configuration and set up experiment paths.</p> <p>Parameters:</p> Name Type Description Default <code>experiment_name</code> <code>Optional[str]</code> <p>Override experiment name from config</p> <code>None</code> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Loaded configuration dictionary</p> Source code in <code>src\\astro_lab\\config\\loader.py</code> <pre><code>def load_config(self, experiment_name: Optional[str] = None) -&gt; Dict[str, Any]:\n    \"\"\"\n    Load configuration and set up experiment paths.\n\n    Args:\n        experiment_name: Override experiment name from config\n\n    Returns:\n        Loaded configuration dictionary\n    \"\"\"\n    config_path = Path(self.config_file)\n    if not config_path.exists():\n        raise FileNotFoundError(f\"Config file not found: {config_path}\")\n    with open(config_path, \"r\", encoding=\"utf-8\") as f:\n        self.config = yaml.safe_load(f)\n    if experiment_name and self.config:\n        self.config[\"experiment_name\"] = experiment_name\n    if self.config:\n        exp_name = self.config[\"experiment_name\"]\n        data_config.ensure_experiment_directories(exp_name)\n        self._update_paths()\n    return self.config or {}\n</code></pre>"},{"location":"api/astro_lab.config/#astro_lab.config.ConfigLoader.save_config","title":"save_config","text":"<pre><code>save_config(output_path: Optional[str] = None)\n</code></pre> <p>Save current configuration to file.</p> <p>Parameters:</p> Name Type Description Default <code>output_path</code> <code>Optional[str]</code> <p>Path to save config. If None, saves to experiment config path.</p> <code>None</code> Source code in <code>src\\astro_lab\\config\\loader.py</code> <pre><code>def save_config(self, output_path: Optional[str] = None):\n    \"\"\"\n    Save current configuration to file.\n\n    Args:\n        output_path: Path to save config. If None, saves to experiment config path.\n    \"\"\"\n    if not self.config:\n        raise ValueError(\"Config not loaded. Call load_config() first.\")\n    if output_path is None:\n        exp_name = self.config[\"experiment_name\"]\n        output_file = data_config.configs_dir / f\"{exp_name}.yaml\"\n    else:\n        output_file = Path(output_path)\n    output_file.parent.mkdir(parents=True, exist_ok=True)\n    with open(output_file, \"w\", encoding=\"utf-8\") as f:\n        yaml.dump(self.config, f, default_flow_style=False, indent=2)\n    print(f\"\ud83d\udcbe Config saved to: {output_file}\")\n</code></pre>"},{"location":"api/astro_lab.config/#astro_lab.config.distribute_config_parameters","title":"distribute_config_parameters","text":"<pre><code>distribute_config_parameters(config: Dict[str, Any]) -&gt; Dict[str, Dict[str, Any]]\n</code></pre> <p>Distributes configuration parameters to the appropriate components.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>Dict[str, Any]</code> <p>Complete configuration dictionary</p> required <p>Returns:</p> Type Description <code>Dict[str, Dict[str, Any]]</code> <p>Dictionary with categorized parameters</p> Source code in <code>src\\astro_lab\\config\\params.py</code> <pre><code>def distribute_config_parameters(config: Dict[str, Any]) -&gt; Dict[str, Dict[str, Any]]:\n    \"\"\"\n    Distributes configuration parameters to the appropriate components.\n\n    Args:\n        config: Complete configuration dictionary\n\n    Returns:\n        Dictionary with categorized parameters\n    \"\"\"\n    distributed = {\n        \"trainer\": {},\n        \"lightning\": {},\n        \"optuna\": {},\n        \"mlflow\": {},\n        \"data\": {},\n        \"excluded\": {},\n    }\n\n    # Training section -&gt; Trainer + Lightning\n    if \"training\" in config:\n        training_config = config[\"training\"]\n        for key, value in training_config.items():\n            if key in TRAINER_PARAMS:\n                distributed[\"trainer\"][key] = value\n            elif key in LIGHTNING_PARAMS:\n                distributed[\"lightning\"][key] = value\n            # Ignore unknown training params\n\n    # MLflow section\n    if \"mlflow\" in config:\n        distributed[\"mlflow\"].update(config[\"mlflow\"])\n\n    # Data section\n    if \"data\" in config:\n        distributed[\"data\"].update(config[\"data\"])\n\n    # Optimization section -&gt; Optuna + Excluded\n    if \"optimization\" in config:\n        opt_config = config[\"optimization\"]\n        for key, value in opt_config.items():\n            if key in EXCLUDED_PARAMS:\n                distributed[\"excluded\"][key] = value\n            elif key in OPTUNA_PARAMS:\n                distributed[\"optuna\"][key] = value\n\n    # Model section -&gt; Lightning (model parameters)\n    if \"model\" in config:\n        model_config = config[\"model\"]\n        # Model type and params go to Lightning\n        if \"type\" in model_config:\n            distributed[\"lightning\"][\"model_type\"] = model_config[\"type\"]\n        if \"params\" in model_config:\n            distributed[\"lightning\"].update(model_config[\"params\"])\n\n    # Modellname auf Top-Level\n    if \"model\" in config and isinstance(config[\"model\"], str):\n        distributed[\"trainer\"][\"model\"] = config[\"model\"]\n        distributed[\"lightning\"][\"model\"] = config[\"model\"]\n\n    return distributed\n</code></pre>"},{"location":"api/astro_lab.config/#astro_lab.config.get_data_params","title":"get_data_params","text":"<pre><code>get_data_params(config: Dict[str, Any]) -&gt; Dict[str, Any]\n</code></pre> <p>Extracts only Data parameters.</p> Source code in <code>src\\astro_lab\\config\\params.py</code> <pre><code>def get_data_params(config: Dict[str, Any]) -&gt; Dict[str, Any]:\n    \"\"\"Extracts only Data parameters.\"\"\"\n    distributed = distribute_config_parameters(config)\n    return distributed[\"data\"]\n</code></pre>"},{"location":"api/astro_lab.config/#astro_lab.config.get_lightning_params","title":"get_lightning_params","text":"<pre><code>get_lightning_params(config: Dict[str, Any]) -&gt; Dict[str, Any]\n</code></pre> <p>Extracts only Lightning parameters.</p> Source code in <code>src\\astro_lab\\config\\params.py</code> <pre><code>def get_lightning_params(config: Dict[str, Any]) -&gt; Dict[str, Any]:\n    \"\"\"Extracts only Lightning parameters.\"\"\"\n    distributed = distribute_config_parameters(config)\n    return distributed[\"lightning\"]\n</code></pre>"},{"location":"api/astro_lab.config/#astro_lab.config.get_mlflow_params","title":"get_mlflow_params","text":"<pre><code>get_mlflow_params(config: Dict[str, Any]) -&gt; Dict[str, Any]\n</code></pre> <p>Extracts only MLflow parameters.</p> Source code in <code>src\\astro_lab\\config\\params.py</code> <pre><code>def get_mlflow_params(config: Dict[str, Any]) -&gt; Dict[str, Any]:\n    \"\"\"Extracts only MLflow parameters.\"\"\"\n    distributed = distribute_config_parameters(config)\n    return distributed[\"mlflow\"]\n</code></pre>"},{"location":"api/astro_lab.config/#astro_lab.config.get_optuna_params","title":"get_optuna_params","text":"<pre><code>get_optuna_params(config: Dict[str, Any]) -&gt; Dict[str, Any]\n</code></pre> <p>Extracts only Optuna parameters.</p> Source code in <code>src\\astro_lab\\config\\params.py</code> <pre><code>def get_optuna_params(config: Dict[str, Any]) -&gt; Dict[str, Any]:\n    \"\"\"Extracts only Optuna parameters.\"\"\"\n    distributed = distribute_config_parameters(config)\n    return distributed[\"optuna\"]\n</code></pre>"},{"location":"api/astro_lab.config/#astro_lab.config.get_survey_config","title":"get_survey_config","text":"<pre><code>get_survey_config(survey: str) -&gt; Dict[str, Any]\n</code></pre> <p>Get configuration for a specific survey.</p> Source code in <code>src\\astro_lab\\config\\surveys.py</code> <pre><code>def get_survey_config(survey: str) -&gt; Dict[str, Any]:\n    \"\"\"Get configuration for a specific survey.\"\"\"\n    if survey not in SURVEY_CONFIGS:\n        raise ValueError(\n            f\"Unknown survey: {survey}. Available: {list(SURVEY_CONFIGS.keys())}\"\n        )\n    return SURVEY_CONFIGS[survey]\n</code></pre>"},{"location":"api/astro_lab.config/#astro_lab.config.get_survey_coordinates","title":"get_survey_coordinates","text":"<pre><code>get_survey_coordinates(survey: str) -&gt; List[str]\n</code></pre> <p>Get coordinate columns for a survey.</p> Source code in <code>src\\astro_lab\\config\\surveys.py</code> <pre><code>def get_survey_coordinates(survey: str) -&gt; List[str]:\n    \"\"\"Get coordinate columns for a survey.\"\"\"\n    config = get_survey_config(survey)\n    return config[\"coord_cols\"]\n</code></pre>"},{"location":"api/astro_lab.config/#astro_lab.config.get_survey_features","title":"get_survey_features","text":"<pre><code>get_survey_features(survey: str) -&gt; List[str]\n</code></pre> <p>Get all feature columns for a survey.</p> Source code in <code>src\\astro_lab\\config\\surveys.py</code> <pre><code>def get_survey_features(survey: str) -&gt; List[str]:\n    \"\"\"Get all feature columns for a survey.\"\"\"\n    config = get_survey_config(survey)\n    features = (\n        config.get(\"coord_cols\", [])\n        + config.get(\"mag_cols\", [])\n        + config.get(\"extra_cols\", [])\n    )\n    return features\n</code></pre>"},{"location":"api/astro_lab.config/#astro_lab.config.get_survey_magnitudes","title":"get_survey_magnitudes","text":"<pre><code>get_survey_magnitudes(survey: str) -&gt; List[str]\n</code></pre> <p>Get magnitude columns for a survey.</p> Source code in <code>src\\astro_lab\\config\\surveys.py</code> <pre><code>def get_survey_magnitudes(survey: str) -&gt; List[str]:\n    \"\"\"Get magnitude columns for a survey.\"\"\"\n    config = get_survey_config(survey)\n    return config[\"mag_cols\"]\n</code></pre>"},{"location":"api/astro_lab.config/#astro_lab.config.get_trainer_params","title":"get_trainer_params","text":"<pre><code>get_trainer_params(config: Dict[str, Any]) -&gt; Dict[str, Any]\n</code></pre> <p>Extracts only Trainer parameters.</p> Source code in <code>src\\astro_lab\\config\\params.py</code> <pre><code>def get_trainer_params(config: Dict[str, Any]) -&gt; Dict[str, Any]:\n    \"\"\"Extracts only Trainer parameters.\"\"\"\n    distributed = distribute_config_parameters(config)\n    return distributed[\"trainer\"]\n</code></pre>"},{"location":"api/astro_lab.config/#astro_lab.config.load_experiment_config","title":"load_experiment_config","text":"<pre><code>load_experiment_config(\n    experiment_name: str, config_file: Optional[str] = None\n) -&gt; Dict[str, Any]\n</code></pre> <p>Convenience function to load experiment configuration.</p> <p>Parameters:</p> Name Type Description Default <code>experiment_name</code> <code>str</code> <p>Name of the experiment</p> required <code>config_file</code> <code>Optional[str]</code> <p>Config file to load (default: configs/default.yaml)</p> <code>None</code> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Loaded configuration dictionary</p> Source code in <code>src\\astro_lab\\config\\loader.py</code> <pre><code>def load_experiment_config(\n    experiment_name: str, config_file: Optional[str] = None\n) -&gt; Dict[str, Any]:\n    \"\"\"\n    Convenience function to load experiment configuration.\n\n    Args:\n        experiment_name: Name of the experiment\n        config_file: Config file to load (default: configs/default.yaml)\n\n    Returns:\n        Loaded configuration dictionary\n    \"\"\"\n    loader = ConfigLoader(config_file)\n    return loader.load_config(experiment_name)\n</code></pre>"},{"location":"api/astro_lab.config/#astro_lab.config.load_survey_config","title":"load_survey_config","text":"<pre><code>load_survey_config(survey: str) -&gt; Dict[str, Any]\n</code></pre> <p>Convenience function to load survey configuration.</p> <p>Parameters:</p> Name Type Description Default <code>survey</code> <code>str</code> <p>Name of the survey (e.g., 'gaia', 'sdss')</p> required <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Survey configuration dictionary</p> Source code in <code>src\\astro_lab\\config\\loader.py</code> <pre><code>def load_survey_config(survey: str) -&gt; Dict[str, Any]:\n    \"\"\"\n    Convenience function to load survey configuration.\n\n    Args:\n        survey: Name of the survey (e.g., 'gaia', 'sdss')\n\n    Returns:\n        Survey configuration dictionary\n    \"\"\"\n    loader = ConfigLoader()\n    return loader.get_survey_config(survey)\n</code></pre>"},{"location":"api/astro_lab.config/#astro_lab.config.print_parameter_distribution","title":"print_parameter_distribution","text":"<pre><code>print_parameter_distribution(config: Dict[str, Any]) -&gt; None\n</code></pre> <p>Debug function: Shows parameter distribution.</p> Source code in <code>src\\astro_lab\\config\\params.py</code> <pre><code>def print_parameter_distribution(config: Dict[str, Any]) -&gt; None:\n    \"\"\"Debug function: Shows parameter distribution.\"\"\"\n    distributed = distribute_config_parameters(config)\n\n    print(\"\ud83d\udd27 Parameter Distribution:\")\n    print(\"=\" * 40)\n\n    for category, params in distributed.items():\n        if params:\n            print(f\"\\n\ud83d\udccb {category.upper()}:\")\n            for key, value in params.items():\n                print(f\"   \u2022 {key}: {value}\")\n\n    # Validation\n    is_valid, error = validate_parameter_conflicts(config)\n    if not is_valid:\n        print(f\"\\n\u274c Validation Error: {error}\")\n    else:\n        print(\"\\n\u2705 Configuration is valid\")\n</code></pre>"},{"location":"api/astro_lab.config/#astro_lab.config.register_survey","title":"register_survey","text":"<pre><code>register_survey(name: str, config: Dict[str, Any]) -&gt; None\n</code></pre> <p>Register a new survey configuration.</p> Source code in <code>src\\astro_lab\\config\\surveys.py</code> <pre><code>def register_survey(name: str, config: Dict[str, Any]) -&gt; None:\n    \"\"\"Register a new survey configuration.\"\"\"\n    required_keys = [\"name\", \"coord_cols\", \"mag_cols\", \"extra_cols\"]\n    for key in required_keys:\n        if key not in config:\n            raise ValueError(f\"Survey config missing required key: {key}\")\n\n    SURVEY_CONFIGS[name] = config\n    print(f\"\u2705 Registered new survey: {name}\")\n</code></pre>"},{"location":"api/astro_lab.config/#astro_lab.config.setup_experiment_from_config","title":"setup_experiment_from_config","text":"<pre><code>setup_experiment_from_config(config_path: str, experiment_name: str)\n</code></pre> <p>Set up experiment directories and environment from config file.</p> <p>Parameters:</p> Name Type Description Default <code>config_path</code> <code>str</code> <p>Path to configuration file</p> required <code>experiment_name</code> <code>str</code> <p>Name of the experiment</p> required Source code in <code>src\\astro_lab\\config\\loader.py</code> <pre><code>def setup_experiment_from_config(config_path: str, experiment_name: str):\n    \"\"\"\n    Set up experiment directories and environment from config file.\n\n    Args:\n        config_path: Path to configuration file\n        experiment_name: Name of the experiment\n    \"\"\"\n    loader = ConfigLoader(config_path)\n    config = loader.load_config(experiment_name)\n\n    print(\"\ud83e\uddea Experiment setup complete:\")\n    print(f\"   - Name: {experiment_name}\")\n    if config:\n        print(f\"   - MLflow: {config['mlflow']['tracking_uri']}\")\n        if \"checkpoints\" in config:\n            print(f\"   - Checkpoints: {config['checkpoints']['dir']}\")\n    print(f\"   - Config saved: {data_config.configs_dir / f'{experiment_name}.yaml'}\")\n\n    return config\n</code></pre>"},{"location":"api/astro_lab.config/#astro_lab.config.validate_parameter_conflicts","title":"validate_parameter_conflicts","text":"<pre><code>validate_parameter_conflicts(config: Dict[str, Any]) -&gt; Tuple[bool, str]\n</code></pre> <p>Validates configuration for parameter conflicts.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>Dict[str, Any]</code> <p>Configuration to validate</p> required <p>Returns:</p> Type Description <code>Tuple[bool, str]</code> <p>(is_valid, error_message)</p> Source code in <code>src\\astro_lab\\config\\params.py</code> <pre><code>def validate_parameter_conflicts(config: Dict[str, Any]) -&gt; Tuple[bool, str]:\n    \"\"\"\n    Validates configuration for parameter conflicts.\n\n    Args:\n        config: Configuration to validate\n\n    Returns:\n        (is_valid, error_message)\n    \"\"\"\n    try:\n        distributed = distribute_config_parameters(config)\n\n        # Check for missing critical parameters\n        if \"learning_rate\" not in distributed[\"lightning\"]:\n            return False, \"learning_rate missing in configuration\"\n\n        # Check for Optuna-specific conflicts\n        if \"optimization\" in config:\n            opt_config = config[\"optimization\"]\n            if \"search_space\" in opt_config:\n                search_space = opt_config[\"search_space\"]\n\n                # Validate learning_rate range\n                if \"learning_rate\" in search_space:\n                    lr_config = search_space[\"learning_rate\"]\n                    if lr_config.get(\"low\", 0) &gt;= lr_config.get(\"high\", 1):\n                        return (\n                            False,\n                            f\"learning_rate: low ({lr_config.get('low')}) &gt;= high ({lr_config.get('high')})\",\n                        )\n\n        return True, \"\"\n\n    except Exception as e:\n        return False, f\"Validation error: {str(e)}\"\n</code></pre>"},{"location":"api/astro_lab.data.datasets/","title":"astro_lab.data.datasets","text":""},{"location":"api/astro_lab.data.datasets/#astro_lab.data.datasets","title":"datasets","text":""},{"location":"api/astro_lab.data.datasets/#astro_lab.data.datasets--astrolab-datasets","title":"AstroLab Datasets","text":"<p>Provides datasets for astronomical data processing and graph building.</p> <p>Modules:</p> Name Description <code>survey_graph_dataset</code> <p>Survey Graph Dataset</p> <p>Classes:</p> Name Description <code>SurveyGraphDataset</code> <p>PyG Dataset that loads SurveyTensorDict data and builds graphs.</p>"},{"location":"api/astro_lab.data.datasets/#astro_lab.data.datasets.SurveyGraphDataset","title":"SurveyGraphDataset","text":"<p>               Bases: <code>InMemoryDataset</code></p> <p>PyG Dataset that loads SurveyTensorDict data and builds graphs.</p> <p>Pipeline: Raw Data \u2192 SurveyTensorDict \u2192 Graph \u2192 InMemoryDataset</p> <p>Methods:</p> Name Description <code>get</code> <p>Get graph by index.</p> <code>get_info</code> <p>Get dataset information.</p> <code>get_survey_tensor</code> <p>Get the underlying SurveyTensorDict (for analysis/debugging).</p> <code>len</code> <p>Number of graphs in dataset.</p> Source code in <code>src\\astro_lab\\data\\datasets\\survey_graph_dataset.py</code> <pre><code>class SurveyGraphDataset(InMemoryDataset):\n    \"\"\"\n    PyG Dataset that loads SurveyTensorDict data and builds graphs.\n\n    Pipeline: Raw Data \u2192 SurveyTensorDict \u2192 Graph \u2192 InMemoryDataset\n    \"\"\"\n\n    def __init__(\n        self,\n        root: str,\n        survey: str,\n        graph_method: str = \"knn\",\n        k_neighbors: int = 8,\n        use_3d_coordinates: bool = True,\n        max_samples: Optional[int] = None,\n        transform: Optional[Any] = None,\n        pre_transform: Optional[Any] = None,\n        pre_filter: Optional[Any] = None,\n        **kwargs,\n    ):\n        self.survey = survey\n        self.graph_method = graph_method\n        self.k_neighbors = k_neighbors\n        self.use_3d_coordinates = use_3d_coordinates\n        self.max_samples = max_samples\n        self.survey_config = get_survey_config(survey)\n\n        super().__init__(root, transform, pre_transform, pre_filter)\n\n        # Setup paths\n        self._processed_dir = Path(self.root) / \"processed\" / self.survey\n        self._processed_dir.mkdir(parents=True, exist_ok=True)\n\n        # Graph file path\n        graph_filename = f\"{self.survey}_{graph_method}_k{k_neighbors}\"\n        if self.use_3d_coordinates:\n            graph_filename += \"_3d\"\n        graph_filename += \".pt\"\n        self._graph_path = self._processed_dir / graph_filename\n\n        # SurveyTensorDict file path (for debugging/analysis)\n        self._tensor_path = self._processed_dir / f\"{self.survey}_tensor.pt\"\n\n        # Load data\n        self._load_data()\n\n    def _load_data(self):\n        \"\"\"Load data and build graph if needed.\"\"\"\n        if self._graph_path.exists():\n            logger.info(f\"\ud83d\udd04 Loading existing graph: {self._graph_path}\")\n            try:\n                graph_data = torch.load(self._graph_path, weights_only=False)\n                self.data, self.slices = self.collate([graph_data])\n                return\n            except Exception as e:\n                logger.warning(f\"\u26a0\ufe0f Failed to load graph: {e}. Rebuilding...\")\n\n        # Build graph from SurveyTensorDict\n        logger.info(f\"\ud83d\udd04 Building graph for {self.survey} using {self.graph_method}\")\n        survey_tensor = self._load_survey_tensor()\n        graph_data = self._build_graph(survey_tensor)\n\n        # Save graph\n        torch.save(graph_data, self._graph_path)\n        logger.info(f\"\ud83d\udcbe Graph saved: {self._graph_path}\")\n\n        # Save SurveyTensorDict for debugging\n        torch.save(survey_tensor, self._tensor_path)\n        logger.info(f\"\ud83d\udcbe SurveyTensorDict saved: {self._tensor_path}\")\n\n        # Set data for PyG\n        self.data, self.slices = self.collate([graph_data])\n\n    def _load_survey_tensor(self) -&gt; SurveyTensorDict:\n        \"\"\"Load or create SurveyTensorDict from raw data.\"\"\"\n        # Try to load existing SurveyTensorDict\n        if self._tensor_path.exists():\n            try:\n                logger.info(f\"\ud83d\udd04 Loading SurveyTensorDict: {self._tensor_path}\")\n                return torch.load(self._tensor_path, weights_only=False)\n            except Exception:\n                logger.warning(\n                    \"\u26a0\ufe0f Failed to load SurveyTensorDict. Creating from raw data...\"\n                )\n\n        # Create from raw data\n        logger.info(f\"\ud83d\udd04 Creating SurveyTensorDict from raw data for {self.survey}\")\n        return self._create_survey_tensor_from_raw()\n\n    def _create_survey_tensor_from_raw(self) -&gt; SurveyTensorDict:\n        \"\"\"Create SurveyTensorDict from raw survey data.\"\"\"\n        # Load raw data (CSV, Parquet, etc.)\n        raw_data_path = self._find_raw_data()\n        if raw_data_path is None:\n            raise FileNotFoundError(f\"No raw data found for survey: {self.survey}\")\n\n        logger.info(f\"\ud83d\udd04 Loading raw data: {raw_data_path}\")\n\n        # Load with Polars\n        if raw_data_path.suffix == \".parquet\":\n            df = pl.read_parquet(raw_data_path)\n        elif raw_data_path.suffix == \".csv\":\n            df = pl.read_csv(raw_data_path)\n        else:\n            raise ValueError(f\"Unsupported file format: {raw_data_path.suffix}\")\n\n        # Apply sampling if requested\n        if self.max_samples and len(df) &gt; self.max_samples:\n            df = df.sample(n=self.max_samples, seed=42)\n            logger.info(f\"\ud83d\udcca Sampled {self.max_samples} objects from {len(df)} total\")\n\n        # Create SurveyTensorDict\n        return self._dataframe_to_survey_tensor(df)\n\n    def _find_raw_data(self) -&gt; Optional[Path]:\n        \"\"\"Find raw data file for the survey.\"\"\"\n        # Common locations - check processed first, then raw\n        search_paths = [\n            Path(\"data/processed\")\n            / self.survey\n            / f\"{self.survey}_processed.parquet\",  # Preprocessing output\n            Path(\"data/processed\") / self.survey / f\"{self.survey}.parquet\",\n            Path(\"data/processed\") / self.survey / f\"{self.survey}.csv\",\n            Path(\"data/raw\") / self.survey / f\"{self.survey}.parquet\",\n            Path(\"data/raw\") / f\"{self.survey}.parquet\",\n            Path(\"data/raw\") / f\"{self.survey}.csv\",\n            Path(\"data\") / f\"{self.survey}.parquet\",\n            Path(\"data\") / f\"{self.survey}.csv\",\n        ]\n\n        for path in search_paths:\n            if path.exists():\n                logger.info(f\"\ud83d\udcc1 Found data file: {path}\")\n                return path\n\n        logger.warning(f\"\u274c No data file found for survey: {self.survey}\")\n        logger.warning(f\"   Searched paths: {search_paths}\")\n        return None\n\n    def _dataframe_to_survey_tensor(self, df: pl.DataFrame) -&gt; SurveyTensorDict:\n        \"\"\"\n        Convert a DataFrame to a SurveyTensorDict.\n        Always pass a tensor to SpatialTensorDict (not a dict),\n        and ensure all meta fields are set as in other TensorDicts.\n        \"\"\"\n        # Check if this is processed data (has processed column config)\n        if \"processed_coord_cols\" in self.survey_config:\n            # Use processed column configuration\n            coord_cols = self.survey_config.get(\n                \"processed_coord_cols\", [\"ra_deg\", \"dec_deg\"]\n            )\n            mag_cols = self.survey_config.get(\"processed_mag_cols\", [])\n            logger.info(f\"\ud83d\udcca Using processed column config for {self.survey}\")\n        else:\n            # Use raw column configuration\n            coord_cols = self.survey_config.get(\"coord_cols\", [\"ra\", \"dec\"])\n            mag_cols = self.survey_config.get(\"mag_cols\", [])\n            logger.info(f\"\ud83d\udcca Using raw column config for {self.survey}\")\n\n        # Extract coordinates as tensor [N, D]\n        coords = []\n        for col in coord_cols:\n            if col in df.columns:\n                coords.append(torch.tensor(df[col].to_numpy(), dtype=torch.float32))\n            else:\n                logger.warning(\n                    f\"\u26a0\ufe0f Coordinate column '{col}' not found in DataFrame. Available: {df.columns}\"\n                )\n\n        if not coords:\n            raise ValueError(f\"No coordinate columns found in DataFrame: {coord_cols}\")\n\n        coordinates = torch.stack(coords, dim=1)  # [N, D]\n        # If only 2D, pad to 3D with zeros (for compatibility)\n        if coordinates.shape[1] == 2:\n            zeros = torch.zeros(coordinates.shape[0], 1, dtype=coordinates.dtype)\n            coordinates = torch.cat([coordinates, zeros], dim=1)\n        spatial_tensor = SpatialTensorDict(coordinates=coordinates)\n\n        # Extract photometric data if available\n        photometric_tensor = None\n        if mag_cols:\n            mags = []\n            bands = []\n            for col in mag_cols:\n                if col in df.columns:\n                    mags.append(torch.tensor(df[col].to_numpy(), dtype=torch.float32))\n                    bands.append(col)\n                else:\n                    logger.warning(f\"\u26a0\ufe0f Magnitude column '{col}' not found in DataFrame\")\n\n            if mags:\n                magnitudes = torch.stack(mags, dim=1)  # [N, B]\n                photometric_tensor = PhotometricTensorDict(\n                    magnitudes=magnitudes, bands=bands, filter_system=\"AB\"\n                )\n\n        # Build SurveyTensorDict with correct signature\n        survey_name = self.survey\n        data_release = self.survey_config.get(\"data_release\", \"unknown\")\n        if photometric_tensor is not None:\n            return SurveyTensorDict(\n                spatial=spatial_tensor,\n                photometric=photometric_tensor,\n                survey_name=survey_name,\n                data_release=data_release,\n            )\n        else:\n            # Minimal SurveyTensorDict (for tests or special cases)\n            # Use dummy photometric tensor if required by constructor\n            dummy_mags = torch.zeros(coordinates.shape[0], 1)\n            dummy_bands = [\"dummy\"]\n            dummy_phot = PhotometricTensorDict(dummy_mags, dummy_bands)\n            return SurveyTensorDict(\n                spatial=spatial_tensor,\n                photometric=dummy_phot,\n                survey_name=survey_name,\n                data_release=data_release,\n            )\n\n    def _build_graph(self, survey_tensor: SurveyTensorDict) -&gt; Data:\n        \"\"\"Build PyG graph from SurveyTensorDict using centralized builders.\"\"\"\n        logger.info(f\"\ud83d\udd17 Building {self.graph_method} graph with k={self.k_neighbors}\")\n\n        if self.graph_method == \"knn\":\n            graph = create_knn_graph(\n                survey_tensor,\n                k_neighbors=self.k_neighbors,\n                use_3d_coordinates=self.use_3d_coordinates,\n            )\n        elif self.graph_method == \"astronomical\":\n            graph = create_astronomical_graph(\n                survey_tensor,\n                k_neighbors=self.k_neighbors,\n                use_3d_coordinates=self.use_3d_coordinates,\n            )\n        else:\n            raise ValueError(f\"Unknown graph method: {self.graph_method}\")\n\n        # Add survey metadata\n        graph.survey_name = self.survey\n        graph.graph_method = self.graph_method\n        graph.k_neighbors = self.k_neighbors\n        graph.use_3d = self.use_3d_coordinates\n\n        # Defensive: Only log node/edge count if present\n        num_nodes = getattr(graph, \"num_nodes\", None)\n        num_edges = getattr(graph, \"num_edges\", None)\n        logger.info(\n            f\"\u2705 Built graph: {num_nodes if num_nodes is not None else '?'} nodes, {num_edges if num_edges is not None else '?'} edges\"\n        )\n        return graph\n\n    def len(self) -&gt; int:\n        \"\"\"Number of graphs in dataset.\"\"\"\n        return 1  # Single graph dataset\n\n    def get(self, idx: int) -&gt; Data:\n        \"\"\"Get graph by index.\"\"\"\n        if idx == 0:\n            return self.data\n        else:\n            raise IndexError(f\"Index {idx} out of range for single graph dataset\")\n\n    def get_survey_tensor(self) -&gt; SurveyTensorDict:\n        \"\"\"Get the underlying SurveyTensorDict (for analysis/debugging).\"\"\"\n        if self._tensor_path.exists():\n            return torch.load(self._tensor_path, weights_only=False)\n        else:\n            raise FileNotFoundError(\n                \"SurveyTensorDict not found. Run _load_data() first.\"\n            )\n\n    def get_info(self) -&gt; Dict[str, Any]:\n        \"\"\"Get dataset information.\"\"\"\n        if len(self) == 0:\n            return {\"error\": \"Dataset empty\"}\n\n        graph = self[0]\n        # Defensive: Only access attributes if present\n        num_nodes = getattr(graph, \"num_nodes\", 0)\n        num_edges = getattr(graph, \"num_edges\", 0)\n        # Use getattr to access 'x' safely\n        x = getattr(graph, \"x\", None)\n        num_features = x.size(1) if x is not None and x.dim() &gt; 1 else 0\n        info = {\n            \"survey\": self.survey,\n            \"graph_method\": self.graph_method,\n            \"k_neighbors\": self.k_neighbors,\n            \"use_3d_coordinates\": self.use_3d_coordinates,\n            \"num_nodes\": num_nodes,\n            \"num_edges\": num_edges,\n            \"num_features\": num_features,\n            \"graph_type\": getattr(graph, \"graph_type\", \"unknown\"),\n        }\n\n        return info\n</code></pre>"},{"location":"api/astro_lab.data.datasets/#astro_lab.data.datasets.SurveyGraphDataset.get","title":"get","text":"<pre><code>get(idx: int) -&gt; Data\n</code></pre> <p>Get graph by index.</p> Source code in <code>src\\astro_lab\\data\\datasets\\survey_graph_dataset.py</code> <pre><code>def get(self, idx: int) -&gt; Data:\n    \"\"\"Get graph by index.\"\"\"\n    if idx == 0:\n        return self.data\n    else:\n        raise IndexError(f\"Index {idx} out of range for single graph dataset\")\n</code></pre>"},{"location":"api/astro_lab.data.datasets/#astro_lab.data.datasets.SurveyGraphDataset.get_info","title":"get_info","text":"<pre><code>get_info() -&gt; Dict[str, Any]\n</code></pre> <p>Get dataset information.</p> Source code in <code>src\\astro_lab\\data\\datasets\\survey_graph_dataset.py</code> <pre><code>def get_info(self) -&gt; Dict[str, Any]:\n    \"\"\"Get dataset information.\"\"\"\n    if len(self) == 0:\n        return {\"error\": \"Dataset empty\"}\n\n    graph = self[0]\n    # Defensive: Only access attributes if present\n    num_nodes = getattr(graph, \"num_nodes\", 0)\n    num_edges = getattr(graph, \"num_edges\", 0)\n    # Use getattr to access 'x' safely\n    x = getattr(graph, \"x\", None)\n    num_features = x.size(1) if x is not None and x.dim() &gt; 1 else 0\n    info = {\n        \"survey\": self.survey,\n        \"graph_method\": self.graph_method,\n        \"k_neighbors\": self.k_neighbors,\n        \"use_3d_coordinates\": self.use_3d_coordinates,\n        \"num_nodes\": num_nodes,\n        \"num_edges\": num_edges,\n        \"num_features\": num_features,\n        \"graph_type\": getattr(graph, \"graph_type\", \"unknown\"),\n    }\n\n    return info\n</code></pre>"},{"location":"api/astro_lab.data.datasets/#astro_lab.data.datasets.SurveyGraphDataset.get_survey_tensor","title":"get_survey_tensor","text":"<pre><code>get_survey_tensor() -&gt; SurveyTensorDict\n</code></pre> <p>Get the underlying SurveyTensorDict (for analysis/debugging).</p> Source code in <code>src\\astro_lab\\data\\datasets\\survey_graph_dataset.py</code> <pre><code>def get_survey_tensor(self) -&gt; SurveyTensorDict:\n    \"\"\"Get the underlying SurveyTensorDict (for analysis/debugging).\"\"\"\n    if self._tensor_path.exists():\n        return torch.load(self._tensor_path, weights_only=False)\n    else:\n        raise FileNotFoundError(\n            \"SurveyTensorDict not found. Run _load_data() first.\"\n        )\n</code></pre>"},{"location":"api/astro_lab.data.datasets/#astro_lab.data.datasets.SurveyGraphDataset.len","title":"len","text":"<pre><code>len() -&gt; int\n</code></pre> <p>Number of graphs in dataset.</p> Source code in <code>src\\astro_lab\\data\\datasets\\survey_graph_dataset.py</code> <pre><code>def len(self) -&gt; int:\n    \"\"\"Number of graphs in dataset.\"\"\"\n    return 1  # Single graph dataset\n</code></pre>"},{"location":"api/astro_lab.data.graphs/","title":"astro_lab.data.graphs","text":""},{"location":"api/astro_lab.data.graphs/#astro_lab.data.graphs","title":"graphs","text":""},{"location":"api/astro_lab.data.graphs/#astro_lab.data.graphs--graph-building-module","title":"Graph Building Module","text":"<p>State-of-the-art graph construction for astronomical data.</p> <p>Modules:</p> Name Description <code>advanced</code> <p>Advanced Graph Building Methods</p> <code>base</code> <p>Base Classes for Graph Building</p> <code>builders</code> <p>Concrete Graph Builders</p> <code>pointcloud</code> <p>Point Cloud Graph Builder</p> <p>Classes:</p> Name Description <code>AdaptiveGraphBuilder</code> <p>Adaptive graph builder that adjusts connectivity based on local structure.</p> <code>AdaptivePointCloudGraphBuilder</code> <p>Advanced point cloud graph builder with adaptive connectivity.</p> <code>AstronomicalGraphBuilder</code> <p>Astronomical graph builder with domain-specific optimizations.</p> <code>BaseGraphBuilder</code> <p>Base class for graph builders with best practices.</p> <code>DynamicGraphBuilder</code> <p>Dynamic graph builder that adapts structure during training.</p> <code>GeometricPriorGraphBuilder</code> <p>Build graphs using astronomical geometric priors.</p> <code>GraphConfig</code> <p>Configuration for graph building with best practices.</p> <code>GraphOfGraphsBuilder</code> <p>Build hierarchical graph-of-graphs structure.</p> <code>HeterogeneousGraphBuilder</code> <p>Builder for heterogeneous graphs with multiple node and edge types.</p> <code>KNNGraphBuilder</code> <p>K-Nearest Neighbors graph builder with optimizations.</p> <code>MultiScaleGraphBuilder</code> <p>Multi-scale graph builder for hierarchical representations.</p> <code>PointCloudGraphBuilder</code> <p>Graph builder optimized for 3D astronomical point clouds.</p> <code>RadiusGraphBuilder</code> <p>Radius-based graph builder with adaptive radius.</p> <code>TemporalGraphBuilder</code> <p>Build temporal graphs for time-series astronomical data.</p> <p>Functions:</p> Name Description <code>create_adaptive_graph</code> <p>Create adaptive graph based on local structure.</p> <code>create_adaptive_pointcloud_graph</code> <p>Create adaptive point cloud graph from survey data.</p> <code>create_astronomical_graph</code> <p>Create astronomical graph with domain-specific optimizations.</p> <code>create_dynamic_graph</code> <p>Create dynamic graph with learnable structure.</p> <code>create_geometric_prior_graph</code> <p>Create graph with astronomical geometric priors.</p> <code>create_heterogeneous_graph</code> <p>Create heterogeneous graph with multiple node types.</p> <code>create_hierarchical_graph</code> <p>Create hierarchical graph-of-graphs.</p> <code>create_knn_graph</code> <p>Create optimized KNN graph from SurveyTensorDict.</p> <code>create_multiscale_graph</code> <p>Create multi-scale graph for hierarchical analysis.</p> <code>create_pointcloud_graph</code> <p>Create point cloud graph from survey data.</p> <code>create_radius_graph</code> <p>Create radius graph with adaptive features.</p> <code>create_temporal_graph</code> <p>Create temporal graph for time-series data.</p>"},{"location":"api/astro_lab.data.graphs/#astro_lab.data.graphs.AdaptiveGraphBuilder","title":"AdaptiveGraphBuilder","text":"<p>               Bases: <code>BaseGraphBuilder</code></p> <p>Adaptive graph builder that adjusts connectivity based on local structure.</p> <p>Methods:</p> Name Description <code>build</code> <p>Build adaptive graph based on local density and features.</p> Source code in <code>src\\astro_lab\\data\\graphs\\builders.py</code> <pre><code>class AdaptiveGraphBuilder(BaseGraphBuilder):\n    \"\"\"Adaptive graph builder that adjusts connectivity based on local structure.\"\"\"\n\n    def __init__(self, **kwargs):\n        config = GraphConfig(method=\"adaptive\", **kwargs)\n        super().__init__(config)\n\n    def build(self, survey_tensor: SurveyTensorDict) -&gt; Data:\n        \"\"\"Build adaptive graph based on local density and features.\"\"\"\n        self.validate_input(survey_tensor)\n\n        # Extract coordinates and features\n        coords = self.extract_coordinates(survey_tensor)\n        features = self.extract_features(survey_tensor)\n\n        # Compute local structure metrics\n        density_scores = self._compute_local_density(coords)\n        feature_similarity = self._compute_feature_similarity(features)\n\n        # Adaptive connectivity\n        edge_index = self._build_adaptive_edges(\n            coords, features, density_scores, feature_similarity\n        )\n\n        # Create PyG Data object\n        data = self.create_data_object(features, edge_index, coords)\n\n        # Add metadata\n        data.graph_type = \"adaptive\"\n        data.density_scores = density_scores\n\n        return data\n\n    def _compute_local_density(self, coords: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"Compute local density for each point.\"\"\"\n        k = min(20, coords.shape[0] - 1)\n        dists = torch.cdist(coords, coords)\n        kth_dists, _ = torch.kthvalue(dists, k + 1, dim=1)\n\n        # Density is inverse of average distance to k neighbors\n        density = 1.0 / (kth_dists + 1e-8)\n\n        # Normalize\n        density = (density - density.min()) / (density.max() - density.min() + 1e-8)\n\n        return density\n\n    def _compute_feature_similarity(self, features: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"Compute feature-based similarity matrix.\"\"\"\n        # Normalize features\n        features_norm = F.normalize(features, p=2, dim=1)\n\n        # Cosine similarity\n        similarity = torch.mm(features_norm, features_norm.t())\n\n        return similarity\n\n    def _build_adaptive_edges(\n        self,\n        coords: torch.Tensor,\n        features: torch.Tensor,\n        density_scores: torch.Tensor,\n        feature_similarity: torch.Tensor,\n    ) -&gt; torch.Tensor:\n        \"\"\"Build edges adaptively based on multiple criteria.\"\"\"\n        n_nodes = coords.shape[0]\n\n        # Spatial distances\n        spatial_dists = torch.cdist(coords, coords)\n\n        # Combined distance metric\n        alpha = 0.7  # Weight for spatial distance\n        beta = 0.3   # Weight for feature similarity\n\n        combined_dists = alpha * spatial_dists - beta * feature_similarity\n\n        # Adaptive k based on density\n        k_base = self.config.k_neighbors\n        adaptive_k = (k_base * (0.5 + density_scores)).long()\n        adaptive_k = torch.clamp(adaptive_k, self.config.k_min, self.config.k_max)\n\n        # Build edges\n        edge_list = []\n        for i in range(n_nodes):\n            k_i = adaptive_k[i].item()\n            _, neighbors = torch.topk(combined_dists[i], k_i + 1, largest=False)\n\n            if not self.config.self_loops:\n                neighbors = neighbors[1:]\n\n            sources = torch.full((len(neighbors),), i)\n            edge_list.append(torch.stack([sources, neighbors]))\n\n        edge_index = torch.cat(edge_list, dim=1)\n\n        return edge_index.to(self.device)\n</code></pre>"},{"location":"api/astro_lab.data.graphs/#astro_lab.data.graphs.AdaptiveGraphBuilder.build","title":"build","text":"<pre><code>build(survey_tensor: SurveyTensorDict) -&gt; Data\n</code></pre> <p>Build adaptive graph based on local density and features.</p> Source code in <code>src\\astro_lab\\data\\graphs\\builders.py</code> <pre><code>def build(self, survey_tensor: SurveyTensorDict) -&gt; Data:\n    \"\"\"Build adaptive graph based on local density and features.\"\"\"\n    self.validate_input(survey_tensor)\n\n    # Extract coordinates and features\n    coords = self.extract_coordinates(survey_tensor)\n    features = self.extract_features(survey_tensor)\n\n    # Compute local structure metrics\n    density_scores = self._compute_local_density(coords)\n    feature_similarity = self._compute_feature_similarity(features)\n\n    # Adaptive connectivity\n    edge_index = self._build_adaptive_edges(\n        coords, features, density_scores, feature_similarity\n    )\n\n    # Create PyG Data object\n    data = self.create_data_object(features, edge_index, coords)\n\n    # Add metadata\n    data.graph_type = \"adaptive\"\n    data.density_scores = density_scores\n\n    return data\n</code></pre>"},{"location":"api/astro_lab.data.graphs/#astro_lab.data.graphs.AdaptivePointCloudGraphBuilder","title":"AdaptivePointCloudGraphBuilder","text":"<p>               Bases: <code>PointCloudGraphBuilder</code></p> <p>Advanced point cloud graph builder with adaptive connectivity.</p> <p>Adjusts graph structure based on local point density and astronomical properties.</p> <p>Methods:</p> Name Description <code>build</code> <p>Build adaptive point cloud graph.</p> Source code in <code>src\\astro_lab\\data\\graphs\\pointcloud.py</code> <pre><code>class AdaptivePointCloudGraphBuilder(PointCloudGraphBuilder):\n    \"\"\"\n    Advanced point cloud graph builder with adaptive connectivity.\n\n    Adjusts graph structure based on local point density and\n    astronomical properties.\n    \"\"\"\n\n    def __init__(\n        self,\n        k_min: int = 8,\n        k_max: int = 32,\n        density_adaptive: bool = True,\n        **kwargs\n    ):\n        super().__init__(k_neighbors=k_min, **kwargs)\n        self.k_min = k_min\n        self.k_max = k_max\n        self.density_adaptive = density_adaptive\n\n    def build(self, survey_tensor: SurveyTensorDict) -&gt; Data:\n        \"\"\"Build adaptive point cloud graph.\"\"\"\n        self.validate_input(survey_tensor)\n\n        # Extract positions and features\n        positions = self.extract_coordinates(survey_tensor)\n        if self.normalize_positions:\n            positions = self._normalize_to_unit_sphere(positions)\n\n        features = self.extract_features(survey_tensor)\n\n        # Compute local density\n        if self.density_adaptive:\n            density_scores = self._compute_local_density(positions)\n            edge_index = self._build_adaptive_graph(positions, density_scores)\n        else:\n            edge_index = self._build_spatial_graph(positions)\n\n        # Create data object\n        data = self.create_data_object(features, edge_index, positions)\n\n        # Add metadata\n        data.graph_type = \"adaptive_pointcloud\"\n        if self.density_adaptive:\n            data.density_scores = density_scores\n\n        return data\n\n    def _compute_local_density(self, positions: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"Compute local point density for each node.\"\"\"\n        # Find distance to k-th nearest neighbor as density proxy\n        k_density = min(20, positions.shape[0] - 1)\n\n        # Compute pairwise distances\n        dists = torch.cdist(positions, positions)\n\n        # Get k-th smallest distance (excluding self)\n        kth_dists, _ = torch.kthvalue(dists, k_density + 1, dim=1)\n\n        # Density is inverse of k-th distance\n        density = 1.0 / (kth_dists + 1e-8)\n\n        # Normalize to [0, 1]\n        density = (density - density.min()) / (density.max() - density.min() + 1e-8)\n\n        return density\n\n    def _build_adaptive_graph(\n        self, \n        positions: torch.Tensor, \n        density_scores: torch.Tensor\n    ) -&gt; torch.Tensor:\n        \"\"\"Build graph with adaptive connectivity based on density.\"\"\"\n        n_nodes = positions.shape[0]\n\n        # Adaptive k: more neighbors in dense regions\n        adaptive_k = (\n            self.k_min + \n            (self.k_max - self.k_min) * density_scores\n        ).long()\n\n        # Build edges with varying k\n        edge_list = []\n\n        # Compute pairwise distances once\n        dists = torch.cdist(positions, positions)\n\n        for i in range(n_nodes):\n            k_i = adaptive_k[i].item()\n\n            # Find k nearest neighbors\n            _, neighbors = torch.topk(dists[i], k_i + 1, largest=False)\n\n            if not self.config.self_loops:\n                neighbors = neighbors[1:]  # Skip self\n\n            # Add edges\n            sources = torch.full((len(neighbors),), i, device=self.device)\n            edge_list.append(torch.stack([sources, neighbors]))\n\n        edge_index = torch.cat(edge_list, dim=1)\n\n        return edge_index\n</code></pre>"},{"location":"api/astro_lab.data.graphs/#astro_lab.data.graphs.AdaptivePointCloudGraphBuilder.build","title":"build","text":"<pre><code>build(survey_tensor: SurveyTensorDict) -&gt; Data\n</code></pre> <p>Build adaptive point cloud graph.</p> Source code in <code>src\\astro_lab\\data\\graphs\\pointcloud.py</code> <pre><code>def build(self, survey_tensor: SurveyTensorDict) -&gt; Data:\n    \"\"\"Build adaptive point cloud graph.\"\"\"\n    self.validate_input(survey_tensor)\n\n    # Extract positions and features\n    positions = self.extract_coordinates(survey_tensor)\n    if self.normalize_positions:\n        positions = self._normalize_to_unit_sphere(positions)\n\n    features = self.extract_features(survey_tensor)\n\n    # Compute local density\n    if self.density_adaptive:\n        density_scores = self._compute_local_density(positions)\n        edge_index = self._build_adaptive_graph(positions, density_scores)\n    else:\n        edge_index = self._build_spatial_graph(positions)\n\n    # Create data object\n    data = self.create_data_object(features, edge_index, positions)\n\n    # Add metadata\n    data.graph_type = \"adaptive_pointcloud\"\n    if self.density_adaptive:\n        data.density_scores = density_scores\n\n    return data\n</code></pre>"},{"location":"api/astro_lab.data.graphs/#astro_lab.data.graphs.AstronomicalGraphBuilder","title":"AstronomicalGraphBuilder","text":"<p>               Bases: <code>BaseGraphBuilder</code></p> <p>Astronomical graph builder with domain-specific optimizations.</p> <p>Methods:</p> Name Description <code>build</code> <p>Build astronomical graph with specialized logic.</p> Source code in <code>src\\astro_lab\\data\\graphs\\builders.py</code> <pre><code>class AstronomicalGraphBuilder(BaseGraphBuilder):\n    \"\"\"Astronomical graph builder with domain-specific optimizations.\"\"\"\n\n    def __init__(self, **kwargs):\n        config = GraphConfig(method=\"astronomical\", **kwargs)\n        super().__init__(config)\n\n    def build(self, survey_tensor: SurveyTensorDict) -&gt; Data:\n        \"\"\"Build astronomical graph with specialized logic.\"\"\"\n        self.validate_input(survey_tensor)\n\n        # Extract coordinates and features\n        coords = self.extract_coordinates(survey_tensor)\n        features = self.extract_features(survey_tensor)\n\n        # Determine graph construction method based on data\n        if self._is_survey_data(survey_tensor):\n            edge_index = self._build_survey_graph(coords, survey_tensor)\n        elif self._is_simulation_data(survey_tensor):\n            edge_index = self._build_simulation_graph(coords, survey_tensor)\n        else:\n            # Default to optimized KNN\n            edge_index = self._build_astronomical_knn(coords)\n\n        # Make undirected if requested\n        if not self.config.directed:\n            edge_index = to_undirected(edge_index, num_nodes=coords.shape[0])\n\n        # Create PyG Data object\n        data = self.create_data_object(features, edge_index, coords)\n\n        # Add astronomical metadata\n        data.graph_type = \"astronomical\"\n        data.coordinate_system = self.config.coordinate_system\n        data.use_3d = self.config.use_3d_coordinates\n\n        # Add survey metadata if available\n        if hasattr(survey_tensor, \"meta\") and survey_tensor.meta:\n            data.survey_name = survey_tensor.meta.get(\"survey_name\", \"unknown\")\n            data.data_release = survey_tensor.meta.get(\"data_release\", \"unknown\")\n\n        return data\n\n    def _is_survey_data(self, survey_tensor: SurveyTensorDict) -&gt; bool:\n        \"\"\"Check if data is from astronomical survey.\"\"\"\n        if hasattr(survey_tensor, \"survey_name\"):\n            return survey_tensor.survey_name in [\"gaia\", \"sdss\", \"lsst\", \"euclid\"]\n        return False\n\n    def _is_simulation_data(self, survey_tensor: SurveyTensorDict) -&gt; bool:\n        \"\"\"Check if data is from simulation.\"\"\"\n        if hasattr(survey_tensor, \"survey_name\"):\n            return survey_tensor.survey_name in [\"tng50\", \"eagle\", \"illustris\"]\n        return False\n\n    def _build_survey_graph(\n        self, coords: torch.Tensor, survey_tensor: SurveyTensorDict\n    ) -&gt; torch.Tensor:\n        \"\"\"Build graph optimized for survey data.\"\"\"\n        survey_name = getattr(survey_tensor, \"survey_name\", \"unknown\")\n\n        if survey_name == \"gaia\" and coords.shape[1] &gt;= 2:\n            # Use angular distance for Gaia\n            return self._build_angular_graph(coords)\n        elif survey_name in [\"sdss\", \"lsst\"] and coords.shape[1] &gt;= 3:\n            # Use 3D distance with redshift\n            return self._build_cosmological_graph(coords)\n        else:\n            # Default KNN\n            return knn_graph(\n                coords,\n                k=self.config.k_neighbors,\n                batch=None,\n                loop=self.config.self_loops,\n            )\n\n    def _build_simulation_graph(\n        self, coords: torch.Tensor, survey_tensor: SurveyTensorDict\n    ) -&gt; torch.Tensor:\n        \"\"\"Build graph optimized for simulation data.\"\"\"\n        # For simulations, use adaptive neighbor search\n        return self._build_adaptive_knn(coords)\n\n    def _build_astronomical_knn(self, coords: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"Build KNN graph with astronomical optimizations.\"\"\"\n        # Use appropriate distance metric\n        if self.config.coordinate_system == \"spherical\":\n            return self._build_angular_graph(coords)\n        else:\n            return knn_graph(\n                coords,\n                k=self.config.k_neighbors,\n                batch=None,\n                loop=self.config.self_loops,\n            )\n\n    def _build_angular_graph(self, coords: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"Build graph using angular distance (optimized).\"\"\"\n        n_nodes = coords.shape[0]\n        k = self.config.k_neighbors\n\n        # Convert to unit vectors for efficient computation\n        if coords.shape[1] == 2:  # RA, Dec\n            ra, dec = coords[:, 0], coords[:, 1]\n            x = torch.cos(dec) * torch.cos(ra)\n            y = torch.cos(dec) * torch.sin(ra)\n            z = torch.sin(dec)\n            unit_vectors = torch.stack([x, y, z], dim=1)\n        else:  # Already 3D\n            norms = torch.norm(coords, dim=1, keepdim=True)\n            unit_vectors = coords / (norms + 1e-8)\n\n        # Compute cosine similarity (dot product of unit vectors)\n        cos_sim = torch.mm(unit_vectors, unit_vectors.t())\n        cos_sim = torch.clamp(cos_sim, -1.0, 1.0)\n\n        # Angular distance\n        angular_dist = torch.acos(cos_sim)\n\n        # Find k nearest neighbors\n        _, indices = torch.topk(angular_dist, k + 1, dim=1, largest=False)\n\n        if not self.config.self_loops:\n            indices = indices[:, 1:]  # Remove self\n\n        # Create edge index\n        source_nodes = torch.arange(n_nodes).unsqueeze(1).expand(-1, indices.shape[1])\n        edge_index = torch.stack([source_nodes.flatten(), indices.flatten()])\n\n        return edge_index.to(self.device)\n\n    def _build_cosmological_graph(self, coords: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"Build graph considering cosmological distances.\"\"\"\n        # For now, use standard KNN with 3D coordinates\n        # TODO: Implement proper cosmological distance metrics\n        return knn_graph(\n            coords,\n            k=self.config.k_neighbors,\n            batch=None,\n            loop=self.config.self_loops,\n        )\n\n    def _build_adaptive_knn(self, coords: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"Build KNN with adaptive k based on local density.\"\"\"\n        n_nodes = coords.shape[0]\n\n        # Compute local density\n        dists = torch.cdist(coords, coords)\n\n        # Find distance to 10th nearest neighbor as density proxy\n        k_density = min(10, n_nodes - 1)\n        density_dists, _ = torch.kthvalue(dists, k_density + 1, dim=1)\n\n        # Normalize densities\n        density_scores = 1.0 / (density_dists + 1e-8)\n        density_scores = density_scores / density_scores.max()\n\n        # Adaptive k: more neighbors in dense regions\n        adaptive_k = (\n            self.config.k_min + \n            (self.config.k_max - self.config.k_min) * density_scores\n        ).long()\n\n        # Build graph with varying k\n        edge_list = []\n        for i in range(n_nodes):\n            k_i = adaptive_k[i].item()\n            _, neighbors = torch.topk(dists[i], k_i + 1, largest=False)\n\n            if not self.config.self_loops:\n                neighbors = neighbors[1:]  # Skip self\n\n            sources = torch.full((len(neighbors),), i)\n            edge_list.append(torch.stack([sources, neighbors]))\n\n        edge_index = torch.cat(edge_list, dim=1)\n\n        return edge_index.to(self.device)\n</code></pre>"},{"location":"api/astro_lab.data.graphs/#astro_lab.data.graphs.AstronomicalGraphBuilder.build","title":"build","text":"<pre><code>build(survey_tensor: SurveyTensorDict) -&gt; Data\n</code></pre> <p>Build astronomical graph with specialized logic.</p> Source code in <code>src\\astro_lab\\data\\graphs\\builders.py</code> <pre><code>def build(self, survey_tensor: SurveyTensorDict) -&gt; Data:\n    \"\"\"Build astronomical graph with specialized logic.\"\"\"\n    self.validate_input(survey_tensor)\n\n    # Extract coordinates and features\n    coords = self.extract_coordinates(survey_tensor)\n    features = self.extract_features(survey_tensor)\n\n    # Determine graph construction method based on data\n    if self._is_survey_data(survey_tensor):\n        edge_index = self._build_survey_graph(coords, survey_tensor)\n    elif self._is_simulation_data(survey_tensor):\n        edge_index = self._build_simulation_graph(coords, survey_tensor)\n    else:\n        # Default to optimized KNN\n        edge_index = self._build_astronomical_knn(coords)\n\n    # Make undirected if requested\n    if not self.config.directed:\n        edge_index = to_undirected(edge_index, num_nodes=coords.shape[0])\n\n    # Create PyG Data object\n    data = self.create_data_object(features, edge_index, coords)\n\n    # Add astronomical metadata\n    data.graph_type = \"astronomical\"\n    data.coordinate_system = self.config.coordinate_system\n    data.use_3d = self.config.use_3d_coordinates\n\n    # Add survey metadata if available\n    if hasattr(survey_tensor, \"meta\") and survey_tensor.meta:\n        data.survey_name = survey_tensor.meta.get(\"survey_name\", \"unknown\")\n        data.data_release = survey_tensor.meta.get(\"data_release\", \"unknown\")\n\n    return data\n</code></pre>"},{"location":"api/astro_lab.data.graphs/#astro_lab.data.graphs.BaseGraphBuilder","title":"BaseGraphBuilder","text":"<p>               Bases: <code>ABC</code></p> <p>Base class for graph builders with best practices.</p> <p>Methods:</p> Name Description <code>build</code> <p>Build graph from SurveyTensorDict.</p> <code>create_data_object</code> <p>Create PyG Data object with enhanced validation.</p> <code>create_hetero_data_object</code> <p>Create heterogeneous graph data object.</p> <code>extract_coordinates</code> <p>Extract coordinates from SurveyTensorDict structure.</p> <code>extract_features</code> <p>Extract features from SurveyTensorDict components.</p> <code>validate_input</code> <p>Enhanced input validation.</p> Source code in <code>src\\astro_lab\\data\\graphs\\base.py</code> <pre><code>class BaseGraphBuilder(ABC):\n    \"\"\"Base class for graph builders with best practices.\"\"\"\n\n    def __init__(self, config: Optional[GraphConfig] = None):\n        self.config = config or GraphConfig()\n        self.device = self._setup_device()\n        self._setup_logging()\n        self._transforms = self._setup_transforms()\n\n    def _setup_device(self) -&gt; torch.device:\n        \"\"\"Setup computation device.\"\"\"\n        if self.config.device:\n            return torch.device(self.config.device)\n\n        if self.config.use_gpu_construction and torch.cuda.is_available():\n            return torch.device(\"cuda\")\n\n        return torch.device(\"cpu\")\n\n    def _setup_logging(self):\n        \"\"\"Setup logging configuration.\"\"\"\n        import logging\n        self.logger = logging.getLogger(self.__class__.__name__)\n\n    def _setup_transforms(self) -&gt; List[Any]:\n        \"\"\"Setup graph transforms.\"\"\"\n        transforms = []\n\n        if self.config.self_loops:\n            transforms.append(AddSelfLoops())\n\n        if self.config.remove_isolated:\n            transforms.append(RemoveIsolatedNodes())\n\n        return transforms\n\n    @abstractmethod\n    def build(self, survey_tensor: SurveyTensorDict) -&gt; Union[Data, HeteroData]:\n        \"\"\"Build graph from SurveyTensorDict.\"\"\"\n        pass\n\n    def validate_input(self, survey_tensor: SurveyTensorDict) -&gt; None:\n        \"\"\"Enhanced input validation.\"\"\"\n        if not isinstance(survey_tensor, SurveyTensorDict):\n            raise TypeError(\n                f\"Input must be SurveyTensorDict, got {type(survey_tensor)}\"\n            )\n\n        # Check for required spatial data\n        if \"spatial\" not in survey_tensor:\n            raise ValueError(\"SurveyTensorDict must contain 'spatial' data\")\n\n        # Validate spatial data has coordinates\n        spatial_data = survey_tensor[\"spatial\"]\n        if \"coordinates\" not in spatial_data:\n            raise ValueError(\"Spatial data must have 'coordinates' key\")\n\n        # Check coordinate dimensions\n        coords = spatial_data[\"coordinates\"]\n        if coords.dim() &lt; 2:\n            raise ValueError(\n                f\"Coordinates must be at least 2D tensor, got shape {coords.shape}\"\n            )\n\n        # Validate k_neighbors parameter\n        n_objects = coords.shape[0]\n        if self.config.k_neighbors &gt;= n_objects:\n            self.logger.warning(\n                f\"k_neighbors ({self.config.k_neighbors}) &gt;= n_objects ({n_objects}). \"\n                f\"Adjusting to {n_objects - 1}\"\n            )\n            self.config.k_neighbors = min(self.config.k_neighbors, n_objects - 1)\n\n    def extract_coordinates(self, survey_tensor: SurveyTensorDict) -&gt; torch.Tensor:\n        \"\"\"Extract coordinates from SurveyTensorDict structure.\"\"\"\n        spatial_data = survey_tensor[\"spatial\"]\n\n        # Get coordinates from TensorDict structure\n        coords = spatial_data[\"coordinates\"]\n\n        # Ensure proper dimensions\n        if coords.dim() == 1:\n            coords = coords.unsqueeze(1)\n\n        # Handle NaN/Inf\n        coords = self._handle_invalid_values(coords, \"coordinates\")\n\n        # Convert coordinate system if needed\n        if self.config.coordinate_system == \"spherical\" and coords.shape[1] &gt;= 2:\n            coords = self._convert_to_spherical(coords)\n        elif self.config.coordinate_system == \"galactic\" and coords.shape[1] &gt;= 2:\n            coords = self._convert_to_galactic(coords)\n\n        return coords.to(self.device)\n\n    def extract_features(self, survey_tensor: SurveyTensorDict) -&gt; torch.Tensor:\n        \"\"\"Extract features from SurveyTensorDict components.\"\"\"\n        features = []\n        feature_names = []\n\n        # Spatial features (coordinates and proper motions)\n        if self.config.use_astrometry and \"spatial\" in survey_tensor:\n            spatial_data = survey_tensor[\"spatial\"]\n\n            # Always include coordinates as features\n            coords = spatial_data[\"coordinates\"]\n            features.append(coords)\n            feature_names.extend([f\"coord_{i}\" for i in range(coords.shape[1])])\n\n            # Proper motions if available in spatial data\n            if \"pmra\" in spatial_data:\n                pmra = spatial_data[\"pmra\"]\n                if pmra.dim() == 1:\n                    pmra = pmra.unsqueeze(1)\n                features.append(pmra)\n                feature_names.append(\"pmra\")\n\n            if \"pmdec\" in spatial_data:\n                pmdec = spatial_data[\"pmdec\"]\n                if pmdec.dim() == 1:\n                    pmdec = pmdec.unsqueeze(1)\n                features.append(pmdec)\n                feature_names.append(\"pmdec\")\n\n            # Parallax if available\n            if \"parallax\" in spatial_data:\n                plx = spatial_data[\"parallax\"]\n                if plx.dim() == 1:\n                    plx = plx.unsqueeze(1)\n                features.append(plx)\n                feature_names.append(\"parallax\")\n\n        # Photometric features\n        if self.config.use_photometry and \"photometric\" in survey_tensor:\n            phot_data = survey_tensor[\"photometric\"]\n\n            if \"magnitudes\" in phot_data:\n                mags = phot_data[\"magnitudes\"]\n                if mags.dim() == 1:\n                    mags = mags.unsqueeze(1)\n                features.append(mags)\n\n                # Add band names if available\n                if hasattr(phot_data, \"bands\"):\n                    feature_names.extend([f\"mag_{band}\" for band in phot_data.bands])\n                else:\n                    feature_names.extend([f\"mag_{i}\" for i in range(mags.shape[1])])\n\n            # Colors if available\n            if \"colors\" in phot_data:\n                colors = phot_data[\"colors\"]\n                if colors.dim() == 1:\n                    colors = colors.unsqueeze(1)\n                features.append(colors)\n                feature_names.extend([f\"color_{i}\" for i in range(colors.shape[1])])\n\n        # Spectral features\n        if self.config.use_spectroscopy and \"spectral\" in survey_tensor:\n            spec_data = survey_tensor[\"spectral\"]\n\n            if \"features\" in spec_data:\n                spec_features = spec_data[\"features\"]\n                if spec_features.dim() == 1:\n                    spec_features = spec_features.unsqueeze(1)\n                features.append(spec_features)\n                feature_names.extend([f\"spec_{i}\" for i in range(spec_features.shape[1])])\n\n        # Additional features if available\n        if \"features\" in survey_tensor:\n            extra_features = survey_tensor[\"features\"]\n            if extra_features.dim() == 1:\n                extra_features = extra_features.unsqueeze(1)\n            features.append(extra_features)\n            feature_names.extend([f\"feat_{i}\" for i in range(extra_features.shape[1])])\n\n        if not features:\n            # Minimal fallback: use coordinates only\n            self.logger.warning(\"No features found, using coordinates as features\")\n            coords = self.extract_coordinates(survey_tensor)\n            features = [coords]\n            feature_names = [f\"coord_{i}\" for i in range(coords.shape[1])]\n\n        # Concatenate all features\n        result = torch.cat(features, dim=-1).to(self.device)\n\n        # Handle invalid values\n        result = self._handle_invalid_values(result, \"features\")\n\n        # Outlier detection\n        if self.config.outlier_detection:\n            result, outlier_mask = self._detect_outliers(result)\n            self.logger.info(f\"Detected {outlier_mask.sum()} outliers\")\n\n        # Normalize features\n        if self.config.normalize_features:\n            result = self._normalize_features(result)\n\n        # Store feature names for interpretability\n        self._feature_names = feature_names\n\n        self.logger.info(\n            f\"Extracted {result.shape[1]} features from {result.shape[0]} objects\"\n        )\n\n        return result\n\n    def _handle_invalid_values(\n        self, tensor: torch.Tensor, name: str\n    ) -&gt; torch.Tensor:\n        \"\"\"Handle NaN and Inf values.\"\"\"\n        # Count invalid values\n        nan_mask = torch.isnan(tensor)\n        inf_mask = torch.isinf(tensor)\n        invalid_mask = nan_mask | inf_mask\n\n        if invalid_mask.any():\n            n_invalid = invalid_mask.sum().item()\n            self.logger.warning(\n                f\"Found {n_invalid} invalid values in {name} \"\n                f\"({100 * n_invalid / tensor.numel():.2f}%)\"\n            )\n\n            if self.config.handle_nan == \"median\":\n                # Replace with median\n                for i in range(tensor.shape[1]):\n                    col = tensor[:, i]\n                    valid_mask = ~invalid_mask[:, i]\n                    if valid_mask.any():\n                        median_val = col[valid_mask].median()\n                        col[invalid_mask[:, i]] = median_val\n                    else:\n                        col[invalid_mask[:, i]] = 0.0\n\n            elif self.config.handle_nan == \"mean\":\n                # Replace with mean\n                for i in range(tensor.shape[1]):\n                    col = tensor[:, i]\n                    valid_mask = ~invalid_mask[:, i]\n                    if valid_mask.any():\n                        mean_val = col[valid_mask].mean()\n                        col[invalid_mask[:, i]] = mean_val\n                    else:\n                        col[invalid_mask[:, i]] = 0.0\n\n            elif self.config.handle_nan == \"zero\":\n                # Replace with zero\n                tensor[invalid_mask] = 0.0\n\n            else:  # \"drop\"\n                # This should be handled at a higher level\n                self.logger.warning(\"'drop' method not implemented at tensor level\")\n                tensor[invalid_mask] = 0.0\n\n        return tensor\n\n    def _detect_outliers(\n        self, features: torch.Tensor\n    ) -&gt; Tuple[torch.Tensor, torch.Tensor]:\n        \"\"\"Detect outliers in features.\"\"\"\n        n_samples, n_features = features.shape\n        outlier_mask = torch.zeros(n_samples, dtype=torch.bool, device=features.device)\n\n        if self.config.outlier_method == \"zscore\":\n            # Z-score based outlier detection\n            for i in range(n_features):\n                col = features[:, i]\n                mean = col.mean()\n                std = col.std()\n                if std &gt; 0:\n                    z_scores = torch.abs((col - mean) / std)\n                    outlier_mask |= z_scores &gt; self.config.outlier_threshold\n\n        elif self.config.outlier_method == \"iqr\":\n            # IQR based outlier detection\n            for i in range(n_features):\n                col = features[:, i]\n                q1 = torch.quantile(col, 0.25)\n                q3 = torch.quantile(col, 0.75)\n                iqr = q3 - q1\n                lower = q1 - self.config.outlier_threshold * iqr\n                upper = q3 + self.config.outlier_threshold * iqr\n                outlier_mask |= (col &lt; lower) | (col &gt; upper)\n\n        # Handle outliers (clip to non-outlier range)\n        if outlier_mask.any():\n            for i in range(n_features):\n                col = features[:, i]\n                non_outlier_mask = ~outlier_mask\n                if non_outlier_mask.any():\n                    min_val = col[non_outlier_mask].min()\n                    max_val = col[non_outlier_mask].max()\n                    col[outlier_mask] = torch.clamp(col[outlier_mask], min_val, max_val)\n\n        return features, outlier_mask\n\n    def _normalize_features(self, features: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"Normalize features.\"\"\"\n        if self.config.normalize_method == \"standardize\":\n            # Standardization (z-score normalization)\n            mean = features.mean(dim=0, keepdim=True)\n            std = features.std(dim=0, keepdim=True)\n            std = torch.where(std &gt; 0, std, torch.ones_like(std))\n            features = (features - mean) / std\n\n        elif self.config.normalize_method == \"minmax\":\n            # Min-max normalization\n            min_vals = features.min(dim=0, keepdim=True)[0]\n            max_vals = features.max(dim=0, keepdim=True)[0]\n            range_vals = max_vals - min_vals\n            range_vals = torch.where(range_vals &gt; 0, range_vals, torch.ones_like(range_vals))\n            features = (features - min_vals) / range_vals\n\n        elif self.config.normalize_method == \"robust\":\n            # Robust normalization (using median and MAD)\n            median = features.median(dim=0, keepdim=True)[0]\n            mad = (features - median).abs().median(dim=0, keepdim=True)[0]\n            mad = torch.where(mad &gt; 0, mad, torch.ones_like(mad))\n            features = (features - median) / (1.4826 * mad)  # 1.4826 \u2248 1/\u03a6^(-1)(0.75)\n\n        return features\n\n    def _convert_to_spherical(self, coords: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"Convert Cartesian to spherical coordinates.\"\"\"\n        if coords.shape[1] &lt; 3:\n            # Already spherical (RA, Dec)\n            return coords\n\n        x, y, z = coords[:, 0], coords[:, 1], coords[:, 2]\n\n        # Radius\n        r = torch.sqrt(x**2 + y**2 + z**2)\n\n        # Azimuth (RA)\n        phi = torch.atan2(y, x)\n\n        # Elevation (Dec)\n        theta = torch.asin(z / (r + 1e-8))\n\n        return torch.stack([phi, theta, r], dim=-1)\n\n    def _convert_to_galactic(self, coords: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"Convert to galactic coordinates (placeholder).\"\"\"\n        self.logger.warning(\"Galactic conversion not implemented, returning original\")\n        return coords\n\n    def create_data_object(\n        self,\n        features: torch.Tensor,\n        edge_index: torch.Tensor,\n        coords: torch.Tensor,\n        **kwargs,\n    ) -&gt; Data:\n        \"\"\"Create PyG Data object with enhanced validation.\"\"\"\n        # Validate dimensions\n        n_nodes = features.size(0)\n        if coords.size(0) != n_nodes:\n            raise ValueError(\n                f\"Size mismatch: features {n_nodes} != coords {coords.size(0)}\"\n            )\n\n        # Validate edge indices\n        if edge_index.size(0) != 2:\n            raise ValueError(\n                f\"Edge index must have shape [2, E], got {edge_index.shape}\"\n            )\n\n        if edge_index.size(1) &gt; 0:\n            max_idx = edge_index.max()\n            if max_idx &gt;= n_nodes:\n                # Filter invalid edges\n                valid_mask = (edge_index[0] &lt; n_nodes) &amp; (edge_index[1] &lt; n_nodes)\n                edge_index = edge_index[:, valid_mask]\n                self.logger.warning(\n                    f\"Filtered {(~valid_mask).sum()} invalid edges\"\n                )\n\n        # Create data object\n        data = Data(\n            x=features,\n            edge_index=edge_index,\n            pos=coords,\n            num_nodes=n_nodes,\n            **kwargs\n        )\n\n        # Apply transforms\n        for transform in self._transforms:\n            data = transform(data)\n\n        # Add metadata\n        data.num_edges = edge_index.size(1)\n        data.graph_method = self.config.method\n        data.device = str(self.device)\n\n        # Add feature names if available\n        if hasattr(self, \"_feature_names\"):\n            data.feature_names = self._feature_names\n\n        # Ensure connectivity if requested\n        if self.config.ensure_connected:\n            data = self._ensure_connected(data)\n\n        self.logger.info(\n            f\"Created graph: {data.num_nodes} nodes, {data.num_edges} edges, \"\n            f\"{features.shape[1]} features\"\n        )\n\n        return data\n\n    def _ensure_connected(self, data: Data) -&gt; Data:\n        \"\"\"Ensure graph is connected (placeholder).\"\"\"\n        # TODO: Implement connected component analysis and connection\n        return data\n\n    def create_hetero_data_object(\n        self,\n        node_features: Dict[str, torch.Tensor],\n        edge_indices: Dict[Tuple[str, str, str], torch.Tensor],\n        node_coords: Dict[str, torch.Tensor],\n        **kwargs,\n    ) -&gt; HeteroData:\n        \"\"\"Create heterogeneous graph data object.\"\"\"\n        data = HeteroData(**kwargs)\n\n        # Add node features\n        for node_type, features in node_features.items():\n            data[node_type].x = features\n            if node_type in node_coords:\n                data[node_type].pos = node_coords[node_type]\n\n        # Add edges\n        for edge_type, edge_index in edge_indices.items():\n            data[edge_type].edge_index = edge_index\n\n        return data\n</code></pre>"},{"location":"api/astro_lab.data.graphs/#astro_lab.data.graphs.BaseGraphBuilder.build","title":"build  <code>abstractmethod</code>","text":"<pre><code>build(survey_tensor: SurveyTensorDict) -&gt; Union[Data, HeteroData]\n</code></pre> <p>Build graph from SurveyTensorDict.</p> Source code in <code>src\\astro_lab\\data\\graphs\\base.py</code> <pre><code>@abstractmethod\ndef build(self, survey_tensor: SurveyTensorDict) -&gt; Union[Data, HeteroData]:\n    \"\"\"Build graph from SurveyTensorDict.\"\"\"\n    pass\n</code></pre>"},{"location":"api/astro_lab.data.graphs/#astro_lab.data.graphs.BaseGraphBuilder.create_data_object","title":"create_data_object","text":"<pre><code>create_data_object(\n    features: Tensor, edge_index: Tensor, coords: Tensor, **kwargs\n) -&gt; Data\n</code></pre> <p>Create PyG Data object with enhanced validation.</p> Source code in <code>src\\astro_lab\\data\\graphs\\base.py</code> <pre><code>def create_data_object(\n    self,\n    features: torch.Tensor,\n    edge_index: torch.Tensor,\n    coords: torch.Tensor,\n    **kwargs,\n) -&gt; Data:\n    \"\"\"Create PyG Data object with enhanced validation.\"\"\"\n    # Validate dimensions\n    n_nodes = features.size(0)\n    if coords.size(0) != n_nodes:\n        raise ValueError(\n            f\"Size mismatch: features {n_nodes} != coords {coords.size(0)}\"\n        )\n\n    # Validate edge indices\n    if edge_index.size(0) != 2:\n        raise ValueError(\n            f\"Edge index must have shape [2, E], got {edge_index.shape}\"\n        )\n\n    if edge_index.size(1) &gt; 0:\n        max_idx = edge_index.max()\n        if max_idx &gt;= n_nodes:\n            # Filter invalid edges\n            valid_mask = (edge_index[0] &lt; n_nodes) &amp; (edge_index[1] &lt; n_nodes)\n            edge_index = edge_index[:, valid_mask]\n            self.logger.warning(\n                f\"Filtered {(~valid_mask).sum()} invalid edges\"\n            )\n\n    # Create data object\n    data = Data(\n        x=features,\n        edge_index=edge_index,\n        pos=coords,\n        num_nodes=n_nodes,\n        **kwargs\n    )\n\n    # Apply transforms\n    for transform in self._transforms:\n        data = transform(data)\n\n    # Add metadata\n    data.num_edges = edge_index.size(1)\n    data.graph_method = self.config.method\n    data.device = str(self.device)\n\n    # Add feature names if available\n    if hasattr(self, \"_feature_names\"):\n        data.feature_names = self._feature_names\n\n    # Ensure connectivity if requested\n    if self.config.ensure_connected:\n        data = self._ensure_connected(data)\n\n    self.logger.info(\n        f\"Created graph: {data.num_nodes} nodes, {data.num_edges} edges, \"\n        f\"{features.shape[1]} features\"\n    )\n\n    return data\n</code></pre>"},{"location":"api/astro_lab.data.graphs/#astro_lab.data.graphs.BaseGraphBuilder.create_hetero_data_object","title":"create_hetero_data_object","text":"<pre><code>create_hetero_data_object(\n    node_features: Dict[str, Tensor],\n    edge_indices: Dict[Tuple[str, str, str], Tensor],\n    node_coords: Dict[str, Tensor],\n    **kwargs\n) -&gt; HeteroData\n</code></pre> <p>Create heterogeneous graph data object.</p> Source code in <code>src\\astro_lab\\data\\graphs\\base.py</code> <pre><code>def create_hetero_data_object(\n    self,\n    node_features: Dict[str, torch.Tensor],\n    edge_indices: Dict[Tuple[str, str, str], torch.Tensor],\n    node_coords: Dict[str, torch.Tensor],\n    **kwargs,\n) -&gt; HeteroData:\n    \"\"\"Create heterogeneous graph data object.\"\"\"\n    data = HeteroData(**kwargs)\n\n    # Add node features\n    for node_type, features in node_features.items():\n        data[node_type].x = features\n        if node_type in node_coords:\n            data[node_type].pos = node_coords[node_type]\n\n    # Add edges\n    for edge_type, edge_index in edge_indices.items():\n        data[edge_type].edge_index = edge_index\n\n    return data\n</code></pre>"},{"location":"api/astro_lab.data.graphs/#astro_lab.data.graphs.BaseGraphBuilder.extract_coordinates","title":"extract_coordinates","text":"<pre><code>extract_coordinates(survey_tensor: SurveyTensorDict) -&gt; Tensor\n</code></pre> <p>Extract coordinates from SurveyTensorDict structure.</p> Source code in <code>src\\astro_lab\\data\\graphs\\base.py</code> <pre><code>def extract_coordinates(self, survey_tensor: SurveyTensorDict) -&gt; torch.Tensor:\n    \"\"\"Extract coordinates from SurveyTensorDict structure.\"\"\"\n    spatial_data = survey_tensor[\"spatial\"]\n\n    # Get coordinates from TensorDict structure\n    coords = spatial_data[\"coordinates\"]\n\n    # Ensure proper dimensions\n    if coords.dim() == 1:\n        coords = coords.unsqueeze(1)\n\n    # Handle NaN/Inf\n    coords = self._handle_invalid_values(coords, \"coordinates\")\n\n    # Convert coordinate system if needed\n    if self.config.coordinate_system == \"spherical\" and coords.shape[1] &gt;= 2:\n        coords = self._convert_to_spherical(coords)\n    elif self.config.coordinate_system == \"galactic\" and coords.shape[1] &gt;= 2:\n        coords = self._convert_to_galactic(coords)\n\n    return coords.to(self.device)\n</code></pre>"},{"location":"api/astro_lab.data.graphs/#astro_lab.data.graphs.BaseGraphBuilder.extract_features","title":"extract_features","text":"<pre><code>extract_features(survey_tensor: SurveyTensorDict) -&gt; Tensor\n</code></pre> <p>Extract features from SurveyTensorDict components.</p> Source code in <code>src\\astro_lab\\data\\graphs\\base.py</code> <pre><code>def extract_features(self, survey_tensor: SurveyTensorDict) -&gt; torch.Tensor:\n    \"\"\"Extract features from SurveyTensorDict components.\"\"\"\n    features = []\n    feature_names = []\n\n    # Spatial features (coordinates and proper motions)\n    if self.config.use_astrometry and \"spatial\" in survey_tensor:\n        spatial_data = survey_tensor[\"spatial\"]\n\n        # Always include coordinates as features\n        coords = spatial_data[\"coordinates\"]\n        features.append(coords)\n        feature_names.extend([f\"coord_{i}\" for i in range(coords.shape[1])])\n\n        # Proper motions if available in spatial data\n        if \"pmra\" in spatial_data:\n            pmra = spatial_data[\"pmra\"]\n            if pmra.dim() == 1:\n                pmra = pmra.unsqueeze(1)\n            features.append(pmra)\n            feature_names.append(\"pmra\")\n\n        if \"pmdec\" in spatial_data:\n            pmdec = spatial_data[\"pmdec\"]\n            if pmdec.dim() == 1:\n                pmdec = pmdec.unsqueeze(1)\n            features.append(pmdec)\n            feature_names.append(\"pmdec\")\n\n        # Parallax if available\n        if \"parallax\" in spatial_data:\n            plx = spatial_data[\"parallax\"]\n            if plx.dim() == 1:\n                plx = plx.unsqueeze(1)\n            features.append(plx)\n            feature_names.append(\"parallax\")\n\n    # Photometric features\n    if self.config.use_photometry and \"photometric\" in survey_tensor:\n        phot_data = survey_tensor[\"photometric\"]\n\n        if \"magnitudes\" in phot_data:\n            mags = phot_data[\"magnitudes\"]\n            if mags.dim() == 1:\n                mags = mags.unsqueeze(1)\n            features.append(mags)\n\n            # Add band names if available\n            if hasattr(phot_data, \"bands\"):\n                feature_names.extend([f\"mag_{band}\" for band in phot_data.bands])\n            else:\n                feature_names.extend([f\"mag_{i}\" for i in range(mags.shape[1])])\n\n        # Colors if available\n        if \"colors\" in phot_data:\n            colors = phot_data[\"colors\"]\n            if colors.dim() == 1:\n                colors = colors.unsqueeze(1)\n            features.append(colors)\n            feature_names.extend([f\"color_{i}\" for i in range(colors.shape[1])])\n\n    # Spectral features\n    if self.config.use_spectroscopy and \"spectral\" in survey_tensor:\n        spec_data = survey_tensor[\"spectral\"]\n\n        if \"features\" in spec_data:\n            spec_features = spec_data[\"features\"]\n            if spec_features.dim() == 1:\n                spec_features = spec_features.unsqueeze(1)\n            features.append(spec_features)\n            feature_names.extend([f\"spec_{i}\" for i in range(spec_features.shape[1])])\n\n    # Additional features if available\n    if \"features\" in survey_tensor:\n        extra_features = survey_tensor[\"features\"]\n        if extra_features.dim() == 1:\n            extra_features = extra_features.unsqueeze(1)\n        features.append(extra_features)\n        feature_names.extend([f\"feat_{i}\" for i in range(extra_features.shape[1])])\n\n    if not features:\n        # Minimal fallback: use coordinates only\n        self.logger.warning(\"No features found, using coordinates as features\")\n        coords = self.extract_coordinates(survey_tensor)\n        features = [coords]\n        feature_names = [f\"coord_{i}\" for i in range(coords.shape[1])]\n\n    # Concatenate all features\n    result = torch.cat(features, dim=-1).to(self.device)\n\n    # Handle invalid values\n    result = self._handle_invalid_values(result, \"features\")\n\n    # Outlier detection\n    if self.config.outlier_detection:\n        result, outlier_mask = self._detect_outliers(result)\n        self.logger.info(f\"Detected {outlier_mask.sum()} outliers\")\n\n    # Normalize features\n    if self.config.normalize_features:\n        result = self._normalize_features(result)\n\n    # Store feature names for interpretability\n    self._feature_names = feature_names\n\n    self.logger.info(\n        f\"Extracted {result.shape[1]} features from {result.shape[0]} objects\"\n    )\n\n    return result\n</code></pre>"},{"location":"api/astro_lab.data.graphs/#astro_lab.data.graphs.BaseGraphBuilder.validate_input","title":"validate_input","text":"<pre><code>validate_input(survey_tensor: SurveyTensorDict) -&gt; None\n</code></pre> <p>Enhanced input validation.</p> Source code in <code>src\\astro_lab\\data\\graphs\\base.py</code> <pre><code>def validate_input(self, survey_tensor: SurveyTensorDict) -&gt; None:\n    \"\"\"Enhanced input validation.\"\"\"\n    if not isinstance(survey_tensor, SurveyTensorDict):\n        raise TypeError(\n            f\"Input must be SurveyTensorDict, got {type(survey_tensor)}\"\n        )\n\n    # Check for required spatial data\n    if \"spatial\" not in survey_tensor:\n        raise ValueError(\"SurveyTensorDict must contain 'spatial' data\")\n\n    # Validate spatial data has coordinates\n    spatial_data = survey_tensor[\"spatial\"]\n    if \"coordinates\" not in spatial_data:\n        raise ValueError(\"Spatial data must have 'coordinates' key\")\n\n    # Check coordinate dimensions\n    coords = spatial_data[\"coordinates\"]\n    if coords.dim() &lt; 2:\n        raise ValueError(\n            f\"Coordinates must be at least 2D tensor, got shape {coords.shape}\"\n        )\n\n    # Validate k_neighbors parameter\n    n_objects = coords.shape[0]\n    if self.config.k_neighbors &gt;= n_objects:\n        self.logger.warning(\n            f\"k_neighbors ({self.config.k_neighbors}) &gt;= n_objects ({n_objects}). \"\n            f\"Adjusting to {n_objects - 1}\"\n        )\n        self.config.k_neighbors = min(self.config.k_neighbors, n_objects - 1)\n</code></pre>"},{"location":"api/astro_lab.data.graphs/#astro_lab.data.graphs.DynamicGraphBuilder","title":"DynamicGraphBuilder","text":"<p>               Bases: <code>BaseGraphBuilder</code></p> <p>Dynamic graph builder that adapts structure during training.</p> <p>Methods:</p> Name Description <code>build</code> <p>Build initial graph structure.</p> <code>refine_graph</code> <p>Refine graph structure based on learned embeddings.</p> Source code in <code>src\\astro_lab\\data\\graphs\\advanced.py</code> <pre><code>class DynamicGraphBuilder(BaseGraphBuilder):\n    \"\"\"Dynamic graph builder that adapts structure during training.\"\"\"\n\n    def __init__(self, initial_k: int = 16, **kwargs):\n        config = GraphConfig(method=\"dynamic\", k_neighbors=initial_k, **kwargs)\n        super().__init__(config)\n        self.edge_weights = None\n        self.learned_edges = None\n\n    def build(self, survey_tensor: SurveyTensorDict) -&gt; Data:\n        \"\"\"Build initial graph structure.\"\"\"\n        self.validate_input(survey_tensor)\n\n        coords = self.extract_coordinates(survey_tensor)\n        features = self.extract_features(survey_tensor)\n\n        # Build initial KNN graph\n        edge_index = knn_graph(\n            coords,\n            k=self.config.k_neighbors,\n            batch=None,\n            loop=self.config.self_loops,\n        )\n\n        # Initialize learnable edge weights\n        n_edges = edge_index.shape[1]\n        edge_weight = torch.ones(n_edges, device=self.device)\n\n        # Create data object\n        data = self.create_data_object(\n            features, edge_index, coords,\n            edge_weight=edge_weight\n        )\n\n        data.graph_type = \"dynamic\"\n        data.supports_edge_learning = True\n\n        return data\n\n    def refine_graph(\n        self, data: Data, node_embeddings: torch.Tensor, temperature: float = 1.0\n    ) -&gt; Data:\n        \"\"\"Refine graph structure based on learned embeddings.\"\"\"\n        # Compute similarity in embedding space\n        embeddings_norm = F.normalize(node_embeddings, p=2, dim=1)\n        similarity = torch.mm(embeddings_norm, embeddings_norm.t())\n\n        # Get current edges\n        edge_index = data.edge_index\n        n_nodes = data.num_nodes\n\n        # Compute edge scores based on similarity\n        edge_scores = similarity[edge_index[0], edge_index[1]]\n\n        # Apply temperature scaling\n        edge_probs = torch.sigmoid(edge_scores / temperature)\n\n        # Prune low-probability edges\n        keep_mask = edge_probs &gt; 0.5\n        refined_edge_index = edge_index[:, keep_mask]\n        refined_edge_weight = edge_probs[keep_mask]\n\n        # Add high-similarity edges not in original graph\n        similarity_threshold = 0.8\n        high_sim_mask = similarity &gt; similarity_threshold\n\n        # Remove self-connections and existing edges\n        high_sim_mask.fill_diagonal_(False)\n        for i in range(edge_index.shape[1]):\n            high_sim_mask[edge_index[0, i], edge_index[1, i]] = False\n\n        # Add new edges\n        new_edges = high_sim_mask.nonzero().t()\n        if new_edges.shape[1] &gt; 0:\n            # Limit number of new edges\n            max_new_edges = n_nodes * 2\n            if new_edges.shape[1] &gt; max_new_edges:\n                perm = torch.randperm(new_edges.shape[1])[:max_new_edges]\n                new_edges = new_edges[:, perm]\n\n            # Combine with refined edges\n            refined_edge_index = torch.cat([refined_edge_index, new_edges], dim=1)\n            new_weights = similarity[new_edges[0], new_edges[1]]\n            refined_edge_weight = torch.cat([refined_edge_weight, new_weights])\n\n        # Update data\n        data.edge_index = refined_edge_index\n        data.edge_weight = refined_edge_weight\n\n        return data\n</code></pre>"},{"location":"api/astro_lab.data.graphs/#astro_lab.data.graphs.DynamicGraphBuilder.build","title":"build","text":"<pre><code>build(survey_tensor: SurveyTensorDict) -&gt; Data\n</code></pre> <p>Build initial graph structure.</p> Source code in <code>src\\astro_lab\\data\\graphs\\advanced.py</code> <pre><code>def build(self, survey_tensor: SurveyTensorDict) -&gt; Data:\n    \"\"\"Build initial graph structure.\"\"\"\n    self.validate_input(survey_tensor)\n\n    coords = self.extract_coordinates(survey_tensor)\n    features = self.extract_features(survey_tensor)\n\n    # Build initial KNN graph\n    edge_index = knn_graph(\n        coords,\n        k=self.config.k_neighbors,\n        batch=None,\n        loop=self.config.self_loops,\n    )\n\n    # Initialize learnable edge weights\n    n_edges = edge_index.shape[1]\n    edge_weight = torch.ones(n_edges, device=self.device)\n\n    # Create data object\n    data = self.create_data_object(\n        features, edge_index, coords,\n        edge_weight=edge_weight\n    )\n\n    data.graph_type = \"dynamic\"\n    data.supports_edge_learning = True\n\n    return data\n</code></pre>"},{"location":"api/astro_lab.data.graphs/#astro_lab.data.graphs.DynamicGraphBuilder.refine_graph","title":"refine_graph","text":"<pre><code>refine_graph(data: Data, node_embeddings: Tensor, temperature: float = 1.0) -&gt; Data\n</code></pre> <p>Refine graph structure based on learned embeddings.</p> Source code in <code>src\\astro_lab\\data\\graphs\\advanced.py</code> <pre><code>def refine_graph(\n    self, data: Data, node_embeddings: torch.Tensor, temperature: float = 1.0\n) -&gt; Data:\n    \"\"\"Refine graph structure based on learned embeddings.\"\"\"\n    # Compute similarity in embedding space\n    embeddings_norm = F.normalize(node_embeddings, p=2, dim=1)\n    similarity = torch.mm(embeddings_norm, embeddings_norm.t())\n\n    # Get current edges\n    edge_index = data.edge_index\n    n_nodes = data.num_nodes\n\n    # Compute edge scores based on similarity\n    edge_scores = similarity[edge_index[0], edge_index[1]]\n\n    # Apply temperature scaling\n    edge_probs = torch.sigmoid(edge_scores / temperature)\n\n    # Prune low-probability edges\n    keep_mask = edge_probs &gt; 0.5\n    refined_edge_index = edge_index[:, keep_mask]\n    refined_edge_weight = edge_probs[keep_mask]\n\n    # Add high-similarity edges not in original graph\n    similarity_threshold = 0.8\n    high_sim_mask = similarity &gt; similarity_threshold\n\n    # Remove self-connections and existing edges\n    high_sim_mask.fill_diagonal_(False)\n    for i in range(edge_index.shape[1]):\n        high_sim_mask[edge_index[0, i], edge_index[1, i]] = False\n\n    # Add new edges\n    new_edges = high_sim_mask.nonzero().t()\n    if new_edges.shape[1] &gt; 0:\n        # Limit number of new edges\n        max_new_edges = n_nodes * 2\n        if new_edges.shape[1] &gt; max_new_edges:\n            perm = torch.randperm(new_edges.shape[1])[:max_new_edges]\n            new_edges = new_edges[:, perm]\n\n        # Combine with refined edges\n        refined_edge_index = torch.cat([refined_edge_index, new_edges], dim=1)\n        new_weights = similarity[new_edges[0], new_edges[1]]\n        refined_edge_weight = torch.cat([refined_edge_weight, new_weights])\n\n    # Update data\n    data.edge_index = refined_edge_index\n    data.edge_weight = refined_edge_weight\n\n    return data\n</code></pre>"},{"location":"api/astro_lab.data.graphs/#astro_lab.data.graphs.GeometricPriorGraphBuilder","title":"GeometricPriorGraphBuilder","text":"<p>               Bases: <code>BaseGraphBuilder</code></p> <p>Build graphs using astronomical geometric priors.</p> <p>Methods:</p> Name Description <code>build</code> <p>Build graph with geometric priors.</p> Source code in <code>src\\astro_lab\\data\\graphs\\advanced.py</code> <pre><code>class GeometricPriorGraphBuilder(BaseGraphBuilder):\n    \"\"\"Build graphs using astronomical geometric priors.\"\"\"\n\n    def __init__(self, prior_type: str = \"filament\", **kwargs):\n        config = GraphConfig(method=\"geometric_prior\", **kwargs)\n        super().__init__(config)\n        self.prior_type = prior_type\n\n    def build(self, survey_tensor: SurveyTensorDict) -&gt; Data:\n        \"\"\"Build graph with geometric priors.\"\"\"\n        self.validate_input(survey_tensor)\n\n        coords = self.extract_coordinates(survey_tensor)\n        features = self.extract_features(survey_tensor)\n\n        if self.prior_type == \"filament\":\n            edge_index = self._build_filament_graph(coords, features)\n        elif self.prior_type == \"cluster\":\n            edge_index = self._build_cluster_aware_graph(coords, features)\n        elif self.prior_type == \"void\":\n            edge_index = self._build_void_aware_graph(coords, features)\n        else:\n            raise ValueError(f\"Unknown prior type: {self.prior_type}\")\n\n        # Create data object\n        data = self.create_data_object(features, edge_index, coords)\n        data.graph_type = f\"geometric_{self.prior_type}\"\n\n        return data\n\n    def _build_filament_graph(\n        self, coords: torch.Tensor, features: torch.Tensor\n    ) -&gt; torch.Tensor:\n        \"\"\"Build graph emphasizing filamentary structures.\"\"\"\n        # Compute local principal directions\n        k_local = 20\n        edge_index = knn_graph(coords, k=k_local, batch=None)\n\n        # For each node, compute local PCA\n        filament_edges = []\n\n        for i in range(coords.shape[0]):\n            # Get neighbors\n            neighbors_mask = edge_index[0] == i\n            neighbor_indices = edge_index[1, neighbors_mask]\n\n            if len(neighbor_indices) &lt; 3:\n                continue\n\n            # Local coordinates\n            local_coords = coords[neighbor_indices] - coords[i]\n\n            # PCA to find principal direction\n            U, S, V = torch.svd(local_coords.t() @ local_coords)\n            principal_dir = V[:, 0]\n\n            # Connect to neighbors aligned with principal direction\n            alignments = torch.abs((local_coords @ principal_dir))\n            aligned_mask = alignments &gt; alignments.mean()\n\n            aligned_neighbors = neighbor_indices[aligned_mask]\n            for j in aligned_neighbors:\n                filament_edges.append([i, j])\n\n        if filament_edges:\n            edge_index = torch.tensor(filament_edges, device=self.device).t()\n        else:\n            # Fallback to regular KNN\n            edge_index = knn_graph(coords, k=self.config.k_neighbors, batch=None)\n\n        return edge_index\n\n    def _build_cluster_aware_graph(\n        self, coords: torch.Tensor, features: torch.Tensor\n    ) -&gt; torch.Tensor:\n        \"\"\"Build graph aware of cluster structures.\"\"\"\n        # Detect high-density regions\n        k_density = 10\n        dists = torch.cdist(coords, coords)\n        kth_dists, _ = torch.kthvalue(dists, k_density + 1, dim=1)\n\n        density = 1.0 / (kth_dists + 1e-8)\n        density_threshold = density.quantile(0.7)\n\n        high_density_mask = density &gt; density_threshold\n\n        # Different connectivity for high/low density\n        edge_list = []\n\n        # High density: more connections\n        if high_density_mask.any():\n            high_density_coords = coords[high_density_mask]\n            high_density_idx = torch.where(high_density_mask)[0]\n\n            k_high = min(self.config.k_neighbors * 2, len(high_density_coords) - 1)\n            if k_high &gt; 0:\n                edges_high = knn_graph(high_density_coords, k=k_high, batch=None)\n                edges_high_global = high_density_idx[edges_high]\n                edge_list.append(edges_high_global)\n\n        # Low density: fewer connections\n        low_density_mask = ~high_density_mask\n        if low_density_mask.any():\n            low_density_coords = coords[low_density_mask]\n            low_density_idx = torch.where(low_density_mask)[0]\n\n            k_low = min(self.config.k_neighbors // 2, len(low_density_coords) - 1)\n            if k_low &gt; 0:\n                edges_low = knn_graph(low_density_coords, k=k_low, batch=None)\n                edges_low_global = low_density_idx[edges_low]\n                edge_list.append(edges_low_global)\n\n        # Inter-density connections\n        if high_density_mask.any() and low_density_mask.any():\n            inter_edges = self._connect_density_regions(\n                coords, high_density_mask, low_density_mask\n            )\n            if inter_edges.shape[1] &gt; 0:\n                edge_list.append(inter_edges)\n\n        if edge_list:\n            edge_index = torch.cat(edge_list, dim=1)\n        else:\n            edge_index = knn_graph(coords, k=self.config.k_neighbors, batch=None)\n\n        return edge_index\n\n    def _build_void_aware_graph(\n        self, coords: torch.Tensor, features: torch.Tensor\n    ) -&gt; torch.Tensor:\n        \"\"\"Build graph aware of void structures.\"\"\"\n        # Similar to cluster-aware but inverted\n        # Emphasize connections around voids\n        return self._build_cluster_aware_graph(coords, features)\n\n    def _connect_density_regions(\n        self,\n        coords: torch.Tensor,\n        high_mask: torch.Tensor,\n        low_mask: torch.Tensor\n    ) -&gt; torch.Tensor:\n        \"\"\"Connect high and low density regions.\"\"\"\n        high_idx = torch.where(high_mask)[0]\n        low_idx = torch.where(low_mask)[0]\n\n        # Sample connections to avoid too many edges\n        n_connections = min(len(high_idx), len(low_idx), 100)\n\n        if n_connections &gt; 0:\n            high_sample = high_idx[torch.randperm(len(high_idx))[:n_connections]]\n            low_sample = low_idx[torch.randperm(len(low_idx))[:n_connections]]\n\n            edges = torch.stack([\n                torch.cat([high_sample, low_sample]),\n                torch.cat([low_sample, high_sample])\n            ])\n\n            return edges\n        else:\n            return torch.empty((2, 0), dtype=torch.long, device=self.device)\n</code></pre>"},{"location":"api/astro_lab.data.graphs/#astro_lab.data.graphs.GeometricPriorGraphBuilder.build","title":"build","text":"<pre><code>build(survey_tensor: SurveyTensorDict) -&gt; Data\n</code></pre> <p>Build graph with geometric priors.</p> Source code in <code>src\\astro_lab\\data\\graphs\\advanced.py</code> <pre><code>def build(self, survey_tensor: SurveyTensorDict) -&gt; Data:\n    \"\"\"Build graph with geometric priors.\"\"\"\n    self.validate_input(survey_tensor)\n\n    coords = self.extract_coordinates(survey_tensor)\n    features = self.extract_features(survey_tensor)\n\n    if self.prior_type == \"filament\":\n        edge_index = self._build_filament_graph(coords, features)\n    elif self.prior_type == \"cluster\":\n        edge_index = self._build_cluster_aware_graph(coords, features)\n    elif self.prior_type == \"void\":\n        edge_index = self._build_void_aware_graph(coords, features)\n    else:\n        raise ValueError(f\"Unknown prior type: {self.prior_type}\")\n\n    # Create data object\n    data = self.create_data_object(features, edge_index, coords)\n    data.graph_type = f\"geometric_{self.prior_type}\"\n\n    return data\n</code></pre>"},{"location":"api/astro_lab.data.graphs/#astro_lab.data.graphs.GraphConfig","title":"GraphConfig  <code>dataclass</code>","text":"<p>Configuration for graph building with best practices.</p> Source code in <code>src\\astro_lab\\data\\graphs\\base.py</code> <pre><code>@dataclass\nclass GraphConfig:\n    \"\"\"Configuration for graph building with best practices.\"\"\"\n\n    # Graph construction method\n    method: str = \"knn\"  # \"knn\", \"radius\", \"astronomical\", \"adaptive\", \"multiscale\"\n\n    # KNN parameters\n    k_neighbors: int = 16  # Increased default for better connectivity\n    k_min: int = 4  # Minimum neighbors for adaptive methods\n    k_max: int = 32  # Maximum neighbors for adaptive methods\n\n    # Radius parameters\n    radius: float = 1.0\n    radius_min: float = 0.1\n    radius_max: float = 10.0\n\n    # Astronomical parameters\n    use_3d_coordinates: bool = True\n    coordinate_system: str = \"cartesian\"  # \"cartesian\", \"spherical\", \"galactic\"\n    distance_metric: str = \"euclidean\"  # \"euclidean\", \"angular\", \"mahalanobis\"\n\n    # Feature selection\n    use_photometry: bool = True\n    use_astrometry: bool = True\n    use_spectroscopy: bool = False\n    use_temporal: bool = False\n\n    # Feature preprocessing\n    normalize_features: bool = True\n    normalize_method: str = \"standardize\"  # \"standardize\", \"minmax\", \"robust\"\n    handle_nan: str = \"median\"  # \"median\", \"mean\", \"zero\", \"drop\"\n    outlier_detection: bool = True\n    outlier_method: str = \"zscore\"  # \"zscore\", \"iqr\", \"isolation_forest\"\n    outlier_threshold: float = 3.0\n\n    # Graph properties\n    directed: bool = False\n    self_loops: bool = False\n    remove_isolated: bool = True\n    ensure_connected: bool = True\n\n    # Multi-scale graph\n    use_multiscale: bool = False\n    scales: List[int] = field(default_factory=lambda: [8, 16, 32])\n\n    # Heterogeneous graph\n    use_hetero: bool = False\n    node_types: List[str] = field(default_factory=lambda: [\"star\", \"galaxy\"])\n    edge_types: List[Tuple[str, str, str]] = field(\n        default_factory=lambda: [(\"star\", \"near\", \"star\"), (\"galaxy\", \"near\", \"galaxy\")]\n    )\n\n    # Performance\n    device: Optional[Union[str, torch.device]] = None\n    use_gpu_construction: bool = True\n    batch_size: Optional[int] = None  # For batch processing large graphs\n    num_workers: int = 0\n    prefetch_factor: int = 2\n\n    # Caching\n    use_cache: bool = True\n    cache_dir: Optional[str] = None\n\n    def __post_init__(self):\n        \"\"\"Validate configuration.\"\"\"\n        valid_methods = [\"knn\", \"radius\", \"astronomical\", \"adaptive\", \"multiscale\"]\n        if self.method not in valid_methods:\n            raise ValueError(f\"Unknown method: {self.method}. Valid: {valid_methods}\")\n\n        if self.coordinate_system not in [\"cartesian\", \"spherical\", \"galactic\"]:\n            raise ValueError(f\"Unknown coordinate system: {self.coordinate_system}\")\n\n        if self.k_neighbors &lt;= 0 or self.k_neighbors &gt; 1000:\n            raise ValueError(f\"k_neighbors must be in (0, 1000], got {self.k_neighbors}\")\n\n        if self.k_min &gt;= self.k_max:\n            raise ValueError(f\"k_min ({self.k_min}) must be &lt; k_max ({self.k_max})\")\n\n        if self.outlier_threshold &lt;= 0:\n            raise ValueError(f\"outlier_threshold must be positive, got {self.outlier_threshold}\")\n</code></pre>"},{"location":"api/astro_lab.data.graphs/#astro_lab.data.graphs.GraphOfGraphsBuilder","title":"GraphOfGraphsBuilder","text":"<p>               Bases: <code>BaseGraphBuilder</code></p> <p>Build hierarchical graph-of-graphs structure.</p> <p>Methods:</p> Name Description <code>build</code> <p>Build hierarchical graph structure.</p> Source code in <code>src\\astro_lab\\data\\graphs\\advanced.py</code> <pre><code>class GraphOfGraphsBuilder(BaseGraphBuilder):\n    \"\"\"Build hierarchical graph-of-graphs structure.\"\"\"\n\n    def __init__(\n        self,\n        cluster_method: str = \"kmeans\",\n        n_clusters: int = 10,\n        **kwargs\n    ):\n        config = GraphConfig(method=\"graph_of_graphs\", **kwargs)\n        super().__init__(config)\n        self.cluster_method = cluster_method\n        self.n_clusters = n_clusters\n\n    def build(self, survey_tensor: SurveyTensorDict) -&gt; Data:\n        \"\"\"Build hierarchical graph structure.\"\"\"\n        self.validate_input(survey_tensor)\n\n        coords = self.extract_coordinates(survey_tensor)\n        features = self.extract_features(survey_tensor)\n\n        # Cluster objects\n        cluster_labels = self._cluster_objects(coords, features)\n\n        # Build subgraphs for each cluster\n        subgraphs = []\n        cluster_features = []\n\n        for cluster_id in range(self.n_clusters):\n            mask = cluster_labels == cluster_id\n            if mask.sum() == 0:\n                continue\n\n            # Extract cluster data\n            cluster_coords = coords[mask]\n            cluster_feats = features[mask]\n\n            # Build subgraph\n            if mask.sum() &gt; 1:\n                subgraph_edges = knn_graph(\n                    cluster_coords,\n                    k=min(self.config.k_neighbors, mask.sum() - 1),\n                    batch=None,\n                )\n            else:\n                subgraph_edges = torch.empty((2, 0), dtype=torch.long)\n\n            subgraphs.append({\n                'node_indices': torch.where(mask)[0],\n                'edge_index': subgraph_edges,\n                'features': cluster_feats,\n                'coords': cluster_coords,\n            })\n\n            # Compute cluster-level features\n            cluster_feat = self._compute_cluster_features(cluster_feats, cluster_coords)\n            cluster_features.append(cluster_feat)\n\n        # Build inter-cluster graph\n        cluster_features = torch.stack(cluster_features)\n        cluster_graph = knn_graph(\n            cluster_features,\n            k=min(5, len(cluster_features) - 1),\n            batch=None,\n        )\n\n        # Combine all edges\n        all_edges = []\n        node_to_cluster = torch.zeros(coords.shape[0], dtype=torch.long)\n\n        for i, subgraph in enumerate(subgraphs):\n            # Map local to global indices\n            global_indices = subgraph['node_indices']\n            local_edges = subgraph['edge_index']\n\n            if local_edges.shape[1] &gt; 0:\n                global_edges = global_indices[local_edges]\n                all_edges.append(global_edges)\n\n            # Store cluster assignment\n            node_to_cluster[global_indices] = i\n\n        # Add inter-cluster edges\n        inter_edges = self._create_inter_cluster_edges(\n            coords, node_to_cluster, cluster_graph\n        )\n        if inter_edges.shape[1] &gt; 0:\n            all_edges.append(inter_edges)\n\n        # Combine all edges\n        if all_edges:\n            edge_index = torch.cat(all_edges, dim=1)\n        else:\n            edge_index = torch.empty((2, 0), dtype=torch.long)\n\n        # Create data object\n        data = self.create_data_object(features, edge_index, coords)\n\n        # Add hierarchical information\n        data.cluster_labels = cluster_labels\n        data.n_clusters = self.n_clusters\n        data.cluster_features = cluster_features\n        data.cluster_graph = cluster_graph\n        data.graph_type = \"hierarchical\"\n\n        return data\n\n    def _cluster_objects(\n        self, coords: torch.Tensor, features: torch.Tensor\n    ) -&gt; torch.Tensor:\n        \"\"\"Cluster objects using specified method.\"\"\"\n        # Combine spatial and feature information\n        combined = torch.cat([\n            coords / coords.std(dim=0, keepdim=True),\n            features / features.std(dim=0, keepdim=True)\n        ], dim=1).cpu().numpy()\n\n        if self.cluster_method == \"kmeans\":\n            kmeans = KMeans(n_clusters=self.n_clusters, random_state=42)\n            labels = kmeans.fit_predict(combined)\n        elif self.cluster_method == \"dbscan\":\n            dbscan = DBSCAN(eps=0.5, min_samples=5)\n            labels = dbscan.fit_predict(combined)\n            # Handle noise points\n            labels[labels == -1] = labels.max() + 1\n            self.n_clusters = len(np.unique(labels))\n        else:\n            raise ValueError(f\"Unknown cluster method: {self.cluster_method}\")\n\n        return torch.tensor(labels, device=self.device)\n\n    def _compute_cluster_features(\n        self, features: torch.Tensor, coords: torch.Tensor\n    ) -&gt; torch.Tensor:\n        \"\"\"Compute aggregated features for a cluster.\"\"\"\n        # Statistical features\n        feat_mean = features.mean(dim=0)\n        feat_std = features.std(dim=0)\n        feat_max = features.max(dim=0)[0]\n        feat_min = features.min(dim=0)[0]\n\n        # Spatial features\n        coord_mean = coords.mean(dim=0)\n        coord_std = coords.std(dim=0)\n\n        # Size feature\n        size = torch.tensor([features.shape[0]], device=features.device)\n\n        # Combine all\n        cluster_feat = torch.cat([\n            feat_mean, feat_std, feat_max, feat_min,\n            coord_mean, coord_std, size\n        ])\n\n        return cluster_feat\n\n    def _create_inter_cluster_edges(\n        self,\n        coords: torch.Tensor,\n        node_to_cluster: torch.Tensor,\n        cluster_graph: torch.Tensor\n    ) -&gt; torch.Tensor:\n        \"\"\"Create edges between clusters.\"\"\"\n        inter_edges = []\n\n        # For each edge in cluster graph\n        for i in range(cluster_graph.shape[1]):\n            c1, c2 = cluster_graph[0, i], cluster_graph[1, i]\n\n            # Find boundary nodes (closest pairs between clusters)\n            mask1 = node_to_cluster == c1\n            mask2 = node_to_cluster == c2\n\n            if mask1.sum() &gt; 0 and mask2.sum() &gt; 0:\n                coords1 = coords[mask1]\n                coords2 = coords[mask2]\n                idx1 = torch.where(mask1)[0]\n                idx2 = torch.where(mask2)[0]\n\n                # Compute pairwise distances\n                dists = torch.cdist(coords1, coords2)\n\n                # Connect k closest pairs\n                k_connect = min(3, dists.shape[0], dists.shape[1])\n                for _ in range(k_connect):\n                    min_idx = dists.argmin()\n                    i1 = min_idx // dists.shape[1]\n                    i2 = min_idx % dists.shape[1]\n\n                    inter_edges.append([idx1[i1], idx2[i2]])\n                    inter_edges.append([idx2[i2], idx1[i1]])  # Bidirectional\n\n                    # Set distance to inf to avoid reselection\n                    dists[i1, i2] = float('inf')\n\n        if inter_edges:\n            return torch.tensor(inter_edges, device=self.device).t()\n        else:\n            return torch.empty((2, 0), dtype=torch.long, device=self.device)\n</code></pre>"},{"location":"api/astro_lab.data.graphs/#astro_lab.data.graphs.GraphOfGraphsBuilder.build","title":"build","text":"<pre><code>build(survey_tensor: SurveyTensorDict) -&gt; Data\n</code></pre> <p>Build hierarchical graph structure.</p> Source code in <code>src\\astro_lab\\data\\graphs\\advanced.py</code> <pre><code>def build(self, survey_tensor: SurveyTensorDict) -&gt; Data:\n    \"\"\"Build hierarchical graph structure.\"\"\"\n    self.validate_input(survey_tensor)\n\n    coords = self.extract_coordinates(survey_tensor)\n    features = self.extract_features(survey_tensor)\n\n    # Cluster objects\n    cluster_labels = self._cluster_objects(coords, features)\n\n    # Build subgraphs for each cluster\n    subgraphs = []\n    cluster_features = []\n\n    for cluster_id in range(self.n_clusters):\n        mask = cluster_labels == cluster_id\n        if mask.sum() == 0:\n            continue\n\n        # Extract cluster data\n        cluster_coords = coords[mask]\n        cluster_feats = features[mask]\n\n        # Build subgraph\n        if mask.sum() &gt; 1:\n            subgraph_edges = knn_graph(\n                cluster_coords,\n                k=min(self.config.k_neighbors, mask.sum() - 1),\n                batch=None,\n            )\n        else:\n            subgraph_edges = torch.empty((2, 0), dtype=torch.long)\n\n        subgraphs.append({\n            'node_indices': torch.where(mask)[0],\n            'edge_index': subgraph_edges,\n            'features': cluster_feats,\n            'coords': cluster_coords,\n        })\n\n        # Compute cluster-level features\n        cluster_feat = self._compute_cluster_features(cluster_feats, cluster_coords)\n        cluster_features.append(cluster_feat)\n\n    # Build inter-cluster graph\n    cluster_features = torch.stack(cluster_features)\n    cluster_graph = knn_graph(\n        cluster_features,\n        k=min(5, len(cluster_features) - 1),\n        batch=None,\n    )\n\n    # Combine all edges\n    all_edges = []\n    node_to_cluster = torch.zeros(coords.shape[0], dtype=torch.long)\n\n    for i, subgraph in enumerate(subgraphs):\n        # Map local to global indices\n        global_indices = subgraph['node_indices']\n        local_edges = subgraph['edge_index']\n\n        if local_edges.shape[1] &gt; 0:\n            global_edges = global_indices[local_edges]\n            all_edges.append(global_edges)\n\n        # Store cluster assignment\n        node_to_cluster[global_indices] = i\n\n    # Add inter-cluster edges\n    inter_edges = self._create_inter_cluster_edges(\n        coords, node_to_cluster, cluster_graph\n    )\n    if inter_edges.shape[1] &gt; 0:\n        all_edges.append(inter_edges)\n\n    # Combine all edges\n    if all_edges:\n        edge_index = torch.cat(all_edges, dim=1)\n    else:\n        edge_index = torch.empty((2, 0), dtype=torch.long)\n\n    # Create data object\n    data = self.create_data_object(features, edge_index, coords)\n\n    # Add hierarchical information\n    data.cluster_labels = cluster_labels\n    data.n_clusters = self.n_clusters\n    data.cluster_features = cluster_features\n    data.cluster_graph = cluster_graph\n    data.graph_type = \"hierarchical\"\n\n    return data\n</code></pre>"},{"location":"api/astro_lab.data.graphs/#astro_lab.data.graphs.HeterogeneousGraphBuilder","title":"HeterogeneousGraphBuilder","text":"<p>               Bases: <code>BaseGraphBuilder</code></p> <p>Builder for heterogeneous graphs with multiple node and edge types.</p> <p>Methods:</p> Name Description <code>build</code> <p>Build heterogeneous graph.</p> Source code in <code>src\\astro_lab\\data\\graphs\\builders.py</code> <pre><code>class HeterogeneousGraphBuilder(BaseGraphBuilder):\n    \"\"\"Builder for heterogeneous graphs with multiple node and edge types.\"\"\"\n\n    def __init__(self, **kwargs):\n        config = GraphConfig(method=\"heterogeneous\", use_hetero=True, **kwargs)\n        super().__init__(config)\n\n    def build(self, survey_tensor: SurveyTensorDict) -&gt; HeteroData:\n        \"\"\"Build heterogeneous graph.\"\"\"\n        self.validate_input(survey_tensor)\n\n        # Extract coordinates and features\n        coords = self.extract_coordinates(survey_tensor)\n        features = self.extract_features(survey_tensor)\n\n        # Classify objects into types\n        node_types, node_masks = self._classify_objects(survey_tensor, features)\n\n        # Split features by node type\n        node_features = {}\n        node_coords = {}\n\n        for node_type, mask in node_masks.items():\n            if mask.any():\n                node_features[node_type] = features[mask]\n                node_coords[node_type] = coords[mask]\n\n        # Build edges between different node types\n        edge_indices = self._build_hetero_edges(node_masks, coords)\n\n        # Create heterogeneous data object\n        data = self.create_hetero_data_object(\n            node_features, edge_indices, node_coords\n        )\n\n        # Add metadata\n        data.graph_type = \"heterogeneous\"\n        data.node_types = list(node_types)\n\n        return data\n\n    def _classify_objects(\n        self, survey_tensor: SurveyTensorDict, features: torch.Tensor\n    ) -&gt; Tuple[List[str], Dict[str, torch.Tensor]]:\n        \"\"\"Classify objects into different types.\"\"\"\n        n_objects = features.shape[0]\n\n        # Simple classification based on survey type\n        # TODO: Implement proper classification based on features\n\n        if hasattr(survey_tensor, \"survey_name\"):\n            if survey_tensor.survey_name == \"gaia\":\n                # Classify stars vs other objects\n                # Placeholder: random classification\n                star_mask = torch.rand(n_objects) &gt; 0.2\n\n                node_types = [\"star\", \"other\"]\n                node_masks = {\n                    \"star\": star_mask,\n                    \"other\": ~star_mask\n                }\n\n            elif survey_tensor.survey_name in [\"sdss\", \"nsa\"]:\n                # Classify galaxies by type\n                # Placeholder: random classification\n                elliptical_mask = torch.rand(n_objects) &lt; 0.3\n                spiral_mask = torch.rand(n_objects) &lt; 0.5\n                spiral_mask &amp;= ~elliptical_mask\n                irregular_mask = ~(elliptical_mask | spiral_mask)\n\n                node_types = [\"elliptical\", \"spiral\", \"irregular\"]\n                node_masks = {\n                    \"elliptical\": elliptical_mask,\n                    \"spiral\": spiral_mask,\n                    \"irregular\": irregular_mask\n                }\n\n            else:\n                # Default: single type\n                node_types = [\"object\"]\n                node_masks = {\"object\": torch.ones(n_objects, dtype=torch.bool)}\n\n        else:\n            # Default: single type\n            node_types = [\"object\"]\n            node_masks = {\"object\": torch.ones(n_objects, dtype=torch.bool)}\n\n        return node_types, node_masks\n\n    def _build_hetero_edges(\n        self, node_masks: Dict[str, torch.Tensor], coords: torch.Tensor\n    ) -&gt; Dict[Tuple[str, str, str], torch.Tensor]:\n        \"\"\"Build edges between different node types.\"\"\"\n        edge_indices = {}\n\n        # Build edges within each node type\n        for node_type, mask in node_masks.items():\n            if mask.sum() &gt; 1:  # Need at least 2 nodes\n                type_coords = coords[mask]\n                type_indices = torch.where(mask)[0]\n\n                # KNN within type\n                edge_index_local = knn_graph(\n                    type_coords,\n                    k=min(self.config.k_neighbors, len(type_coords) - 1),\n                    batch=None,\n                    loop=self.config.self_loops,\n                )\n\n                # Map back to global indices\n                edge_index_global = type_indices[edge_index_local]\n\n                edge_type = (node_type, \"similar\", node_type)\n                edge_indices[edge_type] = edge_index_global\n\n        # Build edges between different node types\n        # TODO: Implement cross-type connections based on spatial proximity\n\n        return edge_indices\n</code></pre>"},{"location":"api/astro_lab.data.graphs/#astro_lab.data.graphs.HeterogeneousGraphBuilder.build","title":"build","text":"<pre><code>build(survey_tensor: SurveyTensorDict) -&gt; HeteroData\n</code></pre> <p>Build heterogeneous graph.</p> Source code in <code>src\\astro_lab\\data\\graphs\\builders.py</code> <pre><code>def build(self, survey_tensor: SurveyTensorDict) -&gt; HeteroData:\n    \"\"\"Build heterogeneous graph.\"\"\"\n    self.validate_input(survey_tensor)\n\n    # Extract coordinates and features\n    coords = self.extract_coordinates(survey_tensor)\n    features = self.extract_features(survey_tensor)\n\n    # Classify objects into types\n    node_types, node_masks = self._classify_objects(survey_tensor, features)\n\n    # Split features by node type\n    node_features = {}\n    node_coords = {}\n\n    for node_type, mask in node_masks.items():\n        if mask.any():\n            node_features[node_type] = features[mask]\n            node_coords[node_type] = coords[mask]\n\n    # Build edges between different node types\n    edge_indices = self._build_hetero_edges(node_masks, coords)\n\n    # Create heterogeneous data object\n    data = self.create_hetero_data_object(\n        node_features, edge_indices, node_coords\n    )\n\n    # Add metadata\n    data.graph_type = \"heterogeneous\"\n    data.node_types = list(node_types)\n\n    return data\n</code></pre>"},{"location":"api/astro_lab.data.graphs/#astro_lab.data.graphs.KNNGraphBuilder","title":"KNNGraphBuilder","text":"<p>               Bases: <code>BaseGraphBuilder</code></p> <p>K-Nearest Neighbors graph builder with optimizations.</p> <p>Methods:</p> Name Description <code>build</code> <p>Build optimized KNN graph.</p> Source code in <code>src\\astro_lab\\data\\graphs\\builders.py</code> <pre><code>class KNNGraphBuilder(BaseGraphBuilder):\n    \"\"\"K-Nearest Neighbors graph builder with optimizations.\"\"\"\n\n    def __init__(self, k_neighbors: int = 16, **kwargs):\n        config = GraphConfig(method=\"knn\", k_neighbors=k_neighbors, **kwargs)\n        super().__init__(config)\n\n    def build(self, survey_tensor: SurveyTensorDict) -&gt; Data:\n        \"\"\"Build optimized KNN graph.\"\"\"\n        self.validate_input(survey_tensor)\n\n        # Extract coordinates and features\n        coords = self.extract_coordinates(survey_tensor)\n        features = self.extract_features(survey_tensor)\n\n        # Build graph based on device and size\n        if coords.shape[0] &gt; 100000 and not self.config.use_gpu_construction:\n            # Use CPU-optimized method for large graphs\n            edge_index = self._build_knn_cpu_optimized(coords)\n        else:\n            # Use PyG's GPU-accelerated method\n            edge_index = knn_graph(\n                coords,\n                k=self.config.k_neighbors,\n                batch=None,\n                loop=self.config.self_loops,\n                flow=\"source_to_target\",\n                cosine=False,\n                num_workers=self.config.num_workers,\n            )\n\n        # Make undirected if requested\n        if not self.config.directed:\n            edge_index = to_undirected(edge_index, num_nodes=coords.shape[0])\n\n        # Create PyG Data object\n        data = self.create_data_object(features, edge_index, coords)\n\n        # Add metadata\n        data.graph_type = \"knn\"\n        data.k_neighbors = self.config.k_neighbors\n\n        return data\n\n    def _build_knn_cpu_optimized(self, coords: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"CPU-optimized KNN using sklearn for large graphs.\"\"\"\n        coords_numpy = coords.cpu().numpy()\n\n        # Use sklearn's optimized implementation\n        nbrs = NearestNeighbors(\n            n_neighbors=self.config.k_neighbors + 1,  # +1 for self\n            algorithm='auto',  # Automatically choose best algorithm\n            metric='euclidean',\n            n_jobs=self.config.num_workers if self.config.num_workers &gt; 0 else -1\n        )\n\n        nbrs.fit(coords_numpy)\n        distances, indices = nbrs.kneighbors(coords_numpy)\n\n        # Remove self-connections if not needed\n        if not self.config.self_loops:\n            indices = indices[:, 1:]  # Skip first neighbor (self)\n\n        # Convert to edge_index format\n        n_nodes = coords.shape[0]\n        k = indices.shape[1]\n\n        source_nodes = torch.arange(n_nodes).unsqueeze(1).expand(-1, k).flatten()\n        target_nodes = torch.from_numpy(indices).flatten()\n\n        edge_index = torch.stack([source_nodes, target_nodes], dim=0)\n\n        return edge_index.to(self.device)\n</code></pre>"},{"location":"api/astro_lab.data.graphs/#astro_lab.data.graphs.KNNGraphBuilder.build","title":"build","text":"<pre><code>build(survey_tensor: SurveyTensorDict) -&gt; Data\n</code></pre> <p>Build optimized KNN graph.</p> Source code in <code>src\\astro_lab\\data\\graphs\\builders.py</code> <pre><code>def build(self, survey_tensor: SurveyTensorDict) -&gt; Data:\n    \"\"\"Build optimized KNN graph.\"\"\"\n    self.validate_input(survey_tensor)\n\n    # Extract coordinates and features\n    coords = self.extract_coordinates(survey_tensor)\n    features = self.extract_features(survey_tensor)\n\n    # Build graph based on device and size\n    if coords.shape[0] &gt; 100000 and not self.config.use_gpu_construction:\n        # Use CPU-optimized method for large graphs\n        edge_index = self._build_knn_cpu_optimized(coords)\n    else:\n        # Use PyG's GPU-accelerated method\n        edge_index = knn_graph(\n            coords,\n            k=self.config.k_neighbors,\n            batch=None,\n            loop=self.config.self_loops,\n            flow=\"source_to_target\",\n            cosine=False,\n            num_workers=self.config.num_workers,\n        )\n\n    # Make undirected if requested\n    if not self.config.directed:\n        edge_index = to_undirected(edge_index, num_nodes=coords.shape[0])\n\n    # Create PyG Data object\n    data = self.create_data_object(features, edge_index, coords)\n\n    # Add metadata\n    data.graph_type = \"knn\"\n    data.k_neighbors = self.config.k_neighbors\n\n    return data\n</code></pre>"},{"location":"api/astro_lab.data.graphs/#astro_lab.data.graphs.MultiScaleGraphBuilder","title":"MultiScaleGraphBuilder","text":"<p>               Bases: <code>BaseGraphBuilder</code></p> <p>Multi-scale graph builder for hierarchical representations.</p> <p>Methods:</p> Name Description <code>build</code> <p>Build multi-scale graph.</p> Source code in <code>src\\astro_lab\\data\\graphs\\builders.py</code> <pre><code>class MultiScaleGraphBuilder(BaseGraphBuilder):\n    \"\"\"Multi-scale graph builder for hierarchical representations.\"\"\"\n\n    def __init__(self, scales: List[int] = None, **kwargs):\n        scales = scales or [8, 16, 32]\n        config = GraphConfig(\n            method=\"multiscale\",\n            scales=scales,\n            use_multiscale=True,\n            **kwargs\n        )\n        super().__init__(config)\n\n    def build(self, survey_tensor: SurveyTensorDict) -&gt; Data:\n        \"\"\"Build multi-scale graph.\"\"\"\n        self.validate_input(survey_tensor)\n\n        # Extract coordinates and features\n        coords = self.extract_coordinates(survey_tensor)\n        features = self.extract_features(survey_tensor)\n\n        # Build graphs at multiple scales\n        all_edges = []\n        edge_attrs = []\n\n        for scale_idx, k in enumerate(self.config.scales):\n            # Build graph at this scale\n            edge_index_scale = knn_graph(\n                coords,\n                k=k,\n                batch=None,\n                loop=False,\n                flow=\"source_to_target\",\n            )\n\n            # Add scale attribute\n            scale_attr = torch.full(\n                (edge_index_scale.shape[1],),\n                scale_idx,\n                dtype=torch.long,\n                device=self.device\n            )\n\n            all_edges.append(edge_index_scale)\n            edge_attrs.append(scale_attr)\n\n        # Combine all scales\n        edge_index = torch.cat(all_edges, dim=1)\n        edge_attr = torch.cat(edge_attrs)\n\n        # Remove duplicate edges, keeping lowest scale\n        edge_index, edge_attr = self._remove_duplicate_edges(edge_index, edge_attr)\n\n        # Make undirected if requested\n        if not self.config.directed:\n            edge_index = to_undirected(edge_index, num_nodes=coords.shape[0])\n            # Duplicate edge attributes for undirected edges\n            edge_attr = torch.cat([edge_attr, edge_attr])\n\n        # Create PyG Data object\n        data = self.create_data_object(\n            features, edge_index, coords,\n            edge_attr=edge_attr\n        )\n\n        # Add metadata\n        data.graph_type = \"multiscale\"\n        data.scales = self.config.scales\n        data.num_scales = len(self.config.scales)\n\n        return data\n\n    def _remove_duplicate_edges(\n        self, edge_index: torch.Tensor, edge_attr: torch.Tensor\n    ) -&gt; Tuple[torch.Tensor, torch.Tensor]:\n        \"\"\"Remove duplicate edges, keeping those with lowest scale.\"\"\"\n        # Create unique edge identifier\n        num_nodes = edge_index.max() + 1\n        edge_ids = edge_index[0] * num_nodes + edge_index[1]\n\n        # Sort by edge_id and scale\n        sort_idx = torch.argsort(edge_ids * len(self.config.scales) + edge_attr)\n        edge_index = edge_index[:, sort_idx]\n        edge_attr = edge_attr[sort_idx]\n        edge_ids = edge_ids[sort_idx]\n\n        # Find unique edges\n        unique_mask = torch.cat([\n            torch.tensor([True], device=edge_ids.device),\n            edge_ids[1:] != edge_ids[:-1]\n        ])\n\n        return edge_index[:, unique_mask], edge_attr[unique_mask]\n</code></pre>"},{"location":"api/astro_lab.data.graphs/#astro_lab.data.graphs.MultiScaleGraphBuilder.build","title":"build","text":"<pre><code>build(survey_tensor: SurveyTensorDict) -&gt; Data\n</code></pre> <p>Build multi-scale graph.</p> Source code in <code>src\\astro_lab\\data\\graphs\\builders.py</code> <pre><code>def build(self, survey_tensor: SurveyTensorDict) -&gt; Data:\n    \"\"\"Build multi-scale graph.\"\"\"\n    self.validate_input(survey_tensor)\n\n    # Extract coordinates and features\n    coords = self.extract_coordinates(survey_tensor)\n    features = self.extract_features(survey_tensor)\n\n    # Build graphs at multiple scales\n    all_edges = []\n    edge_attrs = []\n\n    for scale_idx, k in enumerate(self.config.scales):\n        # Build graph at this scale\n        edge_index_scale = knn_graph(\n            coords,\n            k=k,\n            batch=None,\n            loop=False,\n            flow=\"source_to_target\",\n        )\n\n        # Add scale attribute\n        scale_attr = torch.full(\n            (edge_index_scale.shape[1],),\n            scale_idx,\n            dtype=torch.long,\n            device=self.device\n        )\n\n        all_edges.append(edge_index_scale)\n        edge_attrs.append(scale_attr)\n\n    # Combine all scales\n    edge_index = torch.cat(all_edges, dim=1)\n    edge_attr = torch.cat(edge_attrs)\n\n    # Remove duplicate edges, keeping lowest scale\n    edge_index, edge_attr = self._remove_duplicate_edges(edge_index, edge_attr)\n\n    # Make undirected if requested\n    if not self.config.directed:\n        edge_index = to_undirected(edge_index, num_nodes=coords.shape[0])\n        # Duplicate edge attributes for undirected edges\n        edge_attr = torch.cat([edge_attr, edge_attr])\n\n    # Create PyG Data object\n    data = self.create_data_object(\n        features, edge_index, coords,\n        edge_attr=edge_attr\n    )\n\n    # Add metadata\n    data.graph_type = \"multiscale\"\n    data.scales = self.config.scales\n    data.num_scales = len(self.config.scales)\n\n    return data\n</code></pre>"},{"location":"api/astro_lab.data.graphs/#astro_lab.data.graphs.PointCloudGraphBuilder","title":"PointCloudGraphBuilder","text":"<p>               Bases: <code>BaseGraphBuilder</code></p> <p>Graph builder optimized for 3D astronomical point clouds.</p> <p>Consolidates functionality from: - astro_gnn.surveys.preprocessing.create_survey_pointcloud_graph - Various point cloud processing methods</p> <p>Methods:</p> Name Description <code>build</code> <p>Build optimized point cloud graph.</p> Source code in <code>src\\astro_lab\\data\\graphs\\pointcloud.py</code> <pre><code>class PointCloudGraphBuilder(BaseGraphBuilder):\n    \"\"\"\n    Graph builder optimized for 3D astronomical point clouds.\n\n    Consolidates functionality from:\n    - astro_gnn.surveys.preprocessing.create_survey_pointcloud_graph\n    - Various point cloud processing methods\n    \"\"\"\n\n    def __init__(\n        self, \n        k_neighbors: int = 16,\n        use_gpu_construction: bool = True,\n        normalize_positions: bool = True,\n        add_self_loops: bool = False,\n        **kwargs\n    ):\n        config = GraphConfig(\n            method=\"pointcloud\",\n            k_neighbors=k_neighbors,\n            use_gpu_construction=use_gpu_construction,\n            self_loops=add_self_loops,\n            **kwargs\n        )\n        super().__init__(config)\n        self.normalize_positions = normalize_positions\n\n    def build(self, survey_tensor: SurveyTensorDict) -&gt; Data:\n        \"\"\"Build optimized point cloud graph.\"\"\"\n        self.validate_input(survey_tensor)\n\n        # Extract 3D positions\n        positions = self.extract_coordinates(survey_tensor)\n\n        # Normalize to unit sphere if requested\n        if self.normalize_positions:\n            positions = self._normalize_to_unit_sphere(positions)\n\n        # Extract features\n        features = self.extract_features(survey_tensor)\n\n        # Build spatial graph using positions\n        edge_index = self._build_spatial_graph(positions)\n\n        # Optional: Add feature-based edges\n        if hasattr(self.config, \"use_feature_edges\") and self.config.use_feature_edges:\n            feature_edges = self._build_feature_graph(features)\n            edge_index = torch.cat([edge_index, feature_edges], dim=1)\n            edge_index = torch.unique(edge_index, dim=1)  # Remove duplicates\n\n        # Create data object\n        data = self.create_data_object(features, edge_index, positions)\n\n        # Add point cloud specific metadata\n        data.graph_type = \"pointcloud\"\n        data.normalized_positions = self.normalize_positions\n        data.original_positions = self.extract_coordinates(survey_tensor)  # Keep original\n\n        return data\n\n    def _normalize_to_unit_sphere(self, positions: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"Normalize positions to unit sphere.\"\"\"\n        # Center positions\n        center = positions.mean(dim=0, keepdim=True)\n        positions_centered = positions - center\n\n        # Scale to unit sphere\n        max_radius = torch.norm(positions_centered, dim=1).max()\n        if max_radius &gt; 0:\n            positions_normalized = positions_centered / max_radius\n        else:\n            positions_normalized = positions_centered\n\n        return positions_normalized\n\n    def _build_spatial_graph(self, positions: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"Build graph based on spatial proximity.\"\"\"\n        # Use GPU-accelerated KNN if available\n        if self.config.use_gpu_construction and positions.is_cuda:\n            edge_index = knn_graph(\n                positions,\n                k=self.config.k_neighbors,\n                batch=None,\n                loop=self.config.self_loops,\n                flow=\"source_to_target\",\n                cosine=False,\n                num_workers=1,\n            )\n        else:\n            # CPU fallback for large datasets\n            edge_index = self._build_knn_cpu(positions)\n\n        return edge_index\n\n    def _build_knn_cpu(self, positions: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"CPU-optimized KNN construction.\"\"\"\n        from sklearn.neighbors import NearestNeighbors\n\n        positions_numpy = positions.cpu().numpy()\n\n        nbrs = NearestNeighbors(\n            n_neighbors=self.config.k_neighbors + 1,\n            algorithm='ball_tree',  # Better for 3D data\n            metric='euclidean',\n            n_jobs=-1\n        )\n\n        nbrs.fit(positions_numpy)\n        distances, indices = nbrs.kneighbors(positions_numpy)\n\n        # Remove self-connections if not needed\n        if not self.config.self_loops:\n            indices = indices[:, 1:]\n\n        # Convert to edge_index format\n        n_nodes = positions.shape[0]\n        k = indices.shape[1]\n\n        source_nodes = torch.arange(n_nodes).unsqueeze(1).expand(-1, k).flatten()\n        target_nodes = torch.from_numpy(indices).flatten()\n\n        edge_index = torch.stack([source_nodes, target_nodes], dim=0)\n\n        return edge_index.to(self.device)\n\n    def _build_feature_graph(self, features: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"Build additional edges based on feature similarity.\"\"\"\n        # Normalize features\n        features_norm = torch.nn.functional.normalize(features, p=2, dim=1)\n\n        # Compute feature similarity\n        similarity = torch.mm(features_norm, features_norm.t())\n\n        # Find high similarity pairs (excluding self)\n        similarity.fill_diagonal_(-1)\n        threshold = 0.9  # High similarity threshold\n\n        high_sim_pairs = (similarity &gt; threshold).nonzero().t()\n\n        # Limit number of feature edges\n        max_feature_edges = features.shape[0] * 2\n        if high_sim_pairs.shape[1] &gt; max_feature_edges:\n            perm = torch.randperm(high_sim_pairs.shape[1])[:max_feature_edges]\n            high_sim_pairs = high_sim_pairs[:, perm]\n\n        return high_sim_pairs\n</code></pre>"},{"location":"api/astro_lab.data.graphs/#astro_lab.data.graphs.PointCloudGraphBuilder.build","title":"build","text":"<pre><code>build(survey_tensor: SurveyTensorDict) -&gt; Data\n</code></pre> <p>Build optimized point cloud graph.</p> Source code in <code>src\\astro_lab\\data\\graphs\\pointcloud.py</code> <pre><code>def build(self, survey_tensor: SurveyTensorDict) -&gt; Data:\n    \"\"\"Build optimized point cloud graph.\"\"\"\n    self.validate_input(survey_tensor)\n\n    # Extract 3D positions\n    positions = self.extract_coordinates(survey_tensor)\n\n    # Normalize to unit sphere if requested\n    if self.normalize_positions:\n        positions = self._normalize_to_unit_sphere(positions)\n\n    # Extract features\n    features = self.extract_features(survey_tensor)\n\n    # Build spatial graph using positions\n    edge_index = self._build_spatial_graph(positions)\n\n    # Optional: Add feature-based edges\n    if hasattr(self.config, \"use_feature_edges\") and self.config.use_feature_edges:\n        feature_edges = self._build_feature_graph(features)\n        edge_index = torch.cat([edge_index, feature_edges], dim=1)\n        edge_index = torch.unique(edge_index, dim=1)  # Remove duplicates\n\n    # Create data object\n    data = self.create_data_object(features, edge_index, positions)\n\n    # Add point cloud specific metadata\n    data.graph_type = \"pointcloud\"\n    data.normalized_positions = self.normalize_positions\n    data.original_positions = self.extract_coordinates(survey_tensor)  # Keep original\n\n    return data\n</code></pre>"},{"location":"api/astro_lab.data.graphs/#astro_lab.data.graphs.RadiusGraphBuilder","title":"RadiusGraphBuilder","text":"<p>               Bases: <code>BaseGraphBuilder</code></p> <p>Radius-based graph builder with adaptive radius.</p> <p>Methods:</p> Name Description <code>build</code> <p>Build radius graph with adaptive features.</p> Source code in <code>src\\astro_lab\\data\\graphs\\builders.py</code> <pre><code>class RadiusGraphBuilder(BaseGraphBuilder):\n    \"\"\"Radius-based graph builder with adaptive radius.\"\"\"\n\n    def __init__(self, radius: float = 1.0, **kwargs):\n        config = GraphConfig(method=\"radius\", radius=radius, **kwargs)\n        super().__init__(config)\n\n    def build(self, survey_tensor: SurveyTensorDict) -&gt; Data:\n        \"\"\"Build radius graph with adaptive features.\"\"\"\n        self.validate_input(survey_tensor)\n\n        # Extract coordinates and features\n        coords = self.extract_coordinates(survey_tensor)\n        features = self.extract_features(survey_tensor)\n\n        # Adaptive radius based on local density\n        if hasattr(self.config, \"adaptive_radius\") and self.config.adaptive_radius:\n            radius = self._compute_adaptive_radius(coords)\n        else:\n            radius = self.config.radius\n\n        # Create radius graph\n        edge_index = radius_graph(\n            coords,\n            r=radius,\n            batch=None,\n            loop=self.config.self_loops,\n            max_num_neighbors=self.config.k_neighbors * 2,  # Limit for efficiency\n            flow=\"source_to_target\",\n            num_workers=self.config.num_workers,\n        )\n\n        # Make undirected if requested\n        if not self.config.directed:\n            edge_index = to_undirected(edge_index, num_nodes=coords.shape[0])\n\n        # Create PyG Data object\n        data = self.create_data_object(features, edge_index, coords)\n\n        # Add metadata\n        data.graph_type = \"radius\"\n        data.radius = radius if isinstance(radius, float) else radius.mean().item()\n\n        return data\n\n    def _compute_adaptive_radius(self, coords: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"Compute adaptive radius based on local density.\"\"\"\n        # Find k-th nearest neighbor distance for each point\n        k = min(10, coords.shape[0] - 1)\n\n        # Compute pairwise distances to k nearest neighbors\n        dists = torch.cdist(coords, coords)\n        kth_dists, _ = torch.kthvalue(dists, k + 1, dim=1)  # +1 to skip self\n\n        # Use median of k-th distances as base radius\n        median_dist = kth_dists.median()\n\n        # Scale by configuration\n        radius = median_dist * self.config.radius\n\n        self.logger.info(f\"Adaptive radius: {radius:.4f} (median k-NN dist: {median_dist:.4f})\")\n\n        return radius\n</code></pre>"},{"location":"api/astro_lab.data.graphs/#astro_lab.data.graphs.RadiusGraphBuilder.build","title":"build","text":"<pre><code>build(survey_tensor: SurveyTensorDict) -&gt; Data\n</code></pre> <p>Build radius graph with adaptive features.</p> Source code in <code>src\\astro_lab\\data\\graphs\\builders.py</code> <pre><code>def build(self, survey_tensor: SurveyTensorDict) -&gt; Data:\n    \"\"\"Build radius graph with adaptive features.\"\"\"\n    self.validate_input(survey_tensor)\n\n    # Extract coordinates and features\n    coords = self.extract_coordinates(survey_tensor)\n    features = self.extract_features(survey_tensor)\n\n    # Adaptive radius based on local density\n    if hasattr(self.config, \"adaptive_radius\") and self.config.adaptive_radius:\n        radius = self._compute_adaptive_radius(coords)\n    else:\n        radius = self.config.radius\n\n    # Create radius graph\n    edge_index = radius_graph(\n        coords,\n        r=radius,\n        batch=None,\n        loop=self.config.self_loops,\n        max_num_neighbors=self.config.k_neighbors * 2,  # Limit for efficiency\n        flow=\"source_to_target\",\n        num_workers=self.config.num_workers,\n    )\n\n    # Make undirected if requested\n    if not self.config.directed:\n        edge_index = to_undirected(edge_index, num_nodes=coords.shape[0])\n\n    # Create PyG Data object\n    data = self.create_data_object(features, edge_index, coords)\n\n    # Add metadata\n    data.graph_type = \"radius\"\n    data.radius = radius if isinstance(radius, float) else radius.mean().item()\n\n    return data\n</code></pre>"},{"location":"api/astro_lab.data.graphs/#astro_lab.data.graphs.TemporalGraphBuilder","title":"TemporalGraphBuilder","text":"<p>               Bases: <code>BaseGraphBuilder</code></p> <p>Build temporal graphs for time-series astronomical data.</p> <p>Methods:</p> Name Description <code>build</code> <p>Build temporal graph from time-series data.</p> Source code in <code>src\\astro_lab\\data\\graphs\\advanced.py</code> <pre><code>class TemporalGraphBuilder(BaseGraphBuilder):\n    \"\"\"Build temporal graphs for time-series astronomical data.\"\"\"\n\n    def __init__(\n        self,\n        temporal_window: int = 5,\n        temporal_stride: int = 1,\n        **kwargs\n    ):\n        config = GraphConfig(method=\"temporal\", use_temporal=True, **kwargs)\n        super().__init__(config)\n        self.temporal_window = temporal_window\n        self.temporal_stride = temporal_stride\n\n    def build(self, survey_tensor: SurveyTensorDict) -&gt; Data:\n        \"\"\"Build temporal graph from time-series data.\"\"\"\n        self.validate_input(survey_tensor)\n\n        # Check for temporal data\n        if \"temporal\" not in survey_tensor:\n            raise ValueError(\"SurveyTensorDict must contain 'temporal' data\")\n\n        coords = self.extract_coordinates(survey_tensor)\n        features = self.extract_features(survey_tensor)\n        temporal_data = survey_tensor[\"temporal\"]\n\n        # Build spatial edges\n        spatial_edges = knn_graph(\n            coords,\n            k=self.config.k_neighbors,\n            batch=None,\n            loop=False,\n        )\n\n        # Build temporal edges\n        temporal_edges = self._build_temporal_edges(\n            temporal_data, coords.shape[0]\n        )\n\n        # Combine edges\n        edge_index = torch.cat([spatial_edges, temporal_edges], dim=1)\n\n        # Edge types: 0 for spatial, 1 for temporal\n        edge_type = torch.cat([\n            torch.zeros(spatial_edges.shape[1], dtype=torch.long),\n            torch.ones(temporal_edges.shape[1], dtype=torch.long)\n        ])\n\n        # Create data object\n        data = self.create_data_object(\n            features, edge_index, coords,\n            edge_type=edge_type\n        )\n\n        # Add temporal information\n        if hasattr(temporal_data, \"timestamps\"):\n            data.timestamps = temporal_data.timestamps\n        if hasattr(temporal_data, \"time_features\"):\n            data.time_features = temporal_data.time_features\n\n        data.graph_type = \"temporal\"\n        data.temporal_window = self.temporal_window\n\n        return data\n\n    def _build_temporal_edges(\n        self, temporal_data: Any, n_nodes: int\n    ) -&gt; torch.Tensor:\n        \"\"\"Build edges connecting same object across time.\"\"\"\n        temporal_edges = []\n\n        # If we have time stamps for each observation\n        if hasattr(temporal_data, \"node_timestamps\"):\n            timestamps = temporal_data.node_timestamps\n            unique_times = torch.unique(timestamps, sorted=True)\n\n            # Connect observations of same object across time\n            if hasattr(temporal_data, \"object_ids\"):\n                object_ids = temporal_data.object_ids\n\n                for t_idx in range(len(unique_times) - 1):\n                    t1 = unique_times[t_idx]\n                    t2 = unique_times[t_idx + 1]\n\n                    mask1 = timestamps == t1\n                    mask2 = timestamps == t2\n\n                    # Find matching objects\n                    for obj_id in torch.unique(object_ids):\n                        obj_mask1 = mask1 &amp; (object_ids == obj_id)\n                        obj_mask2 = mask2 &amp; (object_ids == obj_id)\n\n                        if obj_mask1.any() and obj_mask2.any():\n                            idx1 = torch.where(obj_mask1)[0]\n                            idx2 = torch.where(obj_mask2)[0]\n\n                            # Connect all pairs\n                            for i1 in idx1:\n                                for i2 in idx2:\n                                    temporal_edges.append([i1, i2])\n\n        if temporal_edges:\n            return torch.tensor(temporal_edges, device=self.device).t()\n        else:\n            return torch.empty((2, 0), dtype=torch.long, device=self.device)\n</code></pre>"},{"location":"api/astro_lab.data.graphs/#astro_lab.data.graphs.TemporalGraphBuilder.build","title":"build","text":"<pre><code>build(survey_tensor: SurveyTensorDict) -&gt; Data\n</code></pre> <p>Build temporal graph from time-series data.</p> Source code in <code>src\\astro_lab\\data\\graphs\\advanced.py</code> <pre><code>def build(self, survey_tensor: SurveyTensorDict) -&gt; Data:\n    \"\"\"Build temporal graph from time-series data.\"\"\"\n    self.validate_input(survey_tensor)\n\n    # Check for temporal data\n    if \"temporal\" not in survey_tensor:\n        raise ValueError(\"SurveyTensorDict must contain 'temporal' data\")\n\n    coords = self.extract_coordinates(survey_tensor)\n    features = self.extract_features(survey_tensor)\n    temporal_data = survey_tensor[\"temporal\"]\n\n    # Build spatial edges\n    spatial_edges = knn_graph(\n        coords,\n        k=self.config.k_neighbors,\n        batch=None,\n        loop=False,\n    )\n\n    # Build temporal edges\n    temporal_edges = self._build_temporal_edges(\n        temporal_data, coords.shape[0]\n    )\n\n    # Combine edges\n    edge_index = torch.cat([spatial_edges, temporal_edges], dim=1)\n\n    # Edge types: 0 for spatial, 1 for temporal\n    edge_type = torch.cat([\n        torch.zeros(spatial_edges.shape[1], dtype=torch.long),\n        torch.ones(temporal_edges.shape[1], dtype=torch.long)\n    ])\n\n    # Create data object\n    data = self.create_data_object(\n        features, edge_index, coords,\n        edge_type=edge_type\n    )\n\n    # Add temporal information\n    if hasattr(temporal_data, \"timestamps\"):\n        data.timestamps = temporal_data.timestamps\n    if hasattr(temporal_data, \"time_features\"):\n        data.time_features = temporal_data.time_features\n\n    data.graph_type = \"temporal\"\n    data.temporal_window = self.temporal_window\n\n    return data\n</code></pre>"},{"location":"api/astro_lab.data.graphs/#astro_lab.data.graphs.create_adaptive_graph","title":"create_adaptive_graph","text":"<pre><code>create_adaptive_graph(survey_tensor: SurveyTensorDict, **kwargs) -&gt; Data\n</code></pre> <p>Create adaptive graph based on local structure.</p> Source code in <code>src\\astro_lab\\data\\graphs\\builders.py</code> <pre><code>def create_adaptive_graph(survey_tensor: SurveyTensorDict, **kwargs) -&gt; Data:\n    \"\"\"Create adaptive graph based on local structure.\"\"\"\n    builder = AdaptiveGraphBuilder(**kwargs)\n    return builder.build(survey_tensor)\n</code></pre>"},{"location":"api/astro_lab.data.graphs/#astro_lab.data.graphs.create_adaptive_pointcloud_graph","title":"create_adaptive_pointcloud_graph","text":"<pre><code>create_adaptive_pointcloud_graph(\n    survey_tensor: SurveyTensorDict,\n    k_min: int = 8,\n    k_max: int = 32,\n    density_adaptive: bool = True,\n    **kwargs\n) -&gt; Data\n</code></pre> <p>Create adaptive point cloud graph from survey data.</p> Source code in <code>src\\astro_lab\\data\\graphs\\pointcloud.py</code> <pre><code>def create_adaptive_pointcloud_graph(\n    survey_tensor: SurveyTensorDict,\n    k_min: int = 8,\n    k_max: int = 32,\n    density_adaptive: bool = True,\n    **kwargs\n) -&gt; Data:\n    \"\"\"Create adaptive point cloud graph from survey data.\"\"\"\n    builder = AdaptivePointCloudGraphBuilder(\n        k_min=k_min,\n        k_max=k_max,\n        density_adaptive=density_adaptive,\n        **kwargs\n    )\n    return builder.build(survey_tensor)\n</code></pre>"},{"location":"api/astro_lab.data.graphs/#astro_lab.data.graphs.create_astronomical_graph","title":"create_astronomical_graph","text":"<pre><code>create_astronomical_graph(survey_tensor: SurveyTensorDict, **kwargs) -&gt; Data\n</code></pre> <p>Create astronomical graph with domain-specific optimizations.</p> Source code in <code>src\\astro_lab\\data\\graphs\\builders.py</code> <pre><code>def create_astronomical_graph(survey_tensor: SurveyTensorDict, **kwargs) -&gt; Data:\n    \"\"\"Create astronomical graph with domain-specific optimizations.\"\"\"\n    builder = AstronomicalGraphBuilder(**kwargs)\n    return builder.build(survey_tensor)\n</code></pre>"},{"location":"api/astro_lab.data.graphs/#astro_lab.data.graphs.create_dynamic_graph","title":"create_dynamic_graph","text":"<pre><code>create_dynamic_graph(\n    survey_tensor: SurveyTensorDict, initial_k: int = 16, **kwargs\n) -&gt; Data\n</code></pre> <p>Create dynamic graph with learnable structure.</p> Source code in <code>src\\astro_lab\\data\\graphs\\advanced.py</code> <pre><code>def create_dynamic_graph(\n    survey_tensor: SurveyTensorDict, initial_k: int = 16, **kwargs\n) -&gt; Data:\n    \"\"\"Create dynamic graph with learnable structure.\"\"\"\n    builder = DynamicGraphBuilder(initial_k=initial_k, **kwargs)\n    return builder.build(survey_tensor)\n</code></pre>"},{"location":"api/astro_lab.data.graphs/#astro_lab.data.graphs.create_geometric_prior_graph","title":"create_geometric_prior_graph","text":"<pre><code>create_geometric_prior_graph(\n    survey_tensor: SurveyTensorDict, prior_type: str = \"filament\", **kwargs\n) -&gt; Data\n</code></pre> <p>Create graph with astronomical geometric priors.</p> Source code in <code>src\\astro_lab\\data\\graphs\\advanced.py</code> <pre><code>def create_geometric_prior_graph(\n    survey_tensor: SurveyTensorDict,\n    prior_type: str = \"filament\",\n    **kwargs\n) -&gt; Data:\n    \"\"\"Create graph with astronomical geometric priors.\"\"\"\n    builder = GeometricPriorGraphBuilder(\n        prior_type=prior_type,\n        **kwargs\n    )\n    return builder.build(survey_tensor)\n</code></pre>"},{"location":"api/astro_lab.data.graphs/#astro_lab.data.graphs.create_heterogeneous_graph","title":"create_heterogeneous_graph","text":"<pre><code>create_heterogeneous_graph(survey_tensor: SurveyTensorDict, **kwargs) -&gt; HeteroData\n</code></pre> <p>Create heterogeneous graph with multiple node types.</p> Source code in <code>src\\astro_lab\\data\\graphs\\builders.py</code> <pre><code>def create_heterogeneous_graph(\n    survey_tensor: SurveyTensorDict, **kwargs\n) -&gt; HeteroData:\n    \"\"\"Create heterogeneous graph with multiple node types.\"\"\"\n    builder = HeterogeneousGraphBuilder(**kwargs)\n    return builder.build(survey_tensor)\n</code></pre>"},{"location":"api/astro_lab.data.graphs/#astro_lab.data.graphs.create_hierarchical_graph","title":"create_hierarchical_graph","text":"<pre><code>create_hierarchical_graph(\n    survey_tensor: SurveyTensorDict,\n    cluster_method: str = \"kmeans\",\n    n_clusters: int = 10,\n    **kwargs\n) -&gt; Data\n</code></pre> <p>Create hierarchical graph-of-graphs.</p> Source code in <code>src\\astro_lab\\data\\graphs\\advanced.py</code> <pre><code>def create_hierarchical_graph(\n    survey_tensor: SurveyTensorDict,\n    cluster_method: str = \"kmeans\",\n    n_clusters: int = 10,\n    **kwargs\n) -&gt; Data:\n    \"\"\"Create hierarchical graph-of-graphs.\"\"\"\n    builder = GraphOfGraphsBuilder(\n        cluster_method=cluster_method,\n        n_clusters=n_clusters,\n        **kwargs\n    )\n    return builder.build(survey_tensor)\n</code></pre>"},{"location":"api/astro_lab.data.graphs/#astro_lab.data.graphs.create_knn_graph","title":"create_knn_graph","text":"<pre><code>create_knn_graph(\n    survey_tensor: SurveyTensorDict, k_neighbors: int = 16, **kwargs\n) -&gt; Data\n</code></pre> <p>Create optimized KNN graph from SurveyTensorDict.</p> Source code in <code>src\\astro_lab\\data\\graphs\\builders.py</code> <pre><code>def create_knn_graph(\n    survey_tensor: SurveyTensorDict, k_neighbors: int = 16, **kwargs\n) -&gt; Data:\n    \"\"\"Create optimized KNN graph from SurveyTensorDict.\"\"\"\n    builder = KNNGraphBuilder(k_neighbors=k_neighbors, **kwargs)\n    return builder.build(survey_tensor)\n</code></pre>"},{"location":"api/astro_lab.data.graphs/#astro_lab.data.graphs.create_multiscale_graph","title":"create_multiscale_graph","text":"<pre><code>create_multiscale_graph(\n    survey_tensor: SurveyTensorDict, scales: List[int] = None, **kwargs\n) -&gt; Data\n</code></pre> <p>Create multi-scale graph for hierarchical analysis.</p> Source code in <code>src\\astro_lab\\data\\graphs\\builders.py</code> <pre><code>def create_multiscale_graph(\n    survey_tensor: SurveyTensorDict, scales: List[int] = None, **kwargs\n) -&gt; Data:\n    \"\"\"Create multi-scale graph for hierarchical analysis.\"\"\"\n    builder = MultiScaleGraphBuilder(scales=scales, **kwargs)\n    return builder.build(survey_tensor)\n</code></pre>"},{"location":"api/astro_lab.data.graphs/#astro_lab.data.graphs.create_pointcloud_graph","title":"create_pointcloud_graph","text":"<pre><code>create_pointcloud_graph(\n    survey_tensor: SurveyTensorDict,\n    k_neighbors: int = 16,\n    normalize_positions: bool = True,\n    **kwargs\n) -&gt; Data\n</code></pre> <p>Create point cloud graph from survey data.</p> Source code in <code>src\\astro_lab\\data\\graphs\\pointcloud.py</code> <pre><code>def create_pointcloud_graph(\n    survey_tensor: SurveyTensorDict,\n    k_neighbors: int = 16,\n    normalize_positions: bool = True,\n    **kwargs\n) -&gt; Data:\n    \"\"\"Create point cloud graph from survey data.\"\"\"\n    builder = PointCloudGraphBuilder(\n        k_neighbors=k_neighbors,\n        normalize_positions=normalize_positions,\n        **kwargs\n    )\n    return builder.build(survey_tensor)\n</code></pre>"},{"location":"api/astro_lab.data.graphs/#astro_lab.data.graphs.create_radius_graph","title":"create_radius_graph","text":"<pre><code>create_radius_graph(\n    survey_tensor: SurveyTensorDict, radius: float = 1.0, **kwargs\n) -&gt; Data\n</code></pre> <p>Create radius graph with adaptive features.</p> Source code in <code>src\\astro_lab\\data\\graphs\\builders.py</code> <pre><code>def create_radius_graph(\n    survey_tensor: SurveyTensorDict, radius: float = 1.0, **kwargs\n) -&gt; Data:\n    \"\"\"Create radius graph with adaptive features.\"\"\"\n    builder = RadiusGraphBuilder(radius=radius, **kwargs)\n    return builder.build(survey_tensor)\n</code></pre>"},{"location":"api/astro_lab.data.graphs/#astro_lab.data.graphs.create_temporal_graph","title":"create_temporal_graph","text":"<pre><code>create_temporal_graph(\n    survey_tensor: SurveyTensorDict, temporal_window: int = 5, **kwargs\n) -&gt; Data\n</code></pre> <p>Create temporal graph for time-series data.</p> Source code in <code>src\\astro_lab\\data\\graphs\\advanced.py</code> <pre><code>def create_temporal_graph(\n    survey_tensor: SurveyTensorDict,\n    temporal_window: int = 5,\n    **kwargs\n) -&gt; Data:\n    \"\"\"Create temporal graph for time-series data.\"\"\"\n    builder = TemporalGraphBuilder(\n        temporal_window=temporal_window,\n        **kwargs\n    )\n    return builder.build(survey_tensor)\n</code></pre>"},{"location":"api/astro_lab.data/","title":"astro_lab.data","text":""},{"location":"api/astro_lab.data/#astro_lab.data","title":"data","text":"<p>AstroLab Data Module - High-Performance Astronomical Data Processing</p> <p>Clean, unified data loading and processing for astronomical surveys using Polars, PyTorch, and specialized astronomical tensors.</p> Quick Start <p>from astro_lab.data import load_survey_catalog, preprocess_survey df = load_survey_catalog(\"gaia\", max_samples=5000) processed_path = preprocess_survey(\"gaia\")</p> <p>Modules:</p> Name Description <code>config</code> <p>AstroLab Data Configuration</p> <code>datamodule</code> <p>AstroLab DataModule - Lightning DataModule for Astronomical Data</p> <code>datasets</code> <p>AstroLab Datasets</p> <code>graphs</code> <p>Graph Building Module</p> <code>loaders</code> <p>AstroLab Data Loaders - Unified loading and downloading functions.</p> <code>processors</code> <p>AstroLab Data Processors - Unified preprocessing and processing functions.</p> <code>utils</code> <p>AstroLab Data Utils - Essential utility functions.</p> <p>Classes:</p> Name Description <code>AstroDataModule</code> <p>Clean Lightning DataModule for astronomical datasets.</p> <code>DataConfig</code> <p>Centralized data configuration for AstroLab.</p> <p>Functions:</p> Name Description <code>check_astroquery_available</code> <p>Check if astroquery is available.</p> <code>create_astro_datamodule</code> <p>Create AstroDataModule for given survey.</p> <code>create_survey_tensordict</code> <p>Create SurveyTensorDict from processed DataFrame.</p> <code>create_training_splits</code> <p>Create train/validation/test splits.</p> <code>detect_survey_type</code> <p>Detect survey type from DataFrame columns.</p> <code>download_2mass</code> <p>Download 2MASS catalog.</p> <code>download_pan_starrs</code> <p>Download Pan-STARRS catalog.</p> <code>download_sdss</code> <p>Download SDSS catalog.</p> <code>download_survey</code> <p>Download data for a specific survey.</p> <code>download_wise</code> <p>Download WISE catalog.</p> <code>get_data_dir</code> <p>Get the configured data directory.</p> <code>get_data_statistics</code> <p>Get comprehensive statistics for a DataFrame.</p> <code>get_fits_info</code> <p>Get information about FITS file structure.</p> <code>get_processed_dir</code> <p>Get the processed data directory.</p> <code>get_raw_dir</code> <p>Get the raw data directory.</p> <code>get_survey_paths</code> <p>Get all standard paths for a survey.</p> <code>import_fits</code> <p>Import FITS file to processed data directory.</p> <code>import_tng50</code> <p>Import TNG50 HDF5 file.</p> <code>list_available_catalogs</code> <p>List all available catalogs.</p> <code>load_catalog</code> <p>Load a catalog from various formats (FITS, Parquet, CSV).</p> <code>load_fits_optimized</code> <p>Load FITS file optimized for Polars DataFrame output.</p> <code>load_fits_table_optimized</code> <p>Load FITS table with optimization.</p> <code>load_splits_from_parquet</code> <p>Load train/val/test splits from Parquet files.</p> <code>load_survey_catalog</code> <p>Load catalog data for a specific survey.</p> <code>preprocess_survey</code> <p>Unified survey preprocessing function.</p> <code>save_splits_to_parquet</code> <p>Save train/val/test splits to Parquet files.</p>"},{"location":"api/astro_lab.data/#astro_lab.data.AstroDataModule","title":"AstroDataModule","text":"<p>               Bases: <code>LightningDataModule</code></p> <p>Clean Lightning DataModule for astronomical datasets.</p> <p>Handles train/val/test splits and data loading. Uses unified AstroDataset from core.py.</p> <p>2025 Optimizations: - PIN memory for faster GPU transfer - Persistent workers to avoid recreation overhead - Prefetch factor tuning - Drop last for consistent batch sizes - Better distributed sampling - Fixed PyTorch Geometric batch handling</p> <p>Methods:</p> Name Description <code>get_info</code> <p>Get dataset information.</p> <code>load_state_dict</code> <p>Load datamodule state.</p> <code>prepare_data</code> <p>Download or prepare data. Called only on rank 0 in distributed training.</p> <code>setup</code> <p>Setup datasets for training/validation/testing.</p> <code>state_dict</code> <p>Save datamodule state.</p> <code>teardown</code> <p>Clean up after training/testing.</p> <code>test_dataloader</code> <p>Create test dataloader.</p> <code>train_dataloader</code> <p>Create training dataloader with optimizations.</p> <code>val_dataloader</code> <p>Create validation dataloader.</p> Source code in <code>src\\astro_lab\\data\\datamodule.py</code> <pre><code>class AstroDataModule(L.LightningDataModule):\n    \"\"\"\n    Clean Lightning DataModule for astronomical datasets.\n\n    Handles train/val/test splits and data loading.\n    Uses unified AstroDataset from core.py.\n\n    2025 Optimizations:\n    - PIN memory for faster GPU transfer\n    - Persistent workers to avoid recreation overhead\n    - Prefetch factor tuning\n    - Drop last for consistent batch sizes\n    - Better distributed sampling\n    - Fixed PyTorch Geometric batch handling\n    \"\"\"\n\n    def __init__(\n        self,\n        survey: str,\n        data_root: Optional[str] = None,\n        k_neighbors: int = 8,\n        max_samples: Optional[int] = None,\n        batch_size: int = 1,  # Graph datasets typically use batch_size=1\n        train_ratio: float = 0.7,\n        val_ratio: float = 0.15,\n        num_workers: Optional[int] = None,  # Auto-detect optimal workers\n        pin_memory: bool = True,\n        persistent_workers: bool = True,\n        prefetch_factor: int = 2,\n        drop_last: bool = True,\n        use_distributed_sampler: bool = True,\n        # New parameters for laptop optimization\n        max_nodes_per_graph: int = 1000,  # Limit graph size for laptop GPUs\n        use_subgraph_sampling: bool = True,  # Use subgraph sampling for large graphs\n        **kwargs,\n    ):\n        super().__init__()\n        self.save_hyperparameters()\n\n        self.survey = survey\n        self.dataset_name = survey  # For results organization\n        self.data_root = data_root or str(data_config.base_dir)\n        self.k_neighbors = k_neighbors\n        self.max_samples = max_samples\n        print(\n            f\"[DEBUG] batch_size type before logic: {type(batch_size)}, value: {batch_size}\"\n        )\n        self.batch_size = int(batch_size)  # Ensure batch_size is always int\n        self.train_ratio = train_ratio\n        self.val_ratio = val_ratio\n        self.drop_last = drop_last\n        self.use_distributed_sampler = use_distributed_sampler\n\n        # Laptop optimization parameters\n        self.max_nodes_per_graph = max_nodes_per_graph\n        self.use_subgraph_sampling = use_subgraph_sampling\n\n        # Optimize num_workers for laptop\n        if num_workers is None:\n            # Conservative settings for laptop\n            try:\n                cpu_count = os.cpu_count()\n                # Use fewer workers on laptop to avoid memory pressure\n                self.num_workers = max(0, min(cpu_count // 2, 4))\n            except (OSError, AttributeError):\n                self.num_workers = 2\n        else:\n            self.num_workers = num_workers\n\n        # Conservative settings for laptop GPUs\n        if self.batch_size == 1:\n            self.pin_memory = False\n            self.persistent_workers = False\n            self.prefetch_factor = None\n            self.num_workers = 0\n        else:\n            # Conservative memory settings for laptop\n            self.pin_memory = pin_memory and torch.cuda.is_available()\n            self.persistent_workers = persistent_workers and self.num_workers &gt; 0\n            self.prefetch_factor = prefetch_factor if self.num_workers &gt; 0 else None\n\n        # Dataset will be created in setup()\n        self.dataset = None\n\n        # Store the main data object\n        self._main_data = None\n\n        # Class information for Lightning module\n        self.num_classes = None\n        self.num_features = None\n\n    def prepare_data(self):\n        \"\"\"\n        Download or prepare data. Called only on rank 0 in distributed training.\n        \"\"\"\n        # This method is called before setup() and only on rank 0\n        # Use it for downloading or one-time data preparation\n        pass\n\n    def setup(self, stage: Optional[str] = None):\n        \"\"\"Setup datasets for training/validation/testing.\"\"\"\n        if self.dataset is None:\n            self.dataset = SurveyGraphDataset(\n                root=self.data_root,\n                survey=self.survey,\n                k_neighbors=self.k_neighbors,\n                max_samples=self.max_samples,\n            )\n\n        # Get the main data object\n        full_data = self.dataset[0]\n\n        # Apply subgraph sampling if the graph is too large\n        if (\n            self.use_subgraph_sampling\n            and full_data.num_nodes &gt; self.max_nodes_per_graph\n        ):\n            logger.info(\n                f\"Graph too large ({full_data.num_nodes} nodes). Creating subgraph with {self.max_nodes_per_graph} nodes.\"\n            )\n            self._main_data = self._create_subgraph_samples(\n                full_data, self.max_nodes_per_graph\n            )\n        else:\n            self._main_data = full_data\n\n        # Verify graph consistency after setup\n        if not check_graph_consistency(self._main_data):\n            logger.error(\"\u274c Graph consistency check failed after setup\")\n            raise RuntimeError(\"Graph data is inconsistent after setup\")\n\n        # Log graph statistics\n        stats = get_graph_statistics(self._main_data)\n        logger.info(f\"\ud83d\udcca Graph statistics: {stats}\")\n\n        # Extract dataset information for Lightning module\n        self.num_features = None\n        if hasattr(self._main_data, \"x\") and self._main_data.x is not None:\n            if self._main_data.x.dim() &gt;= 2:\n                self.num_features = self._main_data.x.size(1)\n            else:\n                # 1D tensor - use size(0) as feature dimension\n                self.num_features = 1\n\n        if hasattr(self._main_data, \"y\") and self._main_data.y is not None:\n            unique_labels = torch.unique(self._main_data.y)\n            self.num_classes = max(len(unique_labels), 2)  # Ensure at least 2 classes\n\n            # If we only have one class, create synthetic binary labels for demonstration\n            if len(unique_labels) == 1:\n                logger.warning(\n                    f\"Dataset has only 1 unique label ({unique_labels[0].item()}). Creating synthetic binary labels for training.\"\n                )\n                # Create binary labels based on node features or random split\n                # Use feature-based split: nodes with feature sum &gt; median get label 1\n                feature_sums = self._main_data.x.sum(dim=1)\n                median_val = feature_sums.median()\n                self._main_data.y = (feature_sums &gt; median_val).long()\n                self.num_classes = 2\n        else:\n            # No labels available, create synthetic binary labels\n            logger.warning(\n                \"No labels found in dataset. Creating synthetic binary labels for training.\"\n            )\n            if self._main_data.x.dim() &gt;= 2:\n                feature_sums = self._main_data.x.sum(dim=1)\n            else:\n                feature_sums = self._main_data.x.sum()\n                median_val = feature_sums.median()\n                self._main_data.y = (feature_sums &gt; median_val).long()\n                self.num_classes = 2\n\n        # Create train/val/test splits using masks\n        self._create_data_splits()\n\n        # Keep small datasets on CPU to avoid CUDA errors\n        if self._main_data.num_nodes &lt;= 10:\n            self._main_data = self._main_data.cpu()\n            logger.info(\"Small dataset detected, keeping on CPU\")\n\n    def _create_data_splits(self):\n        \"\"\"Create train/val/test masks for the graph data.\"\"\"\n        data = self._main_data\n        num_nodes = data.num_nodes\n\n        # Create train/val/test masks\n        data.train_mask = torch.zeros(num_nodes, dtype=torch.bool)\n        data.val_mask = torch.zeros(num_nodes, dtype=torch.bool)\n        data.test_mask = torch.zeros(num_nodes, dtype=torch.bool)\n\n        # Random split\n        indices = torch.randperm(num_nodes)\n        train_size = int(num_nodes * self.train_ratio)\n        val_size = int(num_nodes * self.val_ratio)\n\n        data.train_mask[indices[:train_size]] = True\n        data.val_mask[indices[train_size : train_size + val_size]] = True\n        data.test_mask[indices[train_size + val_size :]] = True\n\n        # Log split information\n        logger.info(\n            f\"Data splits - Train: {data.train_mask.sum()}, \"\n            f\"Val: {data.val_mask.sum()}, Test: {data.test_mask.sum()}\"\n        )\n\n    def _create_subgraph_samples(self, data, max_nodes: int):\n        \"\"\"Create smaller subgraph samples using utility function.\"\"\"\n        return sample_subgraph_random(data, max_nodes, seed=42)\n\n    def _estimate_memory_usage(self, data) -&gt; float:\n        \"\"\"Estimate memory usage of graph data in MB.\"\"\"\n        total_memory = 0\n\n        # Node features\n        if hasattr(data, \"x\"):\n            total_memory += data.x.numel() * data.x.element_size()\n\n        # Edge indices\n        if hasattr(data, \"edge_index\"):\n            total_memory += data.edge_index.numel() * data.edge_index.element_size()\n\n        # Positions\n        if hasattr(data, \"pos\"):\n            total_memory += data.pos.numel() * data.pos.element_size()\n\n        # Labels\n        if hasattr(data, \"y\"):\n            total_memory += data.y.numel() * data.y.element_size()\n\n        # Masks\n        if hasattr(data, \"train_mask\"):\n            total_memory += data.train_mask.numel() * data.train_mask.element_size()\n\n        return total_memory / (1024 * 1024)  # Convert to MB\n\n    def _get_dataloader_kwargs(self) -&gt; Dict[str, Any]:\n        \"\"\"Get optimized dataloader kwargs based on settings.\"\"\"\n        kwargs = {\n            \"batch_size\": self.batch_size,\n            \"num_workers\": self.num_workers,\n            \"persistent_workers\": self.persistent_workers,\n            \"pin_memory\": self.pin_memory,\n            \"drop_last\": self.drop_last,\n        }\n\n        # Add prefetch_factor only if using workers\n        if self.num_workers &gt; 0 and self.prefetch_factor is not None:\n            kwargs[\"prefetch_factor\"] = self.prefetch_factor\n\n        return kwargs\n\n    def train_dataloader(self):\n        \"\"\"Create training dataloader with optimizations.\"\"\"\n        if self._main_data is None:\n            self.setup()\n\n        # Create masks for training data\n        train_mask = getattr(self._main_data, \"train_mask\", None)\n        if train_mask is not None:\n            train_data = self._main_data.__class__()\n            for attr in self._main_data.keys():\n                value = getattr(self._main_data, attr)\n                if (\n                    isinstance(value, torch.Tensor)\n                    and value.dim() &gt; 0\n                    and value.size(0) == train_mask.size(0)\n                ):\n                    setattr(train_data, attr, value[train_mask])\n                else:\n                    setattr(train_data, attr, value)\n        else:\n            train_data = self._main_data\n\n        return DataLoader(\n            [train_data],\n            batch_size=self.batch_size,\n            shuffle=True,\n            num_workers=self.num_workers,\n            pin_memory=self.pin_memory,\n        )\n\n    def val_dataloader(self):\n        \"\"\"Create validation dataloader.\"\"\"\n        if self._main_data is None:\n            self.setup()\n\n        # Create masks for validation data\n        val_mask = getattr(self._main_data, \"val_mask\", None)\n        if val_mask is not None:\n            val_data = self._main_data.__class__()\n            for attr in self._main_data.keys():\n                value = getattr(self._main_data, attr)\n                if isinstance(value, torch.Tensor) and value.size(0) == val_mask.size(\n                    0\n                ):\n                    setattr(val_data, attr, value[val_mask])\n                else:\n                    setattr(val_data, attr, value)\n        else:\n            val_data = self._main_data\n\n        return DataLoader(\n            [val_data],\n            batch_size=self.batch_size,\n            shuffle=False,\n            num_workers=self.num_workers,\n            pin_memory=self.pin_memory,\n        )\n\n    def test_dataloader(self):\n        \"\"\"Create test dataloader.\"\"\"\n        if self._main_data is None:\n            self.setup()\n\n        # Create masks for test data\n        test_mask = getattr(self._main_data, \"test_mask\", None)\n        if test_mask is not None:\n            test_data = self._main_data.__class__()\n            for attr in self._main_data.keys():\n                value = getattr(self._main_data, attr)\n                if isinstance(value, torch.Tensor) and value.size(0) == test_mask.size(\n                    0\n                ):\n                    setattr(test_data, attr, value[test_mask])\n                else:\n                    setattr(test_data, attr, value)\n        else:\n            test_data = self._main_data\n\n        return DataLoader(\n            [test_data],\n            batch_size=self.batch_size,\n            shuffle=False,\n            num_workers=self.num_workers,\n            pin_memory=self.pin_memory,\n        )\n\n    def teardown(self, stage: Optional[str] = None):\n        \"\"\"Clean up after training/testing.\"\"\"\n        # Clean up cached data\n        self._main_data = None\n\n        # Force garbage collection\n        import gc\n\n        gc.collect()\n\n        # Clear CUDA cache if available\n        if torch.cuda.is_available():\n            torch.cuda.empty_cache()\n\n    def get_info(self) -&gt; Dict[str, Any]:\n        \"\"\"Get dataset information.\"\"\"\n        if self.dataset is None:\n            return {\"error\": \"Dataset not initialized\"}\n\n        info = self.dataset.get_info()\n\n        # Add datamodule-specific info\n        info.update(\n            {\n                \"batch_size\": self.batch_size,\n                \"num_workers\": self.num_workers,\n                \"pin_memory\": self.pin_memory,\n                \"persistent_workers\": self.persistent_workers,\n                \"train_ratio\": self.train_ratio,\n                \"val_ratio\": self.val_ratio,\n                \"num_classes\": self.num_classes,\n                \"num_features\": self.num_features,\n            }\n        )\n\n        return info\n\n    def state_dict(self) -&gt; Dict[str, Any]:\n        \"\"\"Save datamodule state.\"\"\"\n        return {\n            \"survey\": self.survey,\n            \"k_neighbors\": self.k_neighbors,\n            \"max_samples\": self.max_samples,\n            \"batch_size\": self.batch_size,\n            \"train_ratio\": self.train_ratio,\n            \"val_ratio\": self.val_ratio,\n        }\n\n    def load_state_dict(self, state_dict: Dict[str, Any]):\n        \"\"\"Load datamodule state.\"\"\"\n        for key, value in state_dict.items():\n            if hasattr(self, key):\n                setattr(self, key, value)\n</code></pre>"},{"location":"api/astro_lab.data/#astro_lab.data.AstroDataModule.get_info","title":"get_info","text":"<pre><code>get_info() -&gt; Dict[str, Any]\n</code></pre> <p>Get dataset information.</p> Source code in <code>src\\astro_lab\\data\\datamodule.py</code> <pre><code>def get_info(self) -&gt; Dict[str, Any]:\n    \"\"\"Get dataset information.\"\"\"\n    if self.dataset is None:\n        return {\"error\": \"Dataset not initialized\"}\n\n    info = self.dataset.get_info()\n\n    # Add datamodule-specific info\n    info.update(\n        {\n            \"batch_size\": self.batch_size,\n            \"num_workers\": self.num_workers,\n            \"pin_memory\": self.pin_memory,\n            \"persistent_workers\": self.persistent_workers,\n            \"train_ratio\": self.train_ratio,\n            \"val_ratio\": self.val_ratio,\n            \"num_classes\": self.num_classes,\n            \"num_features\": self.num_features,\n        }\n    )\n\n    return info\n</code></pre>"},{"location":"api/astro_lab.data/#astro_lab.data.AstroDataModule.load_state_dict","title":"load_state_dict","text":"<pre><code>load_state_dict(state_dict: Dict[str, Any])\n</code></pre> <p>Load datamodule state.</p> Source code in <code>src\\astro_lab\\data\\datamodule.py</code> <pre><code>def load_state_dict(self, state_dict: Dict[str, Any]):\n    \"\"\"Load datamodule state.\"\"\"\n    for key, value in state_dict.items():\n        if hasattr(self, key):\n            setattr(self, key, value)\n</code></pre>"},{"location":"api/astro_lab.data/#astro_lab.data.AstroDataModule.prepare_data","title":"prepare_data","text":"<pre><code>prepare_data()\n</code></pre> <p>Download or prepare data. Called only on rank 0 in distributed training.</p> Source code in <code>src\\astro_lab\\data\\datamodule.py</code> <pre><code>def prepare_data(self):\n    \"\"\"\n    Download or prepare data. Called only on rank 0 in distributed training.\n    \"\"\"\n    # This method is called before setup() and only on rank 0\n    # Use it for downloading or one-time data preparation\n    pass\n</code></pre>"},{"location":"api/astro_lab.data/#astro_lab.data.AstroDataModule.setup","title":"setup","text":"<pre><code>setup(stage: Optional[str] = None)\n</code></pre> <p>Setup datasets for training/validation/testing.</p> Source code in <code>src\\astro_lab\\data\\datamodule.py</code> <pre><code>def setup(self, stage: Optional[str] = None):\n    \"\"\"Setup datasets for training/validation/testing.\"\"\"\n    if self.dataset is None:\n        self.dataset = SurveyGraphDataset(\n            root=self.data_root,\n            survey=self.survey,\n            k_neighbors=self.k_neighbors,\n            max_samples=self.max_samples,\n        )\n\n    # Get the main data object\n    full_data = self.dataset[0]\n\n    # Apply subgraph sampling if the graph is too large\n    if (\n        self.use_subgraph_sampling\n        and full_data.num_nodes &gt; self.max_nodes_per_graph\n    ):\n        logger.info(\n            f\"Graph too large ({full_data.num_nodes} nodes). Creating subgraph with {self.max_nodes_per_graph} nodes.\"\n        )\n        self._main_data = self._create_subgraph_samples(\n            full_data, self.max_nodes_per_graph\n        )\n    else:\n        self._main_data = full_data\n\n    # Verify graph consistency after setup\n    if not check_graph_consistency(self._main_data):\n        logger.error(\"\u274c Graph consistency check failed after setup\")\n        raise RuntimeError(\"Graph data is inconsistent after setup\")\n\n    # Log graph statistics\n    stats = get_graph_statistics(self._main_data)\n    logger.info(f\"\ud83d\udcca Graph statistics: {stats}\")\n\n    # Extract dataset information for Lightning module\n    self.num_features = None\n    if hasattr(self._main_data, \"x\") and self._main_data.x is not None:\n        if self._main_data.x.dim() &gt;= 2:\n            self.num_features = self._main_data.x.size(1)\n        else:\n            # 1D tensor - use size(0) as feature dimension\n            self.num_features = 1\n\n    if hasattr(self._main_data, \"y\") and self._main_data.y is not None:\n        unique_labels = torch.unique(self._main_data.y)\n        self.num_classes = max(len(unique_labels), 2)  # Ensure at least 2 classes\n\n        # If we only have one class, create synthetic binary labels for demonstration\n        if len(unique_labels) == 1:\n            logger.warning(\n                f\"Dataset has only 1 unique label ({unique_labels[0].item()}). Creating synthetic binary labels for training.\"\n            )\n            # Create binary labels based on node features or random split\n            # Use feature-based split: nodes with feature sum &gt; median get label 1\n            feature_sums = self._main_data.x.sum(dim=1)\n            median_val = feature_sums.median()\n            self._main_data.y = (feature_sums &gt; median_val).long()\n            self.num_classes = 2\n    else:\n        # No labels available, create synthetic binary labels\n        logger.warning(\n            \"No labels found in dataset. Creating synthetic binary labels for training.\"\n        )\n        if self._main_data.x.dim() &gt;= 2:\n            feature_sums = self._main_data.x.sum(dim=1)\n        else:\n            feature_sums = self._main_data.x.sum()\n            median_val = feature_sums.median()\n            self._main_data.y = (feature_sums &gt; median_val).long()\n            self.num_classes = 2\n\n    # Create train/val/test splits using masks\n    self._create_data_splits()\n\n    # Keep small datasets on CPU to avoid CUDA errors\n    if self._main_data.num_nodes &lt;= 10:\n        self._main_data = self._main_data.cpu()\n        logger.info(\"Small dataset detected, keeping on CPU\")\n</code></pre>"},{"location":"api/astro_lab.data/#astro_lab.data.AstroDataModule.state_dict","title":"state_dict","text":"<pre><code>state_dict() -&gt; Dict[str, Any]\n</code></pre> <p>Save datamodule state.</p> Source code in <code>src\\astro_lab\\data\\datamodule.py</code> <pre><code>def state_dict(self) -&gt; Dict[str, Any]:\n    \"\"\"Save datamodule state.\"\"\"\n    return {\n        \"survey\": self.survey,\n        \"k_neighbors\": self.k_neighbors,\n        \"max_samples\": self.max_samples,\n        \"batch_size\": self.batch_size,\n        \"train_ratio\": self.train_ratio,\n        \"val_ratio\": self.val_ratio,\n    }\n</code></pre>"},{"location":"api/astro_lab.data/#astro_lab.data.AstroDataModule.teardown","title":"teardown","text":"<pre><code>teardown(stage: Optional[str] = None)\n</code></pre> <p>Clean up after training/testing.</p> Source code in <code>src\\astro_lab\\data\\datamodule.py</code> <pre><code>def teardown(self, stage: Optional[str] = None):\n    \"\"\"Clean up after training/testing.\"\"\"\n    # Clean up cached data\n    self._main_data = None\n\n    # Force garbage collection\n    import gc\n\n    gc.collect()\n\n    # Clear CUDA cache if available\n    if torch.cuda.is_available():\n        torch.cuda.empty_cache()\n</code></pre>"},{"location":"api/astro_lab.data/#astro_lab.data.AstroDataModule.test_dataloader","title":"test_dataloader","text":"<pre><code>test_dataloader()\n</code></pre> <p>Create test dataloader.</p> Source code in <code>src\\astro_lab\\data\\datamodule.py</code> <pre><code>def test_dataloader(self):\n    \"\"\"Create test dataloader.\"\"\"\n    if self._main_data is None:\n        self.setup()\n\n    # Create masks for test data\n    test_mask = getattr(self._main_data, \"test_mask\", None)\n    if test_mask is not None:\n        test_data = self._main_data.__class__()\n        for attr in self._main_data.keys():\n            value = getattr(self._main_data, attr)\n            if isinstance(value, torch.Tensor) and value.size(0) == test_mask.size(\n                0\n            ):\n                setattr(test_data, attr, value[test_mask])\n            else:\n                setattr(test_data, attr, value)\n    else:\n        test_data = self._main_data\n\n    return DataLoader(\n        [test_data],\n        batch_size=self.batch_size,\n        shuffle=False,\n        num_workers=self.num_workers,\n        pin_memory=self.pin_memory,\n    )\n</code></pre>"},{"location":"api/astro_lab.data/#astro_lab.data.AstroDataModule.train_dataloader","title":"train_dataloader","text":"<pre><code>train_dataloader()\n</code></pre> <p>Create training dataloader with optimizations.</p> Source code in <code>src\\astro_lab\\data\\datamodule.py</code> <pre><code>def train_dataloader(self):\n    \"\"\"Create training dataloader with optimizations.\"\"\"\n    if self._main_data is None:\n        self.setup()\n\n    # Create masks for training data\n    train_mask = getattr(self._main_data, \"train_mask\", None)\n    if train_mask is not None:\n        train_data = self._main_data.__class__()\n        for attr in self._main_data.keys():\n            value = getattr(self._main_data, attr)\n            if (\n                isinstance(value, torch.Tensor)\n                and value.dim() &gt; 0\n                and value.size(0) == train_mask.size(0)\n            ):\n                setattr(train_data, attr, value[train_mask])\n            else:\n                setattr(train_data, attr, value)\n    else:\n        train_data = self._main_data\n\n    return DataLoader(\n        [train_data],\n        batch_size=self.batch_size,\n        shuffle=True,\n        num_workers=self.num_workers,\n        pin_memory=self.pin_memory,\n    )\n</code></pre>"},{"location":"api/astro_lab.data/#astro_lab.data.AstroDataModule.val_dataloader","title":"val_dataloader","text":"<pre><code>val_dataloader()\n</code></pre> <p>Create validation dataloader.</p> Source code in <code>src\\astro_lab\\data\\datamodule.py</code> <pre><code>def val_dataloader(self):\n    \"\"\"Create validation dataloader.\"\"\"\n    if self._main_data is None:\n        self.setup()\n\n    # Create masks for validation data\n    val_mask = getattr(self._main_data, \"val_mask\", None)\n    if val_mask is not None:\n        val_data = self._main_data.__class__()\n        for attr in self._main_data.keys():\n            value = getattr(self._main_data, attr)\n            if isinstance(value, torch.Tensor) and value.size(0) == val_mask.size(\n                0\n            ):\n                setattr(val_data, attr, value[val_mask])\n            else:\n                setattr(val_data, attr, value)\n    else:\n        val_data = self._main_data\n\n    return DataLoader(\n        [val_data],\n        batch_size=self.batch_size,\n        shuffle=False,\n        num_workers=self.num_workers,\n        pin_memory=self.pin_memory,\n    )\n</code></pre>"},{"location":"api/astro_lab.data/#astro_lab.data.DataConfig","title":"DataConfig","text":"<p>Centralized data configuration for AstroLab.</p> <p>Manages all data paths, directory structures, and configuration for astronomical data processing and analysis.</p> <p>Methods:</p> Name Description <code>ensure_experiment_directories</code> <p>Create experiment directories only when needed.</p> <code>ensure_results_directories</code> <p>Ensure directories for model results exist.</p> <code>ensure_survey_directories</code> <p>Create directories for a specific survey only when needed.</p> <code>from_yaml</code> <p>Load DataConfig from YAML file.</p> <code>get_catalog_path</code> <p>Get standard catalog path for survey.</p> <code>get_experiment_paths</code> <p>Get all paths for an experiment.</p> <code>get_graph_path</code> <p>Get graph data path for survey.</p> <code>get_results_structure</code> <p>Get organized results directory structure for survey/model.</p> <code>get_survey_processed_dir</code> <p>Get processed directory for specific survey.</p> <code>get_survey_raw_dir</code> <p>Get raw directory for specific survey.</p> <code>get_tensor_path</code> <p>Get tensor data path for survey.</p> <code>setup_directories</code> <p>Create only core data directory structure (no survey templates).</p> <p>Attributes:</p> Name Type Description <code>artifacts_dir</code> <code>Path</code> <p>MLflow artifacts directory.</p> <code>cache_dir</code> <code>Path</code> <p>Cache directory.</p> <code>checkpoints_dir</code> <code>Path</code> <p>Lightning checkpoints directory.</p> <code>configs_dir</code> <code>Path</code> <p>Configuration files directory.</p> <code>experiments_dir</code> <code>Path</code> <p>Experiments directory for MLflow and checkpoints.</p> <code>logs_dir</code> <code>Path</code> <p>Logs directory for training logs.</p> <code>mlruns_dir</code> <code>Path</code> <p>MLflow tracking directory.</p> <code>processed_dir</code> <code>Path</code> <p>Processed data directory.</p> <code>raw_dir</code> <code>Path</code> <p>Raw data directory.</p> <code>results_dir</code> <code>Path</code> <p>Results directory for organized model outputs (in project root, not data/).</p> Source code in <code>src\\astro_lab\\data\\config.py</code> <pre><code>class DataConfig:\n    \"\"\"\n    Centralized data configuration for AstroLab.\n\n    Manages all data paths, directory structures, and configuration\n    for astronomical data processing and analysis.\n    \"\"\"\n\n    def __init__(self, base_dir: Union[str, Path] = \"data\"):\n        self.base_dir = Path(base_dir)\n\n    @property\n    def raw_dir(self) -&gt; Path:\n        \"\"\"Raw data directory.\"\"\"\n        return self.base_dir / \"raw\"\n\n    @property\n    def processed_dir(self) -&gt; Path:\n        \"\"\"Processed data directory.\"\"\"\n        return self.base_dir / \"processed\"\n\n    @property\n    def cache_dir(self) -&gt; Path:\n        \"\"\"Cache directory.\"\"\"\n        return self.base_dir / \"cache\"\n\n    @property\n    def experiments_dir(self) -&gt; Path:\n        \"\"\"Experiments directory for MLflow and checkpoints.\"\"\"\n        return self.base_dir / \"experiments\"\n\n    @property\n    def mlruns_dir(self) -&gt; Path:\n        \"\"\"MLflow tracking directory.\"\"\"\n        return self.experiments_dir / \"mlruns\"\n\n    @property\n    def checkpoints_dir(self) -&gt; Path:\n        \"\"\"Lightning checkpoints directory.\"\"\"\n        return self.experiments_dir / \"checkpoints\"\n\n    @property\n    def results_dir(self) -&gt; Path:\n        \"\"\"Results directory for organized model outputs (in project root, not data/).\"\"\"\n        return Path(\"results\")\n\n    @property\n    def logs_dir(self) -&gt; Path:\n        \"\"\"Logs directory for training logs.\"\"\"\n        return self.experiments_dir / \"logs\"\n\n    @property\n    def configs_dir(self) -&gt; Path:\n        \"\"\"Configuration files directory.\"\"\"\n        return self.base_dir / \"configs\"\n\n    @property\n    def artifacts_dir(self) -&gt; Path:\n        \"\"\"MLflow artifacts directory.\"\"\"\n        return self.experiments_dir / \"artifacts\"\n\n    def get_survey_raw_dir(self, survey: str) -&gt; Path:\n        \"\"\"Get raw directory for specific survey.\"\"\"\n        return self.raw_dir / survey\n\n    def get_survey_processed_dir(self, survey: str) -&gt; Path:\n        \"\"\"Get processed directory for specific survey.\"\"\"\n        return self.processed_dir / survey\n\n    def get_catalog_path(self, survey: str, processed: bool = True) -&gt; Path:\n        \"\"\"Get standard catalog path for survey.\"\"\"\n        if processed:\n            return self.get_survey_processed_dir(survey) / \"catalog.parquet\"\n        else:\n            # Raw catalog naming depends on survey\n            raw_dir = self.get_survey_raw_dir(survey)\n            return raw_dir / f\"{survey}_catalog.parquet\"\n\n    def get_graph_path(self, survey: str, k_neighbors: int = 8) -&gt; Path:\n        \"\"\"Get graph data path for survey.\"\"\"\n        return self.get_survey_processed_dir(survey) / f\"graphs_k{k_neighbors}.pt\"\n\n    def get_tensor_path(self, survey: str) -&gt; Path:\n        \"\"\"Get tensor data path for survey.\"\"\"\n        return self.get_survey_processed_dir(survey) / \"tensors.pt\"\n\n    def setup_directories(self):\n        \"\"\"Create only core data directory structure (no survey templates).\"\"\"\n        # Only create core directories under data/\n        core_dirs = [\n            self.raw_dir,\n            self.processed_dir,\n            self.experiments_dir,\n        ]\n\n        # Create only core directories\n        for dir_path in core_dirs:\n            dir_path.mkdir(parents=True, exist_ok=True)\n\n        logger.info(f\"\ud83d\udcc1 Core data structure created in: {self.base_dir}\")\n\n        # Results directory is separate in project root\n        self.results_dir.mkdir(parents=True, exist_ok=True)\n        logger.info(f\"\ud83d\udcca Results directory created: {self.results_dir}\")\n\n    def ensure_survey_directories(self, survey: str):\n        \"\"\"Create directories for a specific survey only when needed.\"\"\"\n        raw_dir = self.get_survey_raw_dir(survey)\n        processed_dir = self.get_survey_processed_dir(survey)\n\n        # Create only if they don't exist\n        raw_dir.mkdir(parents=True, exist_ok=True)\n        processed_dir.mkdir(parents=True, exist_ok=True)\n\n        logger.info(f\"\ud83d\udcc1 Created directories for {survey} survey\")\n\n    def ensure_experiment_directories(self, experiment_name: str):\n        \"\"\"Create experiment directories only when needed.\"\"\"\n        mlruns_dir = self.mlruns_dir\n        checkpoint_dir = self.checkpoints_dir / experiment_name\n        logs_dir = self.logs_dir / experiment_name\n        artifacts_dir = self.artifacts_dir\n\n        # Create only if they don't exist\n        mlruns_dir.mkdir(parents=True, exist_ok=True)\n        checkpoint_dir.mkdir(parents=True, exist_ok=True)\n        logs_dir.mkdir(parents=True, exist_ok=True)\n        artifacts_dir.mkdir(parents=True, exist_ok=True)\n\n        logger.info(f\"\ud83e\uddea Created experiment directories for {experiment_name}\")\n\n    def get_results_structure(self, survey: str, model_name: str) -&gt; Dict[str, Path]:\n        \"\"\"Get organized results directory structure for survey/model.\"\"\"\n        base_results = self.results_dir / survey / model_name\n\n        return {\n            \"base\": base_results,\n            \"models\": base_results / \"models\",\n            \"plots\": base_results / \"plots\",\n            \"optuna_plots\": base_results / \"plots\" / \"optuna\",\n        }\n\n    def ensure_results_directories(self, survey: str, model_name: str):\n        \"\"\"Ensure directories for model results exist.\"\"\"\n        results_dir = self.results_dir / survey / model_name\n        results_dir.mkdir(parents=True, exist_ok=True)\n\n        # Create subdirectories for different result types\n        subdirs = [\"checkpoints\", \"logs\", \"plots\", \"predictions\"]\n        for subdir in subdirs:\n            (results_dir / subdir).mkdir(exist_ok=True)\n\n        logger.info(f\"\u2705 Results directories ready: {results_dir}\")\n\n        return {\n            \"base\": results_dir,\n            \"checkpoints\": results_dir / \"checkpoints\",\n            \"logs\": results_dir / \"logs\",\n            \"plots\": results_dir / \"plots\",\n            \"predictions\": results_dir / \"predictions\",\n        }\n\n    def get_experiment_paths(self, experiment_name: str) -&gt; Dict[str, Path]:\n        \"\"\"Get all paths for an experiment.\"\"\"\n        return {\n            \"mlruns\": self.mlruns_dir,\n            \"checkpoints\": self.checkpoints_dir / experiment_name,\n            \"artifacts\": self.artifacts_dir,\n            \"logs\": self.logs_dir / experiment_name,\n        }\n\n    @classmethod\n    def from_yaml(cls, yaml_path: str) -&gt; \"DataConfig\":\n        \"\"\"Load DataConfig from YAML file.\"\"\"\n        import yaml\n\n        with open(yaml_path, \"r\") as f:\n            config_dict = yaml.safe_load(f)\n\n        # Extract data config if nested\n        if \"data\" in config_dict:\n            config_dict = config_dict[\"data\"]\n\n        return cls(**config_dict)\n</code></pre>"},{"location":"api/astro_lab.data/#astro_lab.data.DataConfig.artifacts_dir","title":"artifacts_dir  <code>property</code>","text":"<pre><code>artifacts_dir: Path\n</code></pre> <p>MLflow artifacts directory.</p>"},{"location":"api/astro_lab.data/#astro_lab.data.DataConfig.cache_dir","title":"cache_dir  <code>property</code>","text":"<pre><code>cache_dir: Path\n</code></pre> <p>Cache directory.</p>"},{"location":"api/astro_lab.data/#astro_lab.data.DataConfig.checkpoints_dir","title":"checkpoints_dir  <code>property</code>","text":"<pre><code>checkpoints_dir: Path\n</code></pre> <p>Lightning checkpoints directory.</p>"},{"location":"api/astro_lab.data/#astro_lab.data.DataConfig.configs_dir","title":"configs_dir  <code>property</code>","text":"<pre><code>configs_dir: Path\n</code></pre> <p>Configuration files directory.</p>"},{"location":"api/astro_lab.data/#astro_lab.data.DataConfig.experiments_dir","title":"experiments_dir  <code>property</code>","text":"<pre><code>experiments_dir: Path\n</code></pre> <p>Experiments directory for MLflow and checkpoints.</p>"},{"location":"api/astro_lab.data/#astro_lab.data.DataConfig.logs_dir","title":"logs_dir  <code>property</code>","text":"<pre><code>logs_dir: Path\n</code></pre> <p>Logs directory for training logs.</p>"},{"location":"api/astro_lab.data/#astro_lab.data.DataConfig.mlruns_dir","title":"mlruns_dir  <code>property</code>","text":"<pre><code>mlruns_dir: Path\n</code></pre> <p>MLflow tracking directory.</p>"},{"location":"api/astro_lab.data/#astro_lab.data.DataConfig.processed_dir","title":"processed_dir  <code>property</code>","text":"<pre><code>processed_dir: Path\n</code></pre> <p>Processed data directory.</p>"},{"location":"api/astro_lab.data/#astro_lab.data.DataConfig.raw_dir","title":"raw_dir  <code>property</code>","text":"<pre><code>raw_dir: Path\n</code></pre> <p>Raw data directory.</p>"},{"location":"api/astro_lab.data/#astro_lab.data.DataConfig.results_dir","title":"results_dir  <code>property</code>","text":"<pre><code>results_dir: Path\n</code></pre> <p>Results directory for organized model outputs (in project root, not data/).</p>"},{"location":"api/astro_lab.data/#astro_lab.data.DataConfig.ensure_experiment_directories","title":"ensure_experiment_directories","text":"<pre><code>ensure_experiment_directories(experiment_name: str)\n</code></pre> <p>Create experiment directories only when needed.</p> Source code in <code>src\\astro_lab\\data\\config.py</code> <pre><code>def ensure_experiment_directories(self, experiment_name: str):\n    \"\"\"Create experiment directories only when needed.\"\"\"\n    mlruns_dir = self.mlruns_dir\n    checkpoint_dir = self.checkpoints_dir / experiment_name\n    logs_dir = self.logs_dir / experiment_name\n    artifacts_dir = self.artifacts_dir\n\n    # Create only if they don't exist\n    mlruns_dir.mkdir(parents=True, exist_ok=True)\n    checkpoint_dir.mkdir(parents=True, exist_ok=True)\n    logs_dir.mkdir(parents=True, exist_ok=True)\n    artifacts_dir.mkdir(parents=True, exist_ok=True)\n\n    logger.info(f\"\ud83e\uddea Created experiment directories for {experiment_name}\")\n</code></pre>"},{"location":"api/astro_lab.data/#astro_lab.data.DataConfig.ensure_results_directories","title":"ensure_results_directories","text":"<pre><code>ensure_results_directories(survey: str, model_name: str)\n</code></pre> <p>Ensure directories for model results exist.</p> Source code in <code>src\\astro_lab\\data\\config.py</code> <pre><code>def ensure_results_directories(self, survey: str, model_name: str):\n    \"\"\"Ensure directories for model results exist.\"\"\"\n    results_dir = self.results_dir / survey / model_name\n    results_dir.mkdir(parents=True, exist_ok=True)\n\n    # Create subdirectories for different result types\n    subdirs = [\"checkpoints\", \"logs\", \"plots\", \"predictions\"]\n    for subdir in subdirs:\n        (results_dir / subdir).mkdir(exist_ok=True)\n\n    logger.info(f\"\u2705 Results directories ready: {results_dir}\")\n\n    return {\n        \"base\": results_dir,\n        \"checkpoints\": results_dir / \"checkpoints\",\n        \"logs\": results_dir / \"logs\",\n        \"plots\": results_dir / \"plots\",\n        \"predictions\": results_dir / \"predictions\",\n    }\n</code></pre>"},{"location":"api/astro_lab.data/#astro_lab.data.DataConfig.ensure_survey_directories","title":"ensure_survey_directories","text":"<pre><code>ensure_survey_directories(survey: str)\n</code></pre> <p>Create directories for a specific survey only when needed.</p> Source code in <code>src\\astro_lab\\data\\config.py</code> <pre><code>def ensure_survey_directories(self, survey: str):\n    \"\"\"Create directories for a specific survey only when needed.\"\"\"\n    raw_dir = self.get_survey_raw_dir(survey)\n    processed_dir = self.get_survey_processed_dir(survey)\n\n    # Create only if they don't exist\n    raw_dir.mkdir(parents=True, exist_ok=True)\n    processed_dir.mkdir(parents=True, exist_ok=True)\n\n    logger.info(f\"\ud83d\udcc1 Created directories for {survey} survey\")\n</code></pre>"},{"location":"api/astro_lab.data/#astro_lab.data.DataConfig.from_yaml","title":"from_yaml  <code>classmethod</code>","text":"<pre><code>from_yaml(yaml_path: str) -&gt; DataConfig\n</code></pre> <p>Load DataConfig from YAML file.</p> Source code in <code>src\\astro_lab\\data\\config.py</code> <pre><code>@classmethod\ndef from_yaml(cls, yaml_path: str) -&gt; \"DataConfig\":\n    \"\"\"Load DataConfig from YAML file.\"\"\"\n    import yaml\n\n    with open(yaml_path, \"r\") as f:\n        config_dict = yaml.safe_load(f)\n\n    # Extract data config if nested\n    if \"data\" in config_dict:\n        config_dict = config_dict[\"data\"]\n\n    return cls(**config_dict)\n</code></pre>"},{"location":"api/astro_lab.data/#astro_lab.data.DataConfig.get_catalog_path","title":"get_catalog_path","text":"<pre><code>get_catalog_path(survey: str, processed: bool = True) -&gt; Path\n</code></pre> <p>Get standard catalog path for survey.</p> Source code in <code>src\\astro_lab\\data\\config.py</code> <pre><code>def get_catalog_path(self, survey: str, processed: bool = True) -&gt; Path:\n    \"\"\"Get standard catalog path for survey.\"\"\"\n    if processed:\n        return self.get_survey_processed_dir(survey) / \"catalog.parquet\"\n    else:\n        # Raw catalog naming depends on survey\n        raw_dir = self.get_survey_raw_dir(survey)\n        return raw_dir / f\"{survey}_catalog.parquet\"\n</code></pre>"},{"location":"api/astro_lab.data/#astro_lab.data.DataConfig.get_experiment_paths","title":"get_experiment_paths","text":"<pre><code>get_experiment_paths(experiment_name: str) -&gt; Dict[str, Path]\n</code></pre> <p>Get all paths for an experiment.</p> Source code in <code>src\\astro_lab\\data\\config.py</code> <pre><code>def get_experiment_paths(self, experiment_name: str) -&gt; Dict[str, Path]:\n    \"\"\"Get all paths for an experiment.\"\"\"\n    return {\n        \"mlruns\": self.mlruns_dir,\n        \"checkpoints\": self.checkpoints_dir / experiment_name,\n        \"artifacts\": self.artifacts_dir,\n        \"logs\": self.logs_dir / experiment_name,\n    }\n</code></pre>"},{"location":"api/astro_lab.data/#astro_lab.data.DataConfig.get_graph_path","title":"get_graph_path","text":"<pre><code>get_graph_path(survey: str, k_neighbors: int = 8) -&gt; Path\n</code></pre> <p>Get graph data path for survey.</p> Source code in <code>src\\astro_lab\\data\\config.py</code> <pre><code>def get_graph_path(self, survey: str, k_neighbors: int = 8) -&gt; Path:\n    \"\"\"Get graph data path for survey.\"\"\"\n    return self.get_survey_processed_dir(survey) / f\"graphs_k{k_neighbors}.pt\"\n</code></pre>"},{"location":"api/astro_lab.data/#astro_lab.data.DataConfig.get_results_structure","title":"get_results_structure","text":"<pre><code>get_results_structure(survey: str, model_name: str) -&gt; Dict[str, Path]\n</code></pre> <p>Get organized results directory structure for survey/model.</p> Source code in <code>src\\astro_lab\\data\\config.py</code> <pre><code>def get_results_structure(self, survey: str, model_name: str) -&gt; Dict[str, Path]:\n    \"\"\"Get organized results directory structure for survey/model.\"\"\"\n    base_results = self.results_dir / survey / model_name\n\n    return {\n        \"base\": base_results,\n        \"models\": base_results / \"models\",\n        \"plots\": base_results / \"plots\",\n        \"optuna_plots\": base_results / \"plots\" / \"optuna\",\n    }\n</code></pre>"},{"location":"api/astro_lab.data/#astro_lab.data.DataConfig.get_survey_processed_dir","title":"get_survey_processed_dir","text":"<pre><code>get_survey_processed_dir(survey: str) -&gt; Path\n</code></pre> <p>Get processed directory for specific survey.</p> Source code in <code>src\\astro_lab\\data\\config.py</code> <pre><code>def get_survey_processed_dir(self, survey: str) -&gt; Path:\n    \"\"\"Get processed directory for specific survey.\"\"\"\n    return self.processed_dir / survey\n</code></pre>"},{"location":"api/astro_lab.data/#astro_lab.data.DataConfig.get_survey_raw_dir","title":"get_survey_raw_dir","text":"<pre><code>get_survey_raw_dir(survey: str) -&gt; Path\n</code></pre> <p>Get raw directory for specific survey.</p> Source code in <code>src\\astro_lab\\data\\config.py</code> <pre><code>def get_survey_raw_dir(self, survey: str) -&gt; Path:\n    \"\"\"Get raw directory for specific survey.\"\"\"\n    return self.raw_dir / survey\n</code></pre>"},{"location":"api/astro_lab.data/#astro_lab.data.DataConfig.get_tensor_path","title":"get_tensor_path","text":"<pre><code>get_tensor_path(survey: str) -&gt; Path\n</code></pre> <p>Get tensor data path for survey.</p> Source code in <code>src\\astro_lab\\data\\config.py</code> <pre><code>def get_tensor_path(self, survey: str) -&gt; Path:\n    \"\"\"Get tensor data path for survey.\"\"\"\n    return self.get_survey_processed_dir(survey) / \"tensors.pt\"\n</code></pre>"},{"location":"api/astro_lab.data/#astro_lab.data.DataConfig.setup_directories","title":"setup_directories","text":"<pre><code>setup_directories()\n</code></pre> <p>Create only core data directory structure (no survey templates).</p> Source code in <code>src\\astro_lab\\data\\config.py</code> <pre><code>def setup_directories(self):\n    \"\"\"Create only core data directory structure (no survey templates).\"\"\"\n    # Only create core directories under data/\n    core_dirs = [\n        self.raw_dir,\n        self.processed_dir,\n        self.experiments_dir,\n    ]\n\n    # Create only core directories\n    for dir_path in core_dirs:\n        dir_path.mkdir(parents=True, exist_ok=True)\n\n    logger.info(f\"\ud83d\udcc1 Core data structure created in: {self.base_dir}\")\n\n    # Results directory is separate in project root\n    self.results_dir.mkdir(parents=True, exist_ok=True)\n    logger.info(f\"\ud83d\udcca Results directory created: {self.results_dir}\")\n</code></pre>"},{"location":"api/astro_lab.data/#astro_lab.data.check_astroquery_available","title":"check_astroquery_available","text":"<pre><code>check_astroquery_available() -&gt; bool\n</code></pre> <p>Check if astroquery is available.</p> Source code in <code>src\\astro_lab\\data\\utils.py</code> <pre><code>def check_astroquery_available() -&gt; bool:\n    \"\"\"Check if astroquery is available.\"\"\"\n    try:\n        import astroquery\n\n        return True\n    except ImportError:\n        return False\n</code></pre>"},{"location":"api/astro_lab.data/#astro_lab.data.create_astro_datamodule","title":"create_astro_datamodule","text":"<pre><code>create_astro_datamodule(survey: str, **kwargs) -&gt; AstroDataModule\n</code></pre> <p>Create AstroDataModule for given survey.</p> Source code in <code>src\\astro_lab\\data\\__init__.py</code> <pre><code>def create_astro_datamodule(survey: str, **kwargs) -&gt; AstroDataModule:\n    \"\"\"Create AstroDataModule for given survey.\"\"\"\n    return AstroDataModule(survey=survey, **kwargs)\n</code></pre>"},{"location":"api/astro_lab.data/#astro_lab.data.create_survey_tensordict","title":"create_survey_tensordict","text":"<pre><code>create_survey_tensordict(df: DataFrame, survey: str) -&gt; SurveyTensorDict\n</code></pre> <p>Create SurveyTensorDict from processed DataFrame.</p> Source code in <code>src\\astro_lab\\data\\processors.py</code> <pre><code>def create_survey_tensordict(df: pl.DataFrame, survey: str) -&gt; SurveyTensorDict:\n    \"\"\"Create SurveyTensorDict from processed DataFrame.\"\"\"\n    logger.info(f\"Creating TensorDict for {survey} with {len(df)} objects\")\n\n    # Extract spatial coordinates\n    if \"ra_deg\" in df.columns and \"dec_deg\" in df.columns:\n        # Convert to 3D coordinates for spatial consistency\n        ra_rad = np.radians(df[\"ra_deg\"].to_numpy())\n        dec_rad = np.radians(df[\"dec_deg\"].to_numpy())\n\n        # Unit sphere coordinates\n        x = np.cos(dec_rad) * np.cos(ra_rad)\n        y = np.cos(dec_rad) * np.sin(ra_rad)\n        z = np.sin(dec_rad)\n\n        spatial_data = SpatialTensorDict(\n            {\n                \"x\": torch.tensor(x, dtype=torch.float32),\n                \"y\": torch.tensor(y, dtype=torch.float32),\n                \"z\": torch.tensor(z, dtype=torch.float32),\n            }\n        )\n    else:\n        # Default spatial data\n        n_objects = len(df)\n        spatial_data = SpatialTensorDict(\n            {\n                \"x\": torch.zeros(n_objects, dtype=torch.float32),\n                \"y\": torch.zeros(n_objects, dtype=torch.float32),\n                \"z\": torch.zeros(n_objects, dtype=torch.float32),\n            }\n        )\n\n    # Extract photometric data\n    mag_columns = [col for col in df.columns if \"mag\" in col.lower() and \"_\" not in col]\n    color_columns = [\n        col\n        for col in df.columns\n        if \"_\" in col and any(c in col for c in [\"g\", \"r\", \"i\", \"bp\", \"rp\"])\n    ]\n\n    photometric_features = {}\n    for col in mag_columns:\n        photometric_features[col] = torch.tensor(\n            df[col].to_numpy(), dtype=torch.float32\n        )\n\n    for col in color_columns:\n        photometric_features[col] = torch.tensor(\n            df[col].to_numpy(), dtype=torch.float32\n        )\n\n    photometric_data = (\n        PhotometricTensorDict(photometric_features) if photometric_features else None\n    )\n\n    # Create SurveyTensorDict\n    survey_dict = {\n        \"spatial\": spatial_data,\n        \"survey_name\": survey,\n        \"num_objects\": len(df),\n    }\n\n    if photometric_data:\n        survey_dict[\"photometric\"] = photometric_data\n\n    return SurveyTensorDict(survey_dict)\n</code></pre>"},{"location":"api/astro_lab.data/#astro_lab.data.create_training_splits","title":"create_training_splits","text":"<pre><code>create_training_splits(\n    df: DataFrame,\n    train_ratio: float = 0.7,\n    val_ratio: float = 0.15,\n    test_ratio: float = 0.15,\n    seed: int = 42,\n) -&gt; Tuple[DataFrame, DataFrame, DataFrame]\n</code></pre> <p>Create train/validation/test splits.</p> Source code in <code>src\\astro_lab\\data\\processors.py</code> <pre><code>def create_training_splits(\n    df: pl.DataFrame,\n    train_ratio: float = 0.7,\n    val_ratio: float = 0.15,\n    test_ratio: float = 0.15,\n    seed: int = 42,\n) -&gt; Tuple[pl.DataFrame, pl.DataFrame, pl.DataFrame]:\n    \"\"\"Create train/validation/test splits.\"\"\"\n    assert abs(train_ratio + val_ratio + test_ratio - 1.0) &lt; 1e-6, (\n        \"Ratios must sum to 1.0\"\n    )\n\n    n_total = len(df)\n    n_train = int(n_total * train_ratio)\n    n_val = int(n_total * val_ratio)\n\n    # Shuffle data\n    df_shuffled = df.sample(fraction=1.0, seed=seed)\n\n    # Create splits\n    train_df = df_shuffled.slice(0, n_train)\n    val_df = df_shuffled.slice(n_train, n_val)\n    test_df = df_shuffled.slice(n_train + n_val)\n\n    logger.info(\n        f\"Created splits: train={len(train_df)}, val={len(val_df)}, test={len(test_df)}\"\n    )\n\n    return train_df, val_df, test_df\n</code></pre>"},{"location":"api/astro_lab.data/#astro_lab.data.detect_survey_type","title":"detect_survey_type","text":"<pre><code>detect_survey_type(df: DataFrame) -&gt; str\n</code></pre> <p>Detect survey type from DataFrame columns.</p> Source code in <code>src\\astro_lab\\data\\utils.py</code> <pre><code>def detect_survey_type(df: pl.DataFrame) -&gt; str:\n    \"\"\"Detect survey type from DataFrame columns.\"\"\"\n    columns = [col.lower() for col in df.columns]\n\n    # GAIA indicators\n    gaia_cols = [\n        \"source_id\",\n        \"phot_g_mean_mag\",\n        \"phot_bp_mean_mag\",\n        \"phot_rp_mean_mag\",\n        \"parallax\",\n    ]\n    if any(col in columns for col in gaia_cols):\n        return \"gaia\"\n\n    # SDSS indicators\n    sdss_cols = [\n        \"objid\",\n        \"modelmag_u\",\n        \"modelmag_g\",\n        \"modelmag_r\",\n        \"modelmag_i\",\n        \"modelmag_z\",\n    ]\n    if any(col in columns for col in sdss_cols):\n        return \"sdss\"\n\n    # NSA indicators\n    nsa_cols = [\"nsaid\", \"sersic_n\", \"sersic_th50\"]\n    if any(col in columns for col in nsa_cols):\n        return \"nsa\"\n\n    # LINEAR indicators\n    linear_cols = [\"period\", \"amplitude\", \"linear_id\"]\n    if any(col in columns for col in linear_cols):\n        return \"linear\"\n\n    return \"generic\"\n</code></pre>"},{"location":"api/astro_lab.data/#astro_lab.data.download_2mass","title":"download_2mass","text":"<pre><code>download_2mass(region: str = 'all_sky', magnitude_limit: float = 15.0) -&gt; Path\n</code></pre> <p>Download 2MASS catalog.</p> Source code in <code>src\\astro_lab\\data\\loaders.py</code> <pre><code>def download_2mass(region: str = \"all_sky\", magnitude_limit: float = 15.0) -&gt; Path:\n    \"\"\"Download 2MASS catalog.\"\"\"\n    logger.info(f\"Downloading 2MASS data for region: {region}\")\n    output_path = (\n        data_config.raw_dir / \"2mass\" / f\"2mass_{region}_{magnitude_limit}.fits\"\n    )\n    output_path.parent.mkdir(parents=True, exist_ok=True)\n    return output_path\n</code></pre>"},{"location":"api/astro_lab.data/#astro_lab.data.download_pan_starrs","title":"download_pan_starrs","text":"<pre><code>download_pan_starrs(region: str = 'all_sky', magnitude_limit: float = 15.0) -&gt; Path\n</code></pre> <p>Download Pan-STARRS catalog.</p> Source code in <code>src\\astro_lab\\data\\loaders.py</code> <pre><code>def download_pan_starrs(region: str = \"all_sky\", magnitude_limit: float = 15.0) -&gt; Path:\n    \"\"\"Download Pan-STARRS catalog.\"\"\"\n    logger.info(f\"Downloading Pan-STARRS data for region: {region}\")\n    output_path = (\n        data_config.raw_dir\n        / \"pan_starrs\"\n        / f\"pan_starrs_{region}_{magnitude_limit}.fits\"\n    )\n    output_path.parent.mkdir(parents=True, exist_ok=True)\n    return output_path\n</code></pre>"},{"location":"api/astro_lab.data/#astro_lab.data.download_sdss","title":"download_sdss","text":"<pre><code>download_sdss(region: str = 'all_sky', magnitude_limit: float = 15.0) -&gt; Path\n</code></pre> <p>Download SDSS catalog.</p> Source code in <code>src\\astro_lab\\data\\loaders.py</code> <pre><code>def download_sdss(region: str = \"all_sky\", magnitude_limit: float = 15.0) -&gt; Path:\n    \"\"\"Download SDSS catalog.\"\"\"\n    logger.info(f\"Downloading SDSS data for region: {region}\")\n    output_path = data_config.raw_dir / \"sdss\" / f\"sdss_{region}_{magnitude_limit}.fits\"\n    output_path.parent.mkdir(parents=True, exist_ok=True)\n    return output_path\n</code></pre>"},{"location":"api/astro_lab.data/#astro_lab.data.download_survey","title":"download_survey","text":"<pre><code>download_survey(survey: str, **kwargs) -&gt; Path\n</code></pre> <p>Download data for a specific survey.</p> Source code in <code>src\\astro_lab\\data\\loaders.py</code> <pre><code>def download_survey(survey: str, **kwargs) -&gt; Path:\n    \"\"\"Download data for a specific survey.\"\"\"\n    if survey == \"gaia\":\n        return download_gaia(**kwargs)\n    elif survey == \"sdss\":\n        return download_sdss(**kwargs)\n    elif survey == \"2mass\":\n        return download_2mass(**kwargs)\n    elif survey == \"wise\":\n        return download_wise(**kwargs)\n    elif survey == \"pan_starrs\":\n        return download_pan_starrs(**kwargs)\n    else:\n        raise ValueError(f\"Unsupported survey: {survey}\")\n</code></pre>"},{"location":"api/astro_lab.data/#astro_lab.data.download_wise","title":"download_wise","text":"<pre><code>download_wise(region: str = 'all_sky', magnitude_limit: float = 15.0) -&gt; Path\n</code></pre> <p>Download WISE catalog.</p> Source code in <code>src\\astro_lab\\data\\loaders.py</code> <pre><code>def download_wise(region: str = \"all_sky\", magnitude_limit: float = 15.0) -&gt; Path:\n    \"\"\"Download WISE catalog.\"\"\"\n    logger.info(f\"Downloading WISE data for region: {region}\")\n    output_path = data_config.raw_dir / \"wise\" / f\"wise_{region}_{magnitude_limit}.fits\"\n    output_path.parent.mkdir(parents=True, exist_ok=True)\n    return output_path\n</code></pre>"},{"location":"api/astro_lab.data/#astro_lab.data.get_data_dir","title":"get_data_dir","text":"<pre><code>get_data_dir() -&gt; Path\n</code></pre> <p>Get the configured data directory.</p> Source code in <code>src\\astro_lab\\data\\config.py</code> <pre><code>def get_data_dir() -&gt; Path:\n    \"\"\"Get the configured data directory.\"\"\"\n    return data_config.base_dir\n</code></pre>"},{"location":"api/astro_lab.data/#astro_lab.data.get_data_statistics","title":"get_data_statistics","text":"<pre><code>get_data_statistics(df: DataFrame) -&gt; Dict[str, Any]\n</code></pre> <p>Get comprehensive statistics for a DataFrame.</p> Source code in <code>src\\astro_lab\\data\\utils.py</code> <pre><code>def get_data_statistics(df: pl.DataFrame) -&gt; Dict[str, Any]:\n    \"\"\"Get comprehensive statistics for a DataFrame.\"\"\"\n    stats = {\n        \"n_rows\": len(df),\n        \"n_columns\": len(df.columns),\n        \"memory_usage_mb\": df.estimated_size() / (1024 * 1024),\n        \"columns\": df.columns,\n        \"dtypes\": {col: str(dtype) for col, dtype in df.schema.items()},\n    }\n\n    # Detect magnitude columns\n    mag_cols = _detect_magnitude_columns(df)\n    if mag_cols:\n        stats[\"magnitude_columns\"] = mag_cols\n        for col in mag_cols:\n            if col in df.columns:\n                col_stats = df[col].describe()\n                stats[f\"{col}_stats\"] = {\n                    \"mean\": col_stats[\"mean\"],\n                    \"std\": col_stats[\"std\"],\n                    \"min\": col_stats[\"min\"],\n                    \"max\": col_stats[\"max\"],\n                    \"null_count\": df[col].null_count(),\n                }\n\n    # Detect coordinate columns\n    coord_cols = [\n        col for col in df.columns if col.lower() in [\"ra\", \"dec\", \"x\", \"y\", \"z\"]\n    ]\n    if coord_cols:\n        stats[\"coordinate_columns\"] = coord_cols\n\n    return stats\n</code></pre>"},{"location":"api/astro_lab.data/#astro_lab.data.get_fits_info","title":"get_fits_info","text":"<pre><code>get_fits_info(fits_path: Union[str, Path]) -&gt; Dict[str, Any]\n</code></pre> <p>Get information about FITS file structure.</p> Source code in <code>src\\astro_lab\\data\\utils.py</code> <pre><code>def get_fits_info(fits_path: Union[str, Path]) -&gt; Dict[str, Any]:\n    \"\"\"Get information about FITS file structure.\"\"\"\n    fits_path = Path(fits_path)\n\n    if not fits_path.exists():\n        raise FileNotFoundError(f\"FITS file not found: {fits_path}\")\n\n    with fits.open(fits_path) as hdul:\n        info = {\n            \"filename\": fits_path.name,\n            \"file_size_mb\": fits_path.stat().st_size / (1024 * 1024),\n            \"n_hdus\": len(hdul),\n            \"hdus\": [],\n        }\n\n        for i, hdu in enumerate(hdul):\n            hdu_info = {\n                \"index\": i,\n                \"type\": type(hdu).__name__,\n                \"name\": hdu.name,\n            }\n\n            if hasattr(hdu, \"data\") and hdu.data is not None:\n                if hasattr(hdu.data, \"shape\"):\n                    hdu_info[\"shape\"] = hdu.data.shape\n                if hasattr(hdu.data, \"dtype\"):\n                    hdu_info[\"dtype\"] = str(hdu.data.dtype)\n                if hasattr(hdu.data, \"names\") and hdu.data.names:\n                    hdu_info[\"columns\"] = list(hdu.data.names)\n                    hdu_info[\"n_columns\"] = len(hdu.data.names)\n\n            info[\"hdus\"].append(hdu_info)\n\n    return info\n</code></pre>"},{"location":"api/astro_lab.data/#astro_lab.data.get_processed_dir","title":"get_processed_dir","text":"<pre><code>get_processed_dir() -&gt; Path\n</code></pre> <p>Get the processed data directory.</p> Source code in <code>src\\astro_lab\\data\\config.py</code> <pre><code>def get_processed_dir() -&gt; Path:\n    \"\"\"Get the processed data directory.\"\"\"\n    return data_config.processed_dir\n</code></pre>"},{"location":"api/astro_lab.data/#astro_lab.data.get_raw_dir","title":"get_raw_dir","text":"<pre><code>get_raw_dir() -&gt; Path\n</code></pre> <p>Get the raw data directory.</p> Source code in <code>src\\astro_lab\\data\\config.py</code> <pre><code>def get_raw_dir() -&gt; Path:\n    \"\"\"Get the raw data directory.\"\"\"\n    return data_config.raw_dir\n</code></pre>"},{"location":"api/astro_lab.data/#astro_lab.data.get_survey_paths","title":"get_survey_paths","text":"<pre><code>get_survey_paths(survey: str) -&gt; Dict[str, Path]\n</code></pre> <p>Get all standard paths for a survey.</p> Source code in <code>src\\astro_lab\\data\\config.py</code> <pre><code>def get_survey_paths(survey: str) -&gt; Dict[str, Path]:\n    \"\"\"Get all standard paths for a survey.\"\"\"\n    return {\n        \"raw_dir\": data_config.get_survey_raw_dir(survey),\n        \"processed_dir\": data_config.get_survey_processed_dir(survey),\n        \"catalog\": data_config.get_catalog_path(survey),\n        \"graphs\": data_config.get_graph_path(survey),\n        \"tensors\": data_config.get_tensor_path(survey),\n    }\n</code></pre>"},{"location":"api/astro_lab.data/#astro_lab.data.import_fits","title":"import_fits","text":"<pre><code>import_fits(fits_file: Union[str, Path], catalog_name: str) -&gt; Path\n</code></pre> <p>Import FITS file to processed data directory.</p> Source code in <code>src\\astro_lab\\data\\loaders.py</code> <pre><code>def import_fits(fits_file: Union[str, Path], catalog_name: str) -&gt; Path:\n    \"\"\"Import FITS file to processed data directory.\"\"\"\n    fits_path = Path(fits_file)\n\n    if not fits_path.exists():\n        raise FileNotFoundError(f\"FITS file not found: {fits_path}\")\n\n    output_path = data_config.processed_dir / f\"{catalog_name}.parquet\"\n    output_path.parent.mkdir(parents=True, exist_ok=True)\n\n    # Load and convert to Parquet\n    df = load_fits_optimized(fits_path)\n    df.write_parquet(output_path)\n\n    logger.info(f\"Imported {fits_path} -&gt; {output_path}\")\n    return output_path\n</code></pre>"},{"location":"api/astro_lab.data/#astro_lab.data.import_tng50","title":"import_tng50","text":"<pre><code>import_tng50(hdf5_file: Union[str, Path], dataset_name: str = 'PartType0') -&gt; Path\n</code></pre> <p>Import TNG50 HDF5 file.</p> Source code in <code>src\\astro_lab\\data\\loaders.py</code> <pre><code>def import_tng50(hdf5_file: Union[str, Path], dataset_name: str = \"PartType0\") -&gt; Path:\n    \"\"\"Import TNG50 HDF5 file.\"\"\"\n    import h5py\n\n    hdf5_path = Path(hdf5_file)\n\n    if not hdf5_path.exists():\n        raise FileNotFoundError(f\"HDF5 file not found: {hdf5_path}\")\n\n    output_path = data_config.processed_dir / \"tng50\" / f\"{dataset_name}.parquet\"\n    output_path.parent.mkdir(parents=True, exist_ok=True)\n\n    logger.info(f\"Imported {hdf5_path} -&gt; {output_path}\")\n    return output_path\n</code></pre>"},{"location":"api/astro_lab.data/#astro_lab.data.list_available_catalogs","title":"list_available_catalogs","text":"<pre><code>list_available_catalogs(survey: Optional[str] = None) -&gt; DataFrame\n</code></pre> <p>List all available catalogs.</p> Source code in <code>src\\astro_lab\\data\\loaders.py</code> <pre><code>def list_available_catalogs(survey: Optional[str] = None) -&gt; pl.DataFrame:\n    \"\"\"List all available catalogs.\"\"\"\n    catalogs = []\n\n    search_dirs = [data_config.raw_dir, data_config.processed_dir]\n    if survey:\n        search_dirs = [d / survey for d in search_dirs if (d / survey).exists()]\n\n    for search_dir in search_dirs:\n        if not search_dir.exists():\n            continue\n\n        for file_path in search_dir.rglob(\"*.fits\"):\n            catalogs.append(\n                {\n                    \"name\": file_path.stem,\n                    \"path\": str(file_path),\n                    \"format\": \"fits\",\n                    \"size_mb\": file_path.stat().st_size / 1024 / 1024,\n                    \"survey\": file_path.parent.name,\n                }\n            )\n\n        for file_path in search_dir.rglob(\"*.parquet\"):\n            catalogs.append(\n                {\n                    \"name\": file_path.stem,\n                    \"path\": str(file_path),\n                    \"format\": \"parquet\",\n                    \"size_mb\": file_path.stat().st_size / 1024 / 1024,\n                    \"survey\": file_path.parent.name,\n                }\n            )\n\n    return pl.DataFrame(catalogs)\n</code></pre>"},{"location":"api/astro_lab.data/#astro_lab.data.load_catalog","title":"load_catalog","text":"<pre><code>load_catalog(catalog_path: Union[str, Path]) -&gt; DataFrame\n</code></pre> <p>Load a catalog from various formats (FITS, Parquet, CSV).</p> Source code in <code>src\\astro_lab\\data\\loaders.py</code> <pre><code>def load_catalog(catalog_path: Union[str, Path]) -&gt; pl.DataFrame:\n    \"\"\"Load a catalog from various formats (FITS, Parquet, CSV).\"\"\"\n    path = Path(catalog_path)\n\n    if not path.exists():\n        raise FileNotFoundError(f\"Catalog not found: {path}\")\n\n    logger.info(f\"Loading catalog from {path}\")\n\n    if path.suffix.lower() == \".fits\":\n        return load_fits_optimized(path)\n    elif path.suffix.lower() == \".parquet\":\n        return pl.read_parquet(path)\n    elif path.suffix.lower() == \".csv\":\n        return pl.read_csv(path)\n    else:\n        raise ValueError(f\"Unsupported file format: {path.suffix}\")\n</code></pre>"},{"location":"api/astro_lab.data/#astro_lab.data.load_fits_optimized","title":"load_fits_optimized","text":"<pre><code>load_fits_optimized(\n    fits_path: Union[str, Path],\n    hdu_index: int = 0,\n    memmap: bool = True,\n    max_memory_mb: float = 1000.0,\n) -&gt; DataFrame\n</code></pre> <p>Load FITS file optimized for Polars DataFrame output.</p> Source code in <code>src\\astro_lab\\data\\utils.py</code> <pre><code>def load_fits_optimized(\n    fits_path: Union[str, Path],\n    hdu_index: int = 0,\n    memmap: bool = True,\n    max_memory_mb: float = 1000.0,\n) -&gt; pl.DataFrame:\n    \"\"\"Load FITS file optimized for Polars DataFrame output.\"\"\"\n    try:\n        fits_path = Path(fits_path)\n        if not fits_path.exists():\n            raise FileNotFoundError(f\"FITS file not found: {fits_path}\")\n\n        # Check file size\n        file_size_mb = fits_path.stat().st_size / (1024 * 1024)\n        if file_size_mb &gt; max_memory_mb:\n            logger.warning(f\"Large FITS file: {file_size_mb:.1f} MB\")\n\n        # Load FITS file\n        with fits.open(fits_path, memmap=memmap) as hdul:\n            hdu = hdul[hdu_index]\n\n            if not hasattr(hdu, \"data\") or hdu.data is None:\n                raise ValueError(f\"No data in HDU {hdu_index}\")\n\n            # Convert to astropy Table first\n            table = Table(hdu.data)\n\n            # Filter out multidimensional columns\n            names = [\n                name\n                for name in table.colnames\n                if hasattr(table[name], \"shape\") and len(table[name].shape) &lt;= 1\n            ]\n\n            if len(names) &lt; len(table.colnames):\n                filtered_cols = set(table.colnames) - set(names)\n                logger.info(\n                    f\"Filtered out {len(filtered_cols)} multidimensional columns\"\n                )\n\n            filtered_table = table[names]\n            df_pandas = filtered_table.to_pandas()\n            return pl.from_pandas(df_pandas)\n\n    except Exception as e:\n        logger.error(f\"Error loading FITS file: {e}\")\n        raise\n</code></pre>"},{"location":"api/astro_lab.data/#astro_lab.data.load_fits_table_optimized","title":"load_fits_table_optimized","text":"<pre><code>load_fits_table_optimized(\n    fits_path: Union[str, Path],\n    hdu_index: int = 1,\n    columns: Optional[List[str]] = None,\n    max_rows: Optional[int] = None,\n) -&gt; DataFrame\n</code></pre> <p>Load FITS table with optimization.</p> Source code in <code>src\\astro_lab\\data\\utils.py</code> <pre><code>def load_fits_table_optimized(\n    fits_path: Union[str, Path],\n    hdu_index: int = 1,\n    columns: Optional[List[str]] = None,\n    max_rows: Optional[int] = None,\n) -&gt; pl.DataFrame:\n    \"\"\"Load FITS table with optimization.\"\"\"\n    df = load_fits_optimized(fits_path, hdu_index)\n\n    if columns:\n        available_cols = [col for col in columns if col in df.columns]\n        if available_cols:\n            df = df.select(available_cols)\n\n    if max_rows and len(df) &gt; max_rows:\n        df = df.head(max_rows)\n\n    return df\n</code></pre>"},{"location":"api/astro_lab.data/#astro_lab.data.load_splits_from_parquet","title":"load_splits_from_parquet","text":"<pre><code>load_splits_from_parquet(\n    base_path: Union[str, Path], dataset_name: str\n) -&gt; Tuple[DataFrame, DataFrame, DataFrame]\n</code></pre> <p>Load train/val/test splits from Parquet files.</p> Source code in <code>src\\astro_lab\\data\\utils.py</code> <pre><code>def load_splits_from_parquet(\n    base_path: Union[str, Path], dataset_name: str\n) -&gt; Tuple[pl.DataFrame, pl.DataFrame, pl.DataFrame]:\n    \"\"\"Load train/val/test splits from Parquet files.\"\"\"\n    base_path = Path(base_path)\n\n    train_path = base_path / f\"{dataset_name}_train.parquet\"\n    val_path = base_path / f\"{dataset_name}_val.parquet\"\n    test_path = base_path / f\"{dataset_name}_test.parquet\"\n\n    for path in [train_path, val_path, test_path]:\n        if not path.exists():\n            raise FileNotFoundError(f\"Split file not found: {path}\")\n\n    df_train = pl.read_parquet(train_path)\n    df_val = pl.read_parquet(val_path)\n    df_test = pl.read_parquet(test_path)\n\n    logger.info(\n        f\"Loaded splits: train={len(df_train)}, val={len(df_val)}, test={len(df_test)}\"\n    )\n\n    return df_train, df_val, df_test\n</code></pre>"},{"location":"api/astro_lab.data/#astro_lab.data.load_survey_catalog","title":"load_survey_catalog","text":"<pre><code>load_survey_catalog(\n    survey: str,\n    max_samples: Optional[int] = None,\n    load_processed: bool = True,\n    **kwargs\n) -&gt; DataFrame\n</code></pre> <p>Load catalog data for a specific survey.</p>"},{"location":"api/astro_lab.data/#astro_lab.data.load_survey_catalog--parameters","title":"Parameters","text":"<p>survey : str     Survey name (gaia, sdss, 2mass, wise, pan_starrs) max_samples : int, optional     Maximum number of samples to load load_processed : bool     Whether to load processed version if available</p>"},{"location":"api/astro_lab.data/#astro_lab.data.load_survey_catalog--returns","title":"Returns","text":"<p>pl.DataFrame     Survey catalog data</p> Source code in <code>src\\astro_lab\\data\\loaders.py</code> <pre><code>def load_survey_catalog(\n    survey: str,\n    max_samples: Optional[int] = None,\n    load_processed: bool = True,\n    **kwargs,\n) -&gt; pl.DataFrame:\n    \"\"\"\n    Load catalog data for a specific survey.\n\n    Parameters\n    ----------\n    survey : str\n        Survey name (gaia, sdss, 2mass, wise, pan_starrs)\n    max_samples : int, optional\n        Maximum number of samples to load\n    load_processed : bool\n        Whether to load processed version if available\n\n    Returns\n    -------\n    pl.DataFrame\n        Survey catalog data\n    \"\"\"\n    data_dir = _find_survey_data_dir(survey)\n    catalog_files = list(data_dir.glob(f\"{survey}*.fits\")) + list(\n        data_dir.glob(f\"{survey}*.parquet\")\n    )\n\n    if not catalog_files:\n        raise FileNotFoundError(\n            f\"No catalog files found for survey '{survey}' in {data_dir}\"\n        )\n\n    # Use the first available file\n    catalog_path = catalog_files[0]\n    df = load_catalog(catalog_path)\n\n    if max_samples and len(df) &gt; max_samples:\n        df = df.sample(max_samples, seed=42)\n        logger.info(f\"Sampled {max_samples} objects from {len(df)} total\")\n\n    return df\n</code></pre>"},{"location":"api/astro_lab.data/#astro_lab.data.preprocess_survey","title":"preprocess_survey","text":"<pre><code>preprocess_survey(\n    survey: str,\n    input_path: Optional[Path] = None,\n    output_path: Optional[Path] = None,\n    max_samples: Optional[int] = None,\n    **kwargs\n) -&gt; Path\n</code></pre> <p>Unified survey preprocessing function.</p> Source code in <code>src\\astro_lab\\data\\processors.py</code> <pre><code>def preprocess_survey(\n    survey: str,\n    input_path: Optional[Path] = None,\n    output_path: Optional[Path] = None,\n    max_samples: Optional[int] = None,\n    **kwargs,\n) -&gt; Path:\n    \"\"\"\n    Unified survey preprocessing function.\n    \"\"\"\n    logger.info(f\"Preprocessing survey: {survey}\")\n\n    # Load raw data\n    if input_path is None:\n        from .loaders import load_survey_catalog\n\n        df = load_survey_catalog(survey, max_samples=max_samples)\n    else:\n        from .loaders import load_catalog\n\n        df = load_catalog(input_path)\n        if max_samples and len(df) &gt; max_samples:\n            df = df.sample(max_samples, seed=42)\n\n    # Apply survey-specific preprocessing\n    df_processed = _apply_survey_preprocessing(df, survey)\n\n    # Create output path\n    if output_path is None:\n        output_path = data_config.processed_dir / survey / f\"{survey}_processed.parquet\"\n\n    output_path.parent.mkdir(parents=True, exist_ok=True)\n\n    # Save processed data\n    df_processed.write_parquet(output_path)\n\n    logger.info(f\"Preprocessed {len(df_processed)} objects -&gt; {output_path}\")\n    return output_path\n</code></pre>"},{"location":"api/astro_lab.data/#astro_lab.data.save_splits_to_parquet","title":"save_splits_to_parquet","text":"<pre><code>save_splits_to_parquet(\n    df_train: DataFrame,\n    df_val: DataFrame,\n    df_test: DataFrame,\n    base_path: Union[str, Path],\n    dataset_name: str,\n) -&gt; Dict[str, Path]\n</code></pre> <p>Save train/val/test splits to Parquet files.</p> Source code in <code>src\\astro_lab\\data\\utils.py</code> <pre><code>def save_splits_to_parquet(\n    df_train: pl.DataFrame,\n    df_val: pl.DataFrame,\n    df_test: pl.DataFrame,\n    base_path: Union[str, Path],\n    dataset_name: str,\n) -&gt; Dict[str, Path]:\n    \"\"\"Save train/val/test splits to Parquet files.\"\"\"\n    base_path = Path(base_path)\n    base_path.mkdir(parents=True, exist_ok=True)\n\n    paths = {}\n    for split_name, df in [(\"train\", df_train), (\"val\", df_val), (\"test\", df_test)]:\n        file_path = base_path / f\"{dataset_name}_{split_name}.parquet\"\n        df.write_parquet(file_path)\n        paths[split_name] = file_path\n        logger.info(f\"Saved {split_name} split: {len(df)} rows -&gt; {file_path}\")\n\n    return paths\n</code></pre>"},{"location":"api/astro_lab/","title":"astro_lab","text":""},{"location":"api/astro_lab/#astro_lab","title":"astro_lab","text":"<p>Comprehensive Astronomical Data Analysis Framework</p> <p>A modern Python framework for astronomical data analysis, machine learning, and visualization that combines specialized astronomy libraries with cutting-edge ML tools.</p> <p>Modules:</p> Name Description <code>cli</code> <p>AstroLab CLI - Command Line Interface</p> <code>config</code> <p>AstroLab Configuration Management</p> <code>data</code> <p>AstroLab Data Module - High-Performance Astronomical Data Processing</p> <code>models</code> <p>AstroLab Models - Modern Graph Neural Networks for Astronomical Data</p> <code>tensors</code> <p>Astronomical TensorDict System</p> <code>training</code> <p>AstroLab Training Module (Lightning Edition)</p> <code>ui</code> <p>AstroLab UI Module</p> <code>widgets</code> <p>AstroLab Widgets - Visualization and analysis widgets for astronomical data.</p>"},{"location":"api/astro_lab.models.components/","title":"astro_lab.models.components","text":""},{"location":"api/astro_lab.models.components/#astro_lab.models.components","title":"components","text":"<p>Reusable components for AstroLab models.</p> <p>Modules:</p> Name Description <code>base</code> <p>TensorDict-Native Base Components for AstroLab Models</p> <code>layers</code> <p>Simple layer creation functions for AstroLab models.</p> <code>output_heads</code> <p>Simple output head functions for AstroLab models.</p> <p>Classes:</p> Name Description <code>ClassificationHead</code> <p>Simple classification head for multi-class classification tasks.</p> <code>DeviceMixin</code> <p>Simple device management mixin.</p> <code>GraphProcessor</code> <p>Simple graph processing component.</p> <code>PeriodDetectionHead</code> <p>Head for asteroid period detection with uncertainty estimation.</p> <code>PoolingModule</code> <p>Simple pooling module for graph-level features.</p> <code>RegressionHead</code> <p>Simple regression head for continuous value prediction.</p> <code>ResidualBlock</code> <p>Simple residual block for deeper networks.</p> <code>ShapeModelingHead</code> <p>Head for asteroid shape modeling using spherical harmonics.</p> <p>Functions:</p> Name Description <code>create_conv_layer</code> <p>Simple function to create conv layers.</p> <code>create_mlp</code> <p>Create simple MLP.</p> <code>create_output_head</code> <p>Factory function for creating output heads with robust parameter filtering.</p> <code>get_activation</code> <p>Get activation function by name.</p>"},{"location":"api/astro_lab.models.components/#astro_lab.models.components.ClassificationHead","title":"ClassificationHead","text":"<p>               Bases: <code>Module</code></p> <p>Simple classification head for multi-class classification tasks.</p> <p>Parameters:</p> Name Type Description Default <code>input_dim</code> <code>int</code> <p>Input feature dimension</p> required <code>num_classes</code> <code>int</code> <p>Number of output classes</p> required <code>dropout</code> <code>float</code> <p>Dropout rate for regularization</p> <code>0.1</code> <p>Methods:</p> Name Description <code>forward</code> <p>Forward pass for classification.</p> Source code in <code>src\\astro_lab\\models\\components\\output_heads.py</code> <pre><code>class ClassificationHead(nn.Module):\n    \"\"\"\n    Simple classification head for multi-class classification tasks.\n\n    Args:\n        input_dim: Input feature dimension\n        num_classes: Number of output classes\n        dropout: Dropout rate for regularization\n    \"\"\"\n\n    def __init__(self, input_dim: int, num_classes: int, dropout: float = 0.1):\n        super().__init__()\n        self.classifier = create_mlp(\n            input_dim, num_classes, hidden_dims=[input_dim // 2], dropout=dropout\n        )\n\n    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"\n        Forward pass for classification.\n\n        Args:\n            x: Input tensor of shape (batch_size, input_dim)\n\n        Returns:\n            Classification logits of shape (batch_size, num_classes)\n        \"\"\"\n        return self.classifier(x)\n</code></pre>"},{"location":"api/astro_lab.models.components/#astro_lab.models.components.ClassificationHead.forward","title":"forward","text":"<pre><code>forward(x: Tensor) -&gt; Tensor\n</code></pre> <p>Forward pass for classification.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>Input tensor of shape (batch_size, input_dim)</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Classification logits of shape (batch_size, num_classes)</p> Source code in <code>src\\astro_lab\\models\\components\\output_heads.py</code> <pre><code>def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"\n    Forward pass for classification.\n\n    Args:\n        x: Input tensor of shape (batch_size, input_dim)\n\n    Returns:\n        Classification logits of shape (batch_size, num_classes)\n    \"\"\"\n    return self.classifier(x)\n</code></pre>"},{"location":"api/astro_lab.models.components/#astro_lab.models.components.DeviceMixin","title":"DeviceMixin","text":"<p>Simple device management mixin.</p> <p>Methods:</p> Name Description <code>to_device</code> <p>Move tensors to device.</p> Source code in <code>src\\astro_lab\\models\\components\\base.py</code> <pre><code>class DeviceMixin:\n    \"\"\"Simple device management mixin.\"\"\"\n\n    def __init__(self, device: Optional[Union[str, torch.device]] = None):\n        self.device = torch.device(\n            device or (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n        )\n\n    def to_device(self, *tensors):\n        \"\"\"Move tensors to device.\"\"\"\n        return [t.to(self.device) if t is not None else None for t in tensors]\n</code></pre>"},{"location":"api/astro_lab.models.components/#astro_lab.models.components.DeviceMixin.to_device","title":"to_device","text":"<pre><code>to_device(*tensors)\n</code></pre> <p>Move tensors to device.</p> Source code in <code>src\\astro_lab\\models\\components\\base.py</code> <pre><code>def to_device(self, *tensors):\n    \"\"\"Move tensors to device.\"\"\"\n    return [t.to(self.device) if t is not None else None for t in tensors]\n</code></pre>"},{"location":"api/astro_lab.models.components/#astro_lab.models.components.GraphProcessor","title":"GraphProcessor","text":"<p>               Bases: <code>Module</code></p> <p>Simple graph processing component.</p> <p>Methods:</p> Name Description <code>forward</code> <p>Process graph through conv layers.</p> Source code in <code>src\\astro_lab\\models\\components\\base.py</code> <pre><code>class GraphProcessor(nn.Module):\n    \"\"\"Simple graph processing component.\"\"\"\n\n    def __init__(\n        self,\n        hidden_dim: int,\n        num_layers: int,\n        conv_type: str = \"gcn\",\n        dropout: float = 0.1,\n        **kwargs,\n    ):\n        super().__init__()\n\n        self.convs = nn.ModuleList(\n            [\n                create_conv_layer(conv_type, hidden_dim, hidden_dim, **kwargs)\n                for _ in range(num_layers)\n            ]\n        )\n        self.norms = nn.ModuleList(\n            [nn.LayerNorm(hidden_dim) for _ in range(num_layers)]\n        )\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x: torch.Tensor, edge_index: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"Process graph through conv layers.\"\"\"\n        h = x\n        for conv, norm in zip(self.convs, self.norms):\n            h = conv(h, edge_index)\n            h = norm(h)\n            h = F.relu(h)\n            h = self.dropout(h)\n        return h\n</code></pre>"},{"location":"api/astro_lab.models.components/#astro_lab.models.components.GraphProcessor.forward","title":"forward","text":"<pre><code>forward(x: Tensor, edge_index: Tensor) -&gt; Tensor\n</code></pre> <p>Process graph through conv layers.</p> Source code in <code>src\\astro_lab\\models\\components\\base.py</code> <pre><code>def forward(self, x: torch.Tensor, edge_index: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"Process graph through conv layers.\"\"\"\n    h = x\n    for conv, norm in zip(self.convs, self.norms):\n        h = conv(h, edge_index)\n        h = norm(h)\n        h = F.relu(h)\n        h = self.dropout(h)\n    return h\n</code></pre>"},{"location":"api/astro_lab.models.components/#astro_lab.models.components.PeriodDetectionHead","title":"PeriodDetectionHead","text":"<p>               Bases: <code>Module</code></p> <p>Head for asteroid period detection with uncertainty estimation.</p> <p>Parameters:</p> Name Type Description Default <code>input_dim</code> <code>int</code> <p>Input feature dimension</p> required <code>dropout</code> <code>float</code> <p>Dropout rate for regularization</p> <code>0.1</code> <p>Methods:</p> Name Description <code>forward</code> <p>Forward pass for period detection.</p> Source code in <code>src\\astro_lab\\models\\components\\output_heads.py</code> <pre><code>class PeriodDetectionHead(nn.Module):\n    \"\"\"\n    Head for asteroid period detection with uncertainty estimation.\n\n    Args:\n        input_dim: Input feature dimension\n        dropout: Dropout rate for regularization\n    \"\"\"\n\n    def __init__(self, input_dim: int, dropout: float = 0.1):\n        super().__init__()\n        # Period detection: predict period value and uncertainty\n        self.period_net = create_mlp(\n            input_dim,\n            2,  # period and uncertainty\n            hidden_dims=[input_dim // 2, input_dim // 4],\n            dropout=dropout,\n        )\n\n    def forward(self, x: torch.Tensor) -&gt; Dict[str, torch.Tensor]:\n        \"\"\"\n        Forward pass for period detection.\n\n        Args:\n            x: Input tensor of shape (batch_size, input_dim)\n\n        Returns:\n            Dictionary with 'period' and 'uncertainty' tensors\n        \"\"\"\n        output = self.period_net(x)\n        period = torch.abs(output[:, 0:1])  # Ensure positive\n        uncertainty = torch.abs(output[:, 1:2])\n        return {\"period\": period, \"uncertainty\": uncertainty}\n</code></pre>"},{"location":"api/astro_lab.models.components/#astro_lab.models.components.PeriodDetectionHead.forward","title":"forward","text":"<pre><code>forward(x: Tensor) -&gt; Dict[str, Tensor]\n</code></pre> <p>Forward pass for period detection.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>Input tensor of shape (batch_size, input_dim)</p> required <p>Returns:</p> Type Description <code>Dict[str, Tensor]</code> <p>Dictionary with 'period' and 'uncertainty' tensors</p> Source code in <code>src\\astro_lab\\models\\components\\output_heads.py</code> <pre><code>def forward(self, x: torch.Tensor) -&gt; Dict[str, torch.Tensor]:\n    \"\"\"\n    Forward pass for period detection.\n\n    Args:\n        x: Input tensor of shape (batch_size, input_dim)\n\n    Returns:\n        Dictionary with 'period' and 'uncertainty' tensors\n    \"\"\"\n    output = self.period_net(x)\n    period = torch.abs(output[:, 0:1])  # Ensure positive\n    uncertainty = torch.abs(output[:, 1:2])\n    return {\"period\": period, \"uncertainty\": uncertainty}\n</code></pre>"},{"location":"api/astro_lab.models.components/#astro_lab.models.components.PoolingModule","title":"PoolingModule","text":"<p>               Bases: <code>Module</code></p> <p>Simple pooling module for graph-level features.</p> <p>Methods:</p> Name Description <code>forward</code> <p>Pool node features to graph level.</p> Source code in <code>src\\astro_lab\\models\\components\\base.py</code> <pre><code>class PoolingModule(nn.Module):\n    \"\"\"Simple pooling module for graph-level features.\"\"\"\n\n    def __init__(self, pooling_type: str = \"mean\"):\n        super().__init__()\n        self.pooling_type = pooling_type\n\n    def forward(\n        self, x: torch.Tensor, batch: Optional[torch.Tensor] = None\n    ) -&gt; torch.Tensor:\n        \"\"\"Pool node features to graph level.\"\"\"\n        if batch is None:\n            # If no batch, assume single graph\n            if self.pooling_type == \"mean\":\n                return x.mean(dim=0, keepdim=True)\n            elif self.pooling_type == \"max\":\n                return x.max(dim=0)[0].unsqueeze(0)\n            elif self.pooling_type == \"sum\":\n                return x.sum(dim=0, keepdim=True)\n            else:\n                # Default to mean pooling\n                return x.mean(dim=0, keepdim=True)\n        else:\n            # Use PyG pooling\n            if self.pooling_type == \"mean\":\n                from torch_geometric.nn import global_mean_pool\n\n                return global_mean_pool(x, batch)\n            elif self.pooling_type == \"max\":\n                from torch_geometric.nn import global_max_pool\n\n                return global_max_pool(x, batch)\n            elif self.pooling_type == \"sum\":\n                from torch_geometric.nn import global_add_pool\n\n                return global_add_pool(x, batch)\n            else:\n                # Default to mean pooling\n                from torch_geometric.nn import global_mean_pool\n\n                return global_mean_pool(x, batch)\n</code></pre>"},{"location":"api/astro_lab.models.components/#astro_lab.models.components.PoolingModule.forward","title":"forward","text":"<pre><code>forward(x: Tensor, batch: Optional[Tensor] = None) -&gt; Tensor\n</code></pre> <p>Pool node features to graph level.</p> Source code in <code>src\\astro_lab\\models\\components\\base.py</code> <pre><code>def forward(\n    self, x: torch.Tensor, batch: Optional[torch.Tensor] = None\n) -&gt; torch.Tensor:\n    \"\"\"Pool node features to graph level.\"\"\"\n    if batch is None:\n        # If no batch, assume single graph\n        if self.pooling_type == \"mean\":\n            return x.mean(dim=0, keepdim=True)\n        elif self.pooling_type == \"max\":\n            return x.max(dim=0)[0].unsqueeze(0)\n        elif self.pooling_type == \"sum\":\n            return x.sum(dim=0, keepdim=True)\n        else:\n            # Default to mean pooling\n            return x.mean(dim=0, keepdim=True)\n    else:\n        # Use PyG pooling\n        if self.pooling_type == \"mean\":\n            from torch_geometric.nn import global_mean_pool\n\n            return global_mean_pool(x, batch)\n        elif self.pooling_type == \"max\":\n            from torch_geometric.nn import global_max_pool\n\n            return global_max_pool(x, batch)\n        elif self.pooling_type == \"sum\":\n            from torch_geometric.nn import global_add_pool\n\n            return global_add_pool(x, batch)\n        else:\n            # Default to mean pooling\n            from torch_geometric.nn import global_mean_pool\n\n            return global_mean_pool(x, batch)\n</code></pre>"},{"location":"api/astro_lab.models.components/#astro_lab.models.components.RegressionHead","title":"RegressionHead","text":"<p>               Bases: <code>Module</code></p> <p>Simple regression head for continuous value prediction.</p> <p>Parameters:</p> Name Type Description Default <code>input_dim</code> <code>int</code> <p>Input feature dimension</p> required <code>output_dim</code> <code>int</code> <p>Output dimension (default: 1)</p> <code>1</code> <code>dropout</code> <code>float</code> <p>Dropout rate for regularization</p> <code>0.1</code> <p>Methods:</p> Name Description <code>forward</code> <p>Forward pass for regression.</p> Source code in <code>src\\astro_lab\\models\\components\\output_heads.py</code> <pre><code>class RegressionHead(nn.Module):\n    \"\"\"\n    Simple regression head for continuous value prediction.\n\n    Args:\n        input_dim: Input feature dimension\n        output_dim: Output dimension (default: 1)\n        dropout: Dropout rate for regularization\n    \"\"\"\n\n    def __init__(self, input_dim: int, output_dim: int = 1, dropout: float = 0.1):\n        super().__init__()\n        self.regressor = create_mlp(\n            input_dim, output_dim, hidden_dims=[input_dim // 2], dropout=dropout\n        )\n\n    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"\n        Forward pass for regression.\n\n        Args:\n            x: Input tensor of shape (batch_size, input_dim)\n\n        Returns:\n            Regression output of shape (batch_size, output_dim)\n        \"\"\"\n        return self.regressor(x)\n</code></pre>"},{"location":"api/astro_lab.models.components/#astro_lab.models.components.RegressionHead.forward","title":"forward","text":"<pre><code>forward(x: Tensor) -&gt; Tensor\n</code></pre> <p>Forward pass for regression.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>Input tensor of shape (batch_size, input_dim)</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Regression output of shape (batch_size, output_dim)</p> Source code in <code>src\\astro_lab\\models\\components\\output_heads.py</code> <pre><code>def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"\n    Forward pass for regression.\n\n    Args:\n        x: Input tensor of shape (batch_size, input_dim)\n\n    Returns:\n        Regression output of shape (batch_size, output_dim)\n    \"\"\"\n    return self.regressor(x)\n</code></pre>"},{"location":"api/astro_lab.models.components/#astro_lab.models.components.ResidualBlock","title":"ResidualBlock","text":"<p>               Bases: <code>Module</code></p> <p>Simple residual block for deeper networks.</p> Source code in <code>src\\astro_lab\\models\\components\\layers.py</code> <pre><code>class ResidualBlock(nn.Module):\n    \"\"\"Simple residual block for deeper networks.\"\"\"\n\n    def __init__(self, hidden_dim: int, dropout: float = 0.1):\n        super().__init__()\n        self.fc1 = nn.Linear(hidden_dim, hidden_dim)\n        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n        self.norm1 = nn.LayerNorm(hidden_dim)\n        self.norm2 = nn.LayerNorm(hidden_dim)\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n        residual = x\n        x = self.norm1(x)\n        x = torch.relu(self.fc1(x))\n        x = self.dropout(x)\n        x = self.fc2(x)\n        x = self.norm2(x)\n        return torch.relu(x + residual)\n</code></pre>"},{"location":"api/astro_lab.models.components/#astro_lab.models.components.ShapeModelingHead","title":"ShapeModelingHead","text":"<p>               Bases: <code>Module</code></p> <p>Head for asteroid shape modeling using spherical harmonics.</p> <p>Parameters:</p> Name Type Description Default <code>input_dim</code> <code>int</code> <p>Input feature dimension</p> required <code>num_harmonics</code> <code>int</code> <p>Number of spherical harmonic coefficients</p> <code>10</code> <code>dropout</code> <code>float</code> <p>Dropout rate for regularization</p> <code>0.1</code> <p>Methods:</p> Name Description <code>forward</code> <p>Forward pass for shape modeling.</p> Source code in <code>src\\astro_lab\\models\\components\\output_heads.py</code> <pre><code>class ShapeModelingHead(nn.Module):\n    \"\"\"\n    Head for asteroid shape modeling using spherical harmonics.\n\n    Args:\n        input_dim: Input feature dimension\n        num_harmonics: Number of spherical harmonic coefficients\n        dropout: Dropout rate for regularization\n    \"\"\"\n\n    def __init__(self, input_dim: int, num_harmonics: int = 10, dropout: float = 0.1):\n        super().__init__()\n        self.num_harmonics = num_harmonics\n        # Predict spherical harmonic coefficients\n        self.shape_net = create_mlp(\n            input_dim,\n            num_harmonics * 2,  # Real and imaginary parts\n            hidden_dims=[input_dim // 2],\n            dropout=dropout,\n        )\n\n    def forward(self, x: torch.Tensor) -&gt; Dict[str, torch.Tensor]:\n        \"\"\"\n        Forward pass for shape modeling.\n\n        Args:\n            x: Input tensor of shape (batch_size, input_dim)\n\n        Returns:\n            Dictionary with 'real_coeffs' and 'imag_coeffs' tensors\n        \"\"\"\n        coeffs = self.shape_net(x)\n        real_coeffs = coeffs[:, : self.num_harmonics]\n        imag_coeffs = coeffs[:, self.num_harmonics :]\n        return {\"real_coeffs\": real_coeffs, \"imag_coeffs\": imag_coeffs}\n</code></pre>"},{"location":"api/astro_lab.models.components/#astro_lab.models.components.ShapeModelingHead.forward","title":"forward","text":"<pre><code>forward(x: Tensor) -&gt; Dict[str, Tensor]\n</code></pre> <p>Forward pass for shape modeling.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>Input tensor of shape (batch_size, input_dim)</p> required <p>Returns:</p> Type Description <code>Dict[str, Tensor]</code> <p>Dictionary with 'real_coeffs' and 'imag_coeffs' tensors</p> Source code in <code>src\\astro_lab\\models\\components\\output_heads.py</code> <pre><code>def forward(self, x: torch.Tensor) -&gt; Dict[str, torch.Tensor]:\n    \"\"\"\n    Forward pass for shape modeling.\n\n    Args:\n        x: Input tensor of shape (batch_size, input_dim)\n\n    Returns:\n        Dictionary with 'real_coeffs' and 'imag_coeffs' tensors\n    \"\"\"\n    coeffs = self.shape_net(x)\n    real_coeffs = coeffs[:, : self.num_harmonics]\n    imag_coeffs = coeffs[:, self.num_harmonics :]\n    return {\"real_coeffs\": real_coeffs, \"imag_coeffs\": imag_coeffs}\n</code></pre>"},{"location":"api/astro_lab.models.components/#astro_lab.models.components.create_conv_layer","title":"create_conv_layer","text":"<pre><code>create_conv_layer(\n    conv_type: str, in_channels: int, out_channels: int, **kwargs\n) -&gt; Module\n</code></pre> <p>Simple function to create conv layers.</p> Source code in <code>src\\astro_lab\\models\\components\\layers.py</code> <pre><code>def create_conv_layer(\n    conv_type: str, in_channels: int, out_channels: int, **kwargs\n) -&gt; nn.Module:\n    \"\"\"Simple function to create conv layers.\"\"\"\n    conv_type = conv_type.lower()\n\n    if conv_type == \"gcn\":\n        return GCNConv(in_channels, out_channels)\n    elif conv_type == \"gat\":\n        heads = kwargs.get(\"heads\", 8)\n        return GATConv(in_channels, out_channels // heads, heads=heads)\n    elif conv_type == \"sage\":\n        return SAGEConv(in_channels, out_channels)\n    elif conv_type == \"transformer\":\n        heads = kwargs.get(\"heads\", 8)\n        return TransformerConv(in_channels, out_channels // heads, heads=heads)\n    else:\n        raise ValueError(\n            f\"Unknown conv_type: {conv_type}. Available: gcn, gat, sage, transformer\"\n        )\n</code></pre>"},{"location":"api/astro_lab.models.components/#astro_lab.models.components.create_mlp","title":"create_mlp","text":"<pre><code>create_mlp(\n    input_dim: int,\n    output_dim: int,\n    hidden_dims: Optional[List[int]] = None,\n    dropout: float = 0.1,\n    activation: str = \"relu\",\n    batch_norm: bool = False,\n) -&gt; Module\n</code></pre> <p>Create simple MLP.</p> Source code in <code>src\\astro_lab\\models\\components\\layers.py</code> <pre><code>def create_mlp(\n    input_dim: int,\n    output_dim: int,\n    hidden_dims: Optional[List[int]] = None,\n    dropout: float = 0.1,\n    activation: str = \"relu\",\n    batch_norm: bool = False,\n) -&gt; nn.Module:\n    \"\"\"Create simple MLP.\"\"\"\n    if hidden_dims is None:\n        hidden_dims = [input_dim // 2]\n\n    layers = []\n    prev_dim = input_dim\n\n    # Hidden layers\n    for hidden_dim in hidden_dims:\n        layers.append(nn.Linear(prev_dim, hidden_dim))\n\n        if batch_norm:\n            layers.append(nn.BatchNorm1d(hidden_dim))\n\n        layers.append(get_activation(activation))\n\n        if dropout &gt; 0:\n            layers.append(nn.Dropout(dropout))\n\n        prev_dim = hidden_dim\n\n    # Output layer\n    layers.append(nn.Linear(prev_dim, output_dim))\n\n    return nn.Sequential(*layers)\n</code></pre>"},{"location":"api/astro_lab.models.components/#astro_lab.models.components.create_output_head","title":"create_output_head","text":"<pre><code>create_output_head(\n    head_type: str, input_dim: int, output_dim: Optional[int] = None, **kwargs\n) -&gt; Module\n</code></pre> <p>Factory function for creating output heads with robust parameter filtering.</p> <p>Parameters:</p> Name Type Description Default <code>head_type</code> <code>str</code> <p>Type of output head ('classification', 'regression', etc.)</p> required <code>input_dim</code> <code>int</code> <p>Input feature dimension</p> required <code>output_dim</code> <code>Optional[int]</code> <p>Output dimension (optional, depends on head type)</p> <code>None</code> <code>**kwargs</code> <p>Additional parameters for the head</p> <code>{}</code> <p>Returns:</p> Type Description <code>Module</code> <p>Configured output head module</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If head_type is not supported</p> Source code in <code>src\\astro_lab\\models\\components\\output_heads.py</code> <pre><code>def create_output_head(\n    head_type: str, input_dim: int, output_dim: Optional[int] = None, **kwargs\n) -&gt; nn.Module:\n    \"\"\"\n    Factory function for creating output heads with robust parameter filtering.\n\n    Args:\n        head_type: Type of output head ('classification', 'regression', etc.)\n        input_dim: Input feature dimension\n        output_dim: Output dimension (optional, depends on head type)\n        **kwargs: Additional parameters for the head\n\n    Returns:\n        Configured output head module\n\n    Raises:\n        ValueError: If head_type is not supported\n    \"\"\"\n    if head_type not in OUTPUT_HEADS:\n        available = list(OUTPUT_HEADS.keys())\n        raise ValueError(f\"Unknown head type: {head_type}. Available: {available}\")\n\n    head_class = OUTPUT_HEADS[head_type]\n\n    # Prepare kwargs for each head type\n    if head_type == \"classification\":\n        config = {\"input_dim\": input_dim, \"num_classes\": output_dim or 2}\n        config.update(kwargs)\n        filtered = filter_kwargs(head_class, **config)\n        return head_class(**filtered)\n    elif head_type == \"regression\":\n        config = {\"input_dim\": input_dim, \"output_dim\": output_dim or 1}\n        config.update(kwargs)\n        filtered = filter_kwargs(head_class, **config)\n        return head_class(**filtered)\n    else:\n        # Period detection and shape modeling don't need output_dim\n        config = {\"input_dim\": input_dim}\n        config.update(kwargs)\n        filtered = filter_kwargs(head_class, **config)\n        return head_class(**filtered)\n</code></pre>"},{"location":"api/astro_lab.models.components/#astro_lab.models.components.get_activation","title":"get_activation","text":"<pre><code>get_activation(name: str) -&gt; Module\n</code></pre> <p>Get activation function by name.</p> Source code in <code>src\\astro_lab\\models\\components\\layers.py</code> <pre><code>def get_activation(name: str) -&gt; nn.Module:\n    \"\"\"Get activation function by name.\"\"\"\n    activations = {\n        \"relu\": nn.ReLU(),\n        \"gelu\": nn.GELU(),\n        \"swish\": nn.SiLU(),\n        \"tanh\": nn.Tanh(),\n        \"leaky_relu\": nn.LeakyReLU(),\n        \"elu\": nn.ELU(),\n    }\n\n    name = name.lower()\n    if name not in activations:\n        raise ValueError(\n            f\"Unknown activation: {name}. Available: {list(activations.keys())}\"\n        )\n\n    return activations[name]\n</code></pre>"},{"location":"api/astro_lab.models.core/","title":"astro_lab.models.core","text":""},{"location":"api/astro_lab.models.core/#astro_lab.models.core","title":"core","text":"<p>Core AstroLab models.</p> <p>Modules:</p> Name Description <code>astrophot_gnn</code> <p>TensorDict-Native AstroPhotometry GNN Models</p> <code>point_cloud_gnn</code> <p>Simplified ALCDEF Temporal GNN for lightcurve analysis.</p> <code>pointnet_gnn</code> <p>PointNet++ GNN for Astronomical Point Clouds</p> <code>survey_gnn</code> <p>TensorDict-Native Survey GNN Models</p> <code>temporal_gnn</code> <p>TensorDict-Native Temporal GNN Models</p> <p>Classes:</p> Name Description <code>ALCDEFTemporalGNN</code> <p>Simplified Temporal GNN for ALCDEF lightcurve data.</p> <code>AstroPhotGNN</code> <p>Graph Neural Network for astronomical photometry using native PhotometricTensorDict methods.</p> <code>AstroSurveyGNN</code> <p>Graph Neural Network for astronomical survey data using native TensorDict methods.</p> <code>AstronomicalPointNetGNN</code> <p>PointNet++ model for astronomical point cloud classification and analysis.</p> <code>MultiModalSurveyGNN</code> <p>Extended Survey GNN for multi-modal data fusion using native TensorDict methods.</p> <code>TemporalGCN</code> <p>Temporal Graph Convolutional Network using native LightcurveTensorDict methods.</p> <p>Functions:</p> Name Description <code>create_pointnet_gnn</code> <p>Create an AstronomicalPointNetGNN model.</p>"},{"location":"api/astro_lab.models.core/#astro_lab.models.core.ALCDEFTemporalGNN","title":"ALCDEFTemporalGNN","text":"<p>               Bases: <code>Module</code></p> <p>Simplified Temporal GNN for ALCDEF lightcurve data.</p> <p>Methods:</p> Name Description <code>encode_lightcurve</code> <p>Encode lightcurve sequence with LSTM.</p> <code>forward</code> <p>Forward pass for lightcurve analysis.</p> Source code in <code>src\\astro_lab\\models\\core\\point_cloud_gnn.py</code> <pre><code>class ALCDEFTemporalGNN(nn.Module):\n    \"\"\"Simplified Temporal GNN for ALCDEF lightcurve data.\"\"\"\n\n    def __init__(\n        self,\n        input_dim: int = 1,  # Typically magnitude values\n        hidden_dim: int = 128,\n        output_dim: int = 1,\n        num_layers: int = 3,\n        task: str = \"period_detection\",\n        dropout: float = 0.1,\n        device: Optional[Union[str, torch.device]] = None,\n        **kwargs\n    ):\n        super().__init__()\n\n        self.device = torch.device(device or ('cuda' if torch.cuda.is_available() else 'cpu'))\n        self.hidden_dim = hidden_dim\n        self.task = task\n\n        # Lightcurve encoder - simple LSTM\n        self.lightcurve_encoder = nn.LSTM(\n            input_size=input_dim,\n            hidden_size=hidden_dim,\n            num_layers=2,\n            batch_first=True,\n            dropout=dropout if num_layers &gt; 1 else 0,\n        )\n\n        # Graph convolutions for temporal relationships\n        self.convs = nn.ModuleList([\n            create_conv_layer(\"gcn\", hidden_dim, hidden_dim)\n            for _ in range(num_layers)\n        ])\n\n        self.norms = nn.ModuleList([\n            nn.LayerNorm(hidden_dim) for _ in range(num_layers)\n        ])\n\n        self.dropout = nn.Dropout(dropout)\n\n        # Task-specific output heads\n        if task == \"period_detection\":\n            self.output_head = PeriodDetectionHead(hidden_dim, dropout)\n        elif task == \"shape_modeling\":\n            num_harmonics = kwargs.get(\"num_harmonics\", 10)\n            self.output_head = ShapeModelingHead(hidden_dim, num_harmonics, dropout)\n        elif task == \"classification\":\n            num_classes = kwargs.get(\"num_classes\", 2)\n            self.output_head = ClassificationHead(hidden_dim, num_classes, dropout)\n        else:\n            # Default to simple output head\n            self.output_head = create_output_head(\n                \"regression\",\n                input_dim=hidden_dim,\n                output_dim=output_dim,\n                dropout=dropout\n            )\n\n        self.to(self.device)\n\n    def encode_lightcurve(self, lightcurve_data: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"Encode lightcurve sequence with LSTM.\"\"\"\n        # lightcurve_data shape: (batch, seq_len, features)\n        if lightcurve_data.dim() == 2:\n            # Add feature dimension if needed\n            lightcurve_data = lightcurve_data.unsqueeze(-1)\n\n        # Encode with LSTM\n        lstm_out, (h_n, _) = self.lightcurve_encoder(lightcurve_data)\n\n        # Use last hidden state\n        # h_n shape: (num_layers, batch, hidden_dim)\n        encoded = h_n[-1]  # Take last layer: (batch, hidden_dim)\n\n        return encoded\n\n    def forward(\n        self,\n        lightcurve,\n        edge_index: torch.Tensor,\n        batch: Optional[torch.Tensor] = None,\n        return_embeddings: bool = False,\n    ) -&gt; Union[torch.Tensor, Dict[str, torch.Tensor]]:\n        \"\"\"Forward pass for lightcurve analysis.\"\"\"\n\n        # Move to device\n        edge_index = edge_index.to(self.device)\n        if batch is not None:\n            batch = batch.to(self.device)\n\n        # Handle different input formats\n        if hasattr(lightcurve, 'data'):\n            # LightcurveTensor\n            lc_data = lightcurve.data.to(self.device)\n        elif isinstance(lightcurve, torch.Tensor):\n            lc_data = lightcurve.to(self.device)\n        else:\n            raise ValueError(\"Unsupported lightcurve format\")\n\n        # Encode lightcurve\n        h = self.encode_lightcurve(lc_data)\n\n        # If we have multiple nodes per graph, expand the encoding\n        if edge_index.size(1) &gt; 0:\n            num_nodes = edge_index.max().item() + 1\n            if h.size(0) == 1 and num_nodes &gt; 1:\n                # Broadcast to all nodes\n                h = h.expand(num_nodes, -1)\n\n        # Apply graph convolutions\n        for conv, norm in zip(self.convs, self.norms):\n            h_prev = h\n            h = conv(h, edge_index)\n            h = norm(h)\n            h = F.relu(h)\n            h = self.dropout(h)\n\n            # Residual connection\n            if h_prev.shape == h.shape:\n                h = h + h_prev\n\n        # Pool if needed\n        if batch is not None:\n            pooled = global_mean_pool(h, batch)\n        else:\n            pooled = h.mean(dim=0, keepdim=True)\n\n        # Task-specific output\n        output = self.output_head(pooled)\n\n        if return_embeddings:\n            return {\"predictions\": output, \"embeddings\": pooled}\n        return output \n</code></pre>"},{"location":"api/astro_lab.models.core/#astro_lab.models.core.ALCDEFTemporalGNN.encode_lightcurve","title":"encode_lightcurve","text":"<pre><code>encode_lightcurve(lightcurve_data: Tensor) -&gt; Tensor\n</code></pre> <p>Encode lightcurve sequence with LSTM.</p> Source code in <code>src\\astro_lab\\models\\core\\point_cloud_gnn.py</code> <pre><code>def encode_lightcurve(self, lightcurve_data: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"Encode lightcurve sequence with LSTM.\"\"\"\n    # lightcurve_data shape: (batch, seq_len, features)\n    if lightcurve_data.dim() == 2:\n        # Add feature dimension if needed\n        lightcurve_data = lightcurve_data.unsqueeze(-1)\n\n    # Encode with LSTM\n    lstm_out, (h_n, _) = self.lightcurve_encoder(lightcurve_data)\n\n    # Use last hidden state\n    # h_n shape: (num_layers, batch, hidden_dim)\n    encoded = h_n[-1]  # Take last layer: (batch, hidden_dim)\n\n    return encoded\n</code></pre>"},{"location":"api/astro_lab.models.core/#astro_lab.models.core.ALCDEFTemporalGNN.forward","title":"forward","text":"<pre><code>forward(\n    lightcurve,\n    edge_index: Tensor,\n    batch: Optional[Tensor] = None,\n    return_embeddings: bool = False,\n) -&gt; Union[Tensor, Dict[str, Tensor]]\n</code></pre> <p>Forward pass for lightcurve analysis.</p> Source code in <code>src\\astro_lab\\models\\core\\point_cloud_gnn.py</code> <pre><code>def forward(\n    self,\n    lightcurve,\n    edge_index: torch.Tensor,\n    batch: Optional[torch.Tensor] = None,\n    return_embeddings: bool = False,\n) -&gt; Union[torch.Tensor, Dict[str, torch.Tensor]]:\n    \"\"\"Forward pass for lightcurve analysis.\"\"\"\n\n    # Move to device\n    edge_index = edge_index.to(self.device)\n    if batch is not None:\n        batch = batch.to(self.device)\n\n    # Handle different input formats\n    if hasattr(lightcurve, 'data'):\n        # LightcurveTensor\n        lc_data = lightcurve.data.to(self.device)\n    elif isinstance(lightcurve, torch.Tensor):\n        lc_data = lightcurve.to(self.device)\n    else:\n        raise ValueError(\"Unsupported lightcurve format\")\n\n    # Encode lightcurve\n    h = self.encode_lightcurve(lc_data)\n\n    # If we have multiple nodes per graph, expand the encoding\n    if edge_index.size(1) &gt; 0:\n        num_nodes = edge_index.max().item() + 1\n        if h.size(0) == 1 and num_nodes &gt; 1:\n            # Broadcast to all nodes\n            h = h.expand(num_nodes, -1)\n\n    # Apply graph convolutions\n    for conv, norm in zip(self.convs, self.norms):\n        h_prev = h\n        h = conv(h, edge_index)\n        h = norm(h)\n        h = F.relu(h)\n        h = self.dropout(h)\n\n        # Residual connection\n        if h_prev.shape == h.shape:\n            h = h + h_prev\n\n    # Pool if needed\n    if batch is not None:\n        pooled = global_mean_pool(h, batch)\n    else:\n        pooled = h.mean(dim=0, keepdim=True)\n\n    # Task-specific output\n    output = self.output_head(pooled)\n\n    if return_embeddings:\n        return {\"predictions\": output, \"embeddings\": pooled}\n    return output \n</code></pre>"},{"location":"api/astro_lab.models.core/#astro_lab.models.core.AstroPhotGNN","title":"AstroPhotGNN","text":"<p>               Bases: <code>Module</code></p> <p>Graph Neural Network for astronomical photometry using native PhotometricTensorDict methods.</p> <p>Processes multi-band photometric data through specialized encoders and GNN layers, utilizing the native methods and properties of PhotometricTensorDict.</p> <p>Methods:</p> Name Description <code>forward</code> <p>Forward pass using native PhotometricTensorDict methods.</p> <code>get_photometric_metadata</code> <p>Extract photometric metadata using native methods.</p> Source code in <code>src\\astro_lab\\models\\core\\astrophot_gnn.py</code> <pre><code>class AstroPhotGNN(nn.Module):\n    \"\"\"\n    Graph Neural Network for astronomical photometry using native PhotometricTensorDict methods.\n\n    Processes multi-band photometric data through specialized encoders and GNN layers,\n    utilizing the native methods and properties of PhotometricTensorDict.\n    \"\"\"\n\n    def __init__(\n        self,\n        output_dim: int = 128,\n        hidden_dim: int = 256,\n        num_gnn_layers: int = 3,\n        use_color_features: bool = True,\n        use_magnitude_errors: bool = True,\n        pooling_type: str = \"mean\",\n        dropout: float = 0.1,\n        device: Optional[Union[str, torch.device]] = None,\n        **kwargs,\n    ):\n        super().__init__()\n\n        self.output_dim = output_dim\n        self.hidden_dim = hidden_dim\n        self.num_gnn_layers = num_gnn_layers\n        self.use_color_features = use_color_features\n        self.use_magnitude_errors = use_magnitude_errors\n        self.pooling_type = pooling_type\n        self.dropout = dropout\n\n        self.device = torch.device(\n            device or (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n        )\n\n        # PhotometricTensorDict-native encoder\n        self.photometry_encoder = PhotometryEncoder(\n            output_dim=hidden_dim,\n            hidden_dim=hidden_dim,\n            dropout=dropout,\n            device=device,\n            **kwargs,\n        )\n\n        # Color index processor for multi-band data\n        if use_color_features:\n            self.color_processor = nn.Sequential(\n                nn.Linear(hidden_dim // 2, hidden_dim // 4),\n                nn.ReLU(),\n                nn.Dropout(dropout),\n                nn.Linear(hidden_dim // 4, hidden_dim // 8),\n            )\n\n        # GNN layers for spatial photometric relationships\n        self.gnn_layers = nn.ModuleList(\n            [\n                BaseGNNLayer(\n                    input_dim=hidden_dim,\n                    output_dim=hidden_dim,\n                    layer_type=\"gcn\",\n                    dropout=dropout,\n                    device=device,\n                )\n                for _ in range(num_gnn_layers)\n            ]\n        )\n\n        # Global pooling\n        from ..components.base import PoolingModule\n\n        self.pooling = PoolingModule(pooling_type=pooling_type)\n\n        # Output projection\n        self.output_projection = nn.Sequential(\n            nn.Linear(hidden_dim, hidden_dim // 2),\n            nn.ReLU(),\n            nn.Dropout(dropout),\n            nn.Linear(hidden_dim // 2, output_dim),\n        )\n\n        self.to(self.device)\n\n    def forward(\n        self,\n        data: PhotometricTensorDict,\n        edge_index: Optional[torch.Tensor] = None,\n        batch: Optional[torch.Tensor] = None,\n    ) -&gt; torch.Tensor:\n        \"\"\"\n        Forward pass using native PhotometricTensorDict methods.\n\n        Args:\n            data: PhotometricTensorDict with native photometric access\n            edge_index: Graph edge indices for photometric relationships\n            batch: Batch assignment for objects\n\n        Returns:\n            Encoded photometric features\n        \"\"\"\n        if not isinstance(data, PhotometricTensorDict):\n            raise ValueError(\"AstroPhotGNN requires PhotometricTensorDict input\")\n\n        # Use photometry encoder with native methods\n        node_features = self.photometry_encoder(data)\n\n        # Extract additional color features using native methods\n        if self.use_color_features and data.n_bands &gt; 1:\n            color_features = self._extract_color_features(data)\n            if color_features is not None:\n                # Combine with main features\n                if hasattr(self, \"color_processor\"):\n                    processed_colors = self.color_processor(color_features)\n                    node_features = torch.cat([node_features, processed_colors], dim=-1)\n\n                    # Adjust projection for concatenated features\n                    if not hasattr(self, \"_adjusted_projection\"):\n                        combined_dim = node_features.shape[-1]\n                        self.feature_adjustment = nn.Linear(\n                            combined_dim, self.hidden_dim\n                        ).to(self.device)\n                        self._adjusted_projection = True\n\n                    if hasattr(self, \"feature_adjustment\"):\n                        node_features = self.feature_adjustment(node_features)\n\n        # Create edge index if not provided\n        if edge_index is None:\n            num_nodes = node_features.shape[0]\n            edge_index = self._create_photometric_graph(data, num_nodes)\n\n        edge_index = edge_index.to(self.device)\n\n        # Process through GNN layers\n        h = node_features\n        for gnn_layer in self.gnn_layers:\n            h = gnn_layer(h, edge_index)\n            h = F.relu(h)\n            h = F.dropout(h, p=self.dropout, training=self.training)\n\n        # Global pooling\n        graph_embedding = self.pooling(h, batch)\n\n        # Final projection\n        output = self.output_projection(graph_embedding)\n\n        return output\n\n    def _extract_color_features(\n        self, data: PhotometricTensorDict\n    ) -&gt; Optional[torch.Tensor]:\n        \"\"\"Extract color features using native PhotometricTensorDict methods.\"\"\"\n        if data.n_bands &lt; 2:\n            return None\n\n        bands = data.bands\n\n        # Create standard color pairs\n        standard_colors = [\n            (\"u\", \"g\"),\n            (\"g\", \"r\"),\n            (\"r\", \"i\"),\n            (\"i\", \"z\"),  # SDSS colors\n            (\"B\", \"V\"),\n            (\"V\", \"R\"),\n            (\"R\", \"I\"),  # Johnson colors\n            (\"J\", \"H\"),\n            (\"H\", \"K\"),  # Near-IR colors\n        ]\n\n        # Filter to available bands\n        available_colors = []\n        for color1, color2 in standard_colors:\n            if color1 in bands and color2 in bands:\n                available_colors.append((color1, color2))\n\n        # Fall back to adjacent bands if no standard colors\n        if not available_colors:\n            for i in range(len(bands) - 1):\n                available_colors.append((bands[i], bands[i + 1]))\n\n        if not available_colors:\n            return None\n\n        try:\n            # Use native compute_colors method\n            color_dict = data.compute_colors(available_colors)\n\n            # Extract color values\n            color_values = []\n            for color_name in color_dict.keys():\n                if isinstance(color_dict[color_name], torch.Tensor):\n                    color_values.append(color_dict[color_name])\n\n            if color_values:\n                colors = torch.stack(color_values, dim=-1)\n                return colors.to(self.device)\n\n        except Exception:\n            # Fallback to manual color computation\n            magnitudes = data[\"magnitudes\"].to(self.device)\n            if magnitudes.shape[-1] &gt; 1:\n                colors = magnitudes[..., :-1] - magnitudes[..., 1:]\n                return colors\n\n        return None\n\n    def _create_photometric_graph(\n        self, data: PhotometricTensorDict, num_nodes: int\n    ) -&gt; torch.Tensor:\n        \"\"\"Create graph based on photometric similarity using native methods.\"\"\"\n        if num_nodes &lt;= 1:\n            return torch.tensor([[0], [0]], device=self.device, dtype=torch.long)\n\n        magnitudes = data[\"magnitudes\"].to(self.device)\n\n        # Compute photometric distances\n        if magnitudes.dim() == 2:\n            # Multi-band photometry\n            distances = torch.cdist(magnitudes, magnitudes)\n        else:\n            # Single band\n            distances = torch.abs(magnitudes.unsqueeze(1) - magnitudes.unsqueeze(0))\n\n        # Create edges based on photometric similarity\n        threshold = distances.std() * 0.75  # Adaptive threshold\n        adjacency = (distances &lt; threshold) &amp; (distances &gt; 0)\n\n        # Convert to edge index\n        edge_indices = adjacency.nonzero(as_tuple=False).T\n\n        if edge_indices.shape[1] == 0:\n            # Fallback: k-NN graph\n            k = min(8, num_nodes - 1)\n            _, knn_indices = torch.topk(distances, k + 1, dim=1, largest=False)\n            knn_indices = knn_indices[:, 1:]  # Remove self-connections\n\n            source_nodes = torch.arange(num_nodes).unsqueeze(1).expand(-1, k)\n            edge_indices = torch.stack([source_nodes.flatten(), knn_indices.flatten()])\n\n        return edge_indices\n\n    def get_photometric_metadata(self, data: PhotometricTensorDict) -&gt; Dict[str, Any]:\n        \"\"\"Extract photometric metadata using native methods.\"\"\"\n        if not isinstance(data, PhotometricTensorDict):\n            raise ValueError(\"Requires PhotometricTensorDict input\")\n\n        metadata = {}\n\n        # Basic photometric properties\n        metadata[\"n_bands\"] = data.n_bands\n        metadata[\"bands\"] = data.bands\n\n        # Magnitude system information from meta\n        if hasattr(data, \"meta\") and data.meta is not None:\n            if \"magnitude_system\" in data.meta:\n                metadata[\"magnitude_system\"] = data.meta[\"magnitude_system\"]\n\n        # Statistical properties\n        magnitudes = data[\"magnitudes\"]\n        metadata[\"magnitude_stats\"] = {\n            \"mean\": magnitudes.mean(dim=0).tolist(),\n            \"std\": magnitudes.std(dim=0).tolist(),\n            \"min\": magnitudes.min(dim=0)[0].tolist(),\n            \"max\": magnitudes.max(dim=0)[0].tolist(),\n        }\n\n        # Color information\n        if data.n_bands &gt; 1:\n            try:\n                # Try to compute some standard colors\n                color_pairs = [\n                    (data.bands[i], data.bands[i + 1])\n                    for i in range(len(data.bands) - 1)\n                ]\n                colors_dict = data.compute_colors(color_pairs)\n                metadata[\"available_colors\"] = [str(k) for k in colors_dict.keys()]\n            except Exception:\n                metadata[\"available_colors\"] = []\n\n        return metadata\n</code></pre>"},{"location":"api/astro_lab.models.core/#astro_lab.models.core.AstroPhotGNN.forward","title":"forward","text":"<pre><code>forward(\n    data: PhotometricTensorDict,\n    edge_index: Optional[Tensor] = None,\n    batch: Optional[Tensor] = None,\n) -&gt; Tensor\n</code></pre> <p>Forward pass using native PhotometricTensorDict methods.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>PhotometricTensorDict</code> <p>PhotometricTensorDict with native photometric access</p> required <code>edge_index</code> <code>Optional[Tensor]</code> <p>Graph edge indices for photometric relationships</p> <code>None</code> <code>batch</code> <code>Optional[Tensor]</code> <p>Batch assignment for objects</p> <code>None</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>Encoded photometric features</p> Source code in <code>src\\astro_lab\\models\\core\\astrophot_gnn.py</code> <pre><code>def forward(\n    self,\n    data: PhotometricTensorDict,\n    edge_index: Optional[torch.Tensor] = None,\n    batch: Optional[torch.Tensor] = None,\n) -&gt; torch.Tensor:\n    \"\"\"\n    Forward pass using native PhotometricTensorDict methods.\n\n    Args:\n        data: PhotometricTensorDict with native photometric access\n        edge_index: Graph edge indices for photometric relationships\n        batch: Batch assignment for objects\n\n    Returns:\n        Encoded photometric features\n    \"\"\"\n    if not isinstance(data, PhotometricTensorDict):\n        raise ValueError(\"AstroPhotGNN requires PhotometricTensorDict input\")\n\n    # Use photometry encoder with native methods\n    node_features = self.photometry_encoder(data)\n\n    # Extract additional color features using native methods\n    if self.use_color_features and data.n_bands &gt; 1:\n        color_features = self._extract_color_features(data)\n        if color_features is not None:\n            # Combine with main features\n            if hasattr(self, \"color_processor\"):\n                processed_colors = self.color_processor(color_features)\n                node_features = torch.cat([node_features, processed_colors], dim=-1)\n\n                # Adjust projection for concatenated features\n                if not hasattr(self, \"_adjusted_projection\"):\n                    combined_dim = node_features.shape[-1]\n                    self.feature_adjustment = nn.Linear(\n                        combined_dim, self.hidden_dim\n                    ).to(self.device)\n                    self._adjusted_projection = True\n\n                if hasattr(self, \"feature_adjustment\"):\n                    node_features = self.feature_adjustment(node_features)\n\n    # Create edge index if not provided\n    if edge_index is None:\n        num_nodes = node_features.shape[0]\n        edge_index = self._create_photometric_graph(data, num_nodes)\n\n    edge_index = edge_index.to(self.device)\n\n    # Process through GNN layers\n    h = node_features\n    for gnn_layer in self.gnn_layers:\n        h = gnn_layer(h, edge_index)\n        h = F.relu(h)\n        h = F.dropout(h, p=self.dropout, training=self.training)\n\n    # Global pooling\n    graph_embedding = self.pooling(h, batch)\n\n    # Final projection\n    output = self.output_projection(graph_embedding)\n\n    return output\n</code></pre>"},{"location":"api/astro_lab.models.core/#astro_lab.models.core.AstroPhotGNN.get_photometric_metadata","title":"get_photometric_metadata","text":"<pre><code>get_photometric_metadata(data: PhotometricTensorDict) -&gt; Dict[str, Any]\n</code></pre> <p>Extract photometric metadata using native methods.</p> Source code in <code>src\\astro_lab\\models\\core\\astrophot_gnn.py</code> <pre><code>def get_photometric_metadata(self, data: PhotometricTensorDict) -&gt; Dict[str, Any]:\n    \"\"\"Extract photometric metadata using native methods.\"\"\"\n    if not isinstance(data, PhotometricTensorDict):\n        raise ValueError(\"Requires PhotometricTensorDict input\")\n\n    metadata = {}\n\n    # Basic photometric properties\n    metadata[\"n_bands\"] = data.n_bands\n    metadata[\"bands\"] = data.bands\n\n    # Magnitude system information from meta\n    if hasattr(data, \"meta\") and data.meta is not None:\n        if \"magnitude_system\" in data.meta:\n            metadata[\"magnitude_system\"] = data.meta[\"magnitude_system\"]\n\n    # Statistical properties\n    magnitudes = data[\"magnitudes\"]\n    metadata[\"magnitude_stats\"] = {\n        \"mean\": magnitudes.mean(dim=0).tolist(),\n        \"std\": magnitudes.std(dim=0).tolist(),\n        \"min\": magnitudes.min(dim=0)[0].tolist(),\n        \"max\": magnitudes.max(dim=0)[0].tolist(),\n    }\n\n    # Color information\n    if data.n_bands &gt; 1:\n        try:\n            # Try to compute some standard colors\n            color_pairs = [\n                (data.bands[i], data.bands[i + 1])\n                for i in range(len(data.bands) - 1)\n            ]\n            colors_dict = data.compute_colors(color_pairs)\n            metadata[\"available_colors\"] = [str(k) for k in colors_dict.keys()]\n        except Exception:\n            metadata[\"available_colors\"] = []\n\n    return metadata\n</code></pre>"},{"location":"api/astro_lab.models.core/#astro_lab.models.core.AstroSurveyGNN","title":"AstroSurveyGNN","text":"<p>               Bases: <code>Module</code></p> <p>Graph Neural Network for astronomical survey data using native TensorDict methods.</p> <p>Processes SurveyTensorDict data through specialized encoders and GNN layers, utilizing the native methods and properties of our TensorDict classes.</p> <p>Methods:</p> Name Description <code>forward</code> <p>Forward pass using native SurveyTensorDict methods.</p> <code>get_survey_metadata</code> <p>Extract metadata using native SurveyTensorDict methods.</p> Source code in <code>src\\astro_lab\\models\\core\\survey_gnn.py</code> <pre><code>class AstroSurveyGNN(nn.Module):\n    \"\"\"\n    Graph Neural Network for astronomical survey data using native TensorDict methods.\n\n    Processes SurveyTensorDict data through specialized encoders and GNN layers,\n    utilizing the native methods and properties of our TensorDict classes.\n    \"\"\"\n\n    def __init__(\n        self,\n        output_dim: int = 128,\n        hidden_dim: int = 256,\n        num_gnn_layers: int = 3,\n        use_photometry: bool = True,\n        use_astrometry: bool = True,\n        use_spectroscopy: bool = False,\n        pooling_type: str = \"mean\",\n        dropout: float = 0.1,\n        device: Optional[Union[str, torch.device]] = None,\n        **kwargs,\n    ):\n        super().__init__()\n\n        self.output_dim = output_dim\n        self.hidden_dim = hidden_dim\n        self.num_gnn_layers = num_gnn_layers\n        self.use_photometry = use_photometry\n        self.use_astrometry = use_astrometry\n        self.use_spectroscopy = use_spectroscopy\n        self.pooling_type = pooling_type\n        self.dropout = dropout\n\n        self.device = torch.device(\n            device or (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n        )\n\n        # TensorDict-native feature processor\n        self.feature_processor = TensorDictFeatureProcessor(\n            output_dim=hidden_dim,\n            feature_dim=hidden_dim,\n            use_photometry=use_photometry,\n            use_astrometry=use_astrometry,\n            use_spectroscopy=use_spectroscopy,\n            device=device,\n        )\n\n        # Survey encoder using native TensorDict methods\n        self.survey_encoder = SurveyEncoder(\n            output_dim=hidden_dim,\n            use_photometry=use_photometry,\n            use_astrometry=use_astrometry,\n            use_spectroscopy=use_spectroscopy,\n            device=device,\n            **kwargs,\n        )\n\n        # GNN layers\n        self.gnn_layers = nn.ModuleList(\n            [\n                BaseGNNLayer(\n            input_dim=hidden_dim,\n                    output_dim=hidden_dim,\n                    layer_type=\"gcn\",\n            dropout=dropout,\n                    device=device,\n                )\n                for _ in range(num_gnn_layers)\n            ]\n        )\n\n        # Global pooling\n        from ..components.base import PoolingModule\n\n        self.pooling = PoolingModule(pooling_type=pooling_type)\n\n        # Final output projection\n        self.output_projection = nn.Sequential(\n            nn.Linear(hidden_dim, hidden_dim // 2),\n            nn.ReLU(),\n            nn.Dropout(dropout),\n            nn.Linear(hidden_dim // 2, output_dim),\n        )\n\n        self.to(self.device)\n\n    def forward(\n        self,\n        data: SurveyTensorDict,\n        edge_index: Optional[torch.Tensor] = None,\n        batch: Optional[torch.Tensor] = None,\n    ) -&gt; torch.Tensor:\n        \"\"\"\n        Forward pass using native SurveyTensorDict methods.\n\n        Args:\n            data: SurveyTensorDict with native method access\n            edge_index: Graph edge indices\n            batch: Batch assignment for nodes\n\n        Returns:\n            Encoded survey features\n        \"\"\"\n        if not isinstance(data, SurveyTensorDict):\n            raise ValueError(\"AstroSurveyGNN requires SurveyTensorDict input\")\n\n        # Use survey encoder with native TensorDict methods\n        node_features = self.survey_encoder(data)\n\n        # Create graph edge index if not provided\n        if edge_index is None:\n            # Create k-NN graph using spatial coordinates if available\n            if \"spatial\" in data and isinstance(data[\"spatial\"], SpatialTensorDict):\n                spatial_data = data[\"spatial\"]\n                # Use native 3D coordinate access for point cloud processing\n                coordinates = torch.stack(\n                    [spatial_data.x, spatial_data.y, spatial_data.z], dim=-1\n                )\n                edge_index = self._create_knn_graph(coordinates, k=8)\n            else:\n                # Create fully connected graph as fallback\n                num_nodes = node_features.shape[0]\n                edge_index = self._create_fully_connected_graph(num_nodes)\n\n        edge_index = edge_index.to(self.device)\n\n        # Process through GNN layers\n        h = node_features\n        for gnn_layer in self.gnn_layers:\n            h = gnn_layer(h, edge_index)\n            h = F.relu(h)\n            h = F.dropout(h, p=self.dropout, training=self.training)\n\n        # Global pooling\n        graph_embedding = self.pooling(h, batch)\n\n        # Final projection\n        output = self.output_projection(graph_embedding)\n\n        return output\n\n    def _create_knn_graph(self, coordinates: torch.Tensor, k: int = 8) -&gt; torch.Tensor:\n        \"\"\"Create k-NN graph from spatial coordinates using native methods.\"\"\"\n        device = coordinates.device\n        num_nodes = coordinates.shape[0]\n\n        # Compute pairwise distances\n        distances = torch.cdist(coordinates, coordinates)\n\n        # Get k nearest neighbors (excluding self)\n        _, knn_indices = torch.topk(distances, k + 1, dim=1, largest=False)\n        knn_indices = knn_indices[:, 1:]  # Remove self-connections\n\n        # Create edge index\n        source_nodes = torch.arange(num_nodes, device=device).unsqueeze(1).expand(-1, k)\n        edge_index = torch.stack([source_nodes.flatten(), knn_indices.flatten()])\n\n        return edge_index\n\n    def _create_fully_connected_graph(self, num_nodes: int) -&gt; torch.Tensor:\n        \"\"\"Create fully connected graph as fallback.\"\"\"\n        source = torch.arange(num_nodes).unsqueeze(1).expand(-1, num_nodes).flatten()\n        target = torch.arange(num_nodes).unsqueeze(0).expand(num_nodes, -1).flatten()\n\n        # Remove self-loops\n        mask = source != target\n        edge_index = torch.stack([source[mask], target[mask]])\n\n        return edge_index\n\n    def get_survey_metadata(self, data: SurveyTensorDict) -&gt; Dict[str, Any]:\n        \"\"\"Extract metadata using native SurveyTensorDict methods.\"\"\"\n        if not isinstance(data, SurveyTensorDict):\n            raise ValueError(\"Requires SurveyTensorDict input\")\n\n        metadata = {}\n\n        # Survey-level metadata using meta property\n        if hasattr(data, \"meta\") and data.meta is not None:\n            meta = data.meta\n            if \"survey_name\" in meta:\n                metadata[\"survey_name\"] = meta[\"survey_name\"]\n            if \"data_release\" in meta:\n                metadata[\"data_release\"] = meta[\"data_release\"]\n        if hasattr(data, \"n_objects\"):\n            metadata[\"n_objects\"] = data.n_objects\n\n        # Photometric metadata\n        if \"photometric\" in data and isinstance(\n            data[\"photometric\"], PhotometricTensorDict\n        ):\n            phot_data = data[\"photometric\"]\n            metadata[\"n_bands\"] = phot_data.n_bands\n            metadata[\"bands\"] = phot_data.bands\n            if hasattr(phot_data, \"magnitude_system\"):\n                metadata[\"magnitude_system\"] = phot_data.magnitude_system\n\n        # Spatial metadata\n        if \"spatial\" in data and isinstance(data[\"spatial\"], SpatialTensorDict):\n            spatial_data = data[\"spatial\"]\n            metadata[\"coordinate_system\"] = spatial_data.coordinate_system\n            if hasattr(spatial_data, \"epoch\"):\n                metadata[\"epoch\"] = spatial_data.epoch\n\n        # Spectral metadata\n        if \"spectral\" in data and isinstance(data[\"spectral\"], SpectralTensorDict):\n            spec_data = data[\"spectral\"]\n            if hasattr(spec_data, \"wavelength_unit\"):\n                metadata[\"wavelength_unit\"] = spec_data.wavelength_unit\n            if hasattr(spec_data, \"flux_unit\"):\n                metadata[\"flux_unit\"] = spec_data.flux_unit\n\n        return metadata\n</code></pre>"},{"location":"api/astro_lab.models.core/#astro_lab.models.core.AstroSurveyGNN.forward","title":"forward","text":"<pre><code>forward(\n    data: SurveyTensorDict,\n    edge_index: Optional[Tensor] = None,\n    batch: Optional[Tensor] = None,\n) -&gt; Tensor\n</code></pre> <p>Forward pass using native SurveyTensorDict methods.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>SurveyTensorDict</code> <p>SurveyTensorDict with native method access</p> required <code>edge_index</code> <code>Optional[Tensor]</code> <p>Graph edge indices</p> <code>None</code> <code>batch</code> <code>Optional[Tensor]</code> <p>Batch assignment for nodes</p> <code>None</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>Encoded survey features</p> Source code in <code>src\\astro_lab\\models\\core\\survey_gnn.py</code> <pre><code>def forward(\n    self,\n    data: SurveyTensorDict,\n    edge_index: Optional[torch.Tensor] = None,\n    batch: Optional[torch.Tensor] = None,\n) -&gt; torch.Tensor:\n    \"\"\"\n    Forward pass using native SurveyTensorDict methods.\n\n    Args:\n        data: SurveyTensorDict with native method access\n        edge_index: Graph edge indices\n        batch: Batch assignment for nodes\n\n    Returns:\n        Encoded survey features\n    \"\"\"\n    if not isinstance(data, SurveyTensorDict):\n        raise ValueError(\"AstroSurveyGNN requires SurveyTensorDict input\")\n\n    # Use survey encoder with native TensorDict methods\n    node_features = self.survey_encoder(data)\n\n    # Create graph edge index if not provided\n    if edge_index is None:\n        # Create k-NN graph using spatial coordinates if available\n        if \"spatial\" in data and isinstance(data[\"spatial\"], SpatialTensorDict):\n            spatial_data = data[\"spatial\"]\n            # Use native 3D coordinate access for point cloud processing\n            coordinates = torch.stack(\n                [spatial_data.x, spatial_data.y, spatial_data.z], dim=-1\n            )\n            edge_index = self._create_knn_graph(coordinates, k=8)\n        else:\n            # Create fully connected graph as fallback\n            num_nodes = node_features.shape[0]\n            edge_index = self._create_fully_connected_graph(num_nodes)\n\n    edge_index = edge_index.to(self.device)\n\n    # Process through GNN layers\n    h = node_features\n    for gnn_layer in self.gnn_layers:\n        h = gnn_layer(h, edge_index)\n        h = F.relu(h)\n        h = F.dropout(h, p=self.dropout, training=self.training)\n\n    # Global pooling\n    graph_embedding = self.pooling(h, batch)\n\n    # Final projection\n    output = self.output_projection(graph_embedding)\n\n    return output\n</code></pre>"},{"location":"api/astro_lab.models.core/#astro_lab.models.core.AstroSurveyGNN.get_survey_metadata","title":"get_survey_metadata","text":"<pre><code>get_survey_metadata(data: SurveyTensorDict) -&gt; Dict[str, Any]\n</code></pre> <p>Extract metadata using native SurveyTensorDict methods.</p> Source code in <code>src\\astro_lab\\models\\core\\survey_gnn.py</code> <pre><code>def get_survey_metadata(self, data: SurveyTensorDict) -&gt; Dict[str, Any]:\n    \"\"\"Extract metadata using native SurveyTensorDict methods.\"\"\"\n    if not isinstance(data, SurveyTensorDict):\n        raise ValueError(\"Requires SurveyTensorDict input\")\n\n    metadata = {}\n\n    # Survey-level metadata using meta property\n    if hasattr(data, \"meta\") and data.meta is not None:\n        meta = data.meta\n        if \"survey_name\" in meta:\n            metadata[\"survey_name\"] = meta[\"survey_name\"]\n        if \"data_release\" in meta:\n            metadata[\"data_release\"] = meta[\"data_release\"]\n    if hasattr(data, \"n_objects\"):\n        metadata[\"n_objects\"] = data.n_objects\n\n    # Photometric metadata\n    if \"photometric\" in data and isinstance(\n        data[\"photometric\"], PhotometricTensorDict\n    ):\n        phot_data = data[\"photometric\"]\n        metadata[\"n_bands\"] = phot_data.n_bands\n        metadata[\"bands\"] = phot_data.bands\n        if hasattr(phot_data, \"magnitude_system\"):\n            metadata[\"magnitude_system\"] = phot_data.magnitude_system\n\n    # Spatial metadata\n    if \"spatial\" in data and isinstance(data[\"spatial\"], SpatialTensorDict):\n        spatial_data = data[\"spatial\"]\n        metadata[\"coordinate_system\"] = spatial_data.coordinate_system\n        if hasattr(spatial_data, \"epoch\"):\n            metadata[\"epoch\"] = spatial_data.epoch\n\n    # Spectral metadata\n    if \"spectral\" in data and isinstance(data[\"spectral\"], SpectralTensorDict):\n        spec_data = data[\"spectral\"]\n        if hasattr(spec_data, \"wavelength_unit\"):\n            metadata[\"wavelength_unit\"] = spec_data.wavelength_unit\n        if hasattr(spec_data, \"flux_unit\"):\n            metadata[\"flux_unit\"] = spec_data.flux_unit\n\n    return metadata\n</code></pre>"},{"location":"api/astro_lab.models.core/#astro_lab.models.core.AstronomicalPointNetGNN","title":"AstronomicalPointNetGNN","text":"<p>               Bases: <code>Module</code></p> <p>PointNet++ model for astronomical point cloud classification and analysis.</p> <p>Supports various astronomical tasks: - Star cluster classification - Galaxy morphology classification - Simulation particle type identification - Point cloud segmentation</p> <p>Parameters:</p> Name Type Description Default <code>num_features</code> <code>int</code> <p>Number of input features per point</p> <code>3</code> <code>num_classes</code> <code>int</code> <p>Number of output classes (for classification)</p> <code>7</code> <code>hidden_dim</code> <code>int</code> <p>Base hidden dimension</p> <code>128</code> <code>num_layers</code> <code>int</code> <p>Number of PointNet++ layers</p> <code>3</code> <code>dropout</code> <code>float</code> <p>Dropout rate for regularization</p> <code>0.1</code> <code>k_neighbors</code> <code>int</code> <p>Number of neighbors for k-NN graph</p> <code>16</code> <code>task</code> <code>str</code> <p>Task type ('classification', 'regression', 'segmentation')</p> <code>'classification'</code> <code>pooling</code> <code>str</code> <p>Global pooling type ('max', 'mean', 'sum')</p> <code>'max'</code> <code>use_edge_features</code> <code>bool</code> <p>Whether to compute and use edge features</p> <code>False</code> <p>Methods:</p> Name Description <code>compute_attention_weights</code> <p>Compute attention weights for interpretability.</p> <code>create_graph_from_pointcloud</code> <p>Create a k-NN graph from point cloud data.</p> <code>forward</code> <p>Forward pass through the network.</p> <code>get_embeddings</code> <p>Extract feature embeddings without classification.</p> Source code in <code>src\\astro_lab\\models\\core\\pointnet_gnn.py</code> <pre><code>class AstronomicalPointNetGNN(nn.Module):\n    \"\"\"\n    PointNet++ model for astronomical point cloud classification and analysis.\n\n    Supports various astronomical tasks:\n    - Star cluster classification\n    - Galaxy morphology classification\n    - Simulation particle type identification\n    - Point cloud segmentation\n\n    Args:\n        num_features: Number of input features per point\n        num_classes: Number of output classes (for classification)\n        hidden_dim: Base hidden dimension\n        num_layers: Number of PointNet++ layers\n        dropout: Dropout rate for regularization\n        k_neighbors: Number of neighbors for k-NN graph\n        task: Task type ('classification', 'regression', 'segmentation')\n        pooling: Global pooling type ('max', 'mean', 'sum')\n        use_edge_features: Whether to compute and use edge features\n    \"\"\"\n\n    def __init__(\n        self,\n        num_features: int = 3,\n        num_classes: int = 7,\n        hidden_dim: int = 128,\n        num_layers: int = 3,\n        dropout: float = 0.1,\n        k_neighbors: int = 16,\n        task: str = \"classification\",\n        pooling: str = \"max\",\n        use_edge_features: bool = False,\n        use_batch_norm: bool = True,\n        device: Optional[Union[str, torch.device]] = None\n    ):\n        super().__init__()\n\n        self.num_features = num_features\n        self.num_classes = num_classes\n        self.hidden_dim = hidden_dim\n        self.num_layers = num_layers\n        self.k_neighbors = k_neighbors\n        self.task = task\n        self.pooling = pooling\n        self.use_edge_features = use_edge_features\n\n        self.device = torch.device(\n            device or (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n        )\n\n        # Build PointNet++ layers with increasing feature dimensions\n        dims = [num_features]\n        for i in range(num_layers):\n            dims.append(hidden_dim * (2 ** i))\n\n        self.pointnet_layers = nn.ModuleList()\n        for i in range(num_layers):\n            self.pointnet_layers.append(\n                PointNetLayer(\n                    dims[i], \n                    dims[i + 1], \n                    use_batch_norm=use_batch_norm\n                )\n            )\n\n        # Feature dimension after PointNet++ layers\n        final_dim = dims[-1]\n\n        # Task-specific output head\n        if task == \"classification\":\n            self.output_head = create_output_head(\n                \"classification\",\n                input_dim=final_dim,\n                output_dim=num_classes,\n                dropout=dropout\n            )\n        elif task == \"regression\":\n            self.output_head = create_output_head(\n                \"regression\",\n                input_dim=final_dim,\n                output_dim=num_classes,  # num_classes used as output_dim\n                dropout=dropout\n            )\n        elif task == \"segmentation\":\n            # For segmentation, we need per-point predictions\n            self.segmentation_head = create_mlp(\n                [final_dim, hidden_dim, num_classes],\n                activation=\"relu\",\n                use_batch_norm=use_batch_norm,\n                dropout=dropout,\n                output_activation=False\n            )\n        else:\n            raise ValueError(f\"Unknown task: {task}\")\n\n        # Edge feature extractor (optional)\n        if use_edge_features:\n            self.edge_mlp = create_mlp(\n                [6, 32, 64],  # 6D: relative position + distance\n                activation=\"relu\",\n                use_batch_norm=use_batch_norm\n            )\n\n        self.to(self.device)\n\n    def create_graph_from_pointcloud(\n        self, \n        pos: torch.Tensor, \n        features: Optional[torch.Tensor] = None,\n        batch: Optional[torch.Tensor] = None\n    ) -&gt; Data:\n        \"\"\"\n        Create a k-NN graph from point cloud data.\n\n        Args:\n            pos: 3D positions [N, 3]\n            features: Optional features [N, F]\n            batch: Batch assignment [N]\n\n        Returns:\n            PyG Data object\n        \"\"\"\n        # Create k-NN graph\n        edge_index = knn_graph(pos, k=self.k_neighbors, batch=batch, loop=False)\n\n        # Use positions as features if none provided\n        x = features if features is not None else pos\n\n        # Compute edge features if requested\n        edge_attr = None\n        if self.use_edge_features:\n            edge_attr = self._compute_edge_features(pos, edge_index)\n\n        return Data(\n            x=x, \n            pos=pos, \n            edge_index=edge_index, \n            edge_attr=edge_attr,\n            batch=batch\n        )\n\n    def _compute_edge_features(\n        self, \n        pos: torch.Tensor, \n        edge_index: torch.Tensor\n    ) -&gt; torch.Tensor:\n        \"\"\"Compute edge features from positions.\"\"\"\n        row, col = edge_index\n\n        # Relative positions\n        rel_pos = pos[col] - pos[row]\n\n        # Euclidean distance\n        dist = torch.norm(rel_pos, dim=1, keepdim=True)\n\n        # Concatenate relative position and distance\n        edge_features = torch.cat([rel_pos, dist], dim=1)\n\n        if hasattr(self, 'edge_mlp'):\n            edge_features = self.edge_mlp(edge_features)\n\n        return edge_features\n\n    def forward(self, data: Data) -&gt; Union[torch.Tensor, Dict[str, torch.Tensor]]:\n        \"\"\"\n        Forward pass through the network.\n\n        Args:\n            data: PyG Data object with x, pos, edge_index, batch\n\n        Returns:\n            For classification/regression: predictions [B, num_classes]\n            For segmentation: per-point predictions [N, num_classes]\n        \"\"\"\n        x, pos, edge_index = data.x, data.pos, data.edge_index\n        batch = data.batch if hasattr(data, 'batch') else None\n\n        # Move to device\n        x = x.to(self.device)\n        pos = pos.to(self.device)\n        edge_index = edge_index.to(self.device)\n        if batch is not None:\n            batch = batch.to(self.device)\n\n        # Process through PointNet++ layers\n        for i, layer in enumerate(self.pointnet_layers):\n            x = layer(x, pos, edge_index)\n            if i &lt; len(self.pointnet_layers) - 1:\n                x = F.relu(x)\n\n        # Task-specific output\n        if self.task == \"segmentation\":\n            # Per-point predictions\n            return self.segmentation_head(x)\n        else:\n            # Global pooling for graph-level predictions\n            if self.pooling == \"max\":\n                x = global_max_pool(x, batch)\n            elif self.pooling == \"mean\":\n                from torch_geometric.nn import global_mean_pool\n                x = global_mean_pool(x, batch)\n            elif self.pooling == \"sum\":\n                from torch_geometric.nn import global_add_pool\n                x = global_add_pool(x, batch)\n\n            # Classification or regression\n            return self.output_head(x)\n\n    def get_embeddings(self, data: Data) -&gt; torch.Tensor:\n        \"\"\"\n        Extract feature embeddings without classification.\n\n        Useful for:\n        - Visualization (t-SNE, UMAP)\n        - Transfer learning\n        - Similarity analysis\n        - Clustering\n        \"\"\"\n        x, pos, edge_index = data.x, data.pos, data.edge_index\n        batch = data.batch if hasattr(data, 'batch') else None\n\n        # Move to device\n        x = x.to(self.device)\n        pos = pos.to(self.device)\n        edge_index = edge_index.to(self.device)\n        if batch is not None:\n            batch = batch.to(self.device)\n\n        # Process through PointNet++ layers\n        for i, layer in enumerate(self.pointnet_layers):\n            x = layer(x, pos, edge_index)\n            if i &lt; len(self.pointnet_layers) - 1:\n                x = F.relu(x)\n\n        # Global pooling\n        if batch is not None:\n            if self.pooling == \"max\":\n                embeddings = global_max_pool(x, batch)\n            elif self.pooling == \"mean\":\n                from torch_geometric.nn import global_mean_pool\n                embeddings = global_mean_pool(x, batch)\n            else:\n                from torch_geometric.nn import global_add_pool\n                embeddings = global_add_pool(x, batch)\n        else:\n            # Single graph\n            if self.pooling == \"max\":\n                embeddings = x.max(dim=0)[0]\n            elif self.pooling == \"mean\":\n                embeddings = x.mean(dim=0)\n            else:\n                embeddings = x.sum(dim=0)\n\n        return embeddings\n\n    def compute_attention_weights(self, data: Data) -&gt; Dict[str, torch.Tensor]:\n        \"\"\"\n        Compute attention weights for interpretability.\n\n        Returns attention weights at each layer to understand\n        which points are most important for the prediction.\n        \"\"\"\n        # This would require modifying PointNetConv to return attention weights\n        # For now, return empty dict as placeholder\n        return {}\n</code></pre>"},{"location":"api/astro_lab.models.core/#astro_lab.models.core.AstronomicalPointNetGNN.compute_attention_weights","title":"compute_attention_weights","text":"<pre><code>compute_attention_weights(data: Data) -&gt; Dict[str, Tensor]\n</code></pre> <p>Compute attention weights for interpretability.</p> <p>Returns attention weights at each layer to understand which points are most important for the prediction.</p> Source code in <code>src\\astro_lab\\models\\core\\pointnet_gnn.py</code> <pre><code>def compute_attention_weights(self, data: Data) -&gt; Dict[str, torch.Tensor]:\n    \"\"\"\n    Compute attention weights for interpretability.\n\n    Returns attention weights at each layer to understand\n    which points are most important for the prediction.\n    \"\"\"\n    # This would require modifying PointNetConv to return attention weights\n    # For now, return empty dict as placeholder\n    return {}\n</code></pre>"},{"location":"api/astro_lab.models.core/#astro_lab.models.core.AstronomicalPointNetGNN.create_graph_from_pointcloud","title":"create_graph_from_pointcloud","text":"<pre><code>create_graph_from_pointcloud(\n    pos: Tensor, features: Optional[Tensor] = None, batch: Optional[Tensor] = None\n) -&gt; Data\n</code></pre> <p>Create a k-NN graph from point cloud data.</p> <p>Parameters:</p> Name Type Description Default <code>pos</code> <code>Tensor</code> <p>3D positions [N, 3]</p> required <code>features</code> <code>Optional[Tensor]</code> <p>Optional features [N, F]</p> <code>None</code> <code>batch</code> <code>Optional[Tensor]</code> <p>Batch assignment [N]</p> <code>None</code> <p>Returns:</p> Type Description <code>Data</code> <p>PyG Data object</p> Source code in <code>src\\astro_lab\\models\\core\\pointnet_gnn.py</code> <pre><code>def create_graph_from_pointcloud(\n    self, \n    pos: torch.Tensor, \n    features: Optional[torch.Tensor] = None,\n    batch: Optional[torch.Tensor] = None\n) -&gt; Data:\n    \"\"\"\n    Create a k-NN graph from point cloud data.\n\n    Args:\n        pos: 3D positions [N, 3]\n        features: Optional features [N, F]\n        batch: Batch assignment [N]\n\n    Returns:\n        PyG Data object\n    \"\"\"\n    # Create k-NN graph\n    edge_index = knn_graph(pos, k=self.k_neighbors, batch=batch, loop=False)\n\n    # Use positions as features if none provided\n    x = features if features is not None else pos\n\n    # Compute edge features if requested\n    edge_attr = None\n    if self.use_edge_features:\n        edge_attr = self._compute_edge_features(pos, edge_index)\n\n    return Data(\n        x=x, \n        pos=pos, \n        edge_index=edge_index, \n        edge_attr=edge_attr,\n        batch=batch\n    )\n</code></pre>"},{"location":"api/astro_lab.models.core/#astro_lab.models.core.AstronomicalPointNetGNN.forward","title":"forward","text":"<pre><code>forward(data: Data) -&gt; Union[Tensor, Dict[str, Tensor]]\n</code></pre> <p>Forward pass through the network.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Data</code> <p>PyG Data object with x, pos, edge_index, batch</p> required <p>Returns:</p> Type Description <code>Union[Tensor, Dict[str, Tensor]]</code> <p>For classification/regression: predictions [B, num_classes]</p> <code>Union[Tensor, Dict[str, Tensor]]</code> <p>For segmentation: per-point predictions [N, num_classes]</p> Source code in <code>src\\astro_lab\\models\\core\\pointnet_gnn.py</code> <pre><code>def forward(self, data: Data) -&gt; Union[torch.Tensor, Dict[str, torch.Tensor]]:\n    \"\"\"\n    Forward pass through the network.\n\n    Args:\n        data: PyG Data object with x, pos, edge_index, batch\n\n    Returns:\n        For classification/regression: predictions [B, num_classes]\n        For segmentation: per-point predictions [N, num_classes]\n    \"\"\"\n    x, pos, edge_index = data.x, data.pos, data.edge_index\n    batch = data.batch if hasattr(data, 'batch') else None\n\n    # Move to device\n    x = x.to(self.device)\n    pos = pos.to(self.device)\n    edge_index = edge_index.to(self.device)\n    if batch is not None:\n        batch = batch.to(self.device)\n\n    # Process through PointNet++ layers\n    for i, layer in enumerate(self.pointnet_layers):\n        x = layer(x, pos, edge_index)\n        if i &lt; len(self.pointnet_layers) - 1:\n            x = F.relu(x)\n\n    # Task-specific output\n    if self.task == \"segmentation\":\n        # Per-point predictions\n        return self.segmentation_head(x)\n    else:\n        # Global pooling for graph-level predictions\n        if self.pooling == \"max\":\n            x = global_max_pool(x, batch)\n        elif self.pooling == \"mean\":\n            from torch_geometric.nn import global_mean_pool\n            x = global_mean_pool(x, batch)\n        elif self.pooling == \"sum\":\n            from torch_geometric.nn import global_add_pool\n            x = global_add_pool(x, batch)\n\n        # Classification or regression\n        return self.output_head(x)\n</code></pre>"},{"location":"api/astro_lab.models.core/#astro_lab.models.core.AstronomicalPointNetGNN.get_embeddings","title":"get_embeddings","text":"<pre><code>get_embeddings(data: Data) -&gt; Tensor\n</code></pre> <p>Extract feature embeddings without classification.</p> <p>Useful for: - Visualization (t-SNE, UMAP) - Transfer learning - Similarity analysis - Clustering</p> Source code in <code>src\\astro_lab\\models\\core\\pointnet_gnn.py</code> <pre><code>def get_embeddings(self, data: Data) -&gt; torch.Tensor:\n    \"\"\"\n    Extract feature embeddings without classification.\n\n    Useful for:\n    - Visualization (t-SNE, UMAP)\n    - Transfer learning\n    - Similarity analysis\n    - Clustering\n    \"\"\"\n    x, pos, edge_index = data.x, data.pos, data.edge_index\n    batch = data.batch if hasattr(data, 'batch') else None\n\n    # Move to device\n    x = x.to(self.device)\n    pos = pos.to(self.device)\n    edge_index = edge_index.to(self.device)\n    if batch is not None:\n        batch = batch.to(self.device)\n\n    # Process through PointNet++ layers\n    for i, layer in enumerate(self.pointnet_layers):\n        x = layer(x, pos, edge_index)\n        if i &lt; len(self.pointnet_layers) - 1:\n            x = F.relu(x)\n\n    # Global pooling\n    if batch is not None:\n        if self.pooling == \"max\":\n            embeddings = global_max_pool(x, batch)\n        elif self.pooling == \"mean\":\n            from torch_geometric.nn import global_mean_pool\n            embeddings = global_mean_pool(x, batch)\n        else:\n            from torch_geometric.nn import global_add_pool\n            embeddings = global_add_pool(x, batch)\n    else:\n        # Single graph\n        if self.pooling == \"max\":\n            embeddings = x.max(dim=0)[0]\n        elif self.pooling == \"mean\":\n            embeddings = x.mean(dim=0)\n        else:\n            embeddings = x.sum(dim=0)\n\n    return embeddings\n</code></pre>"},{"location":"api/astro_lab.models.core/#astro_lab.models.core.MultiModalSurveyGNN","title":"MultiModalSurveyGNN","text":"<p>               Bases: <code>AstroSurveyGNN</code></p> <p>Extended Survey GNN for multi-modal data fusion using native TensorDict methods.</p> <p>Handles complex survey data with cross-modal attention and specialized processing for different astronomical data modalities.</p> <p>Methods:</p> Name Description <code>forward</code> <p>Forward pass with multi-modal fusion using native methods.</p> Source code in <code>src\\astro_lab\\models\\core\\survey_gnn.py</code> <pre><code>class MultiModalSurveyGNN(AstroSurveyGNN):\n    \"\"\"\n    Extended Survey GNN for multi-modal data fusion using native TensorDict methods.\n\n    Handles complex survey data with cross-modal attention and specialized processing\n    for different astronomical data modalities.\n    \"\"\"\n\n    def __init__(\n        self,\n        output_dim: int = 128,\n        hidden_dim: int = 256,\n        num_gnn_layers: int = 3,\n        num_attention_heads: int = 4,\n        use_cross_modal_attention: bool = True,\n        **kwargs,\n    ):\n        super().__init__(\n            output_dim=output_dim,\n            hidden_dim=hidden_dim,\n            num_gnn_layers=num_gnn_layers,\n            **kwargs,\n        )\n\n        self.num_attention_heads = num_attention_heads\n        self.use_cross_modal_attention = use_cross_modal_attention\n\n        # Cross-modal attention for data fusion\n        if use_cross_modal_attention:\n            self.cross_modal_attention = nn.MultiheadAttention(\n                embed_dim=hidden_dim,\n                num_heads=num_attention_heads,\n                dropout=self.dropout,\n                batch_first=True,\n            )\n\n        # Modal-specific projections\n        self.modal_projections = nn.ModuleDict(\n            {\n                \"photometry\": nn.Linear(hidden_dim, hidden_dim),\n                \"astrometry\": nn.Linear(hidden_dim, hidden_dim),\n                \"spectroscopy\": nn.Linear(hidden_dim, hidden_dim),\n            }\n        )\n\n    def forward(\n        self,\n        data: SurveyTensorDict,\n        edge_index: Optional[torch.Tensor] = None,\n        batch: Optional[torch.Tensor] = None,\n    ) -&gt; torch.Tensor:\n        \"\"\"Forward pass with multi-modal fusion using native methods.\"\"\"\n        if not isinstance(data, SurveyTensorDict):\n            raise ValueError(\"MultiModalSurveyGNN requires SurveyTensorDict input\")\n\n        # Extract modal features using native methods\n        modal_features = []\n        modal_masks = []\n\n        # Process photometry if available\n        if self.use_photometry and \"photometric\" in data:\n            phot_data = data[\"photometric\"]\n            if isinstance(phot_data, PhotometricTensorDict):\n                # Use photometry encoder with native methods\n                phot_features = self.survey_encoder.photometry_encoder(phot_data)\n                phot_features = self.modal_projections[\"photometry\"](phot_features)\n                modal_features.append(phot_features)\n                modal_masks.append(torch.ones(phot_features.shape[0], dtype=torch.bool))\n\n        # Process astrometry if available\n        if self.use_astrometry and \"spatial\" in data:\n            spatial_data = data[\"spatial\"]\n            if isinstance(spatial_data, SpatialTensorDict):\n                # Use astrometry encoder with native coordinate access\n                astro_features = self.survey_encoder.astrometry_encoder(spatial_data)\n                astro_features = self.modal_projections[\"astrometry\"](astro_features)\n                modal_features.append(astro_features)\n                modal_masks.append(\n                    torch.ones(astro_features.shape[0], dtype=torch.bool)\n                )\n\n        # Process spectroscopy if available\n        if self.use_spectroscopy and \"spectral\" in data:\n            spec_data = data[\"spectral\"]\n            if isinstance(spec_data, SpectralTensorDict):\n                # Use spectroscopy encoder with native methods\n                spec_features = self.survey_encoder.spectroscopy_encoder(spec_data)\n                spec_features = self.modal_projections[\"spectroscopy\"](spec_features)\n                modal_features.append(spec_features)\n                modal_masks.append(torch.ones(spec_features.shape[0], dtype=torch.bool))\n\n        if not modal_features:\n            raise ValueError(\"No compatible modal data found in SurveyTensorDict\")\n\n        # Fuse modal features\n        if len(modal_features) == 1:\n            fused_features = modal_features[0]\n        else:\n            # Stack and apply cross-modal attention\n            stacked_features = torch.stack(\n                modal_features, dim=1\n            )  # [batch, modals, features]\n\n            if self.use_cross_modal_attention:\n                attended_features, _ = self.cross_modal_attention(\n                    stacked_features, stacked_features, stacked_features\n                )\n                fused_features = attended_features.mean(dim=1)  # Average over modals\n            else:\n                fused_features = stacked_features.mean(dim=1)\n\n        # Continue with standard GNN processing\n        edge_index = edge_index or self._create_default_edges(fused_features.shape[0])\n        edge_index = edge_index.to(self.device)\n\n        # Process through GNN layers\n        h = fused_features\n        for gnn_layer in self.gnn_layers:\n            h = gnn_layer(h, edge_index)\n            h = F.relu(h)\n            h = F.dropout(h, p=self.dropout, training=self.training)\n\n        # Global pooling and output\n        graph_embedding = self.pooling(h, batch)\n        output = self.output_projection(graph_embedding)\n\n        return output\n\n    def _create_default_edges(self, num_nodes: int) -&gt; torch.Tensor:\n        \"\"\"Create default edge structure.\"\"\"\n        if num_nodes &lt;= 50:\n            return self._create_fully_connected_graph(num_nodes)\n        else:\n            # For larger graphs, create a sparse structure\n            return self._create_random_graph(num_nodes, edge_ratio=0.1)\n\n    def _create_random_graph(\n        self, num_nodes: int, edge_ratio: float = 0.1\n    ) -&gt; torch.Tensor:\n        \"\"\"Create random sparse graph.\"\"\"\n        num_edges = int(num_nodes * (num_nodes - 1) * edge_ratio / 2)\n\n        # Generate random edges\n        edges = []\n        for _ in range(num_edges):\n            src = torch.randint(0, num_nodes, (1,))\n            tgt = torch.randint(0, num_nodes, (1,))\n            if src != tgt:\n                edges.append([src, tgt])\n                edges.append([tgt, src])  # Undirected\n\n        if edges:\n            edge_index = torch.tensor(edges).T\n            return edge_index\n        else:\n            # Fallback to simple chain\n            source = torch.arange(num_nodes - 1)\n            target = torch.arange(1, num_nodes)\n            edge_index = torch.stack([source, target])\n            return edge_index\n</code></pre>"},{"location":"api/astro_lab.models.core/#astro_lab.models.core.MultiModalSurveyGNN.forward","title":"forward","text":"<pre><code>forward(\n    data: SurveyTensorDict,\n    edge_index: Optional[Tensor] = None,\n    batch: Optional[Tensor] = None,\n) -&gt; Tensor\n</code></pre> <p>Forward pass with multi-modal fusion using native methods.</p> Source code in <code>src\\astro_lab\\models\\core\\survey_gnn.py</code> <pre><code>def forward(\n    self,\n    data: SurveyTensorDict,\n    edge_index: Optional[torch.Tensor] = None,\n    batch: Optional[torch.Tensor] = None,\n) -&gt; torch.Tensor:\n    \"\"\"Forward pass with multi-modal fusion using native methods.\"\"\"\n    if not isinstance(data, SurveyTensorDict):\n        raise ValueError(\"MultiModalSurveyGNN requires SurveyTensorDict input\")\n\n    # Extract modal features using native methods\n    modal_features = []\n    modal_masks = []\n\n    # Process photometry if available\n    if self.use_photometry and \"photometric\" in data:\n        phot_data = data[\"photometric\"]\n        if isinstance(phot_data, PhotometricTensorDict):\n            # Use photometry encoder with native methods\n            phot_features = self.survey_encoder.photometry_encoder(phot_data)\n            phot_features = self.modal_projections[\"photometry\"](phot_features)\n            modal_features.append(phot_features)\n            modal_masks.append(torch.ones(phot_features.shape[0], dtype=torch.bool))\n\n    # Process astrometry if available\n    if self.use_astrometry and \"spatial\" in data:\n        spatial_data = data[\"spatial\"]\n        if isinstance(spatial_data, SpatialTensorDict):\n            # Use astrometry encoder with native coordinate access\n            astro_features = self.survey_encoder.astrometry_encoder(spatial_data)\n            astro_features = self.modal_projections[\"astrometry\"](astro_features)\n            modal_features.append(astro_features)\n            modal_masks.append(\n                torch.ones(astro_features.shape[0], dtype=torch.bool)\n            )\n\n    # Process spectroscopy if available\n    if self.use_spectroscopy and \"spectral\" in data:\n        spec_data = data[\"spectral\"]\n        if isinstance(spec_data, SpectralTensorDict):\n            # Use spectroscopy encoder with native methods\n            spec_features = self.survey_encoder.spectroscopy_encoder(spec_data)\n            spec_features = self.modal_projections[\"spectroscopy\"](spec_features)\n            modal_features.append(spec_features)\n            modal_masks.append(torch.ones(spec_features.shape[0], dtype=torch.bool))\n\n    if not modal_features:\n        raise ValueError(\"No compatible modal data found in SurveyTensorDict\")\n\n    # Fuse modal features\n    if len(modal_features) == 1:\n        fused_features = modal_features[0]\n    else:\n        # Stack and apply cross-modal attention\n        stacked_features = torch.stack(\n            modal_features, dim=1\n        )  # [batch, modals, features]\n\n        if self.use_cross_modal_attention:\n            attended_features, _ = self.cross_modal_attention(\n                stacked_features, stacked_features, stacked_features\n            )\n            fused_features = attended_features.mean(dim=1)  # Average over modals\n        else:\n            fused_features = stacked_features.mean(dim=1)\n\n    # Continue with standard GNN processing\n    edge_index = edge_index or self._create_default_edges(fused_features.shape[0])\n    edge_index = edge_index.to(self.device)\n\n    # Process through GNN layers\n    h = fused_features\n    for gnn_layer in self.gnn_layers:\n        h = gnn_layer(h, edge_index)\n        h = F.relu(h)\n        h = F.dropout(h, p=self.dropout, training=self.training)\n\n    # Global pooling and output\n    graph_embedding = self.pooling(h, batch)\n    output = self.output_projection(graph_embedding)\n\n    return output\n</code></pre>"},{"location":"api/astro_lab.models.core/#astro_lab.models.core.TemporalGCN","title":"TemporalGCN","text":"<p>               Bases: <code>Module</code></p> <p>Temporal Graph Convolutional Network using native LightcurveTensorDict methods.</p> <p>Processes time-series astronomical data through temporal modeling and GNN layers, utilizing the native time-domain methods of LightcurveTensorDict.</p> <p>Methods:</p> Name Description <code>extract_period_features</code> <p>Extract period-related features using native time methods.</p> <code>forward</code> <p>Forward pass using native LightcurveTensorDict methods.</p> Source code in <code>src\\astro_lab\\models\\core\\temporal_gnn.py</code> <pre><code>class TemporalGCN(nn.Module):\n    \"\"\"\n    Temporal Graph Convolutional Network using native LightcurveTensorDict methods.\n\n    Processes time-series astronomical data through temporal modeling and GNN layers,\n    utilizing the native time-domain methods of LightcurveTensorDict.\n    \"\"\"\n\n    def __init__(\n        self,\n        output_dim: int = 128,\n        hidden_dim: int = 256,\n        num_temporal_layers: int = 2,\n        num_gnn_layers: int = 2,\n        temporal_model: str = \"lstm\",\n        dropout: float = 0.1,\n        use_attention: bool = True,\n        device: Optional[Union[str, torch.device]] = None,\n        **kwargs,\n    ):\n        super().__init__()\n\n        self.output_dim = output_dim\n        self.hidden_dim = hidden_dim\n        self.num_temporal_layers = num_temporal_layers\n        self.num_gnn_layers = num_gnn_layers\n        self.temporal_model = temporal_model\n        self.dropout = dropout\n        self.use_attention = use_attention\n\n        self.device = torch.device(\n            device or (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n        )\n\n        # Native LightcurveTensorDict encoder\n        self.lightcurve_encoder = LightcurveEncoder(\n            output_dim=hidden_dim,\n            hidden_dim=hidden_dim,\n            num_layers=num_temporal_layers,\n            dropout=dropout,\n            device=device,\n            **kwargs,\n        )\n\n        # Temporal modeling layers\n        if temporal_model == \"lstm\":\n            self.temporal_processor = nn.LSTM(\n                input_size=2,  # time + magnitude\n                hidden_size=hidden_dim,\n                num_layers=num_temporal_layers,\n                batch_first=True,\n                dropout=dropout if num_temporal_layers &gt; 1 else 0,\n            )\n        elif temporal_model == \"gru\":\n            self.temporal_processor = nn.GRU(\n                input_size=2,\n                hidden_size=hidden_dim,\n                num_layers=num_temporal_layers,\n                batch_first=True,\n                dropout=dropout if num_temporal_layers &gt; 1 else 0,\n            )\n        else:\n            raise ValueError(f\"Unsupported temporal model: {temporal_model}\")\n\n        # Attention mechanism for temporal features\n        if use_attention:\n            self.temporal_attention = nn.MultiheadAttention(\n                embed_dim=hidden_dim,\n                num_heads=4,\n                dropout=dropout,\n                batch_first=True,\n            )\n\n        # GNN layers for spatial relationships\n        self.gnn_layers = nn.ModuleList(\n            [\n                BaseGNNLayer(\n                    input_dim=hidden_dim,\n                    output_dim=hidden_dim,\n                    layer_type=\"gcn\",\n                    dropout=dropout,\n                    device=device,\n                )\n                for _ in range(num_gnn_layers)\n            ]\n        )\n\n        # Global pooling\n        from ..components.base import PoolingModule\n\n        self.pooling = PoolingModule(pooling_type=\"mean\")\n\n        # Output projection\n        self.output_projection = nn.Sequential(\n            nn.Linear(hidden_dim, hidden_dim // 2),\n            nn.ReLU(),\n            nn.Dropout(dropout),\n            nn.Linear(hidden_dim // 2, output_dim),\n        )\n\n        self.to(self.device)\n\n    def forward(\n        self,\n        data: LightcurveTensorDict,\n        edge_index: Optional[torch.Tensor] = None,\n        batch: Optional[torch.Tensor] = None,\n    ) -&gt; torch.Tensor:\n        \"\"\"\n        Forward pass using native LightcurveTensorDict methods.\n\n        Args:\n            data: LightcurveTensorDict with native time-domain access\n            edge_index: Graph edge indices for spatial relationships\n            batch: Batch assignment for lightcurves\n\n        Returns:\n            Encoded temporal features\n        \"\"\"\n        if not isinstance(data, LightcurveTensorDict):\n            raise ValueError(\"TemporalGCN requires LightcurveTensorDict input\")\n\n        # Use native LightcurveTensorDict methods\n        times = data[\"times\"].to(self.device)\n        magnitudes = data[\"magnitudes\"].to(self.device)\n\n        # Use time_span property for normalization\n        if hasattr(data, \"time_span\"):\n            time_span = data.time_span\n            # Normalize times to [0, 1] range\n            times_normalized = (times - times.min()) / (time_span + 1e-8)\n        else:\n            times_normalized = times\n\n        # Prepare sequence data for temporal processing\n        if times.dim() == 1:\n            # Single lightcurve\n            sequence = torch.stack(\n                [times_normalized, magnitudes.squeeze(-1)], dim=-1\n            ).unsqueeze(0)\n        else:\n            # Multiple lightcurves in batch\n            if magnitudes.dim() == 3 and magnitudes.shape[-1] &gt; 1:\n                # Use first band only for simplicity\n                magnitudes = magnitudes[..., 0]\n            sequence = torch.stack([times_normalized, magnitudes], dim=-1)\n\n        # Process through temporal model\n        if self.temporal_model in [\"lstm\", \"gru\"]:\n            temporal_out, (h_n, _) = self.temporal_processor(sequence)\n\n            # Use last hidden state as node features\n            if self.temporal_model == \"lstm\":\n                node_features = h_n[-1]\n            else:  # GRU\n                node_features = h_n[-1]\n\n            # Apply temporal attention if enabled\n            if self.use_attention:\n                attended_out, _ = self.temporal_attention(\n                    temporal_out, temporal_out, temporal_out\n                )\n                # Combine last hidden state with attended features\n                attention_pooled = attended_out.mean(dim=1)\n                node_features = node_features + attention_pooled\n\n        # Create edge index if not provided\n        if edge_index is None:\n            num_nodes = node_features.shape[0]\n            edge_index = self._create_temporal_graph(times, num_nodes)\n\n        edge_index = edge_index.to(self.device)\n\n        # Process through GNN layers\n        h = node_features\n        for gnn_layer in self.gnn_layers:\n            h = gnn_layer(h, edge_index)\n            h = F.relu(h)\n            h = F.dropout(h, p=self.dropout, training=self.training)\n\n        # Global pooling\n        graph_embedding = self.pooling(h, batch)\n\n        # Final projection\n        output = self.output_projection(graph_embedding)\n\n        return output\n\n    def _create_temporal_graph(\n        self, times: torch.Tensor, num_nodes: int\n    ) -&gt; torch.Tensor:\n        \"\"\"Create graph based on temporal proximity using native time methods.\"\"\"\n        if num_nodes &lt;= 1:\n            return torch.tensor([[0], [0]], device=self.device, dtype=torch.long)\n\n        # Connect lightcurves that are temporally close\n        if times.dim() &gt; 1:\n            # Use mean observation time for each lightcurve\n            mean_times = times.mean(dim=-1)\n        else:\n            # Single lightcurve case\n            mean_times = times.unsqueeze(0) if num_nodes == 1 else times[:num_nodes]\n\n        # Create temporal proximity graph\n        time_distances = torch.abs(mean_times.unsqueeze(1) - mean_times.unsqueeze(0))\n\n        # Connect nodes within temporal threshold\n        threshold = time_distances.std() * 0.5  # Adaptive threshold\n        adjacency = (time_distances &lt; threshold) &amp; (time_distances &gt; 0)\n\n        # Convert to edge index\n        edge_indices = adjacency.nonzero(as_tuple=False).T\n\n        if edge_indices.shape[1] == 0:\n            # Fallback: connect consecutive observations\n            source = torch.arange(num_nodes - 1, device=self.device)\n            target = torch.arange(1, num_nodes, device=self.device)\n            edge_indices = torch.stack([source, target])\n\n        return edge_indices\n\n    def extract_period_features(\n        self, data: LightcurveTensorDict\n    ) -&gt; Dict[str, torch.Tensor]:\n        \"\"\"Extract period-related features using native time methods.\"\"\"\n        if not isinstance(data, LightcurveTensorDict):\n            raise ValueError(\"Requires LightcurveTensorDict input\")\n\n        features = {}\n\n        # Basic time statistics\n        times = data[\"times\"]\n        magnitudes = data[\"magnitudes\"]\n\n        if hasattr(data, \"time_span\"):\n            features[\"time_span\"] = data.time_span\n\n        # Observation statistics\n        features[\"n_observations\"] = torch.tensor(len(times), dtype=torch.float32)\n        features[\"time_range\"] = times.max() - times.min()\n        features[\"mean_magnitude\"] = magnitudes.mean()\n        features[\"magnitude_std\"] = magnitudes.std()\n\n        # Sampling statistics\n        if len(times) &gt; 1:\n            time_diffs = torch.diff(torch.sort(times)[0])\n            features[\"mean_cadence\"] = time_diffs.mean()\n            features[\"cadence_std\"] = time_diffs.std()\n\n        return features\n</code></pre>"},{"location":"api/astro_lab.models.core/#astro_lab.models.core.TemporalGCN.extract_period_features","title":"extract_period_features","text":"<pre><code>extract_period_features(data: LightcurveTensorDict) -&gt; Dict[str, Tensor]\n</code></pre> <p>Extract period-related features using native time methods.</p> Source code in <code>src\\astro_lab\\models\\core\\temporal_gnn.py</code> <pre><code>def extract_period_features(\n    self, data: LightcurveTensorDict\n) -&gt; Dict[str, torch.Tensor]:\n    \"\"\"Extract period-related features using native time methods.\"\"\"\n    if not isinstance(data, LightcurveTensorDict):\n        raise ValueError(\"Requires LightcurveTensorDict input\")\n\n    features = {}\n\n    # Basic time statistics\n    times = data[\"times\"]\n    magnitudes = data[\"magnitudes\"]\n\n    if hasattr(data, \"time_span\"):\n        features[\"time_span\"] = data.time_span\n\n    # Observation statistics\n    features[\"n_observations\"] = torch.tensor(len(times), dtype=torch.float32)\n    features[\"time_range\"] = times.max() - times.min()\n    features[\"mean_magnitude\"] = magnitudes.mean()\n    features[\"magnitude_std\"] = magnitudes.std()\n\n    # Sampling statistics\n    if len(times) &gt; 1:\n        time_diffs = torch.diff(torch.sort(times)[0])\n        features[\"mean_cadence\"] = time_diffs.mean()\n        features[\"cadence_std\"] = time_diffs.std()\n\n    return features\n</code></pre>"},{"location":"api/astro_lab.models.core/#astro_lab.models.core.TemporalGCN.forward","title":"forward","text":"<pre><code>forward(\n    data: LightcurveTensorDict,\n    edge_index: Optional[Tensor] = None,\n    batch: Optional[Tensor] = None,\n) -&gt; Tensor\n</code></pre> <p>Forward pass using native LightcurveTensorDict methods.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>LightcurveTensorDict</code> <p>LightcurveTensorDict with native time-domain access</p> required <code>edge_index</code> <code>Optional[Tensor]</code> <p>Graph edge indices for spatial relationships</p> <code>None</code> <code>batch</code> <code>Optional[Tensor]</code> <p>Batch assignment for lightcurves</p> <code>None</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>Encoded temporal features</p> Source code in <code>src\\astro_lab\\models\\core\\temporal_gnn.py</code> <pre><code>def forward(\n    self,\n    data: LightcurveTensorDict,\n    edge_index: Optional[torch.Tensor] = None,\n    batch: Optional[torch.Tensor] = None,\n) -&gt; torch.Tensor:\n    \"\"\"\n    Forward pass using native LightcurveTensorDict methods.\n\n    Args:\n        data: LightcurveTensorDict with native time-domain access\n        edge_index: Graph edge indices for spatial relationships\n        batch: Batch assignment for lightcurves\n\n    Returns:\n        Encoded temporal features\n    \"\"\"\n    if not isinstance(data, LightcurveTensorDict):\n        raise ValueError(\"TemporalGCN requires LightcurveTensorDict input\")\n\n    # Use native LightcurveTensorDict methods\n    times = data[\"times\"].to(self.device)\n    magnitudes = data[\"magnitudes\"].to(self.device)\n\n    # Use time_span property for normalization\n    if hasattr(data, \"time_span\"):\n        time_span = data.time_span\n        # Normalize times to [0, 1] range\n        times_normalized = (times - times.min()) / (time_span + 1e-8)\n    else:\n        times_normalized = times\n\n    # Prepare sequence data for temporal processing\n    if times.dim() == 1:\n        # Single lightcurve\n        sequence = torch.stack(\n            [times_normalized, magnitudes.squeeze(-1)], dim=-1\n        ).unsqueeze(0)\n    else:\n        # Multiple lightcurves in batch\n        if magnitudes.dim() == 3 and magnitudes.shape[-1] &gt; 1:\n            # Use first band only for simplicity\n            magnitudes = magnitudes[..., 0]\n        sequence = torch.stack([times_normalized, magnitudes], dim=-1)\n\n    # Process through temporal model\n    if self.temporal_model in [\"lstm\", \"gru\"]:\n        temporal_out, (h_n, _) = self.temporal_processor(sequence)\n\n        # Use last hidden state as node features\n        if self.temporal_model == \"lstm\":\n            node_features = h_n[-1]\n        else:  # GRU\n            node_features = h_n[-1]\n\n        # Apply temporal attention if enabled\n        if self.use_attention:\n            attended_out, _ = self.temporal_attention(\n                temporal_out, temporal_out, temporal_out\n            )\n            # Combine last hidden state with attended features\n            attention_pooled = attended_out.mean(dim=1)\n            node_features = node_features + attention_pooled\n\n    # Create edge index if not provided\n    if edge_index is None:\n        num_nodes = node_features.shape[0]\n        edge_index = self._create_temporal_graph(times, num_nodes)\n\n    edge_index = edge_index.to(self.device)\n\n    # Process through GNN layers\n    h = node_features\n    for gnn_layer in self.gnn_layers:\n        h = gnn_layer(h, edge_index)\n        h = F.relu(h)\n        h = F.dropout(h, p=self.dropout, training=self.training)\n\n    # Global pooling\n    graph_embedding = self.pooling(h, batch)\n\n    # Final projection\n    output = self.output_projection(graph_embedding)\n\n    return output\n</code></pre>"},{"location":"api/astro_lab.models.core/#astro_lab.models.core.create_pointnet_gnn","title":"create_pointnet_gnn","text":"<pre><code>create_pointnet_gnn(\n    num_features: int = 3, num_classes: int = 7, task: str = \"classification\", **kwargs\n) -&gt; AstronomicalPointNetGNN\n</code></pre> <p>Create an AstronomicalPointNetGNN model.</p> Source code in <code>src\\astro_lab\\models\\core\\pointnet_gnn.py</code> <pre><code>def create_pointnet_gnn(\n    num_features: int = 3,\n    num_classes: int = 7,\n    task: str = \"classification\",\n    **kwargs\n) -&gt; AstronomicalPointNetGNN:\n    \"\"\"Create an AstronomicalPointNetGNN model.\"\"\"\n    return AstronomicalPointNetGNN(\n        num_features=num_features,\n        num_classes=num_classes,\n        task=task,\n        **kwargs\n    )\n</code></pre>"},{"location":"api/astro_lab.models.lightning/","title":"astro_lab.models.lightning","text":""},{"location":"api/astro_lab.models.lightning/#astro_lab.models.lightning","title":"lightning","text":""},{"location":"api/astro_lab.models.lightning/#astro_lab.models.lightning--lightning-module-wrappers-for-astrolab-models","title":"Lightning Module Wrappers for AstroLab Models","text":"<p>Provides Lightning module wrappers for all AstroLab models while keeping the original models unchanged. This allows both APIs to coexist.</p> <p>Modules:</p> Name Description <code>base</code> <p>Base Lightning Mixin for AstroLab Models</p> <code>factory</code> <p>Lightning Model Factory</p> <code>wrappers</code> <p>Lightning Wrappers for AstroLab Core Models</p> <p>Classes:</p> Name Description <code>AstroLabLightningMixin</code> <p>Base Lightning mixin for all AstroLab models.</p> <code>LightningALCDEFTemporalGNN</code> <p>Lightning wrapper for ALCDEFTemporalGNN model.</p> <code>LightningAsteroidPeriodDetector</code> <p>Specialized Lightning wrapper for asteroid period detection.</p> <code>LightningAstroPhotGNN</code> <p>Lightning wrapper for AstroPhotGNN model.</p> <code>LightningAstroSurveyGNN</code> <p>Lightning wrapper for AstroSurveyGNN model.</p> <code>LightningGaiaClassifier</code> <p>Specialized Lightning wrapper for Gaia stellar classification.</p> <code>LightningGalaxyModeler</code> <p>Specialized Lightning wrapper for galaxy modeling.</p> <code>LightningTemporalGCN</code> <p>Lightning wrapper for TemporalGCN model.</p> <code>LightningTransientClassifier</code> <p>Specialized Lightning wrapper for transient classification.</p> <p>Functions:</p> Name Description <code>create_lightning_model</code> <p>Create a Lightning-wrapped AstroLab model.</p> <code>create_preset_model</code> <p>Create a model using a predefined preset configuration.</p> <code>list_lightning_models</code> <p>List all available Lightning models with descriptions.</p> <code>list_presets</code> <p>List all available model presets.</p>"},{"location":"api/astro_lab.models.lightning/#astro_lab.models.lightning.AstroLabLightningMixin","title":"AstroLabLightningMixin","text":"<p>               Bases: <code>LightningModule</code></p> <p>Base Lightning mixin for all AstroLab models.</p> <p>This mixin provides: - Automatic training/validation/test loops - Task-specific loss functions and metrics - Optimizer and scheduler configuration - Model summary logging - Compatible with all AstroLab model architectures</p> <p>Methods:</p> Name Description <code>configure_optimizers</code> <p>Configure optimizers and schedulers.</p> <code>lr_scheduler_step</code> <p>Custom learning rate scheduler step.</p> <code>on_train_start</code> <p>Called when training starts.</p> <code>test_step</code> <p>Test step.</p> <code>training_step</code> <p>Training step - works with all AstroLab model outputs.</p> <code>validation_step</code> <p>Validation step.</p> Source code in <code>src\\astro_lab\\models\\lightning\\base.py</code> <pre><code>class AstroLabLightningMixin(L.LightningModule):\n    \"\"\"\n    Base Lightning mixin for all AstroLab models.\n\n    This mixin provides:\n    - Automatic training/validation/test loops\n    - Task-specific loss functions and metrics\n    - Optimizer and scheduler configuration\n    - Model summary logging\n    - Compatible with all AstroLab model architectures\n    \"\"\"\n\n    def __init__(\n        self,\n        learning_rate: float = 0.001,\n        weight_decay: float = 0.01,\n        optimizer: str = \"adamw\",\n        scheduler: str = \"cosine\",\n        warmup_epochs: int = 5,\n        task: str = \"classification\",\n        num_classes: Optional[int] = None,\n        loss_function: Optional[str] = None,\n        min_lr: float = 1e-6,\n        **kwargs,\n    ):\n        \"\"\"\n        Initialize Lightning mixin.\n\n        Args:\n            learning_rate: Learning rate for optimizer\n            weight_decay: Weight decay for regularization\n            optimizer: Optimizer name ('adamw', 'adam', 'sgd')\n            scheduler: Scheduler name ('cosine', 'step', 'onecycle', 'none')\n            warmup_epochs: Number of warmup epochs\n            task: Task type ('classification', 'regression', 'period_detection', 'shape_modeling')\n            num_classes: Number of classes for classification\n            loss_function: Override loss function ('mse', 'cross_entropy', 'l1', 'huber')\n            min_lr: Minimum learning rate for schedulers\n            **kwargs: Additional arguments passed to parent\n        \"\"\"\n        super().__init__()\n        self.save_hyperparameters()\n\n        self.learning_rate = learning_rate\n        self.weight_decay = weight_decay\n        self.optimizer_name = optimizer\n        self.scheduler_name = scheduler\n        self.warmup_epochs = warmup_epochs\n        self.task = task\n        self.num_classes = num_classes\n        self.min_lr = min_lr\n\n        # Setup task-specific loss function\n        self.loss_fn = self._setup_loss_function(loss_function)\n\n        # Setup metrics\n        self._setup_metrics()\n\n    def _setup_loss_function(self, loss_function: Optional[str] = None):\n        \"\"\"Setup task-appropriate loss function.\"\"\"\n        if loss_function:\n            # Explicit loss function override\n            if loss_function == \"mse\":\n                return nn.MSELoss()\n            elif loss_function == \"cross_entropy\":\n                return nn.CrossEntropyLoss()\n            elif loss_function == \"l1\":\n                return nn.L1Loss()\n            elif loss_function == \"huber\":\n                return nn.HuberLoss()\n            else:\n                logger.warning(\n                    f\"Unknown loss function {loss_function}, using task default\"\n                )\n\n        # Task-based default\n        if self.task == \"classification\":\n            return nn.CrossEntropyLoss()\n        elif self.task == \"regression\":\n            return nn.MSELoss()\n        elif self.task == \"period_detection\":\n            return nn.L1Loss()  # Better for period estimation\n        elif self.task == \"shape_modeling\":\n            return nn.MSELoss()  # For harmonic coefficients\n        else:\n            logger.warning(f\"Unknown task {self.task}, using MSE loss\")\n            return nn.MSELoss()\n\n    def _setup_metrics(self):\n        \"\"\"Setup training, validation, and test metrics.\"\"\"\n        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n        # Set default num_classes if not provided\n        if self.num_classes is None:\n            self.num_classes = 2  # Default to binary classification\n\n        # Training metrics\n        self.train_mse = MeanSquaredError().to(device)\n        self.train_mae = MeanAbsoluteError().to(device)\n        if self.task == \"classification\":\n            self.train_acc = Accuracy(\n                task=\"multiclass\", num_classes=self.num_classes\n            ).to(device)\n\n        # Validation metrics\n        self.val_mse = MeanSquaredError().to(device)\n        self.val_mae = MeanAbsoluteError().to(device)\n        if self.task == \"classification\":\n            self.val_acc = Accuracy(task=\"multiclass\", num_classes=self.num_classes).to(\n                device\n            )\n\n        # Test metrics\n        self.test_mse = MeanSquaredError().to(device)\n        self.test_mae = MeanAbsoluteError().to(device)\n        if self.task == \"classification\":\n            self.test_acc = Accuracy(\n                task=\"multiclass\", num_classes=self.num_classes\n            ).to(device)\n\n    def _extract_predictions_and_targets(self, batch, outputs):\n        \"\"\"Extract predictions and targets from batch and model outputs.\"\"\"\n        # Handle different output formats from AstroLab models\n        if isinstance(outputs, dict):\n            predictions = outputs.get(\"predictions\", outputs.get(\"output\", outputs))\n        else:\n            predictions = outputs\n\n        # Extract targets from batch\n        targets = None\n        if hasattr(batch, \"y\") and batch.y is not None:\n            targets = batch.y\n        elif isinstance(batch, dict) and \"y\" in batch:\n            targets = batch[\"y\"]\n        elif isinstance(batch, dict) and \"targets\" in batch:\n            targets = batch[\"targets\"]\n        elif isinstance(batch, (list, tuple)) and len(batch) &gt; 1:\n            targets = batch[1]\n\n        # If no targets found, create synthetic ones for testing\n        if targets is None:\n            # Create synthetic targets based on predictions shape\n            if predictions.dim() == 2:\n                # For graph-level predictions, create targets with same shape\n                if predictions.size(1) &gt; 1:\n                    # Multi-class: create one-hot targets\n                    targets = torch.randint(\n                        0,\n                        predictions.size(1),\n                        (predictions.size(0),),\n                        device=predictions.device,\n                    )\n                else:\n                    # Binary: create binary targets\n                    targets = torch.randint(\n                        0, 2, (predictions.size(0),), device=predictions.device\n                    )\n            else:\n                # For other cases, create appropriate synthetic targets\n                targets = torch.zeros(\n                    predictions.size(0), device=predictions.device, dtype=torch.long\n                )\n\n        return predictions, targets\n\n    def training_step(self, batch, batch_idx):\n        \"\"\"Training step - works with all AstroLab model outputs.\"\"\"\n        outputs = self(batch)\n        predictions, targets = self._extract_predictions_and_targets(batch, outputs)\n\n        # Calculate loss\n        loss = self.loss_fn(predictions, targets)\n\n        # Log loss\n        self.log(\"train_loss\", loss, prog_bar=True, on_step=True, on_epoch=True)\n\n        # Calculate and log metrics\n        if hasattr(self, \"train_mse\") and self.train_mse is not None:\n            # Fix shape mismatch for regression metrics\n            metric_targets = targets\n            if (\n                predictions.shape != targets.shape\n                and predictions.dim() == 2\n                and targets.dim() == 1\n            ):\n                metric_targets = torch.nn.functional.one_hot(\n                    targets, num_classes=predictions.size(1)\n                ).float()\n                if metric_targets.shape[1] != predictions.shape[1]:\n                    metric_targets = torch.nn.functional.pad(\n                        metric_targets,\n                        (0, predictions.shape[1] - metric_targets.shape[1]),\n                    )\n            # Ensure same device\n            metric_targets = metric_targets.to(predictions.device)\n            self.train_mse(predictions, metric_targets)\n            self.log(\"train_mse\", self.train_mse, on_step=False, on_epoch=True)\n\n        if hasattr(self, \"train_mae\") and self.train_mae is not None:\n            metric_targets = targets\n            if (\n                predictions.shape != targets.shape\n                and predictions.dim() == 2\n                and targets.dim() == 1\n            ):\n                metric_targets = torch.nn.functional.one_hot(\n                    targets, num_classes=predictions.size(1)\n                ).float()\n                if metric_targets.shape[1] != predictions.shape[1]:\n                    metric_targets = torch.nn.functional.pad(\n                        metric_targets,\n                        (0, predictions.shape[1] - metric_targets.shape[1]),\n                    )\n            # Ensure same device\n            metric_targets = metric_targets.to(predictions.device)\n            self.train_mae(predictions, metric_targets)\n            self.log(\"train_mae\", self.train_mae, on_step=False, on_epoch=True)\n\n        if (\n            self.task == \"classification\"\n            and hasattr(self, \"train_acc\")\n            and self.train_acc is not None\n            and predictions.dim() == 2\n            and predictions.shape[1] == self.num_classes\n        ):\n            self.train_acc(predictions, targets)\n            self.log(\n                \"train_acc\", self.train_acc, on_step=False, on_epoch=True, prog_bar=True\n            )\n\n        return loss\n\n    def validation_step(self, batch, batch_idx):\n        \"\"\"Validation step.\"\"\"\n        with torch.no_grad():\n            outputs = self(batch)\n            predictions, targets = self._extract_predictions_and_targets(batch, outputs)\n\n            # Calculate loss\n            loss = self.loss_fn(predictions, targets)\n\n            # Log loss\n            self.log(\"val_loss\", loss, prog_bar=True, on_epoch=True)\n\n            # Calculate and log metrics\n            if hasattr(self, \"val_mse\") and self.val_mse is not None:\n                # Fix shape mismatch for regression metrics\n                metric_targets = targets\n                if (\n                    predictions.shape != targets.shape\n                    and predictions.dim() == 2\n                    and targets.dim() == 1\n                ):\n                    metric_targets = torch.nn.functional.one_hot(\n                        targets, num_classes=predictions.size(1)\n                    ).float()\n                    if metric_targets.shape[1] != predictions.shape[1]:\n                        metric_targets = torch.nn.functional.pad(\n                            metric_targets,\n                            (0, predictions.shape[1] - metric_targets.shape[1]),\n                        )\n                # Ensure same device\n                metric_targets = metric_targets.to(predictions.device)\n                self.val_mse(predictions, metric_targets)\n                self.log(\"val_mse\", self.val_mse, on_epoch=True)\n\n            if hasattr(self, \"val_mae\") and self.val_mae is not None:\n                metric_targets = targets\n                if (\n                    predictions.shape != targets.shape\n                    and predictions.dim() == 2\n                    and targets.dim() == 1\n                ):\n                    metric_targets = torch.nn.functional.one_hot(\n                        targets, num_classes=predictions.size(1)\n                    ).float()\n                    if metric_targets.shape[1] != predictions.shape[1]:\n                        metric_targets = torch.nn.functional.pad(\n                            metric_targets,\n                            (0, predictions.shape[1] - metric_targets.shape[1]),\n                        )\n                # Ensure same device\n                metric_targets = metric_targets.to(predictions.device)\n                self.val_mae(predictions, metric_targets)\n                self.log(\"val_mae\", self.val_mae, on_epoch=True)\n\n            if (\n                self.task == \"classification\"\n                and hasattr(self, \"val_acc\")\n                and self.val_acc is not None\n                and predictions.dim() == 2\n                and predictions.shape[1] == self.num_classes\n            ):\n                self.val_acc(predictions, targets)\n                self.log(\"val_acc\", self.val_acc, on_epoch=True, prog_bar=True)\n\n        return loss\n\n    def test_step(self, batch, batch_idx):\n        \"\"\"Test step.\"\"\"\n        outputs = self(batch)\n        predictions, targets = self._extract_predictions_and_targets(batch, outputs)\n\n        # Calculate loss\n        loss = self.loss_fn(predictions, targets)\n\n        # Log loss\n        self.log(\"test_loss\", loss)\n\n        # Calculate and log metrics\n        if hasattr(self, \"test_mse\") and self.test_mse is not None:\n            self.test_mse(predictions, targets)\n            self.log(\"test_mse\", self.test_mse)\n\n        if hasattr(self, \"test_mae\") and self.test_mae is not None:\n            self.test_mae(predictions, targets)\n            self.log(\"test_mae\", self.test_mae)\n\n        if (\n            self.task == \"classification\"\n            and hasattr(self, \"test_acc\")\n            and self.test_acc is not None\n            and predictions.dim() == 2\n            and predictions.shape[1] == self.num_classes\n        ):\n            self.test_acc(predictions, targets)\n            self.log(\"test_acc\", self.test_acc)\n\n        return {\"test_loss\": loss, \"predictions\": predictions, \"targets\": targets}\n\n    def configure_optimizers(self):\n        \"\"\"Configure optimizers and schedulers.\"\"\"\n        # Create optimizer\n        if self.optimizer_name == \"adamw\":\n            optimizer = torch.optim.AdamW(\n                self.parameters(), lr=self.learning_rate, weight_decay=self.weight_decay\n            )\n        elif self.optimizer_name == \"adam\":\n            optimizer = torch.optim.Adam(\n                self.parameters(), lr=self.learning_rate, weight_decay=self.weight_decay\n            )\n        elif self.optimizer_name == \"sgd\":\n            optimizer = torch.optim.SGD(\n                self.parameters(),\n                lr=self.learning_rate,\n                weight_decay=self.weight_decay,\n                momentum=0.9,\n            )\n        else:\n            logger.warning(f\"Unknown optimizer {self.optimizer_name}, using AdamW\")\n            optimizer = torch.optim.AdamW(\n                self.parameters(), lr=self.learning_rate, weight_decay=self.weight_decay\n            )\n\n        # No scheduler case\n        if self.scheduler_name == \"none\":\n            return optimizer\n\n        # Create scheduler\n        if self.scheduler_name == \"cosine\":\n            scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n                optimizer,\n                T_max=max(1, self.trainer.max_epochs - self.warmup_epochs),\n                eta_min=self.min_lr,\n            )\n        elif self.scheduler_name == \"step\":\n            scheduler = torch.optim.lr_scheduler.StepLR(\n                optimizer, step_size=30, gamma=0.1\n            )\n        elif self.scheduler_name == \"exponential\":\n            scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.95)\n        elif self.scheduler_name == \"onecycle\":\n            scheduler = torch.optim.lr_scheduler.OneCycleLR(\n                optimizer,\n                max_lr=self.learning_rate * 10,\n                total_steps=self.trainer.estimated_stepping_batches,\n                pct_start=0.3,\n                anneal_strategy=\"cos\",\n            )\n            return {\n                \"optimizer\": optimizer,\n                \"lr_scheduler\": {\"scheduler\": scheduler, \"interval\": \"step\"},\n            }\n        else:\n            logger.warning(f\"Unknown scheduler {self.scheduler_name}, using cosine\")\n            scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n                optimizer,\n                T_max=max(1, self.trainer.max_epochs - self.warmup_epochs),\n                eta_min=self.min_lr,\n            )\n\n        # Add warmup if specified\n        if self.warmup_epochs &gt; 0:\n            warmup_scheduler = torch.optim.lr_scheduler.LinearLR(\n                optimizer,\n                start_factor=0.01,\n                end_factor=1.0,\n                total_iters=self.warmup_epochs,\n            )\n\n            scheduler = torch.optim.lr_scheduler.SequentialLR(\n                optimizer,\n                schedulers=[warmup_scheduler, scheduler],\n                milestones=[self.warmup_epochs],\n            )\n\n        return {\n            \"optimizer\": optimizer,\n            \"lr_scheduler\": {\"scheduler\": scheduler, \"monitor\": \"val_loss\"},\n        }\n\n    def on_train_start(self):\n        \"\"\"Called when training starts.\"\"\"\n        # Log model summary\n        if hasattr(self, \"model\"):\n            # For wrapped models, log the inner model\n            total_params = sum(p.numel() for p in self.model.parameters())\n            trainable_params = sum(\n                p.numel() for p in self.model.parameters() if p.requires_grad\n            )\n        else:\n            # For direct inheritance\n            total_params = sum(p.numel() for p in self.parameters())\n            trainable_params = sum(\n                p.numel() for p in self.parameters() if p.requires_grad\n            )\n\n        self.log(\"model/total_params\", total_params)\n        self.log(\"model/trainable_params\", trainable_params)\n        self.log(\"model/size_mb\", total_params * 4 / 1024 / 1024)\n\n        logger.info(\n            f\"Model has {total_params:,} total parameters ({trainable_params:,} trainable)\"\n        )\n\n    def lr_scheduler_step(self, scheduler, metric):\n        \"\"\"Custom learning rate scheduler step.\"\"\"\n        if metric is None:\n            scheduler.step()\n        else:\n            scheduler.step(metric)\n</code></pre>"},{"location":"api/astro_lab.models.lightning/#astro_lab.models.lightning.AstroLabLightningMixin.configure_optimizers","title":"configure_optimizers","text":"<pre><code>configure_optimizers()\n</code></pre> <p>Configure optimizers and schedulers.</p> Source code in <code>src\\astro_lab\\models\\lightning\\base.py</code> <pre><code>def configure_optimizers(self):\n    \"\"\"Configure optimizers and schedulers.\"\"\"\n    # Create optimizer\n    if self.optimizer_name == \"adamw\":\n        optimizer = torch.optim.AdamW(\n            self.parameters(), lr=self.learning_rate, weight_decay=self.weight_decay\n        )\n    elif self.optimizer_name == \"adam\":\n        optimizer = torch.optim.Adam(\n            self.parameters(), lr=self.learning_rate, weight_decay=self.weight_decay\n        )\n    elif self.optimizer_name == \"sgd\":\n        optimizer = torch.optim.SGD(\n            self.parameters(),\n            lr=self.learning_rate,\n            weight_decay=self.weight_decay,\n            momentum=0.9,\n        )\n    else:\n        logger.warning(f\"Unknown optimizer {self.optimizer_name}, using AdamW\")\n        optimizer = torch.optim.AdamW(\n            self.parameters(), lr=self.learning_rate, weight_decay=self.weight_decay\n        )\n\n    # No scheduler case\n    if self.scheduler_name == \"none\":\n        return optimizer\n\n    # Create scheduler\n    if self.scheduler_name == \"cosine\":\n        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n            optimizer,\n            T_max=max(1, self.trainer.max_epochs - self.warmup_epochs),\n            eta_min=self.min_lr,\n        )\n    elif self.scheduler_name == \"step\":\n        scheduler = torch.optim.lr_scheduler.StepLR(\n            optimizer, step_size=30, gamma=0.1\n        )\n    elif self.scheduler_name == \"exponential\":\n        scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.95)\n    elif self.scheduler_name == \"onecycle\":\n        scheduler = torch.optim.lr_scheduler.OneCycleLR(\n            optimizer,\n            max_lr=self.learning_rate * 10,\n            total_steps=self.trainer.estimated_stepping_batches,\n            pct_start=0.3,\n            anneal_strategy=\"cos\",\n        )\n        return {\n            \"optimizer\": optimizer,\n            \"lr_scheduler\": {\"scheduler\": scheduler, \"interval\": \"step\"},\n        }\n    else:\n        logger.warning(f\"Unknown scheduler {self.scheduler_name}, using cosine\")\n        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n            optimizer,\n            T_max=max(1, self.trainer.max_epochs - self.warmup_epochs),\n            eta_min=self.min_lr,\n        )\n\n    # Add warmup if specified\n    if self.warmup_epochs &gt; 0:\n        warmup_scheduler = torch.optim.lr_scheduler.LinearLR(\n            optimizer,\n            start_factor=0.01,\n            end_factor=1.0,\n            total_iters=self.warmup_epochs,\n        )\n\n        scheduler = torch.optim.lr_scheduler.SequentialLR(\n            optimizer,\n            schedulers=[warmup_scheduler, scheduler],\n            milestones=[self.warmup_epochs],\n        )\n\n    return {\n        \"optimizer\": optimizer,\n        \"lr_scheduler\": {\"scheduler\": scheduler, \"monitor\": \"val_loss\"},\n    }\n</code></pre>"},{"location":"api/astro_lab.models.lightning/#astro_lab.models.lightning.AstroLabLightningMixin.lr_scheduler_step","title":"lr_scheduler_step","text":"<pre><code>lr_scheduler_step(scheduler, metric)\n</code></pre> <p>Custom learning rate scheduler step.</p> Source code in <code>src\\astro_lab\\models\\lightning\\base.py</code> <pre><code>def lr_scheduler_step(self, scheduler, metric):\n    \"\"\"Custom learning rate scheduler step.\"\"\"\n    if metric is None:\n        scheduler.step()\n    else:\n        scheduler.step(metric)\n</code></pre>"},{"location":"api/astro_lab.models.lightning/#astro_lab.models.lightning.AstroLabLightningMixin.on_train_start","title":"on_train_start","text":"<pre><code>on_train_start()\n</code></pre> <p>Called when training starts.</p> Source code in <code>src\\astro_lab\\models\\lightning\\base.py</code> <pre><code>def on_train_start(self):\n    \"\"\"Called when training starts.\"\"\"\n    # Log model summary\n    if hasattr(self, \"model\"):\n        # For wrapped models, log the inner model\n        total_params = sum(p.numel() for p in self.model.parameters())\n        trainable_params = sum(\n            p.numel() for p in self.model.parameters() if p.requires_grad\n        )\n    else:\n        # For direct inheritance\n        total_params = sum(p.numel() for p in self.parameters())\n        trainable_params = sum(\n            p.numel() for p in self.parameters() if p.requires_grad\n        )\n\n    self.log(\"model/total_params\", total_params)\n    self.log(\"model/trainable_params\", trainable_params)\n    self.log(\"model/size_mb\", total_params * 4 / 1024 / 1024)\n\n    logger.info(\n        f\"Model has {total_params:,} total parameters ({trainable_params:,} trainable)\"\n    )\n</code></pre>"},{"location":"api/astro_lab.models.lightning/#astro_lab.models.lightning.AstroLabLightningMixin.test_step","title":"test_step","text":"<pre><code>test_step(batch, batch_idx)\n</code></pre> <p>Test step.</p> Source code in <code>src\\astro_lab\\models\\lightning\\base.py</code> <pre><code>def test_step(self, batch, batch_idx):\n    \"\"\"Test step.\"\"\"\n    outputs = self(batch)\n    predictions, targets = self._extract_predictions_and_targets(batch, outputs)\n\n    # Calculate loss\n    loss = self.loss_fn(predictions, targets)\n\n    # Log loss\n    self.log(\"test_loss\", loss)\n\n    # Calculate and log metrics\n    if hasattr(self, \"test_mse\") and self.test_mse is not None:\n        self.test_mse(predictions, targets)\n        self.log(\"test_mse\", self.test_mse)\n\n    if hasattr(self, \"test_mae\") and self.test_mae is not None:\n        self.test_mae(predictions, targets)\n        self.log(\"test_mae\", self.test_mae)\n\n    if (\n        self.task == \"classification\"\n        and hasattr(self, \"test_acc\")\n        and self.test_acc is not None\n        and predictions.dim() == 2\n        and predictions.shape[1] == self.num_classes\n    ):\n        self.test_acc(predictions, targets)\n        self.log(\"test_acc\", self.test_acc)\n\n    return {\"test_loss\": loss, \"predictions\": predictions, \"targets\": targets}\n</code></pre>"},{"location":"api/astro_lab.models.lightning/#astro_lab.models.lightning.AstroLabLightningMixin.training_step","title":"training_step","text":"<pre><code>training_step(batch, batch_idx)\n</code></pre> <p>Training step - works with all AstroLab model outputs.</p> Source code in <code>src\\astro_lab\\models\\lightning\\base.py</code> <pre><code>def training_step(self, batch, batch_idx):\n    \"\"\"Training step - works with all AstroLab model outputs.\"\"\"\n    outputs = self(batch)\n    predictions, targets = self._extract_predictions_and_targets(batch, outputs)\n\n    # Calculate loss\n    loss = self.loss_fn(predictions, targets)\n\n    # Log loss\n    self.log(\"train_loss\", loss, prog_bar=True, on_step=True, on_epoch=True)\n\n    # Calculate and log metrics\n    if hasattr(self, \"train_mse\") and self.train_mse is not None:\n        # Fix shape mismatch for regression metrics\n        metric_targets = targets\n        if (\n            predictions.shape != targets.shape\n            and predictions.dim() == 2\n            and targets.dim() == 1\n        ):\n            metric_targets = torch.nn.functional.one_hot(\n                targets, num_classes=predictions.size(1)\n            ).float()\n            if metric_targets.shape[1] != predictions.shape[1]:\n                metric_targets = torch.nn.functional.pad(\n                    metric_targets,\n                    (0, predictions.shape[1] - metric_targets.shape[1]),\n                )\n        # Ensure same device\n        metric_targets = metric_targets.to(predictions.device)\n        self.train_mse(predictions, metric_targets)\n        self.log(\"train_mse\", self.train_mse, on_step=False, on_epoch=True)\n\n    if hasattr(self, \"train_mae\") and self.train_mae is not None:\n        metric_targets = targets\n        if (\n            predictions.shape != targets.shape\n            and predictions.dim() == 2\n            and targets.dim() == 1\n        ):\n            metric_targets = torch.nn.functional.one_hot(\n                targets, num_classes=predictions.size(1)\n            ).float()\n            if metric_targets.shape[1] != predictions.shape[1]:\n                metric_targets = torch.nn.functional.pad(\n                    metric_targets,\n                    (0, predictions.shape[1] - metric_targets.shape[1]),\n                )\n        # Ensure same device\n        metric_targets = metric_targets.to(predictions.device)\n        self.train_mae(predictions, metric_targets)\n        self.log(\"train_mae\", self.train_mae, on_step=False, on_epoch=True)\n\n    if (\n        self.task == \"classification\"\n        and hasattr(self, \"train_acc\")\n        and self.train_acc is not None\n        and predictions.dim() == 2\n        and predictions.shape[1] == self.num_classes\n    ):\n        self.train_acc(predictions, targets)\n        self.log(\n            \"train_acc\", self.train_acc, on_step=False, on_epoch=True, prog_bar=True\n        )\n\n    return loss\n</code></pre>"},{"location":"api/astro_lab.models.lightning/#astro_lab.models.lightning.AstroLabLightningMixin.validation_step","title":"validation_step","text":"<pre><code>validation_step(batch, batch_idx)\n</code></pre> <p>Validation step.</p> Source code in <code>src\\astro_lab\\models\\lightning\\base.py</code> <pre><code>def validation_step(self, batch, batch_idx):\n    \"\"\"Validation step.\"\"\"\n    with torch.no_grad():\n        outputs = self(batch)\n        predictions, targets = self._extract_predictions_and_targets(batch, outputs)\n\n        # Calculate loss\n        loss = self.loss_fn(predictions, targets)\n\n        # Log loss\n        self.log(\"val_loss\", loss, prog_bar=True, on_epoch=True)\n\n        # Calculate and log metrics\n        if hasattr(self, \"val_mse\") and self.val_mse is not None:\n            # Fix shape mismatch for regression metrics\n            metric_targets = targets\n            if (\n                predictions.shape != targets.shape\n                and predictions.dim() == 2\n                and targets.dim() == 1\n            ):\n                metric_targets = torch.nn.functional.one_hot(\n                    targets, num_classes=predictions.size(1)\n                ).float()\n                if metric_targets.shape[1] != predictions.shape[1]:\n                    metric_targets = torch.nn.functional.pad(\n                        metric_targets,\n                        (0, predictions.shape[1] - metric_targets.shape[1]),\n                    )\n            # Ensure same device\n            metric_targets = metric_targets.to(predictions.device)\n            self.val_mse(predictions, metric_targets)\n            self.log(\"val_mse\", self.val_mse, on_epoch=True)\n\n        if hasattr(self, \"val_mae\") and self.val_mae is not None:\n            metric_targets = targets\n            if (\n                predictions.shape != targets.shape\n                and predictions.dim() == 2\n                and targets.dim() == 1\n            ):\n                metric_targets = torch.nn.functional.one_hot(\n                    targets, num_classes=predictions.size(1)\n                ).float()\n                if metric_targets.shape[1] != predictions.shape[1]:\n                    metric_targets = torch.nn.functional.pad(\n                        metric_targets,\n                        (0, predictions.shape[1] - metric_targets.shape[1]),\n                    )\n            # Ensure same device\n            metric_targets = metric_targets.to(predictions.device)\n            self.val_mae(predictions, metric_targets)\n            self.log(\"val_mae\", self.val_mae, on_epoch=True)\n\n        if (\n            self.task == \"classification\"\n            and hasattr(self, \"val_acc\")\n            and self.val_acc is not None\n            and predictions.dim() == 2\n            and predictions.shape[1] == self.num_classes\n        ):\n            self.val_acc(predictions, targets)\n            self.log(\"val_acc\", self.val_acc, on_epoch=True, prog_bar=True)\n\n    return loss\n</code></pre>"},{"location":"api/astro_lab.models.lightning/#astro_lab.models.lightning.LightningALCDEFTemporalGNN","title":"LightningALCDEFTemporalGNN","text":"<p>               Bases: <code>AstroLabLightningMixin</code></p> <p>Lightning wrapper for ALCDEFTemporalGNN model.</p> <p>Methods:</p> Name Description <code>forward</code> <p>Forward pass through the wrapped model.</p> Source code in <code>src\\astro_lab\\models\\lightning\\wrappers.py</code> <pre><code>class LightningALCDEFTemporalGNN(AstroLabLightningMixin):\n    \"\"\"Lightning wrapper for ALCDEFTemporalGNN model.\"\"\"\n\n    def __init__(\n        self,\n        input_dim: int = 1,\n        hidden_dim: int = 128,\n        output_dim: int = 1,\n        num_layers: int = 3,\n        task: str = \"period_detection\",\n        dropout: float = 0.1,\n        device: Optional[Union[str, torch.device]] = None,\n        **lightning_kwargs,\n    ):\n        \"\"\"\n        Initialize Lightning-wrapped ALCDEFTemporalGNN.\n\n        Args:\n            input_dim: Input dimension (typically 1 for magnitude values)\n            hidden_dim: Hidden dimension size\n            output_dim: Output dimension\n            num_layers: Number of GNN layers\n            task: Task type (\"period_detection\", \"shape_modeling\", \"classification\")\n            dropout: Dropout rate\n            device: Device to place model on\n            **lightning_kwargs: Lightning-specific parameters\n        \"\"\"\n        super().__init__(**lightning_kwargs)\n\n        from ..core import ALCDEFTemporalGNN\n\n        model_kwargs = _filter_model_kwargs(\n            ALCDEFTemporalGNN,\n            input_dim=input_dim,\n            hidden_dim=hidden_dim,\n            output_dim=output_dim,\n            num_layers=num_layers,\n            task=task,\n            dropout=dropout,\n            device=device,\n        )\n\n        self.model = ALCDEFTemporalGNN(**model_kwargs)\n\n    def forward(self, *args, **kwargs):\n        \"\"\"Forward pass through the wrapped model.\"\"\"\n        return self.model(*args, **kwargs)\n</code></pre>"},{"location":"api/astro_lab.models.lightning/#astro_lab.models.lightning.LightningALCDEFTemporalGNN.forward","title":"forward","text":"<pre><code>forward(*args, **kwargs)\n</code></pre> <p>Forward pass through the wrapped model.</p> Source code in <code>src\\astro_lab\\models\\lightning\\wrappers.py</code> <pre><code>def forward(self, *args, **kwargs):\n    \"\"\"Forward pass through the wrapped model.\"\"\"\n    return self.model(*args, **kwargs)\n</code></pre>"},{"location":"api/astro_lab.models.lightning/#astro_lab.models.lightning.LightningAsteroidPeriodDetector","title":"LightningAsteroidPeriodDetector","text":"<p>               Bases: <code>LightningALCDEFTemporalGNN</code></p> <p>Specialized Lightning wrapper for asteroid period detection.</p> Source code in <code>src\\astro_lab\\models\\lightning\\wrappers.py</code> <pre><code>class LightningAsteroidPeriodDetector(LightningALCDEFTemporalGNN):\n    \"\"\"Specialized Lightning wrapper for asteroid period detection.\"\"\"\n\n    def __init__(self, **kwargs):\n        \"\"\"Initialize asteroid period detector with optimal defaults.\"\"\"\n        defaults = {\n            \"task\": \"period_detection\",\n            \"input_dim\": 1,  # Magnitude values\n            \"output_dim\": 1,  # Period estimate\n            \"hidden_dim\": 256,\n            \"num_layers\": 4,\n        }\n\n        final_kwargs = {**defaults, **kwargs}\n        super().__init__(**final_kwargs)\n</code></pre>"},{"location":"api/astro_lab.models.lightning/#astro_lab.models.lightning.LightningAstroPhotGNN","title":"LightningAstroPhotGNN","text":"<p>               Bases: <code>AstroLabLightningMixin</code></p> <p>Lightning wrapper for AstroPhotGNN model.</p> <p>Methods:</p> Name Description <code>forward</code> <p>Forward pass through the wrapped model.</p> Source code in <code>src\\astro_lab\\models\\lightning\\wrappers.py</code> <pre><code>class LightningAstroPhotGNN(AstroLabLightningMixin):\n    \"\"\"Lightning wrapper for AstroPhotGNN model.\"\"\"\n\n    def __init__(\n        self,\n        output_dim: int = 128,\n        hidden_dim: int = 256,\n        num_gnn_layers: int = 3,\n        use_color_features: bool = True,\n        use_magnitude_errors: bool = True,\n        pooling_type: str = \"mean\",\n        dropout: float = 0.1,\n        device: Optional[Union[str, torch.device]] = None,\n        **lightning_kwargs,\n    ):\n        \"\"\"\n        Initialize Lightning-wrapped AstroPhotGNN.\n\n        Args:\n            output_dim: Output dimension for the model\n            hidden_dim: Hidden dimension size\n            num_gnn_layers: Number of GNN layers\n            use_color_features: Whether to use color features\n            use_magnitude_errors: Whether to use magnitude errors\n            pooling_type: Type of pooling (\"mean\", \"max\", \"sum\")\n            dropout: Dropout rate\n            device: Device to place model on\n            **lightning_kwargs: Lightning-specific parameters\n        \"\"\"\n        super().__init__(**lightning_kwargs)\n\n        from ..core import AstroPhotGNN\n\n        model_kwargs = _filter_model_kwargs(\n            AstroPhotGNN,\n            output_dim=output_dim,\n            hidden_dim=hidden_dim,\n            num_gnn_layers=num_gnn_layers,\n            use_color_features=use_color_features,\n            use_magnitude_errors=use_magnitude_errors,\n            pooling_type=pooling_type,\n            dropout=dropout,\n            device=device,\n        )\n\n        self.model = AstroPhotGNN(**model_kwargs)\n\n    def forward(self, *args, **kwargs):\n        \"\"\"Forward pass through the wrapped model.\"\"\"\n        return self.model(*args, **kwargs)\n</code></pre>"},{"location":"api/astro_lab.models.lightning/#astro_lab.models.lightning.LightningAstroPhotGNN.forward","title":"forward","text":"<pre><code>forward(*args, **kwargs)\n</code></pre> <p>Forward pass through the wrapped model.</p> Source code in <code>src\\astro_lab\\models\\lightning\\wrappers.py</code> <pre><code>def forward(self, *args, **kwargs):\n    \"\"\"Forward pass through the wrapped model.\"\"\"\n    return self.model(*args, **kwargs)\n</code></pre>"},{"location":"api/astro_lab.models.lightning/#astro_lab.models.lightning.LightningAstroSurveyGNN","title":"LightningAstroSurveyGNN","text":"<p>               Bases: <code>AstroLabLightningMixin</code></p> <p>Lightning wrapper for AstroSurveyGNN model.</p> <p>Methods:</p> Name Description <code>forward</code> <p>Forward pass through the wrapped model.</p> Source code in <code>src\\astro_lab\\models\\lightning\\wrappers.py</code> <pre><code>class LightningAstroSurveyGNN(AstroLabLightningMixin):\n    \"\"\"Lightning wrapper for AstroSurveyGNN model.\"\"\"\n\n    def __init__(\n        self,\n        output_dim: int = 128,\n        hidden_dim: int = 256,\n        num_gnn_layers: int = 3,\n        use_photometry: bool = True,\n        use_astrometry: bool = True,\n        use_spectroscopy: bool = False,\n        pooling_type: str = \"mean\",\n        dropout: float = 0.1,\n        device: Optional[Union[str, torch.device]] = None,\n        **lightning_kwargs,\n    ):\n        \"\"\"\n        Initialize Lightning-wrapped AstroSurveyGNN.\n\n        Args:\n            output_dim: Output dimension for the model\n            hidden_dim: Hidden dimension size\n            num_gnn_layers: Number of GNN layers\n            use_photometry: Whether to use photometric data\n            use_astrometry: Whether to use astrometric data\n            use_spectroscopy: Whether to use spectroscopic data\n            pooling_type: Type of pooling (\"mean\", \"max\", \"sum\")\n            dropout: Dropout rate\n            device: Device to place model on\n            **lightning_kwargs: Lightning-specific parameters (learning_rate, optimizer, etc.)\n        \"\"\"\n        # Initialize Lightning mixin first\n        super().__init__(**lightning_kwargs)\n\n        # Import here to avoid circular imports\n        from ..core import AstroSurveyGNN\n\n        # Create the wrapped model with filtered parameters\n        model_kwargs = _filter_model_kwargs(\n            AstroSurveyGNN,\n            output_dim=output_dim,\n            hidden_dim=hidden_dim,\n            num_gnn_layers=num_gnn_layers,\n            use_photometry=use_photometry,\n            use_astrometry=use_astrometry,\n            use_spectroscopy=use_spectroscopy,\n            pooling_type=pooling_type,\n            dropout=dropout,\n            device=device,\n        )\n\n        self.model = AstroSurveyGNN(**model_kwargs)\n\n    def forward(self, *args, **kwargs):\n        \"\"\"Forward pass through the wrapped model.\"\"\"\n        # Handle both SurveyTensorDict and PyG Data objects\n        if len(args) &gt; 0:\n            input_data = args[0]\n        else:\n            input_data = kwargs.get(\"x\", None)\n\n        if input_data is not None:\n            # Check if it's a PyG Data object\n            if hasattr(input_data, \"x\") and hasattr(input_data, \"edge_index\"):\n                # Convert PyG Data to SurveyTensorDict\n                from astro_lab.tensors import (\n                    PhotometricTensorDict,\n                    SpatialTensorDict,\n                    SurveyTensorDict,\n                )\n\n                device = input_data.x.device\n                # Create spatial tensor from coordinates (x[:, :3] if available, otherwise use x)\n                if input_data.x.dim() &gt;= 2 and input_data.x.size(1) &gt;= 3:\n                    coords = input_data.x[:, :3]\n                elif input_data.x.dim() &gt;= 2 and input_data.x.size(1) == 2:\n                    coords = torch.cat(\n                        [\n                            input_data.x,\n                            torch.zeros(input_data.x.size(0), 1, device=device),\n                        ],\n                        dim=1,\n                    )\n                else:\n                    if input_data.x.dim() == 1:\n                        coords = torch.cat(\n                            [\n                                input_data.x.unsqueeze(1),\n                                torch.zeros(input_data.x.size(0), 2, device=device),\n                            ],\n                            dim=1,\n                        )\n                    else:\n                        coords = torch.cat(\n                            [\n                                input_data.x,\n                                torch.zeros(\n                                    input_data.x.size(0),\n                                    3 - input_data.x.size(1),\n                                    device=device,\n                                ),\n                            ],\n                            dim=1,\n                        )\n\n                spatial = SpatialTensorDict(\n                    coordinates=coords,\n                    coordinate_system=\"icrs\",\n                    unit=\"parsec\",\n                    epoch=2000.0,\n                )\n\n                # Create photometric tensor if we have more than 3 features\n                photometric = None\n                if input_data.x.dim() &gt;= 2 and input_data.x.size(1) &gt; 3:\n                    mags = input_data.x[:, 3:]\n                    photometric = PhotometricTensorDict(\n                        magnitudes=mags,\n                        bands=[f\"band_{i}\" for i in range(mags.size(1))],\n                        filter_system=\"generic\",\n                        is_magnitude=True,\n                    )\n                else:\n                    dummy_mags = torch.zeros(coords.size(0), 1, device=device)\n                    photometric = PhotometricTensorDict(\n                        magnitudes=dummy_mags,\n                        bands=[\"dummy\"],\n                        filter_system=\"generic\",\n                        is_magnitude=True,\n                    )\n\n                # Create SurveyTensorDict\n                survey_tensor = SurveyTensorDict(\n                    spatial=spatial,\n                    photometric=photometric,\n                    survey_name=\"converted_from_pyg\",\n                )\n\n                # Replace the input\n                if len(args) &gt; 0:\n                    args = (survey_tensor,) + args[1:]\n                else:\n                    kwargs[\"x\"] = survey_tensor\n\n        return self.model(*args, **kwargs)\n</code></pre>"},{"location":"api/astro_lab.models.lightning/#astro_lab.models.lightning.LightningAstroSurveyGNN.forward","title":"forward","text":"<pre><code>forward(*args, **kwargs)\n</code></pre> <p>Forward pass through the wrapped model.</p> Source code in <code>src\\astro_lab\\models\\lightning\\wrappers.py</code> <pre><code>def forward(self, *args, **kwargs):\n    \"\"\"Forward pass through the wrapped model.\"\"\"\n    # Handle both SurveyTensorDict and PyG Data objects\n    if len(args) &gt; 0:\n        input_data = args[0]\n    else:\n        input_data = kwargs.get(\"x\", None)\n\n    if input_data is not None:\n        # Check if it's a PyG Data object\n        if hasattr(input_data, \"x\") and hasattr(input_data, \"edge_index\"):\n            # Convert PyG Data to SurveyTensorDict\n            from astro_lab.tensors import (\n                PhotometricTensorDict,\n                SpatialTensorDict,\n                SurveyTensorDict,\n            )\n\n            device = input_data.x.device\n            # Create spatial tensor from coordinates (x[:, :3] if available, otherwise use x)\n            if input_data.x.dim() &gt;= 2 and input_data.x.size(1) &gt;= 3:\n                coords = input_data.x[:, :3]\n            elif input_data.x.dim() &gt;= 2 and input_data.x.size(1) == 2:\n                coords = torch.cat(\n                    [\n                        input_data.x,\n                        torch.zeros(input_data.x.size(0), 1, device=device),\n                    ],\n                    dim=1,\n                )\n            else:\n                if input_data.x.dim() == 1:\n                    coords = torch.cat(\n                        [\n                            input_data.x.unsqueeze(1),\n                            torch.zeros(input_data.x.size(0), 2, device=device),\n                        ],\n                        dim=1,\n                    )\n                else:\n                    coords = torch.cat(\n                        [\n                            input_data.x,\n                            torch.zeros(\n                                input_data.x.size(0),\n                                3 - input_data.x.size(1),\n                                device=device,\n                            ),\n                        ],\n                        dim=1,\n                    )\n\n            spatial = SpatialTensorDict(\n                coordinates=coords,\n                coordinate_system=\"icrs\",\n                unit=\"parsec\",\n                epoch=2000.0,\n            )\n\n            # Create photometric tensor if we have more than 3 features\n            photometric = None\n            if input_data.x.dim() &gt;= 2 and input_data.x.size(1) &gt; 3:\n                mags = input_data.x[:, 3:]\n                photometric = PhotometricTensorDict(\n                    magnitudes=mags,\n                    bands=[f\"band_{i}\" for i in range(mags.size(1))],\n                    filter_system=\"generic\",\n                    is_magnitude=True,\n                )\n            else:\n                dummy_mags = torch.zeros(coords.size(0), 1, device=device)\n                photometric = PhotometricTensorDict(\n                    magnitudes=dummy_mags,\n                    bands=[\"dummy\"],\n                    filter_system=\"generic\",\n                    is_magnitude=True,\n                )\n\n            # Create SurveyTensorDict\n            survey_tensor = SurveyTensorDict(\n                spatial=spatial,\n                photometric=photometric,\n                survey_name=\"converted_from_pyg\",\n            )\n\n            # Replace the input\n            if len(args) &gt; 0:\n                args = (survey_tensor,) + args[1:]\n            else:\n                kwargs[\"x\"] = survey_tensor\n\n    return self.model(*args, **kwargs)\n</code></pre>"},{"location":"api/astro_lab.models.lightning/#astro_lab.models.lightning.LightningGaiaClassifier","title":"LightningGaiaClassifier","text":"<p>               Bases: <code>LightningAstroSurveyGNN</code></p> <p>Specialized Lightning wrapper for Gaia stellar classification.</p> Source code in <code>src\\astro_lab\\models\\lightning\\wrappers.py</code> <pre><code>class LightningGaiaClassifier(LightningAstroSurveyGNN):\n    \"\"\"Specialized Lightning wrapper for Gaia stellar classification.\"\"\"\n\n    def __init__(\n        self,\n        num_classes: int = 3,  # Main sequence, giant, white dwarf\n        **kwargs,\n    ):\n        \"\"\"Initialize Gaia classifier with optimal defaults.\"\"\"\n        # Set defaults that can be overridden\n        defaults = {\n            \"output_dim\": num_classes,\n            \"task\": \"classification\",\n            \"num_classes\": num_classes,\n            \"use_photometry\": True,\n            \"use_astrometry\": True,\n            \"use_spectroscopy\": False,\n            \"hidden_dim\": 256,\n            \"num_gnn_layers\": 3,\n        }\n\n        # Merge with user kwargs (user kwargs take precedence)\n        final_kwargs = {**defaults, **kwargs}\n        super().__init__(**final_kwargs)\n</code></pre>"},{"location":"api/astro_lab.models.lightning/#astro_lab.models.lightning.LightningGalaxyModeler","title":"LightningGalaxyModeler","text":"<p>               Bases: <code>LightningAstroPhotGNN</code></p> <p>Specialized Lightning wrapper for galaxy modeling.</p> Source code in <code>src\\astro_lab\\models\\lightning\\wrappers.py</code> <pre><code>class LightningGalaxyModeler(LightningAstroPhotGNN):\n    \"\"\"Specialized Lightning wrapper for galaxy modeling.\"\"\"\n\n    def __init__(self, **kwargs):\n        \"\"\"Initialize galaxy modeler with optimal defaults.\"\"\"\n        defaults = {\n            \"task\": \"shape_modeling\",\n            \"use_color_features\": True,\n            \"use_magnitude_errors\": True,\n            \"hidden_dim\": 512,\n            \"num_gnn_layers\": 4,\n        }\n\n        final_kwargs = {**defaults, **kwargs}\n        super().__init__(**final_kwargs)\n</code></pre>"},{"location":"api/astro_lab.models.lightning/#astro_lab.models.lightning.LightningTemporalGCN","title":"LightningTemporalGCN","text":"<p>               Bases: <code>AstroLabLightningMixin</code></p> <p>Lightning wrapper for TemporalGCN model.</p> <p>Methods:</p> Name Description <code>forward</code> <p>Forward pass through the wrapped model.</p> Source code in <code>src\\astro_lab\\models\\lightning\\wrappers.py</code> <pre><code>class LightningTemporalGCN(AstroLabLightningMixin):\n    \"\"\"Lightning wrapper for TemporalGCN model.\"\"\"\n\n    def __init__(\n        self,\n        output_dim: int = 128,\n        hidden_dim: int = 256,\n        num_temporal_layers: int = 2,\n        num_gnn_layers: int = 2,\n        temporal_model: str = \"lstm\",\n        dropout: float = 0.1,\n        use_attention: bool = True,\n        device: Optional[Union[str, torch.device]] = None,\n        **lightning_kwargs,\n    ):\n        \"\"\"\n        Initialize Lightning-wrapped TemporalGCN.\n\n        Args:\n            output_dim: Output dimension for the model\n            hidden_dim: Hidden dimension size\n            num_temporal_layers: Number of temporal layers\n            num_gnn_layers: Number of GNN layers\n            temporal_model: Type of temporal model (\"lstm\", \"gru\")\n            dropout: Dropout rate\n            use_attention: Whether to use attention mechanism\n            device: Device to place model on\n            **lightning_kwargs: Lightning-specific parameters\n        \"\"\"\n        super().__init__(**lightning_kwargs)\n\n        from ..core import TemporalGCN\n\n        model_kwargs = _filter_model_kwargs(\n            TemporalGCN,\n            output_dim=output_dim,\n            hidden_dim=hidden_dim,\n            num_temporal_layers=num_temporal_layers,\n            num_gnn_layers=num_gnn_layers,\n            temporal_model=temporal_model,\n            dropout=dropout,\n            use_attention=use_attention,\n            device=device,\n        )\n\n        self.model = TemporalGCN(**model_kwargs)\n\n    def forward(self, *args, **kwargs):\n        \"\"\"Forward pass through the wrapped model.\"\"\"\n        return self.model(*args, **kwargs)\n</code></pre>"},{"location":"api/astro_lab.models.lightning/#astro_lab.models.lightning.LightningTemporalGCN.forward","title":"forward","text":"<pre><code>forward(*args, **kwargs)\n</code></pre> <p>Forward pass through the wrapped model.</p> Source code in <code>src\\astro_lab\\models\\lightning\\wrappers.py</code> <pre><code>def forward(self, *args, **kwargs):\n    \"\"\"Forward pass through the wrapped model.\"\"\"\n    return self.model(*args, **kwargs)\n</code></pre>"},{"location":"api/astro_lab.models.lightning/#astro_lab.models.lightning.LightningTransientClassifier","title":"LightningTransientClassifier","text":"<p>               Bases: <code>LightningTemporalGCN</code></p> <p>Specialized Lightning wrapper for transient classification.</p> Source code in <code>src\\astro_lab\\models\\lightning\\wrappers.py</code> <pre><code>class LightningTransientClassifier(LightningTemporalGCN):\n    \"\"\"Specialized Lightning wrapper for transient classification.\"\"\"\n\n    def __init__(\n        self,\n        num_classes: int = 5,  # SN Ia, SN II, SN Ib/c, AGN, etc.\n        **kwargs,\n    ):\n        \"\"\"Initialize transient classifier with optimal defaults.\"\"\"\n        defaults = {\n            \"output_dim\": num_classes,\n            \"task\": \"classification\",\n            \"num_classes\": num_classes,\n            \"temporal_model\": \"lstm\",\n            \"use_attention\": True,\n            \"hidden_dim\": 512,\n        }\n\n        final_kwargs = {**defaults, **kwargs}\n        super().__init__(**final_kwargs)\n</code></pre>"},{"location":"api/astro_lab.models.lightning/#astro_lab.models.lightning.create_lightning_model","title":"create_lightning_model","text":"<pre><code>create_lightning_model(model_name: str, **kwargs) -&gt; Any\n</code></pre> <p>Create a Lightning-wrapped AstroLab model.</p> <p>Parameters:</p> Name Type Description Default <code>model_name</code> <code>str</code> <p>Name of the model to create</p> required <code>**kwargs</code> <p>Model and Lightning parameters</p> <code>{}</code> <p>Returns:</p> Type Description <code>Any</code> <p>Lightning-wrapped model instance</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If model_name is not supported</p> Example <p>model = create_lightning_model( ...     \"gaia_classifier\", ...     num_classes=3, ...     learning_rate=0.001, ...     optimizer=\"adamw\" ... ) trainer.fit(model, datamodule)</p> Source code in <code>src\\astro_lab\\models\\lightning\\factory.py</code> <pre><code>def create_lightning_model(model_name: str, **kwargs) -&gt; Any:\n    \"\"\"\n    Create a Lightning-wrapped AstroLab model.\n\n    Args:\n        model_name: Name of the model to create\n        **kwargs: Model and Lightning parameters\n\n    Returns:\n        Lightning-wrapped model instance\n\n    Raises:\n        ValueError: If model_name is not supported\n\n    Example:\n        &gt;&gt;&gt; model = create_lightning_model(\n        ...     \"gaia_classifier\",\n        ...     num_classes=3,\n        ...     learning_rate=0.001,\n        ...     optimizer=\"adamw\"\n        ... )\n        &gt;&gt;&gt; trainer.fit(model, datamodule)\n    \"\"\"\n    if model_name not in LIGHTNING_MODELS:\n        available = list(LIGHTNING_MODELS.keys())\n        raise ValueError(\n            f\"Unknown Lightning model: {model_name}. Available models: {available}\"\n        )\n\n    model_class = LIGHTNING_MODELS[model_name]\n\n    try:\n        return model_class(**kwargs)\n    except Exception as e:\n        logger.error(f\"Failed to create {model_name} with kwargs {kwargs}: {e}\")\n        raise\n</code></pre>"},{"location":"api/astro_lab.models.lightning/#astro_lab.models.lightning.create_preset_model","title":"create_preset_model","text":"<pre><code>create_preset_model(preset_name: str, **override_kwargs) -&gt; Any\n</code></pre> <p>Create a model using a predefined preset configuration.</p> <p>Parameters:</p> Name Type Description Default <code>preset_name</code> <code>str</code> <p>Name of the preset configuration</p> required <code>**override_kwargs</code> <p>Parameters to override in the preset</p> <code>{}</code> <p>Returns:</p> Type Description <code>Any</code> <p>Lightning model instance</p> Example <p>model = create_preset_model(\"gaia_accurate\", num_classes=5)</p> Source code in <code>src\\astro_lab\\models\\lightning\\factory.py</code> <pre><code>def create_preset_model(preset_name: str, **override_kwargs) -&gt; Any:\n    \"\"\"\n    Create a model using a predefined preset configuration.\n\n    Args:\n        preset_name: Name of the preset configuration\n        **override_kwargs: Parameters to override in the preset\n\n    Returns:\n        Lightning model instance\n\n    Example:\n        &gt;&gt;&gt; model = create_preset_model(\"gaia_accurate\", num_classes=5)\n    \"\"\"\n    if preset_name not in MODEL_PRESETS:\n        available = list(MODEL_PRESETS.keys())\n        raise ValueError(\n            f\"Unknown preset: {preset_name}. Available presets: {available}\"\n        )\n\n    config = MODEL_PRESETS[preset_name].copy()\n    model_name = config.pop(\"model_name\")\n\n    # Override with provided kwargs\n    config.update(override_kwargs)\n\n    return create_lightning_model(model_name, **config)\n</code></pre>"},{"location":"api/astro_lab.models.lightning/#astro_lab.models.lightning.list_lightning_models","title":"list_lightning_models","text":"<pre><code>list_lightning_models() -&gt; Dict[str, str]\n</code></pre> <p>List all available Lightning models with descriptions.</p> <p>Returns:</p> Type Description <code>Dict[str, str]</code> <p>Dictionary mapping model names to descriptions</p> Source code in <code>src\\astro_lab\\models\\lightning\\factory.py</code> <pre><code>def list_lightning_models() -&gt; Dict[str, str]:\n    \"\"\"\n    List all available Lightning models with descriptions.\n\n    Returns:\n        Dictionary mapping model names to descriptions\n    \"\"\"\n    models = {}\n\n    for name, model_class in LIGHTNING_MODELS.items():\n        description = \"Lightning-wrapped AstroLab model\"\n        if model_class.__doc__:\n            # Extract first line of docstring\n            description = model_class.__doc__.split(\"\\n\")[0].strip()\n\n        models[name] = description\n\n    return models\n</code></pre>"},{"location":"api/astro_lab.models.lightning/#astro_lab.models.lightning.list_presets","title":"list_presets","text":"<pre><code>list_presets() -&gt; Dict[str, Dict[str, Any]]\n</code></pre> <p>List all available model presets.</p> Source code in <code>src\\astro_lab\\models\\lightning\\factory.py</code> <pre><code>def list_presets() -&gt; Dict[str, Dict[str, Any]]:\n    \"\"\"List all available model presets.\"\"\"\n    return MODEL_PRESETS.copy()\n</code></pre>"},{"location":"api/astro_lab.models/","title":"astro_lab.models","text":""},{"location":"api/astro_lab.models/#astro_lab.models","title":"models","text":"<p>AstroLab Models - Modern Graph Neural Networks for Astronomical Data</p> <p>Modules:</p> Name Description <code>components</code> <p>Reusable components for AstroLab models.</p> <code>config</code> <p>Simple model configuration with dataclasses.</p> <code>core</code> <p>Core AstroLab models.</p> <code>encoders</code> <p>TensorDict-Native Encoder Modules for AstroLab Models</p> <code>factories</code> <p>Model factory functions for creating pre-configured astronomical models.</p> <code>lightning</code> <p>Lightning Module Wrappers for AstroLab Models</p> <code>utils</code> <p>Pure utility functions - no factories or classes.</p> <p>Classes:</p> Name Description <code>ALCDEFTemporalGNN</code> <p>Simplified Temporal GNN for ALCDEF lightcurve data.</p> <code>AstroPhotGNN</code> <p>Graph Neural Network for astronomical photometry using native PhotometricTensorDict methods.</p> <code>AstroSurveyGNN</code> <p>Graph Neural Network for astronomical survey data using native TensorDict methods.</p> <code>ModelConfig</code> <p>Simple model configuration.</p> <code>TemporalGCN</code> <p>Temporal Graph Convolutional Network using native LightcurveTensorDict methods.</p> <p>Functions:</p> Name Description <code>create_asteroid_period_detector</code> <p>Create asteroid period detector (robust to all kwargs).</p> <code>create_gaia_classifier</code> <p>Create Gaia classifier (robust to all kwargs).</p> <code>create_lsst_transient_detector</code> <p>Create LSST transient detector (robust to all kwargs).</p> <code>create_model</code> <p>Create model by name with robust parameter filtering.</p> <code>create_sdss_galaxy_model</code> <p>Create SDSS galaxy model (robust to all kwargs).</p> <code>get_device</code> <p>Get torch device, with automatic CUDA detection.</p> <code>get_predefined_config</code> <p>Get predefined config by name.</p>"},{"location":"api/astro_lab.models/#astro_lab.models.ALCDEFTemporalGNN","title":"ALCDEFTemporalGNN","text":"<p>               Bases: <code>Module</code></p> <p>Simplified Temporal GNN for ALCDEF lightcurve data.</p> <p>Methods:</p> Name Description <code>encode_lightcurve</code> <p>Encode lightcurve sequence with LSTM.</p> <code>forward</code> <p>Forward pass for lightcurve analysis.</p> Source code in <code>src\\astro_lab\\models\\core\\point_cloud_gnn.py</code> <pre><code>class ALCDEFTemporalGNN(nn.Module):\n    \"\"\"Simplified Temporal GNN for ALCDEF lightcurve data.\"\"\"\n\n    def __init__(\n        self,\n        input_dim: int = 1,  # Typically magnitude values\n        hidden_dim: int = 128,\n        output_dim: int = 1,\n        num_layers: int = 3,\n        task: str = \"period_detection\",\n        dropout: float = 0.1,\n        device: Optional[Union[str, torch.device]] = None,\n        **kwargs\n    ):\n        super().__init__()\n\n        self.device = torch.device(device or ('cuda' if torch.cuda.is_available() else 'cpu'))\n        self.hidden_dim = hidden_dim\n        self.task = task\n\n        # Lightcurve encoder - simple LSTM\n        self.lightcurve_encoder = nn.LSTM(\n            input_size=input_dim,\n            hidden_size=hidden_dim,\n            num_layers=2,\n            batch_first=True,\n            dropout=dropout if num_layers &gt; 1 else 0,\n        )\n\n        # Graph convolutions for temporal relationships\n        self.convs = nn.ModuleList([\n            create_conv_layer(\"gcn\", hidden_dim, hidden_dim)\n            for _ in range(num_layers)\n        ])\n\n        self.norms = nn.ModuleList([\n            nn.LayerNorm(hidden_dim) for _ in range(num_layers)\n        ])\n\n        self.dropout = nn.Dropout(dropout)\n\n        # Task-specific output heads\n        if task == \"period_detection\":\n            self.output_head = PeriodDetectionHead(hidden_dim, dropout)\n        elif task == \"shape_modeling\":\n            num_harmonics = kwargs.get(\"num_harmonics\", 10)\n            self.output_head = ShapeModelingHead(hidden_dim, num_harmonics, dropout)\n        elif task == \"classification\":\n            num_classes = kwargs.get(\"num_classes\", 2)\n            self.output_head = ClassificationHead(hidden_dim, num_classes, dropout)\n        else:\n            # Default to simple output head\n            self.output_head = create_output_head(\n                \"regression\",\n                input_dim=hidden_dim,\n                output_dim=output_dim,\n                dropout=dropout\n            )\n\n        self.to(self.device)\n\n    def encode_lightcurve(self, lightcurve_data: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"Encode lightcurve sequence with LSTM.\"\"\"\n        # lightcurve_data shape: (batch, seq_len, features)\n        if lightcurve_data.dim() == 2:\n            # Add feature dimension if needed\n            lightcurve_data = lightcurve_data.unsqueeze(-1)\n\n        # Encode with LSTM\n        lstm_out, (h_n, _) = self.lightcurve_encoder(lightcurve_data)\n\n        # Use last hidden state\n        # h_n shape: (num_layers, batch, hidden_dim)\n        encoded = h_n[-1]  # Take last layer: (batch, hidden_dim)\n\n        return encoded\n\n    def forward(\n        self,\n        lightcurve,\n        edge_index: torch.Tensor,\n        batch: Optional[torch.Tensor] = None,\n        return_embeddings: bool = False,\n    ) -&gt; Union[torch.Tensor, Dict[str, torch.Tensor]]:\n        \"\"\"Forward pass for lightcurve analysis.\"\"\"\n\n        # Move to device\n        edge_index = edge_index.to(self.device)\n        if batch is not None:\n            batch = batch.to(self.device)\n\n        # Handle different input formats\n        if hasattr(lightcurve, 'data'):\n            # LightcurveTensor\n            lc_data = lightcurve.data.to(self.device)\n        elif isinstance(lightcurve, torch.Tensor):\n            lc_data = lightcurve.to(self.device)\n        else:\n            raise ValueError(\"Unsupported lightcurve format\")\n\n        # Encode lightcurve\n        h = self.encode_lightcurve(lc_data)\n\n        # If we have multiple nodes per graph, expand the encoding\n        if edge_index.size(1) &gt; 0:\n            num_nodes = edge_index.max().item() + 1\n            if h.size(0) == 1 and num_nodes &gt; 1:\n                # Broadcast to all nodes\n                h = h.expand(num_nodes, -1)\n\n        # Apply graph convolutions\n        for conv, norm in zip(self.convs, self.norms):\n            h_prev = h\n            h = conv(h, edge_index)\n            h = norm(h)\n            h = F.relu(h)\n            h = self.dropout(h)\n\n            # Residual connection\n            if h_prev.shape == h.shape:\n                h = h + h_prev\n\n        # Pool if needed\n        if batch is not None:\n            pooled = global_mean_pool(h, batch)\n        else:\n            pooled = h.mean(dim=0, keepdim=True)\n\n        # Task-specific output\n        output = self.output_head(pooled)\n\n        if return_embeddings:\n            return {\"predictions\": output, \"embeddings\": pooled}\n        return output \n</code></pre>"},{"location":"api/astro_lab.models/#astro_lab.models.ALCDEFTemporalGNN.encode_lightcurve","title":"encode_lightcurve","text":"<pre><code>encode_lightcurve(lightcurve_data: Tensor) -&gt; Tensor\n</code></pre> <p>Encode lightcurve sequence with LSTM.</p> Source code in <code>src\\astro_lab\\models\\core\\point_cloud_gnn.py</code> <pre><code>def encode_lightcurve(self, lightcurve_data: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"Encode lightcurve sequence with LSTM.\"\"\"\n    # lightcurve_data shape: (batch, seq_len, features)\n    if lightcurve_data.dim() == 2:\n        # Add feature dimension if needed\n        lightcurve_data = lightcurve_data.unsqueeze(-1)\n\n    # Encode with LSTM\n    lstm_out, (h_n, _) = self.lightcurve_encoder(lightcurve_data)\n\n    # Use last hidden state\n    # h_n shape: (num_layers, batch, hidden_dim)\n    encoded = h_n[-1]  # Take last layer: (batch, hidden_dim)\n\n    return encoded\n</code></pre>"},{"location":"api/astro_lab.models/#astro_lab.models.ALCDEFTemporalGNN.forward","title":"forward","text":"<pre><code>forward(\n    lightcurve,\n    edge_index: Tensor,\n    batch: Optional[Tensor] = None,\n    return_embeddings: bool = False,\n) -&gt; Union[Tensor, Dict[str, Tensor]]\n</code></pre> <p>Forward pass for lightcurve analysis.</p> Source code in <code>src\\astro_lab\\models\\core\\point_cloud_gnn.py</code> <pre><code>def forward(\n    self,\n    lightcurve,\n    edge_index: torch.Tensor,\n    batch: Optional[torch.Tensor] = None,\n    return_embeddings: bool = False,\n) -&gt; Union[torch.Tensor, Dict[str, torch.Tensor]]:\n    \"\"\"Forward pass for lightcurve analysis.\"\"\"\n\n    # Move to device\n    edge_index = edge_index.to(self.device)\n    if batch is not None:\n        batch = batch.to(self.device)\n\n    # Handle different input formats\n    if hasattr(lightcurve, 'data'):\n        # LightcurveTensor\n        lc_data = lightcurve.data.to(self.device)\n    elif isinstance(lightcurve, torch.Tensor):\n        lc_data = lightcurve.to(self.device)\n    else:\n        raise ValueError(\"Unsupported lightcurve format\")\n\n    # Encode lightcurve\n    h = self.encode_lightcurve(lc_data)\n\n    # If we have multiple nodes per graph, expand the encoding\n    if edge_index.size(1) &gt; 0:\n        num_nodes = edge_index.max().item() + 1\n        if h.size(0) == 1 and num_nodes &gt; 1:\n            # Broadcast to all nodes\n            h = h.expand(num_nodes, -1)\n\n    # Apply graph convolutions\n    for conv, norm in zip(self.convs, self.norms):\n        h_prev = h\n        h = conv(h, edge_index)\n        h = norm(h)\n        h = F.relu(h)\n        h = self.dropout(h)\n\n        # Residual connection\n        if h_prev.shape == h.shape:\n            h = h + h_prev\n\n    # Pool if needed\n    if batch is not None:\n        pooled = global_mean_pool(h, batch)\n    else:\n        pooled = h.mean(dim=0, keepdim=True)\n\n    # Task-specific output\n    output = self.output_head(pooled)\n\n    if return_embeddings:\n        return {\"predictions\": output, \"embeddings\": pooled}\n    return output \n</code></pre>"},{"location":"api/astro_lab.models/#astro_lab.models.AstroPhotGNN","title":"AstroPhotGNN","text":"<p>               Bases: <code>Module</code></p> <p>Graph Neural Network for astronomical photometry using native PhotometricTensorDict methods.</p> <p>Processes multi-band photometric data through specialized encoders and GNN layers, utilizing the native methods and properties of PhotometricTensorDict.</p> <p>Methods:</p> Name Description <code>forward</code> <p>Forward pass using native PhotometricTensorDict methods.</p> <code>get_photometric_metadata</code> <p>Extract photometric metadata using native methods.</p> Source code in <code>src\\astro_lab\\models\\core\\astrophot_gnn.py</code> <pre><code>class AstroPhotGNN(nn.Module):\n    \"\"\"\n    Graph Neural Network for astronomical photometry using native PhotometricTensorDict methods.\n\n    Processes multi-band photometric data through specialized encoders and GNN layers,\n    utilizing the native methods and properties of PhotometricTensorDict.\n    \"\"\"\n\n    def __init__(\n        self,\n        output_dim: int = 128,\n        hidden_dim: int = 256,\n        num_gnn_layers: int = 3,\n        use_color_features: bool = True,\n        use_magnitude_errors: bool = True,\n        pooling_type: str = \"mean\",\n        dropout: float = 0.1,\n        device: Optional[Union[str, torch.device]] = None,\n        **kwargs,\n    ):\n        super().__init__()\n\n        self.output_dim = output_dim\n        self.hidden_dim = hidden_dim\n        self.num_gnn_layers = num_gnn_layers\n        self.use_color_features = use_color_features\n        self.use_magnitude_errors = use_magnitude_errors\n        self.pooling_type = pooling_type\n        self.dropout = dropout\n\n        self.device = torch.device(\n            device or (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n        )\n\n        # PhotometricTensorDict-native encoder\n        self.photometry_encoder = PhotometryEncoder(\n            output_dim=hidden_dim,\n            hidden_dim=hidden_dim,\n            dropout=dropout,\n            device=device,\n            **kwargs,\n        )\n\n        # Color index processor for multi-band data\n        if use_color_features:\n            self.color_processor = nn.Sequential(\n                nn.Linear(hidden_dim // 2, hidden_dim // 4),\n                nn.ReLU(),\n                nn.Dropout(dropout),\n                nn.Linear(hidden_dim // 4, hidden_dim // 8),\n            )\n\n        # GNN layers for spatial photometric relationships\n        self.gnn_layers = nn.ModuleList(\n            [\n                BaseGNNLayer(\n                    input_dim=hidden_dim,\n                    output_dim=hidden_dim,\n                    layer_type=\"gcn\",\n                    dropout=dropout,\n                    device=device,\n                )\n                for _ in range(num_gnn_layers)\n            ]\n        )\n\n        # Global pooling\n        from ..components.base import PoolingModule\n\n        self.pooling = PoolingModule(pooling_type=pooling_type)\n\n        # Output projection\n        self.output_projection = nn.Sequential(\n            nn.Linear(hidden_dim, hidden_dim // 2),\n            nn.ReLU(),\n            nn.Dropout(dropout),\n            nn.Linear(hidden_dim // 2, output_dim),\n        )\n\n        self.to(self.device)\n\n    def forward(\n        self,\n        data: PhotometricTensorDict,\n        edge_index: Optional[torch.Tensor] = None,\n        batch: Optional[torch.Tensor] = None,\n    ) -&gt; torch.Tensor:\n        \"\"\"\n        Forward pass using native PhotometricTensorDict methods.\n\n        Args:\n            data: PhotometricTensorDict with native photometric access\n            edge_index: Graph edge indices for photometric relationships\n            batch: Batch assignment for objects\n\n        Returns:\n            Encoded photometric features\n        \"\"\"\n        if not isinstance(data, PhotometricTensorDict):\n            raise ValueError(\"AstroPhotGNN requires PhotometricTensorDict input\")\n\n        # Use photometry encoder with native methods\n        node_features = self.photometry_encoder(data)\n\n        # Extract additional color features using native methods\n        if self.use_color_features and data.n_bands &gt; 1:\n            color_features = self._extract_color_features(data)\n            if color_features is not None:\n                # Combine with main features\n                if hasattr(self, \"color_processor\"):\n                    processed_colors = self.color_processor(color_features)\n                    node_features = torch.cat([node_features, processed_colors], dim=-1)\n\n                    # Adjust projection for concatenated features\n                    if not hasattr(self, \"_adjusted_projection\"):\n                        combined_dim = node_features.shape[-1]\n                        self.feature_adjustment = nn.Linear(\n                            combined_dim, self.hidden_dim\n                        ).to(self.device)\n                        self._adjusted_projection = True\n\n                    if hasattr(self, \"feature_adjustment\"):\n                        node_features = self.feature_adjustment(node_features)\n\n        # Create edge index if not provided\n        if edge_index is None:\n            num_nodes = node_features.shape[0]\n            edge_index = self._create_photometric_graph(data, num_nodes)\n\n        edge_index = edge_index.to(self.device)\n\n        # Process through GNN layers\n        h = node_features\n        for gnn_layer in self.gnn_layers:\n            h = gnn_layer(h, edge_index)\n            h = F.relu(h)\n            h = F.dropout(h, p=self.dropout, training=self.training)\n\n        # Global pooling\n        graph_embedding = self.pooling(h, batch)\n\n        # Final projection\n        output = self.output_projection(graph_embedding)\n\n        return output\n\n    def _extract_color_features(\n        self, data: PhotometricTensorDict\n    ) -&gt; Optional[torch.Tensor]:\n        \"\"\"Extract color features using native PhotometricTensorDict methods.\"\"\"\n        if data.n_bands &lt; 2:\n            return None\n\n        bands = data.bands\n\n        # Create standard color pairs\n        standard_colors = [\n            (\"u\", \"g\"),\n            (\"g\", \"r\"),\n            (\"r\", \"i\"),\n            (\"i\", \"z\"),  # SDSS colors\n            (\"B\", \"V\"),\n            (\"V\", \"R\"),\n            (\"R\", \"I\"),  # Johnson colors\n            (\"J\", \"H\"),\n            (\"H\", \"K\"),  # Near-IR colors\n        ]\n\n        # Filter to available bands\n        available_colors = []\n        for color1, color2 in standard_colors:\n            if color1 in bands and color2 in bands:\n                available_colors.append((color1, color2))\n\n        # Fall back to adjacent bands if no standard colors\n        if not available_colors:\n            for i in range(len(bands) - 1):\n                available_colors.append((bands[i], bands[i + 1]))\n\n        if not available_colors:\n            return None\n\n        try:\n            # Use native compute_colors method\n            color_dict = data.compute_colors(available_colors)\n\n            # Extract color values\n            color_values = []\n            for color_name in color_dict.keys():\n                if isinstance(color_dict[color_name], torch.Tensor):\n                    color_values.append(color_dict[color_name])\n\n            if color_values:\n                colors = torch.stack(color_values, dim=-1)\n                return colors.to(self.device)\n\n        except Exception:\n            # Fallback to manual color computation\n            magnitudes = data[\"magnitudes\"].to(self.device)\n            if magnitudes.shape[-1] &gt; 1:\n                colors = magnitudes[..., :-1] - magnitudes[..., 1:]\n                return colors\n\n        return None\n\n    def _create_photometric_graph(\n        self, data: PhotometricTensorDict, num_nodes: int\n    ) -&gt; torch.Tensor:\n        \"\"\"Create graph based on photometric similarity using native methods.\"\"\"\n        if num_nodes &lt;= 1:\n            return torch.tensor([[0], [0]], device=self.device, dtype=torch.long)\n\n        magnitudes = data[\"magnitudes\"].to(self.device)\n\n        # Compute photometric distances\n        if magnitudes.dim() == 2:\n            # Multi-band photometry\n            distances = torch.cdist(magnitudes, magnitudes)\n        else:\n            # Single band\n            distances = torch.abs(magnitudes.unsqueeze(1) - magnitudes.unsqueeze(0))\n\n        # Create edges based on photometric similarity\n        threshold = distances.std() * 0.75  # Adaptive threshold\n        adjacency = (distances &lt; threshold) &amp; (distances &gt; 0)\n\n        # Convert to edge index\n        edge_indices = adjacency.nonzero(as_tuple=False).T\n\n        if edge_indices.shape[1] == 0:\n            # Fallback: k-NN graph\n            k = min(8, num_nodes - 1)\n            _, knn_indices = torch.topk(distances, k + 1, dim=1, largest=False)\n            knn_indices = knn_indices[:, 1:]  # Remove self-connections\n\n            source_nodes = torch.arange(num_nodes).unsqueeze(1).expand(-1, k)\n            edge_indices = torch.stack([source_nodes.flatten(), knn_indices.flatten()])\n\n        return edge_indices\n\n    def get_photometric_metadata(self, data: PhotometricTensorDict) -&gt; Dict[str, Any]:\n        \"\"\"Extract photometric metadata using native methods.\"\"\"\n        if not isinstance(data, PhotometricTensorDict):\n            raise ValueError(\"Requires PhotometricTensorDict input\")\n\n        metadata = {}\n\n        # Basic photometric properties\n        metadata[\"n_bands\"] = data.n_bands\n        metadata[\"bands\"] = data.bands\n\n        # Magnitude system information from meta\n        if hasattr(data, \"meta\") and data.meta is not None:\n            if \"magnitude_system\" in data.meta:\n                metadata[\"magnitude_system\"] = data.meta[\"magnitude_system\"]\n\n        # Statistical properties\n        magnitudes = data[\"magnitudes\"]\n        metadata[\"magnitude_stats\"] = {\n            \"mean\": magnitudes.mean(dim=0).tolist(),\n            \"std\": magnitudes.std(dim=0).tolist(),\n            \"min\": magnitudes.min(dim=0)[0].tolist(),\n            \"max\": magnitudes.max(dim=0)[0].tolist(),\n        }\n\n        # Color information\n        if data.n_bands &gt; 1:\n            try:\n                # Try to compute some standard colors\n                color_pairs = [\n                    (data.bands[i], data.bands[i + 1])\n                    for i in range(len(data.bands) - 1)\n                ]\n                colors_dict = data.compute_colors(color_pairs)\n                metadata[\"available_colors\"] = [str(k) for k in colors_dict.keys()]\n            except Exception:\n                metadata[\"available_colors\"] = []\n\n        return metadata\n</code></pre>"},{"location":"api/astro_lab.models/#astro_lab.models.AstroPhotGNN.forward","title":"forward","text":"<pre><code>forward(\n    data: PhotometricTensorDict,\n    edge_index: Optional[Tensor] = None,\n    batch: Optional[Tensor] = None,\n) -&gt; Tensor\n</code></pre> <p>Forward pass using native PhotometricTensorDict methods.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>PhotometricTensorDict</code> <p>PhotometricTensorDict with native photometric access</p> required <code>edge_index</code> <code>Optional[Tensor]</code> <p>Graph edge indices for photometric relationships</p> <code>None</code> <code>batch</code> <code>Optional[Tensor]</code> <p>Batch assignment for objects</p> <code>None</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>Encoded photometric features</p> Source code in <code>src\\astro_lab\\models\\core\\astrophot_gnn.py</code> <pre><code>def forward(\n    self,\n    data: PhotometricTensorDict,\n    edge_index: Optional[torch.Tensor] = None,\n    batch: Optional[torch.Tensor] = None,\n) -&gt; torch.Tensor:\n    \"\"\"\n    Forward pass using native PhotometricTensorDict methods.\n\n    Args:\n        data: PhotometricTensorDict with native photometric access\n        edge_index: Graph edge indices for photometric relationships\n        batch: Batch assignment for objects\n\n    Returns:\n        Encoded photometric features\n    \"\"\"\n    if not isinstance(data, PhotometricTensorDict):\n        raise ValueError(\"AstroPhotGNN requires PhotometricTensorDict input\")\n\n    # Use photometry encoder with native methods\n    node_features = self.photometry_encoder(data)\n\n    # Extract additional color features using native methods\n    if self.use_color_features and data.n_bands &gt; 1:\n        color_features = self._extract_color_features(data)\n        if color_features is not None:\n            # Combine with main features\n            if hasattr(self, \"color_processor\"):\n                processed_colors = self.color_processor(color_features)\n                node_features = torch.cat([node_features, processed_colors], dim=-1)\n\n                # Adjust projection for concatenated features\n                if not hasattr(self, \"_adjusted_projection\"):\n                    combined_dim = node_features.shape[-1]\n                    self.feature_adjustment = nn.Linear(\n                        combined_dim, self.hidden_dim\n                    ).to(self.device)\n                    self._adjusted_projection = True\n\n                if hasattr(self, \"feature_adjustment\"):\n                    node_features = self.feature_adjustment(node_features)\n\n    # Create edge index if not provided\n    if edge_index is None:\n        num_nodes = node_features.shape[0]\n        edge_index = self._create_photometric_graph(data, num_nodes)\n\n    edge_index = edge_index.to(self.device)\n\n    # Process through GNN layers\n    h = node_features\n    for gnn_layer in self.gnn_layers:\n        h = gnn_layer(h, edge_index)\n        h = F.relu(h)\n        h = F.dropout(h, p=self.dropout, training=self.training)\n\n    # Global pooling\n    graph_embedding = self.pooling(h, batch)\n\n    # Final projection\n    output = self.output_projection(graph_embedding)\n\n    return output\n</code></pre>"},{"location":"api/astro_lab.models/#astro_lab.models.AstroPhotGNN.get_photometric_metadata","title":"get_photometric_metadata","text":"<pre><code>get_photometric_metadata(data: PhotometricTensorDict) -&gt; Dict[str, Any]\n</code></pre> <p>Extract photometric metadata using native methods.</p> Source code in <code>src\\astro_lab\\models\\core\\astrophot_gnn.py</code> <pre><code>def get_photometric_metadata(self, data: PhotometricTensorDict) -&gt; Dict[str, Any]:\n    \"\"\"Extract photometric metadata using native methods.\"\"\"\n    if not isinstance(data, PhotometricTensorDict):\n        raise ValueError(\"Requires PhotometricTensorDict input\")\n\n    metadata = {}\n\n    # Basic photometric properties\n    metadata[\"n_bands\"] = data.n_bands\n    metadata[\"bands\"] = data.bands\n\n    # Magnitude system information from meta\n    if hasattr(data, \"meta\") and data.meta is not None:\n        if \"magnitude_system\" in data.meta:\n            metadata[\"magnitude_system\"] = data.meta[\"magnitude_system\"]\n\n    # Statistical properties\n    magnitudes = data[\"magnitudes\"]\n    metadata[\"magnitude_stats\"] = {\n        \"mean\": magnitudes.mean(dim=0).tolist(),\n        \"std\": magnitudes.std(dim=0).tolist(),\n        \"min\": magnitudes.min(dim=0)[0].tolist(),\n        \"max\": magnitudes.max(dim=0)[0].tolist(),\n    }\n\n    # Color information\n    if data.n_bands &gt; 1:\n        try:\n            # Try to compute some standard colors\n            color_pairs = [\n                (data.bands[i], data.bands[i + 1])\n                for i in range(len(data.bands) - 1)\n            ]\n            colors_dict = data.compute_colors(color_pairs)\n            metadata[\"available_colors\"] = [str(k) for k in colors_dict.keys()]\n        except Exception:\n            metadata[\"available_colors\"] = []\n\n    return metadata\n</code></pre>"},{"location":"api/astro_lab.models/#astro_lab.models.AstroSurveyGNN","title":"AstroSurveyGNN","text":"<p>               Bases: <code>Module</code></p> <p>Graph Neural Network for astronomical survey data using native TensorDict methods.</p> <p>Processes SurveyTensorDict data through specialized encoders and GNN layers, utilizing the native methods and properties of our TensorDict classes.</p> <p>Methods:</p> Name Description <code>forward</code> <p>Forward pass using native SurveyTensorDict methods.</p> <code>get_survey_metadata</code> <p>Extract metadata using native SurveyTensorDict methods.</p> Source code in <code>src\\astro_lab\\models\\core\\survey_gnn.py</code> <pre><code>class AstroSurveyGNN(nn.Module):\n    \"\"\"\n    Graph Neural Network for astronomical survey data using native TensorDict methods.\n\n    Processes SurveyTensorDict data through specialized encoders and GNN layers,\n    utilizing the native methods and properties of our TensorDict classes.\n    \"\"\"\n\n    def __init__(\n        self,\n        output_dim: int = 128,\n        hidden_dim: int = 256,\n        num_gnn_layers: int = 3,\n        use_photometry: bool = True,\n        use_astrometry: bool = True,\n        use_spectroscopy: bool = False,\n        pooling_type: str = \"mean\",\n        dropout: float = 0.1,\n        device: Optional[Union[str, torch.device]] = None,\n        **kwargs,\n    ):\n        super().__init__()\n\n        self.output_dim = output_dim\n        self.hidden_dim = hidden_dim\n        self.num_gnn_layers = num_gnn_layers\n        self.use_photometry = use_photometry\n        self.use_astrometry = use_astrometry\n        self.use_spectroscopy = use_spectroscopy\n        self.pooling_type = pooling_type\n        self.dropout = dropout\n\n        self.device = torch.device(\n            device or (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n        )\n\n        # TensorDict-native feature processor\n        self.feature_processor = TensorDictFeatureProcessor(\n            output_dim=hidden_dim,\n            feature_dim=hidden_dim,\n            use_photometry=use_photometry,\n            use_astrometry=use_astrometry,\n            use_spectroscopy=use_spectroscopy,\n            device=device,\n        )\n\n        # Survey encoder using native TensorDict methods\n        self.survey_encoder = SurveyEncoder(\n            output_dim=hidden_dim,\n            use_photometry=use_photometry,\n            use_astrometry=use_astrometry,\n            use_spectroscopy=use_spectroscopy,\n            device=device,\n            **kwargs,\n        )\n\n        # GNN layers\n        self.gnn_layers = nn.ModuleList(\n            [\n                BaseGNNLayer(\n            input_dim=hidden_dim,\n                    output_dim=hidden_dim,\n                    layer_type=\"gcn\",\n            dropout=dropout,\n                    device=device,\n                )\n                for _ in range(num_gnn_layers)\n            ]\n        )\n\n        # Global pooling\n        from ..components.base import PoolingModule\n\n        self.pooling = PoolingModule(pooling_type=pooling_type)\n\n        # Final output projection\n        self.output_projection = nn.Sequential(\n            nn.Linear(hidden_dim, hidden_dim // 2),\n            nn.ReLU(),\n            nn.Dropout(dropout),\n            nn.Linear(hidden_dim // 2, output_dim),\n        )\n\n        self.to(self.device)\n\n    def forward(\n        self,\n        data: SurveyTensorDict,\n        edge_index: Optional[torch.Tensor] = None,\n        batch: Optional[torch.Tensor] = None,\n    ) -&gt; torch.Tensor:\n        \"\"\"\n        Forward pass using native SurveyTensorDict methods.\n\n        Args:\n            data: SurveyTensorDict with native method access\n            edge_index: Graph edge indices\n            batch: Batch assignment for nodes\n\n        Returns:\n            Encoded survey features\n        \"\"\"\n        if not isinstance(data, SurveyTensorDict):\n            raise ValueError(\"AstroSurveyGNN requires SurveyTensorDict input\")\n\n        # Use survey encoder with native TensorDict methods\n        node_features = self.survey_encoder(data)\n\n        # Create graph edge index if not provided\n        if edge_index is None:\n            # Create k-NN graph using spatial coordinates if available\n            if \"spatial\" in data and isinstance(data[\"spatial\"], SpatialTensorDict):\n                spatial_data = data[\"spatial\"]\n                # Use native 3D coordinate access for point cloud processing\n                coordinates = torch.stack(\n                    [spatial_data.x, spatial_data.y, spatial_data.z], dim=-1\n                )\n                edge_index = self._create_knn_graph(coordinates, k=8)\n            else:\n                # Create fully connected graph as fallback\n                num_nodes = node_features.shape[0]\n                edge_index = self._create_fully_connected_graph(num_nodes)\n\n        edge_index = edge_index.to(self.device)\n\n        # Process through GNN layers\n        h = node_features\n        for gnn_layer in self.gnn_layers:\n            h = gnn_layer(h, edge_index)\n            h = F.relu(h)\n            h = F.dropout(h, p=self.dropout, training=self.training)\n\n        # Global pooling\n        graph_embedding = self.pooling(h, batch)\n\n        # Final projection\n        output = self.output_projection(graph_embedding)\n\n        return output\n\n    def _create_knn_graph(self, coordinates: torch.Tensor, k: int = 8) -&gt; torch.Tensor:\n        \"\"\"Create k-NN graph from spatial coordinates using native methods.\"\"\"\n        device = coordinates.device\n        num_nodes = coordinates.shape[0]\n\n        # Compute pairwise distances\n        distances = torch.cdist(coordinates, coordinates)\n\n        # Get k nearest neighbors (excluding self)\n        _, knn_indices = torch.topk(distances, k + 1, dim=1, largest=False)\n        knn_indices = knn_indices[:, 1:]  # Remove self-connections\n\n        # Create edge index\n        source_nodes = torch.arange(num_nodes, device=device).unsqueeze(1).expand(-1, k)\n        edge_index = torch.stack([source_nodes.flatten(), knn_indices.flatten()])\n\n        return edge_index\n\n    def _create_fully_connected_graph(self, num_nodes: int) -&gt; torch.Tensor:\n        \"\"\"Create fully connected graph as fallback.\"\"\"\n        source = torch.arange(num_nodes).unsqueeze(1).expand(-1, num_nodes).flatten()\n        target = torch.arange(num_nodes).unsqueeze(0).expand(num_nodes, -1).flatten()\n\n        # Remove self-loops\n        mask = source != target\n        edge_index = torch.stack([source[mask], target[mask]])\n\n        return edge_index\n\n    def get_survey_metadata(self, data: SurveyTensorDict) -&gt; Dict[str, Any]:\n        \"\"\"Extract metadata using native SurveyTensorDict methods.\"\"\"\n        if not isinstance(data, SurveyTensorDict):\n            raise ValueError(\"Requires SurveyTensorDict input\")\n\n        metadata = {}\n\n        # Survey-level metadata using meta property\n        if hasattr(data, \"meta\") and data.meta is not None:\n            meta = data.meta\n            if \"survey_name\" in meta:\n                metadata[\"survey_name\"] = meta[\"survey_name\"]\n            if \"data_release\" in meta:\n                metadata[\"data_release\"] = meta[\"data_release\"]\n        if hasattr(data, \"n_objects\"):\n            metadata[\"n_objects\"] = data.n_objects\n\n        # Photometric metadata\n        if \"photometric\" in data and isinstance(\n            data[\"photometric\"], PhotometricTensorDict\n        ):\n            phot_data = data[\"photometric\"]\n            metadata[\"n_bands\"] = phot_data.n_bands\n            metadata[\"bands\"] = phot_data.bands\n            if hasattr(phot_data, \"magnitude_system\"):\n                metadata[\"magnitude_system\"] = phot_data.magnitude_system\n\n        # Spatial metadata\n        if \"spatial\" in data and isinstance(data[\"spatial\"], SpatialTensorDict):\n            spatial_data = data[\"spatial\"]\n            metadata[\"coordinate_system\"] = spatial_data.coordinate_system\n            if hasattr(spatial_data, \"epoch\"):\n                metadata[\"epoch\"] = spatial_data.epoch\n\n        # Spectral metadata\n        if \"spectral\" in data and isinstance(data[\"spectral\"], SpectralTensorDict):\n            spec_data = data[\"spectral\"]\n            if hasattr(spec_data, \"wavelength_unit\"):\n                metadata[\"wavelength_unit\"] = spec_data.wavelength_unit\n            if hasattr(spec_data, \"flux_unit\"):\n                metadata[\"flux_unit\"] = spec_data.flux_unit\n\n        return metadata\n</code></pre>"},{"location":"api/astro_lab.models/#astro_lab.models.AstroSurveyGNN.forward","title":"forward","text":"<pre><code>forward(\n    data: SurveyTensorDict,\n    edge_index: Optional[Tensor] = None,\n    batch: Optional[Tensor] = None,\n) -&gt; Tensor\n</code></pre> <p>Forward pass using native SurveyTensorDict methods.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>SurveyTensorDict</code> <p>SurveyTensorDict with native method access</p> required <code>edge_index</code> <code>Optional[Tensor]</code> <p>Graph edge indices</p> <code>None</code> <code>batch</code> <code>Optional[Tensor]</code> <p>Batch assignment for nodes</p> <code>None</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>Encoded survey features</p> Source code in <code>src\\astro_lab\\models\\core\\survey_gnn.py</code> <pre><code>def forward(\n    self,\n    data: SurveyTensorDict,\n    edge_index: Optional[torch.Tensor] = None,\n    batch: Optional[torch.Tensor] = None,\n) -&gt; torch.Tensor:\n    \"\"\"\n    Forward pass using native SurveyTensorDict methods.\n\n    Args:\n        data: SurveyTensorDict with native method access\n        edge_index: Graph edge indices\n        batch: Batch assignment for nodes\n\n    Returns:\n        Encoded survey features\n    \"\"\"\n    if not isinstance(data, SurveyTensorDict):\n        raise ValueError(\"AstroSurveyGNN requires SurveyTensorDict input\")\n\n    # Use survey encoder with native TensorDict methods\n    node_features = self.survey_encoder(data)\n\n    # Create graph edge index if not provided\n    if edge_index is None:\n        # Create k-NN graph using spatial coordinates if available\n        if \"spatial\" in data and isinstance(data[\"spatial\"], SpatialTensorDict):\n            spatial_data = data[\"spatial\"]\n            # Use native 3D coordinate access for point cloud processing\n            coordinates = torch.stack(\n                [spatial_data.x, spatial_data.y, spatial_data.z], dim=-1\n            )\n            edge_index = self._create_knn_graph(coordinates, k=8)\n        else:\n            # Create fully connected graph as fallback\n            num_nodes = node_features.shape[0]\n            edge_index = self._create_fully_connected_graph(num_nodes)\n\n    edge_index = edge_index.to(self.device)\n\n    # Process through GNN layers\n    h = node_features\n    for gnn_layer in self.gnn_layers:\n        h = gnn_layer(h, edge_index)\n        h = F.relu(h)\n        h = F.dropout(h, p=self.dropout, training=self.training)\n\n    # Global pooling\n    graph_embedding = self.pooling(h, batch)\n\n    # Final projection\n    output = self.output_projection(graph_embedding)\n\n    return output\n</code></pre>"},{"location":"api/astro_lab.models/#astro_lab.models.AstroSurveyGNN.get_survey_metadata","title":"get_survey_metadata","text":"<pre><code>get_survey_metadata(data: SurveyTensorDict) -&gt; Dict[str, Any]\n</code></pre> <p>Extract metadata using native SurveyTensorDict methods.</p> Source code in <code>src\\astro_lab\\models\\core\\survey_gnn.py</code> <pre><code>def get_survey_metadata(self, data: SurveyTensorDict) -&gt; Dict[str, Any]:\n    \"\"\"Extract metadata using native SurveyTensorDict methods.\"\"\"\n    if not isinstance(data, SurveyTensorDict):\n        raise ValueError(\"Requires SurveyTensorDict input\")\n\n    metadata = {}\n\n    # Survey-level metadata using meta property\n    if hasattr(data, \"meta\") and data.meta is not None:\n        meta = data.meta\n        if \"survey_name\" in meta:\n            metadata[\"survey_name\"] = meta[\"survey_name\"]\n        if \"data_release\" in meta:\n            metadata[\"data_release\"] = meta[\"data_release\"]\n    if hasattr(data, \"n_objects\"):\n        metadata[\"n_objects\"] = data.n_objects\n\n    # Photometric metadata\n    if \"photometric\" in data and isinstance(\n        data[\"photometric\"], PhotometricTensorDict\n    ):\n        phot_data = data[\"photometric\"]\n        metadata[\"n_bands\"] = phot_data.n_bands\n        metadata[\"bands\"] = phot_data.bands\n        if hasattr(phot_data, \"magnitude_system\"):\n            metadata[\"magnitude_system\"] = phot_data.magnitude_system\n\n    # Spatial metadata\n    if \"spatial\" in data and isinstance(data[\"spatial\"], SpatialTensorDict):\n        spatial_data = data[\"spatial\"]\n        metadata[\"coordinate_system\"] = spatial_data.coordinate_system\n        if hasattr(spatial_data, \"epoch\"):\n            metadata[\"epoch\"] = spatial_data.epoch\n\n    # Spectral metadata\n    if \"spectral\" in data and isinstance(data[\"spectral\"], SpectralTensorDict):\n        spec_data = data[\"spectral\"]\n        if hasattr(spec_data, \"wavelength_unit\"):\n            metadata[\"wavelength_unit\"] = spec_data.wavelength_unit\n        if hasattr(spec_data, \"flux_unit\"):\n            metadata[\"flux_unit\"] = spec_data.flux_unit\n\n    return metadata\n</code></pre>"},{"location":"api/astro_lab.models/#astro_lab.models.ModelConfig","title":"ModelConfig  <code>dataclass</code>","text":"<p>Simple model configuration.</p> <p>Methods:</p> Name Description <code>to_dict</code> <p>Convert to dictionary.</p> Source code in <code>src\\astro_lab\\models\\config.py</code> <pre><code>@dataclass\nclass ModelConfig:\n    \"\"\"Simple model configuration.\"\"\"\n\n    name: str\n    hidden_dim: int = 128\n    num_layers: int = 3\n    conv_type: str = \"gcn\"\n    dropout: float = 0.1\n    task: str = \"classification\"\n\n    # Survey-specific settings\n    use_photometry: bool = True\n    use_astrometry: bool = True\n    use_spectroscopy: bool = False\n\n    # Additional settings\n    output_dim: Optional[int] = None\n    pooling: str = \"mean\"\n    activation: str = \"relu\"\n\n    # GAT/Transformer specific\n    num_heads: int = 8\n\n    def to_dict(self) -&gt; Dict[str, Any]:\n        \"\"\"Convert to dictionary.\"\"\"\n        return {\n            \"name\": self.name,\n            \"hidden_dim\": self.hidden_dim,\n            \"num_layers\": self.num_layers,\n            \"conv_type\": self.conv_type,\n            \"dropout\": self.dropout,\n            \"task\": self.task,\n            \"use_photometry\": self.use_photometry,\n            \"use_astrometry\": self.use_astrometry,\n            \"use_spectroscopy\": self.use_spectroscopy,\n            \"output_dim\": self.output_dim,\n            \"pooling\": self.pooling,\n            \"activation\": self.activation,\n            \"num_heads\": self.num_heads,\n        }\n</code></pre>"},{"location":"api/astro_lab.models/#astro_lab.models.ModelConfig.to_dict","title":"to_dict","text":"<pre><code>to_dict() -&gt; Dict[str, Any]\n</code></pre> <p>Convert to dictionary.</p> Source code in <code>src\\astro_lab\\models\\config.py</code> <pre><code>def to_dict(self) -&gt; Dict[str, Any]:\n    \"\"\"Convert to dictionary.\"\"\"\n    return {\n        \"name\": self.name,\n        \"hidden_dim\": self.hidden_dim,\n        \"num_layers\": self.num_layers,\n        \"conv_type\": self.conv_type,\n        \"dropout\": self.dropout,\n        \"task\": self.task,\n        \"use_photometry\": self.use_photometry,\n        \"use_astrometry\": self.use_astrometry,\n        \"use_spectroscopy\": self.use_spectroscopy,\n        \"output_dim\": self.output_dim,\n        \"pooling\": self.pooling,\n        \"activation\": self.activation,\n        \"num_heads\": self.num_heads,\n    }\n</code></pre>"},{"location":"api/astro_lab.models/#astro_lab.models.TemporalGCN","title":"TemporalGCN","text":"<p>               Bases: <code>Module</code></p> <p>Temporal Graph Convolutional Network using native LightcurveTensorDict methods.</p> <p>Processes time-series astronomical data through temporal modeling and GNN layers, utilizing the native time-domain methods of LightcurveTensorDict.</p> <p>Methods:</p> Name Description <code>extract_period_features</code> <p>Extract period-related features using native time methods.</p> <code>forward</code> <p>Forward pass using native LightcurveTensorDict methods.</p> Source code in <code>src\\astro_lab\\models\\core\\temporal_gnn.py</code> <pre><code>class TemporalGCN(nn.Module):\n    \"\"\"\n    Temporal Graph Convolutional Network using native LightcurveTensorDict methods.\n\n    Processes time-series astronomical data through temporal modeling and GNN layers,\n    utilizing the native time-domain methods of LightcurveTensorDict.\n    \"\"\"\n\n    def __init__(\n        self,\n        output_dim: int = 128,\n        hidden_dim: int = 256,\n        num_temporal_layers: int = 2,\n        num_gnn_layers: int = 2,\n        temporal_model: str = \"lstm\",\n        dropout: float = 0.1,\n        use_attention: bool = True,\n        device: Optional[Union[str, torch.device]] = None,\n        **kwargs,\n    ):\n        super().__init__()\n\n        self.output_dim = output_dim\n        self.hidden_dim = hidden_dim\n        self.num_temporal_layers = num_temporal_layers\n        self.num_gnn_layers = num_gnn_layers\n        self.temporal_model = temporal_model\n        self.dropout = dropout\n        self.use_attention = use_attention\n\n        self.device = torch.device(\n            device or (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n        )\n\n        # Native LightcurveTensorDict encoder\n        self.lightcurve_encoder = LightcurveEncoder(\n            output_dim=hidden_dim,\n            hidden_dim=hidden_dim,\n            num_layers=num_temporal_layers,\n            dropout=dropout,\n            device=device,\n            **kwargs,\n        )\n\n        # Temporal modeling layers\n        if temporal_model == \"lstm\":\n            self.temporal_processor = nn.LSTM(\n                input_size=2,  # time + magnitude\n                hidden_size=hidden_dim,\n                num_layers=num_temporal_layers,\n                batch_first=True,\n                dropout=dropout if num_temporal_layers &gt; 1 else 0,\n            )\n        elif temporal_model == \"gru\":\n            self.temporal_processor = nn.GRU(\n                input_size=2,\n                hidden_size=hidden_dim,\n                num_layers=num_temporal_layers,\n                batch_first=True,\n                dropout=dropout if num_temporal_layers &gt; 1 else 0,\n            )\n        else:\n            raise ValueError(f\"Unsupported temporal model: {temporal_model}\")\n\n        # Attention mechanism for temporal features\n        if use_attention:\n            self.temporal_attention = nn.MultiheadAttention(\n                embed_dim=hidden_dim,\n                num_heads=4,\n                dropout=dropout,\n                batch_first=True,\n            )\n\n        # GNN layers for spatial relationships\n        self.gnn_layers = nn.ModuleList(\n            [\n                BaseGNNLayer(\n                    input_dim=hidden_dim,\n                    output_dim=hidden_dim,\n                    layer_type=\"gcn\",\n                    dropout=dropout,\n                    device=device,\n                )\n                for _ in range(num_gnn_layers)\n            ]\n        )\n\n        # Global pooling\n        from ..components.base import PoolingModule\n\n        self.pooling = PoolingModule(pooling_type=\"mean\")\n\n        # Output projection\n        self.output_projection = nn.Sequential(\n            nn.Linear(hidden_dim, hidden_dim // 2),\n            nn.ReLU(),\n            nn.Dropout(dropout),\n            nn.Linear(hidden_dim // 2, output_dim),\n        )\n\n        self.to(self.device)\n\n    def forward(\n        self,\n        data: LightcurveTensorDict,\n        edge_index: Optional[torch.Tensor] = None,\n        batch: Optional[torch.Tensor] = None,\n    ) -&gt; torch.Tensor:\n        \"\"\"\n        Forward pass using native LightcurveTensorDict methods.\n\n        Args:\n            data: LightcurveTensorDict with native time-domain access\n            edge_index: Graph edge indices for spatial relationships\n            batch: Batch assignment for lightcurves\n\n        Returns:\n            Encoded temporal features\n        \"\"\"\n        if not isinstance(data, LightcurveTensorDict):\n            raise ValueError(\"TemporalGCN requires LightcurveTensorDict input\")\n\n        # Use native LightcurveTensorDict methods\n        times = data[\"times\"].to(self.device)\n        magnitudes = data[\"magnitudes\"].to(self.device)\n\n        # Use time_span property for normalization\n        if hasattr(data, \"time_span\"):\n            time_span = data.time_span\n            # Normalize times to [0, 1] range\n            times_normalized = (times - times.min()) / (time_span + 1e-8)\n        else:\n            times_normalized = times\n\n        # Prepare sequence data for temporal processing\n        if times.dim() == 1:\n            # Single lightcurve\n            sequence = torch.stack(\n                [times_normalized, magnitudes.squeeze(-1)], dim=-1\n            ).unsqueeze(0)\n        else:\n            # Multiple lightcurves in batch\n            if magnitudes.dim() == 3 and magnitudes.shape[-1] &gt; 1:\n                # Use first band only for simplicity\n                magnitudes = magnitudes[..., 0]\n            sequence = torch.stack([times_normalized, magnitudes], dim=-1)\n\n        # Process through temporal model\n        if self.temporal_model in [\"lstm\", \"gru\"]:\n            temporal_out, (h_n, _) = self.temporal_processor(sequence)\n\n            # Use last hidden state as node features\n            if self.temporal_model == \"lstm\":\n                node_features = h_n[-1]\n            else:  # GRU\n                node_features = h_n[-1]\n\n            # Apply temporal attention if enabled\n            if self.use_attention:\n                attended_out, _ = self.temporal_attention(\n                    temporal_out, temporal_out, temporal_out\n                )\n                # Combine last hidden state with attended features\n                attention_pooled = attended_out.mean(dim=1)\n                node_features = node_features + attention_pooled\n\n        # Create edge index if not provided\n        if edge_index is None:\n            num_nodes = node_features.shape[0]\n            edge_index = self._create_temporal_graph(times, num_nodes)\n\n        edge_index = edge_index.to(self.device)\n\n        # Process through GNN layers\n        h = node_features\n        for gnn_layer in self.gnn_layers:\n            h = gnn_layer(h, edge_index)\n            h = F.relu(h)\n            h = F.dropout(h, p=self.dropout, training=self.training)\n\n        # Global pooling\n        graph_embedding = self.pooling(h, batch)\n\n        # Final projection\n        output = self.output_projection(graph_embedding)\n\n        return output\n\n    def _create_temporal_graph(\n        self, times: torch.Tensor, num_nodes: int\n    ) -&gt; torch.Tensor:\n        \"\"\"Create graph based on temporal proximity using native time methods.\"\"\"\n        if num_nodes &lt;= 1:\n            return torch.tensor([[0], [0]], device=self.device, dtype=torch.long)\n\n        # Connect lightcurves that are temporally close\n        if times.dim() &gt; 1:\n            # Use mean observation time for each lightcurve\n            mean_times = times.mean(dim=-1)\n        else:\n            # Single lightcurve case\n            mean_times = times.unsqueeze(0) if num_nodes == 1 else times[:num_nodes]\n\n        # Create temporal proximity graph\n        time_distances = torch.abs(mean_times.unsqueeze(1) - mean_times.unsqueeze(0))\n\n        # Connect nodes within temporal threshold\n        threshold = time_distances.std() * 0.5  # Adaptive threshold\n        adjacency = (time_distances &lt; threshold) &amp; (time_distances &gt; 0)\n\n        # Convert to edge index\n        edge_indices = adjacency.nonzero(as_tuple=False).T\n\n        if edge_indices.shape[1] == 0:\n            # Fallback: connect consecutive observations\n            source = torch.arange(num_nodes - 1, device=self.device)\n            target = torch.arange(1, num_nodes, device=self.device)\n            edge_indices = torch.stack([source, target])\n\n        return edge_indices\n\n    def extract_period_features(\n        self, data: LightcurveTensorDict\n    ) -&gt; Dict[str, torch.Tensor]:\n        \"\"\"Extract period-related features using native time methods.\"\"\"\n        if not isinstance(data, LightcurveTensorDict):\n            raise ValueError(\"Requires LightcurveTensorDict input\")\n\n        features = {}\n\n        # Basic time statistics\n        times = data[\"times\"]\n        magnitudes = data[\"magnitudes\"]\n\n        if hasattr(data, \"time_span\"):\n            features[\"time_span\"] = data.time_span\n\n        # Observation statistics\n        features[\"n_observations\"] = torch.tensor(len(times), dtype=torch.float32)\n        features[\"time_range\"] = times.max() - times.min()\n        features[\"mean_magnitude\"] = magnitudes.mean()\n        features[\"magnitude_std\"] = magnitudes.std()\n\n        # Sampling statistics\n        if len(times) &gt; 1:\n            time_diffs = torch.diff(torch.sort(times)[0])\n            features[\"mean_cadence\"] = time_diffs.mean()\n            features[\"cadence_std\"] = time_diffs.std()\n\n        return features\n</code></pre>"},{"location":"api/astro_lab.models/#astro_lab.models.TemporalGCN.extract_period_features","title":"extract_period_features","text":"<pre><code>extract_period_features(data: LightcurveTensorDict) -&gt; Dict[str, Tensor]\n</code></pre> <p>Extract period-related features using native time methods.</p> Source code in <code>src\\astro_lab\\models\\core\\temporal_gnn.py</code> <pre><code>def extract_period_features(\n    self, data: LightcurveTensorDict\n) -&gt; Dict[str, torch.Tensor]:\n    \"\"\"Extract period-related features using native time methods.\"\"\"\n    if not isinstance(data, LightcurveTensorDict):\n        raise ValueError(\"Requires LightcurveTensorDict input\")\n\n    features = {}\n\n    # Basic time statistics\n    times = data[\"times\"]\n    magnitudes = data[\"magnitudes\"]\n\n    if hasattr(data, \"time_span\"):\n        features[\"time_span\"] = data.time_span\n\n    # Observation statistics\n    features[\"n_observations\"] = torch.tensor(len(times), dtype=torch.float32)\n    features[\"time_range\"] = times.max() - times.min()\n    features[\"mean_magnitude\"] = magnitudes.mean()\n    features[\"magnitude_std\"] = magnitudes.std()\n\n    # Sampling statistics\n    if len(times) &gt; 1:\n        time_diffs = torch.diff(torch.sort(times)[0])\n        features[\"mean_cadence\"] = time_diffs.mean()\n        features[\"cadence_std\"] = time_diffs.std()\n\n    return features\n</code></pre>"},{"location":"api/astro_lab.models/#astro_lab.models.TemporalGCN.forward","title":"forward","text":"<pre><code>forward(\n    data: LightcurveTensorDict,\n    edge_index: Optional[Tensor] = None,\n    batch: Optional[Tensor] = None,\n) -&gt; Tensor\n</code></pre> <p>Forward pass using native LightcurveTensorDict methods.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>LightcurveTensorDict</code> <p>LightcurveTensorDict with native time-domain access</p> required <code>edge_index</code> <code>Optional[Tensor]</code> <p>Graph edge indices for spatial relationships</p> <code>None</code> <code>batch</code> <code>Optional[Tensor]</code> <p>Batch assignment for lightcurves</p> <code>None</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>Encoded temporal features</p> Source code in <code>src\\astro_lab\\models\\core\\temporal_gnn.py</code> <pre><code>def forward(\n    self,\n    data: LightcurveTensorDict,\n    edge_index: Optional[torch.Tensor] = None,\n    batch: Optional[torch.Tensor] = None,\n) -&gt; torch.Tensor:\n    \"\"\"\n    Forward pass using native LightcurveTensorDict methods.\n\n    Args:\n        data: LightcurveTensorDict with native time-domain access\n        edge_index: Graph edge indices for spatial relationships\n        batch: Batch assignment for lightcurves\n\n    Returns:\n        Encoded temporal features\n    \"\"\"\n    if not isinstance(data, LightcurveTensorDict):\n        raise ValueError(\"TemporalGCN requires LightcurveTensorDict input\")\n\n    # Use native LightcurveTensorDict methods\n    times = data[\"times\"].to(self.device)\n    magnitudes = data[\"magnitudes\"].to(self.device)\n\n    # Use time_span property for normalization\n    if hasattr(data, \"time_span\"):\n        time_span = data.time_span\n        # Normalize times to [0, 1] range\n        times_normalized = (times - times.min()) / (time_span + 1e-8)\n    else:\n        times_normalized = times\n\n    # Prepare sequence data for temporal processing\n    if times.dim() == 1:\n        # Single lightcurve\n        sequence = torch.stack(\n            [times_normalized, magnitudes.squeeze(-1)], dim=-1\n        ).unsqueeze(0)\n    else:\n        # Multiple lightcurves in batch\n        if magnitudes.dim() == 3 and magnitudes.shape[-1] &gt; 1:\n            # Use first band only for simplicity\n            magnitudes = magnitudes[..., 0]\n        sequence = torch.stack([times_normalized, magnitudes], dim=-1)\n\n    # Process through temporal model\n    if self.temporal_model in [\"lstm\", \"gru\"]:\n        temporal_out, (h_n, _) = self.temporal_processor(sequence)\n\n        # Use last hidden state as node features\n        if self.temporal_model == \"lstm\":\n            node_features = h_n[-1]\n        else:  # GRU\n            node_features = h_n[-1]\n\n        # Apply temporal attention if enabled\n        if self.use_attention:\n            attended_out, _ = self.temporal_attention(\n                temporal_out, temporal_out, temporal_out\n            )\n            # Combine last hidden state with attended features\n            attention_pooled = attended_out.mean(dim=1)\n            node_features = node_features + attention_pooled\n\n    # Create edge index if not provided\n    if edge_index is None:\n        num_nodes = node_features.shape[0]\n        edge_index = self._create_temporal_graph(times, num_nodes)\n\n    edge_index = edge_index.to(self.device)\n\n    # Process through GNN layers\n    h = node_features\n    for gnn_layer in self.gnn_layers:\n        h = gnn_layer(h, edge_index)\n        h = F.relu(h)\n        h = F.dropout(h, p=self.dropout, training=self.training)\n\n    # Global pooling\n    graph_embedding = self.pooling(h, batch)\n\n    # Final projection\n    output = self.output_projection(graph_embedding)\n\n    return output\n</code></pre>"},{"location":"api/astro_lab.models/#astro_lab.models.create_asteroid_period_detector","title":"create_asteroid_period_detector","text":"<pre><code>create_asteroid_period_detector(\n    hidden_dim: int = 128, device: Optional[Union[str, device]] = None, **kwargs: Any\n) -&gt; ALCDEFTemporalGNN\n</code></pre> <p>Create asteroid period detector (robust to all kwargs).</p> <p>Parameters:</p> Name Type Description Default <code>hidden_dim</code> <code>int</code> <p>Hidden dimension size</p> <code>128</code> <code>device</code> <code>Optional[Union[str, device]]</code> <p>Device to place model on</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Additional model parameters</p> <code>{}</code> <p>Returns:</p> Type Description <code>ALCDEFTemporalGNN</code> <p>Configured ALCDEFTemporalGNN model</p> Source code in <code>src\\astro_lab\\models\\factories.py</code> <pre><code>def create_asteroid_period_detector(\n    hidden_dim: int = 128,\n    device: Optional[Union[str, torch.device]] = None,\n    **kwargs: Any,\n) -&gt; ALCDEFTemporalGNN:\n    \"\"\"\n    Create asteroid period detector (robust to all kwargs).\n\n    Args:\n        hidden_dim: Hidden dimension size\n        device: Device to place model on\n        **kwargs: Additional model parameters\n\n    Returns:\n        Configured ALCDEFTemporalGNN model\n    \"\"\"\n    config = get_predefined_config(\"asteroid_period\").to_dict()\n    config.update(kwargs)\n    config.setdefault(\"hidden_dim\", hidden_dim)\n    config.setdefault(\"device\", device)\n    config.setdefault(\"task\", \"period_detection\")\n\n    # Filter to only valid ALCDEFTemporalGNN parameters\n    filtered = filter_kwargs(ALCDEFTemporalGNN, **config)\n    return ALCDEFTemporalGNN(**filtered)\n</code></pre>"},{"location":"api/astro_lab.models/#astro_lab.models.create_gaia_classifier","title":"create_gaia_classifier","text":"<pre><code>create_gaia_classifier(\n    num_classes: Optional[int] = None,\n    hidden_dim: int = 128,\n    device: Optional[Union[str, device]] = None,\n    **kwargs: Any\n) -&gt; AstroSurveyGNN\n</code></pre> <p>Create Gaia classifier (robust to all kwargs).</p> <p>Parameters:</p> Name Type Description Default <code>num_classes</code> <code>Optional[int]</code> <p>Number of output classes (required for classification)</p> <code>None</code> <code>hidden_dim</code> <code>int</code> <p>Hidden dimension size</p> <code>128</code> <code>device</code> <code>Optional[Union[str, device]]</code> <p>Device to place model on</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Additional model parameters</p> <code>{}</code> <p>Returns:</p> Type Description <code>AstroSurveyGNN</code> <p>Configured AstroSurveyGNN model</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If num_classes is not provided or invalid</p> Source code in <code>src\\astro_lab\\models\\factories.py</code> <pre><code>def create_gaia_classifier(\n    num_classes: Optional[int] = None,\n    hidden_dim: int = 128,\n    device: Optional[Union[str, torch.device]] = None,\n    **kwargs: Any,\n) -&gt; AstroSurveyGNN:\n    \"\"\"\n    Create Gaia classifier (robust to all kwargs).\n\n    Args:\n        num_classes: Number of output classes (required for classification)\n        hidden_dim: Hidden dimension size\n        device: Device to place model on\n        **kwargs: Additional model parameters\n\n    Returns:\n        Configured AstroSurveyGNN model\n\n    Raises:\n        ValueError: If num_classes is not provided or invalid\n    \"\"\"\n    if num_classes is None:\n        raise ValueError(\n            \"num_classes must be specified for classification models. \"\n            \"It should be determined from your data.\"\n        )\n    if num_classes &lt; 2:\n        raise ValueError(f\"num_classes must be at least 2, got {num_classes}\")\n\n    config = get_predefined_config(\"gaia_classifier\").to_dict()\n    config.update(kwargs)\n    config[\"output_dim\"] = num_classes\n    config[\"hidden_dim\"] = hidden_dim\n    if device is not None:\n        config[\"device\"] = device\n    config[\"task\"] = \"classification\"\n\n    logger.info(f\"Creating Gaia classifier with {num_classes} classes\")\n\n    filtered = filter_kwargs_strict(AstroSurveyGNN, **config)\n    return AstroSurveyGNN(**filtered)\n</code></pre>"},{"location":"api/astro_lab.models/#astro_lab.models.create_lsst_transient_detector","title":"create_lsst_transient_detector","text":"<pre><code>create_lsst_transient_detector(\n    num_classes: Optional[int] = None,\n    hidden_dim: int = 128,\n    device: Optional[Union[str, device]] = None,\n    **kwargs: Any\n) -&gt; AstroSurveyGNN\n</code></pre> <p>Create LSST transient detector (robust to all kwargs).</p> <p>Parameters:</p> Name Type Description Default <code>num_classes</code> <code>Optional[int]</code> <p>Number of output classes (required)</p> <code>None</code> <code>hidden_dim</code> <code>int</code> <p>Hidden dimension size</p> <code>128</code> <code>device</code> <code>Optional[Union[str, device]]</code> <p>Device to place model on</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Additional model parameters</p> <code>{}</code> <p>Returns:</p> Type Description <code>AstroSurveyGNN</code> <p>Configured AstroSurveyGNN model</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If num_classes is not provided</p> Source code in <code>src\\astro_lab\\models\\factories.py</code> <pre><code>def create_lsst_transient_detector(\n    num_classes: Optional[int] = None,\n    hidden_dim: int = 128,\n    device: Optional[Union[str, torch.device]] = None,\n    **kwargs: Any,\n) -&gt; AstroSurveyGNN:\n    \"\"\"\n    Create LSST transient detector (robust to all kwargs).\n\n    Args:\n        num_classes: Number of output classes (required)\n        hidden_dim: Hidden dimension size\n        device: Device to place model on\n        **kwargs: Additional model parameters\n\n    Returns:\n        Configured AstroSurveyGNN model\n\n    Raises:\n        ValueError: If num_classes is not provided\n    \"\"\"\n    if num_classes is None:\n        raise ValueError(\n            \"num_classes must be specified for classification models. \"\n            \"It should be determined from your data.\"\n        )\n\n    config = get_predefined_config(\"lsst_transient\").to_dict()\n    config.update(kwargs)\n    config[\"output_dim\"] = num_classes\n    config[\"hidden_dim\"] = hidden_dim\n    config[\"device\"] = device\n    config[\"task\"] = \"classification\"\n\n    # Filter to only valid AstroSurveyGNN parameters\n    filtered = filter_kwargs(AstroSurveyGNN, **config)\n    return AstroSurveyGNN(**filtered)\n</code></pre>"},{"location":"api/astro_lab.models/#astro_lab.models.create_model","title":"create_model","text":"<pre><code>create_model(model_name: str, **kwargs: Any)\n</code></pre> <p>Create model by name with robust parameter filtering.</p> <p>Parameters:</p> Name Type Description Default <code>model_name</code> <code>str</code> <p>Name of the model to create</p> required <code>**kwargs</code> <code>Any</code> <p>Model parameters</p> <code>{}</code> <p>Returns:</p> Type Description <p>Configured model instance</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If model_name is not supported</p> Source code in <code>src\\astro_lab\\models\\factories.py</code> <pre><code>def create_model(model_name: str, **kwargs: Any):\n    \"\"\"\n    Create model by name with robust parameter filtering.\n\n    Args:\n        model_name: Name of the model to create\n        **kwargs: Model parameters\n\n    Returns:\n        Configured model instance\n\n    Raises:\n        ValueError: If model_name is not supported\n    \"\"\"\n    if model_name not in MODELS:\n        available = list(MODELS.keys())\n        raise ValueError(f\"Unknown model: {model_name}. Available: {available}\")\n\n    factory_fn = MODELS[model_name]\n    return factory_fn(**kwargs)\n</code></pre>"},{"location":"api/astro_lab.models/#astro_lab.models.create_sdss_galaxy_model","title":"create_sdss_galaxy_model","text":"<pre><code>create_sdss_galaxy_model(\n    output_dim: Optional[int] = None,\n    hidden_dim: int = 256,\n    device: Optional[Union[str, device]] = None,\n    **kwargs: Any\n) -&gt; AstroSurveyGNN\n</code></pre> <p>Create SDSS galaxy model (robust to all kwargs).</p> <p>Parameters:</p> Name Type Description Default <code>output_dim</code> <code>Optional[int]</code> <p>Output dimension (required)</p> <code>None</code> <code>hidden_dim</code> <code>int</code> <p>Hidden dimension size</p> <code>256</code> <code>device</code> <code>Optional[Union[str, device]]</code> <p>Device to place model on</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Additional model parameters</p> <code>{}</code> <p>Returns:</p> Type Description <code>AstroSurveyGNN</code> <p>Configured AstroSurveyGNN model</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If output_dim is not provided</p> Source code in <code>src\\astro_lab\\models\\factories.py</code> <pre><code>def create_sdss_galaxy_model(\n    output_dim: Optional[int] = None,\n    hidden_dim: int = 256,\n    device: Optional[Union[str, torch.device]] = None,\n    **kwargs: Any,\n) -&gt; AstroSurveyGNN:\n    \"\"\"\n    Create SDSS galaxy model (robust to all kwargs).\n\n    Args:\n        output_dim: Output dimension (required)\n        hidden_dim: Hidden dimension size\n        device: Device to place model on\n        **kwargs: Additional model parameters\n\n    Returns:\n        Configured AstroSurveyGNN model\n\n    Raises:\n        ValueError: If output_dim is not provided\n    \"\"\"\n    if output_dim is None:\n        raise ValueError(\n            \"output_dim must be specified for regression models. \"\n            \"It depends on the number of properties you want to predict.\"\n        )\n\n    config = get_predefined_config(\"sdss_galaxy\").to_dict()\n    config.update(kwargs)\n    if output_dim is not None:\n        config[\"output_dim\"] = output_dim\n    config[\"hidden_dim\"] = hidden_dim\n    if device is not None:\n        config[\"device\"] = device\n    config[\"task\"] = \"regression\"\n\n    filtered = filter_kwargs_strict(AstroSurveyGNN, **config)\n    return AstroSurveyGNN(**filtered)\n</code></pre>"},{"location":"api/astro_lab.models/#astro_lab.models.get_device","title":"get_device","text":"<pre><code>get_device(device: Optional[Union[str, device]] = None) -&gt; device\n</code></pre> <p>Get torch device, with automatic CUDA detection.</p> Source code in <code>src\\astro_lab\\models\\utils.py</code> <pre><code>def get_device(device: Optional[Union[str, torch.device]] = None) -&gt; torch.device:\n    \"\"\"Get torch device, with automatic CUDA detection.\"\"\"\n    if device is None:\n        return torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    return torch.device(device)\n</code></pre>"},{"location":"api/astro_lab.models/#astro_lab.models.get_predefined_config","title":"get_predefined_config","text":"<pre><code>get_predefined_config(name: str) -&gt; ModelConfig\n</code></pre> <p>Get predefined config by name.</p> Source code in <code>src\\astro_lab\\models\\config.py</code> <pre><code>def get_predefined_config(name: str) -&gt; ModelConfig:\n    \"\"\"Get predefined config by name.\"\"\"\n    if name not in CONFIGS:\n        available = list(CONFIGS.keys())\n        raise ValueError(f\"Unknown config: {name}. Available: {available}\")\n\n    # Return a copy to avoid modifications\n    config = CONFIGS[name]\n    return ModelConfig(**config.to_dict())\n</code></pre>"},{"location":"api/astro_lab.tensors/","title":"astro_lab.tensors","text":""},{"location":"api/astro_lab.tensors/#astro_lab.tensors","title":"tensors","text":""},{"location":"api/astro_lab.tensors/#astro_lab.tensors--astronomical-tensordict-system","title":"Astronomical TensorDict System","text":"<p>Modern tensor-based astronomical data processing using TensorDict architecture. Fully modernized for PyTorch 2.0+ and Lightning integration.</p> <p>This module provides specialized TensorDicts for astronomical data types: - Spatial coordinates with coordinate transformations - Photometric measurements across multiple bands - Spectroscopic data with wavelength operations - Time series and lightcurve analysis - Survey data coordination and management - Orbital mechanics and satellite tracking - Cosmological simulation data</p> <p>All TensorDicts include: - Native PyTorch integration - Memory-efficient hierarchical data structures - Zero-copy operations where possible - Astronomical metadata preservation - Lightning DataModule compatibility</p> <p>Classes:</p> Name Description <code>LightcurveTensorConfig</code> <p>Configuration for LightcurveTensorDict.</p> <code>OrbitTensorConfig</code> <p>Configuration for OrbitTensorDict.</p> <code>PhotometricTensorConfig</code> <p>Configuration for PhotometricTensorDict.</p> <code>SpatialTensorConfig</code> <p>Configuration for SpatialTensorDict.</p> <code>SpectralTensorConfig</code> <p>Configuration for SpectralTensorDict.</p> <code>SurveyTensorConfig</code> <p>Configuration for SurveyTensorDict.</p> <p>Functions:</p> Name Description <code>create_photometric_tensor</code> <p>Create PhotometricTensorDict from magnitude data.</p> <code>create_simulation_tensor</code> <p>Create SimulationTensorDict for N-body data.</p> <code>create_spatial_tensor</code> <p>Create SpatialTensorDict from coordinates.</p> <code>create_survey_tensor</code> <p>Create SurveyTensorDict from components.</p> <code>from_astrometric_data</code> <p>Create SpatialTensorDict from astrometric measurements.</p> <code>from_lightcurve_data</code> <p>Create LightcurveTensorDict from lightcurve data.</p> <code>from_orbital_elements</code> <p>Create OrbitTensorDict from orbital elements.</p>"},{"location":"api/astro_lab.tensors/#astro_lab.tensors.LightcurveTensorConfig","title":"LightcurveTensorConfig","text":"<p>               Bases: <code>BaseModel</code></p> <p>Configuration for LightcurveTensorDict.</p> Source code in <code>src\\astro_lab\\tensors\\__init__.py</code> <pre><code>class LightcurveTensorConfig(BaseModel):\n    \"\"\"Configuration for LightcurveTensorDict.\"\"\"\n\n    model_config = ConfigDict(validate_assignment=True, extra=\"allow\")\n    time_format: str = Field(default=\"mjd\", description=\"Time format\")\n    time_scale: str = Field(default=\"utc\", description=\"Time scale\")\n    bands: List[str] = Field(default_factory=lambda: [\"V\", \"I\"])\n</code></pre>"},{"location":"api/astro_lab.tensors/#astro_lab.tensors.OrbitTensorConfig","title":"OrbitTensorConfig","text":"<p>               Bases: <code>BaseModel</code></p> <p>Configuration for OrbitTensorDict.</p> Source code in <code>src\\astro_lab\\tensors\\__init__.py</code> <pre><code>class OrbitTensorConfig(BaseModel):\n    \"\"\"Configuration for OrbitTensorDict.\"\"\"\n\n    model_config = ConfigDict(validate_assignment=True, extra=\"allow\")\n    frame: str = Field(default=\"ecliptic\", description=\"Reference frame\")\n    units: Dict[str, str] = Field(\n        default_factory=lambda: {\"a\": \"au\", \"e\": \"dimensionless\", \"i\": \"degrees\"}\n    )\n</code></pre>"},{"location":"api/astro_lab.tensors/#astro_lab.tensors.PhotometricTensorConfig","title":"PhotometricTensorConfig","text":"<p>               Bases: <code>BaseModel</code></p> <p>Configuration for PhotometricTensorDict.</p> Source code in <code>src\\astro_lab\\tensors\\__init__.py</code> <pre><code>class PhotometricTensorConfig(BaseModel):\n    \"\"\"Configuration for PhotometricTensorDict.\"\"\"\n\n    model_config = ConfigDict(validate_assignment=True, extra=\"allow\")\n    bands: List[str] = Field(default_factory=lambda: [\"u\", \"g\", \"r\", \"i\", \"z\"])\n    magnitude_system: str = Field(default=\"AB\", description=\"Magnitude system\")\n    zeropoints: Optional[Dict[str, float]] = Field(default=None)\n</code></pre>"},{"location":"api/astro_lab.tensors/#astro_lab.tensors.SpatialTensorConfig","title":"SpatialTensorConfig","text":"<p>               Bases: <code>BaseModel</code></p> <p>Configuration for SpatialTensorDict.</p> Source code in <code>src\\astro_lab\\tensors\\__init__.py</code> <pre><code>class SpatialTensorConfig(BaseModel):\n    \"\"\"Configuration for SpatialTensorDict.\"\"\"\n\n    model_config = ConfigDict(validate_assignment=True, extra=\"allow\")\n    coordinate_system: str = Field(\n        default=\"icrs\", description=\"Coordinate reference system\"\n    )\n    units: Dict[str, str] = Field(\n        default_factory=lambda: {\"ra\": \"degrees\", \"dec\": \"degrees\", \"distance\": \"kpc\"}\n    )\n    epoch: Optional[str] = Field(default=\"J2000.0\", description=\"Coordinate epoch\")\n</code></pre>"},{"location":"api/astro_lab.tensors/#astro_lab.tensors.SpectralTensorConfig","title":"SpectralTensorConfig","text":"<p>               Bases: <code>BaseModel</code></p> <p>Configuration for SpectralTensorDict.</p> Source code in <code>src\\astro_lab\\tensors\\__init__.py</code> <pre><code>class SpectralTensorConfig(BaseModel):\n    \"\"\"Configuration for SpectralTensorDict.\"\"\"\n\n    model_config = ConfigDict(validate_assignment=True, extra=\"allow\")\n    wavelength_unit: str = Field(default=\"angstrom\", description=\"Wavelength units\")\n    flux_unit: str = Field(default=\"erg/s/cm2/A\", description=\"Flux units\")\n    spectral_resolution: Optional[float] = Field(default=None, description=\"R = \u03bb/\u0394\u03bb\")\n</code></pre>"},{"location":"api/astro_lab.tensors/#astro_lab.tensors.SurveyTensorConfig","title":"SurveyTensorConfig","text":"<p>               Bases: <code>BaseModel</code></p> <p>Configuration for SurveyTensorDict.</p> Source code in <code>src\\astro_lab\\tensors\\__init__.py</code> <pre><code>class SurveyTensorConfig(BaseModel):\n    \"\"\"Configuration for SurveyTensorDict.\"\"\"\n\n    model_config = ConfigDict(validate_assignment=True, extra=\"allow\")\n    survey_name: str = Field(..., description=\"Name of the survey\")\n    data_release: Optional[str] = Field(\n        default=None, description=\"Data release version\"\n    )\n    selection_function: Optional[str] = Field(\n        default=None, description=\"Selection function applied\"\n    )\n    created_at: str = Field(default_factory=lambda: datetime.datetime.now().isoformat())\n</code></pre>"},{"location":"api/astro_lab.tensors/#astro_lab.tensors.create_photometric_tensor","title":"create_photometric_tensor","text":"<pre><code>create_photometric_tensor(magnitudes, bands, **kwargs)\n</code></pre> <p>Create PhotometricTensorDict from magnitude data.</p> Source code in <code>src\\astro_lab\\tensors\\__init__.py</code> <pre><code>def create_photometric_tensor(magnitudes, bands, **kwargs):\n    \"\"\"Create PhotometricTensorDict from magnitude data.\"\"\"\n    import torch\n\n    if not isinstance(magnitudes, torch.Tensor):\n        magnitudes = torch.tensor(magnitudes, dtype=torch.float32)\n\n    return PhotometricTensorDict(magnitudes, bands, **kwargs)\n</code></pre>"},{"location":"api/astro_lab.tensors/#astro_lab.tensors.create_simulation_tensor","title":"create_simulation_tensor","text":"<pre><code>create_simulation_tensor(positions, features=None, **kwargs)\n</code></pre> <p>Create SimulationTensorDict for N-body data.</p> Source code in <code>src\\astro_lab\\tensors\\__init__.py</code> <pre><code>def create_simulation_tensor(positions, features=None, **kwargs):\n    \"\"\"Create SimulationTensorDict for N-body data.\"\"\"\n    return SimulationTensorDict(\n        positions=positions,\n        velocities=kwargs.get(\"velocities\", positions * 0),\n        masses=kwargs.get(\"masses\", positions.new_ones(positions.shape[0])),\n        **kwargs,\n    )\n</code></pre>"},{"location":"api/astro_lab.tensors/#astro_lab.tensors.create_spatial_tensor","title":"create_spatial_tensor","text":"<pre><code>create_spatial_tensor(coordinates, coordinate_system='icrs', **kwargs)\n</code></pre> <p>Create SpatialTensorDict from coordinates.</p> Source code in <code>src\\astro_lab\\tensors\\__init__.py</code> <pre><code>def create_spatial_tensor(coordinates, coordinate_system=\"icrs\", **kwargs):\n    \"\"\"Create SpatialTensorDict from coordinates.\"\"\"\n    import torch\n\n    if not isinstance(coordinates, torch.Tensor):\n        coordinates = torch.tensor(coordinates, dtype=torch.float32)\n\n    return SpatialTensorDict(coordinates, coordinate_system=coordinate_system, **kwargs)\n</code></pre>"},{"location":"api/astro_lab.tensors/#astro_lab.tensors.create_survey_tensor","title":"create_survey_tensor","text":"<pre><code>create_survey_tensor(spatial, photometric, survey_name, **kwargs)\n</code></pre> <p>Create SurveyTensorDict from components.</p> Source code in <code>src\\astro_lab\\tensors\\__init__.py</code> <pre><code>def create_survey_tensor(spatial, photometric, survey_name, **kwargs):\n    \"\"\"Create SurveyTensorDict from components.\"\"\"\n    return SurveyTensorDict(\n        spatial=spatial, photometric=photometric, survey_name=survey_name, **kwargs\n    )\n</code></pre>"},{"location":"api/astro_lab.tensors/#astro_lab.tensors.from_astrometric_data","title":"from_astrometric_data","text":"<pre><code>from_astrometric_data(ra, dec, parallax=None, pmra=None, pmdec=None, **kwargs)\n</code></pre> <p>Create SpatialTensorDict from astrometric measurements.</p> Source code in <code>src\\astro_lab\\tensors\\__init__.py</code> <pre><code>def from_astrometric_data(ra, dec, parallax=None, pmra=None, pmdec=None, **kwargs):\n    \"\"\"Create SpatialTensorDict from astrometric measurements.\"\"\"\n    import torch\n\n    # Convert to tensors\n    if not isinstance(ra, torch.Tensor):\n        ra = torch.tensor(ra, dtype=torch.float32)\n    if not isinstance(dec, torch.Tensor):\n        dec = torch.tensor(dec, dtype=torch.float32)\n\n    # Create coordinates tensor\n    coords = torch.stack([ra, dec, torch.zeros_like(ra)], dim=-1)\n\n    # Add distance from parallax if available\n    if parallax is not None:\n        if not isinstance(parallax, torch.Tensor):\n            parallax = torch.tensor(parallax, dtype=torch.float32)\n        distance = 1000.0 / (torch.abs(parallax) + 1e-6)  # mas to parsec\n        coords[..., 2] = distance\n\n    return SpatialTensorDict(coords, coordinate_system=\"icrs\", **kwargs)\n</code></pre>"},{"location":"api/astro_lab.tensors/#astro_lab.tensors.from_lightcurve_data","title":"from_lightcurve_data","text":"<pre><code>from_lightcurve_data(times, magnitudes, errors=None, **kwargs)\n</code></pre> <p>Create LightcurveTensorDict from lightcurve data.</p> Source code in <code>src\\astro_lab\\tensors\\__init__.py</code> <pre><code>def from_lightcurve_data(times, magnitudes, errors=None, **kwargs):\n    \"\"\"Create LightcurveTensorDict from lightcurve data.\"\"\"\n    import torch\n\n    if not isinstance(times, torch.Tensor):\n        times = torch.tensor(times, dtype=torch.float32)\n    if not isinstance(magnitudes, torch.Tensor):\n        magnitudes = torch.tensor(magnitudes, dtype=torch.float32)\n\n    # Ensure proper shape for LightcurveTensorDict\n    if magnitudes.dim() == 2:\n        magnitudes = magnitudes.unsqueeze(-1)  # Add band dimension\n\n    if errors is not None:\n        if not isinstance(errors, torch.Tensor):\n            errors = torch.tensor(errors, dtype=torch.float32)\n        if errors.dim() == 2:\n            errors = errors.unsqueeze(-1)\n\n    return LightcurveTensorDict(\n        times=times, magnitudes=magnitudes, bands=[\"V\"], errors=errors, **kwargs\n    )\n</code></pre>"},{"location":"api/astro_lab.tensors/#astro_lab.tensors.from_orbital_elements","title":"from_orbital_elements","text":"<pre><code>from_orbital_elements(elements, element_type='keplerian', **kwargs)\n</code></pre> <p>Create OrbitTensorDict from orbital elements.</p> Source code in <code>src\\astro_lab\\tensors\\__init__.py</code> <pre><code>def from_orbital_elements(elements, element_type=\"keplerian\", **kwargs):\n    \"\"\"Create OrbitTensorDict from orbital elements.\"\"\"\n    import torch\n\n    if not isinstance(elements, torch.Tensor):\n        elements = torch.tensor(elements, dtype=torch.float32)\n\n    return OrbitTensorDict(elements=elements, **kwargs)\n</code></pre>"},{"location":"api/astro_lab.training/","title":"astro_lab.training","text":""},{"location":"api/astro_lab.training/#astro_lab.training","title":"training","text":""},{"location":"api/astro_lab.training/#astro_lab.training--astrolab-training-module-lightning-edition","title":"AstroLab Training Module (Lightning Edition)","text":"<p>Simplified training utilities for Lightning-based astronomical ML. The main training functionality is now handled by Lightning wrappers in astro_lab.models.lightning.</p> <p>Modules:</p> Name Description <code>astro_trainer</code> <p>AstroLab Trainer for Lightning Models</p> <code>config</code> <p>Training Configuration for AstroLab</p> <code>mlflow_logger</code> <p>Lightning MLflow Logger for AstroLab</p> <code>utils</code> <p>Training Utilities</p> <p>Classes:</p> Name Description <code>AstroTrainer</code> <p>Unified trainer for AstroLab Lightning models.</p> <code>LightningMLflowLogger</code> <p>Enhanced MLflow logger for PyTorch Lightning with AstroLab integration.</p> <code>MLflowModelCheckpoint</code> <p>Lightning callback to log best models to MLflow.</p> <code>TrainingConfig</code> <p>Configuration for model training.</p> <p>Functions:</p> Name Description <code>set_random_seed</code> <p>Set random seed for reproducibility across all libraries.</p> <code>setup_logging</code> <p>Setup logging configuration for training.</p> <code>train_model</code> <p>Convenience function to train a model with configuration.</p>"},{"location":"api/astro_lab.training/#astro_lab.training.AstroTrainer","title":"AstroTrainer","text":"<p>Unified trainer for AstroLab Lightning models.</p> <p>Handles model creation, data loading, training setup, and execution.</p> <p>Methods:</p> Name Description <code>create_datamodule</code> <p>Create data module for training.</p> <code>create_model</code> <p>Create Lightning model based on configuration.</p> <code>create_trainer</code> <p>Create Lightning trainer with configuration.</p> <code>get_model</code> <p>Get the created model.</p> <code>get_trainer</code> <p>Get the created trainer.</p> <code>train</code> <p>Execute complete training pipeline.</p> Source code in <code>src\\astro_lab\\training\\astro_trainer.py</code> <pre><code>class AstroTrainer:\n    \"\"\"\n    Unified trainer for AstroLab Lightning models.\n\n    Handles model creation, data loading, training setup, and execution.\n    \"\"\"\n\n    def __init__(self, config: Dict[str, Any]):\n        \"\"\"\n        Initialize AstroTrainer with configuration.\n\n        Args:\n            config: Training configuration dictionary\n        \"\"\"\n        self.config = config\n        self.model = None\n        self.datamodule = None\n        self.trainer = None\n        self.mlflow_logger = None\n\n    def create_model(self) -&gt; pl.LightningModule:\n        \"\"\"Create Lightning model based on configuration.\"\"\"\n        # Build model kwargs from config\n        model_kwargs = {}\n        for key in [\n            \"learning_rate\",\n            \"optimizer\",\n            \"scheduler\",\n            \"hidden_dim\",\n            \"num_layers\",\n            \"num_classes\",\n        ]:\n            if key in self.config and self.config[key] is not None:\n                model_kwargs[key] = self.config[key]\n\n        # Create model\n        if self.config.get(\"preset\"):\n            logger.info(f\"Creating model with preset: {self.config['preset']}\")\n            model = create_preset_model(self.config[\"preset\"], **model_kwargs)\n        elif self.config.get(\"model\"):\n            logger.info(f\"Creating model: {self.config['model']}\")\n            model = create_lightning_model(self.config[\"model\"], **model_kwargs)\n        else:\n            raise ValueError(\"Must specify either 'model' or 'preset' in config\")\n\n        self.model = model\n        return model\n\n    def create_datamodule(self) -&gt; AstroDataModule:\n        \"\"\"Create data module for training.\"\"\"\n        survey = self.config.get(\"survey\", \"gaia\")\n        batch_size = self.config.get(\"batch_size\", 32)\n        max_samples = self.config.get(\"max_samples\")\n\n        logger.info(f\"Setting up data for survey: {survey}\")\n        datamodule = AstroDataModule(\n            survey=survey,\n            batch_size=batch_size,\n            num_workers=4,\n            max_samples=max_samples,\n        )\n\n        self.datamodule = datamodule\n        return datamodule\n\n    def create_trainer(self) -&gt; pl.Trainer:\n        \"\"\"Create Lightning trainer with configuration.\"\"\"\n        logger.info(\"Configuring Lightning trainer\")\n\n        # Setup callbacks\n        checkpoint_dir = self.config.get(\"checkpoint_dir\", Path(\"checkpoints\"))\n        callbacks = [\n            ModelCheckpoint(\n                dirpath=checkpoint_dir,\n                filename=\"{epoch:02d}-{val_loss:.3f}\",\n                monitor=\"val_loss\",\n                mode=\"min\",\n                save_top_k=3,\n            ),\n            EarlyStopping(\n                monitor=\"val_loss\",\n                patience=10,\n                mode=\"min\",\n            ),\n        ]\n\n        # Setup MLflow logger\n        experiment_name = self.config.get(\"experiment_name\", \"astrolab_experiment\")\n        model_name = self.config.get(\"model\") or self.config.get(\"preset\", \"unknown\")\n        survey = self.config.get(\"survey\", \"gaia\")\n\n        mlflow_logger = LightningMLflowLogger(\n            experiment_name=experiment_name,\n            run_name=f\"{model_name}_{survey}\",\n            tags={\n                \"model\": model_name,\n                \"survey\": survey,\n                \"preset\": self.config.get(\"preset\") or \"custom\",\n            },\n        )\n\n        self.mlflow_logger = mlflow_logger\n\n        # Create trainer\n        trainer_kwargs = {\n            \"max_epochs\": self.config.get(\"epochs\", 50),\n            \"accelerator\": \"auto\",\n            \"devices\": self.config.get(\"devices\", 1),\n            \"precision\": self.config.get(\"precision\", \"16-mixed\"),\n            \"strategy\": self.config.get(\"strategy\", \"auto\"),\n            \"callbacks\": callbacks,\n            \"logger\": mlflow_logger,\n            \"gradient_clip_val\": 1.0,\n            \"accumulate_grad_batches\": 2,\n        }\n\n        # Development options\n        if self.config.get(\"fast_dev_run\"):\n            trainer_kwargs[\"fast_dev_run\"] = True\n        if self.config.get(\"overfit_batches\"):\n            trainer_kwargs[\"overfit_batches\"] = self.config[\"overfit_batches\"]\n\n        trainer = pl.Trainer(**trainer_kwargs)\n        self.trainer = trainer\n        return trainer\n\n    def train(self) -&gt; bool:\n        \"\"\"\n        Execute complete training pipeline.\n\n        Returns:\n            True if training succeeded, False otherwise\n        \"\"\"\n        try:\n            logger.info(\"\ud83d\ude80 Starting AstroLab Lightning training\")\n\n            # Create components\n            self.create_model()\n            self.create_datamodule()\n            self.create_trainer()\n\n            # Training\n            logger.info(\"Starting training...\")\n            if self.model is None or self.datamodule is None:\n                raise RuntimeError(\n                    \"Model and datamodule must be created before training\"\n                )\n            self.trainer.fit(self.model, self.datamodule)\n\n            # Testing\n            logger.info(\"Running final evaluation...\")\n            self.trainer.test(self.model, self.datamodule)\n\n            logger.info(\"\u2705 Training completed successfully!\")\n            return True\n\n        except Exception as e:\n            logger.error(f\"\u274c Training failed: {e}\")\n            return False\n\n    def get_model(self) -&gt; Optional[pl.LightningModule]:\n        \"\"\"Get the created model.\"\"\"\n        return self.model\n\n    def get_trainer(self) -&gt; Optional[pl.Trainer]:\n        \"\"\"Get the created trainer.\"\"\"\n        return self.trainer\n</code></pre>"},{"location":"api/astro_lab.training/#astro_lab.training.AstroTrainer.create_datamodule","title":"create_datamodule","text":"<pre><code>create_datamodule() -&gt; AstroDataModule\n</code></pre> <p>Create data module for training.</p> Source code in <code>src\\astro_lab\\training\\astro_trainer.py</code> <pre><code>def create_datamodule(self) -&gt; AstroDataModule:\n    \"\"\"Create data module for training.\"\"\"\n    survey = self.config.get(\"survey\", \"gaia\")\n    batch_size = self.config.get(\"batch_size\", 32)\n    max_samples = self.config.get(\"max_samples\")\n\n    logger.info(f\"Setting up data for survey: {survey}\")\n    datamodule = AstroDataModule(\n        survey=survey,\n        batch_size=batch_size,\n        num_workers=4,\n        max_samples=max_samples,\n    )\n\n    self.datamodule = datamodule\n    return datamodule\n</code></pre>"},{"location":"api/astro_lab.training/#astro_lab.training.AstroTrainer.create_model","title":"create_model","text":"<pre><code>create_model() -&gt; LightningModule\n</code></pre> <p>Create Lightning model based on configuration.</p> Source code in <code>src\\astro_lab\\training\\astro_trainer.py</code> <pre><code>def create_model(self) -&gt; pl.LightningModule:\n    \"\"\"Create Lightning model based on configuration.\"\"\"\n    # Build model kwargs from config\n    model_kwargs = {}\n    for key in [\n        \"learning_rate\",\n        \"optimizer\",\n        \"scheduler\",\n        \"hidden_dim\",\n        \"num_layers\",\n        \"num_classes\",\n    ]:\n        if key in self.config and self.config[key] is not None:\n            model_kwargs[key] = self.config[key]\n\n    # Create model\n    if self.config.get(\"preset\"):\n        logger.info(f\"Creating model with preset: {self.config['preset']}\")\n        model = create_preset_model(self.config[\"preset\"], **model_kwargs)\n    elif self.config.get(\"model\"):\n        logger.info(f\"Creating model: {self.config['model']}\")\n        model = create_lightning_model(self.config[\"model\"], **model_kwargs)\n    else:\n        raise ValueError(\"Must specify either 'model' or 'preset' in config\")\n\n    self.model = model\n    return model\n</code></pre>"},{"location":"api/astro_lab.training/#astro_lab.training.AstroTrainer.create_trainer","title":"create_trainer","text":"<pre><code>create_trainer() -&gt; Trainer\n</code></pre> <p>Create Lightning trainer with configuration.</p> Source code in <code>src\\astro_lab\\training\\astro_trainer.py</code> <pre><code>def create_trainer(self) -&gt; pl.Trainer:\n    \"\"\"Create Lightning trainer with configuration.\"\"\"\n    logger.info(\"Configuring Lightning trainer\")\n\n    # Setup callbacks\n    checkpoint_dir = self.config.get(\"checkpoint_dir\", Path(\"checkpoints\"))\n    callbacks = [\n        ModelCheckpoint(\n            dirpath=checkpoint_dir,\n            filename=\"{epoch:02d}-{val_loss:.3f}\",\n            monitor=\"val_loss\",\n            mode=\"min\",\n            save_top_k=3,\n        ),\n        EarlyStopping(\n            monitor=\"val_loss\",\n            patience=10,\n            mode=\"min\",\n        ),\n    ]\n\n    # Setup MLflow logger\n    experiment_name = self.config.get(\"experiment_name\", \"astrolab_experiment\")\n    model_name = self.config.get(\"model\") or self.config.get(\"preset\", \"unknown\")\n    survey = self.config.get(\"survey\", \"gaia\")\n\n    mlflow_logger = LightningMLflowLogger(\n        experiment_name=experiment_name,\n        run_name=f\"{model_name}_{survey}\",\n        tags={\n            \"model\": model_name,\n            \"survey\": survey,\n            \"preset\": self.config.get(\"preset\") or \"custom\",\n        },\n    )\n\n    self.mlflow_logger = mlflow_logger\n\n    # Create trainer\n    trainer_kwargs = {\n        \"max_epochs\": self.config.get(\"epochs\", 50),\n        \"accelerator\": \"auto\",\n        \"devices\": self.config.get(\"devices\", 1),\n        \"precision\": self.config.get(\"precision\", \"16-mixed\"),\n        \"strategy\": self.config.get(\"strategy\", \"auto\"),\n        \"callbacks\": callbacks,\n        \"logger\": mlflow_logger,\n        \"gradient_clip_val\": 1.0,\n        \"accumulate_grad_batches\": 2,\n    }\n\n    # Development options\n    if self.config.get(\"fast_dev_run\"):\n        trainer_kwargs[\"fast_dev_run\"] = True\n    if self.config.get(\"overfit_batches\"):\n        trainer_kwargs[\"overfit_batches\"] = self.config[\"overfit_batches\"]\n\n    trainer = pl.Trainer(**trainer_kwargs)\n    self.trainer = trainer\n    return trainer\n</code></pre>"},{"location":"api/astro_lab.training/#astro_lab.training.AstroTrainer.get_model","title":"get_model","text":"<pre><code>get_model() -&gt; Optional[LightningModule]\n</code></pre> <p>Get the created model.</p> Source code in <code>src\\astro_lab\\training\\astro_trainer.py</code> <pre><code>def get_model(self) -&gt; Optional[pl.LightningModule]:\n    \"\"\"Get the created model.\"\"\"\n    return self.model\n</code></pre>"},{"location":"api/astro_lab.training/#astro_lab.training.AstroTrainer.get_trainer","title":"get_trainer","text":"<pre><code>get_trainer() -&gt; Optional[Trainer]\n</code></pre> <p>Get the created trainer.</p> Source code in <code>src\\astro_lab\\training\\astro_trainer.py</code> <pre><code>def get_trainer(self) -&gt; Optional[pl.Trainer]:\n    \"\"\"Get the created trainer.\"\"\"\n    return self.trainer\n</code></pre>"},{"location":"api/astro_lab.training/#astro_lab.training.AstroTrainer.train","title":"train","text":"<pre><code>train() -&gt; bool\n</code></pre> <p>Execute complete training pipeline.</p> <p>Returns:</p> Type Description <code>bool</code> <p>True if training succeeded, False otherwise</p> Source code in <code>src\\astro_lab\\training\\astro_trainer.py</code> <pre><code>def train(self) -&gt; bool:\n    \"\"\"\n    Execute complete training pipeline.\n\n    Returns:\n        True if training succeeded, False otherwise\n    \"\"\"\n    try:\n        logger.info(\"\ud83d\ude80 Starting AstroLab Lightning training\")\n\n        # Create components\n        self.create_model()\n        self.create_datamodule()\n        self.create_trainer()\n\n        # Training\n        logger.info(\"Starting training...\")\n        if self.model is None or self.datamodule is None:\n            raise RuntimeError(\n                \"Model and datamodule must be created before training\"\n            )\n        self.trainer.fit(self.model, self.datamodule)\n\n        # Testing\n        logger.info(\"Running final evaluation...\")\n        self.trainer.test(self.model, self.datamodule)\n\n        logger.info(\"\u2705 Training completed successfully!\")\n        return True\n\n    except Exception as e:\n        logger.error(f\"\u274c Training failed: {e}\")\n        return False\n</code></pre>"},{"location":"api/astro_lab.training/#astro_lab.training.LightningMLflowLogger","title":"LightningMLflowLogger","text":"<p>               Bases: <code>MLFlowLogger</code></p> <p>Enhanced MLflow logger for PyTorch Lightning with AstroLab integration.</p> <p>Features: - Automatic experiment organization - Model architecture tracking - Configuration persistence - Enhanced artifact logging - Integration with AstroLab model factories</p> <p>Methods:</p> Name Description <code>finalize</code> <p>Finalize the experiment run.</p> <code>log_astrolab_model_info</code> <p>Log AstroLab specific model information.</p> <code>log_configuration</code> <p>Log configuration as artifact.</p> <code>log_hyperparams</code> <p>Log hyperparameters with enhanced handling for AstroLab models.</p> <code>log_metrics</code> <p>Log metrics with validation.</p> <code>log_model_summary</code> <p>Log model architecture summary.</p> <code>log_text</code> <p>Log text as artifact.</p> <p>Attributes:</p> Name Type Description <code>name</code> <code>str</code> <p>Return logger name.</p> <code>version</code> <code>str</code> <p>Return run id.</p> Source code in <code>src\\astro_lab\\training\\mlflow_logger.py</code> <pre><code>class LightningMLflowLogger(MLFlowLogger):\n    \"\"\"\n    Enhanced MLflow logger for PyTorch Lightning with AstroLab integration.\n\n    Features:\n    - Automatic experiment organization\n    - Model architecture tracking\n    - Configuration persistence\n    - Enhanced artifact logging\n    - Integration with AstroLab model factories\n    \"\"\"\n\n    def __init__(\n        self,\n        experiment_name: str = \"astrolab_experiment\",\n        tracking_uri: Optional[str] = None,\n        run_name: Optional[str] = None,\n        tags: Optional[Dict[str, str]] = None,\n        save_dir: Optional[str] = \"./mlruns\",\n        log_model: bool = True,\n        prefix: str = \"\",\n    ):\n        \"\"\"\n        Initialize Lightning MLflow logger.\n\n        Args:\n            experiment_name: Name of the MLflow experiment\n            tracking_uri: URI for MLflow tracking server\n            run_name: Name for this specific run\n            tags: Additional tags for the run\n            save_dir: Directory to save MLflow runs\n            log_model: Whether to log models automatically\n            prefix: Prefix for metric keys\n        \"\"\"\n        # Set tracking URI with proper file:// prefix\n        if tracking_uri is None:\n            save_path = Path(save_dir).resolve()\n            # Convert to proper file URI format\n            tracking_uri = save_path.as_uri()\n\n        # Prepare tags with defaults\n        default_tags = {\n            \"framework\": \"lightning\",\n            \"library\": \"astrolab\",\n            \"created_at\": datetime.now().isoformat(),\n        }\n        if tags:\n            default_tags.update(tags)\n\n        # Initialize parent MLFlowLogger\n        super().__init__(\n            experiment_name=experiment_name,\n            tracking_uri=tracking_uri,\n            tags=default_tags,\n            save_dir=save_dir,\n            log_model=log_model,\n            prefix=prefix,\n            run_name=run_name,\n        )\n\n        self._log_model = log_model\n        self._logged_model_summary = False\n\n    @property\n    def name(self) -&gt; str:\n        \"\"\"Return logger name.\"\"\"\n        return \"AstroLabMLflow\"\n\n    @property\n    def version(self) -&gt; str:\n        \"\"\"Return run id.\"\"\"\n        return self.run_id if self.run_id else \"unknown\"\n\n    @rank_zero_only\n    def log_hyperparams(self, params: Union[Dict[str, Any], Any]) -&gt; None:\n        \"\"\"\n        Log hyperparameters with enhanced handling for AstroLab models.\n\n        Args:\n            params: Parameters to log (dict or namespace)\n        \"\"\"\n        # Convert to dict if needed\n        if hasattr(params, \"hparams\"):\n            # Lightning module with hparams\n            params_dict = dict(params.hparams)\n        elif hasattr(params, \"model_dump\"):\n            # Pydantic model\n            params_dict = params.model_dump()\n        elif hasattr(params, \"__dict__\"):\n            # Object with attributes\n            params_dict = {k: v for k, v in params.__dict__.items() \n                          if not k.startswith(\"_\")}\n        else:\n            params_dict = dict(params) if isinstance(params, dict) else {}\n\n        # Handle AstroLab specific parameters\n        if \"model_config\" in params_dict:\n            model_config = params_dict.pop(\"model_config\")\n            if hasattr(model_config, \"to_dict\"):\n                params_dict[\"model\"] = model_config.to_dict()\n            else:\n                params_dict[\"model\"] = str(model_config)\n\n        # Flatten nested parameters\n        flat_params = self._flatten_dict(params_dict)\n\n        # Filter out non-loggable values\n        loggable_params = {\n            k: v for k, v in flat_params.items() \n            if v is not None and self._is_loggable_value(v)\n        }\n\n        # Log via parent method\n        super().log_hyperparams(loggable_params)\n\n    @rank_zero_only\n    def log_metrics(self, metrics: Dict[str, float], step: Optional[int] = None) -&gt; None:\n        \"\"\"\n        Log metrics with validation.\n\n        Args:\n            metrics: Dictionary of metrics\n            step: Global step number\n        \"\"\"\n        # Filter out non-numeric values\n        numeric_metrics = {}\n        for k, v in metrics.items():\n            if isinstance(v, (int, float)) and not isinstance(v, bool):\n                numeric_metrics[k] = float(v)\n            elif hasattr(v, \"item\"):  # Tensor\n                numeric_metrics[k] = float(v.item())\n\n        # Log via parent method\n        super().log_metrics(numeric_metrics, step)\n\n    @rank_zero_only\n    def log_model_summary(self, model: torch.nn.Module, max_depth: int = 2) -&gt; None:\n        \"\"\"\n        Log model architecture summary.\n\n        Args:\n            model: PyTorch model\n            max_depth: Maximum depth for summary\n        \"\"\"\n        if self._logged_model_summary:\n            return\n\n        try:\n            from lightning.pytorch.utilities.model_summary import ModelSummary\n\n            # Generate summary\n            summary = ModelSummary(model, max_depth=max_depth)\n            summary_str = str(summary)\n\n            # Log as text artifact\n            self.log_text(summary_str, \"model_summary.txt\")\n\n            # Log key statistics as metrics\n            total_params = sum(p.numel() for p in model.parameters())\n            trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n\n            self.experiment.log_metric(self.run_id, \"model/total_params\", total_params)\n            self.experiment.log_metric(self.run_id, \"model/trainable_params\", trainable_params)\n            self.experiment.log_metric(self.run_id, \"model/size_mb\", total_params * 4 / 1024 / 1024)\n\n            # Log model type if it's an AstroLab model\n            model_type = type(model).__name__\n            self.experiment.set_tag(self.run_id, \"model_type\", model_type)\n\n            self._logged_model_summary = True\n\n        except Exception as e:\n            logger.warning(f\"Could not log model summary: {e}\")\n\n    @rank_zero_only\n    def log_configuration(self, config: Any, filename: str = \"config.yaml\") -&gt; None:\n        \"\"\"\n        Log configuration as artifact.\n\n        Args:\n            config: Configuration object\n            filename: Name for config file\n        \"\"\"\n        # Convert to dict\n        if hasattr(config, \"model_dump\"):\n            config_dict = config.model_dump()\n        elif hasattr(config, \"to_dict\"):\n            config_dict = config.to_dict()\n        elif hasattr(config, \"__dict__\"):\n            config_dict = {k: v for k, v in config.__dict__.items() \n                          if not k.startswith(\"_\")}\n        else:\n            config_dict = config if isinstance(config, dict) else {\"config\": str(config)}\n\n        # Handle nested objects\n        config_dict = self._make_serializable(config_dict)\n\n        # Save based on file extension\n        config_path = Path(filename)\n\n        try:\n            if config_path.suffix in [\".yaml\", \".yml\"]:\n                import yaml\n                with open(config_path, \"w\") as f:\n                    yaml.dump(config_dict, f, default_flow_style=False)\n            else:\n                with open(config_path, \"w\") as f:\n                    json.dump(config_dict, f, indent=2, default=str)\n\n            # Log artifact\n            self.experiment.log_artifact(self.run_id, str(config_path))\n            config_path.unlink()\n\n        except Exception as e:\n            logger.warning(f\"Could not log configuration: {e}\")\n\n    @rank_zero_only\n    def log_text(self, text: str, filename: str) -&gt; None:\n        \"\"\"\n        Log text as artifact.\n\n        Args:\n            text: Text content\n            filename: Filename for artifact\n        \"\"\"\n        text_path = Path(filename)\n        text_path.write_text(text)\n        self.experiment.log_artifact(self.run_id, str(text_path))\n        text_path.unlink()\n\n    @rank_zero_only\n    def log_astrolab_model_info(self, model_name: str, model_config: Dict[str, Any]) -&gt; None:\n        \"\"\"\n        Log AstroLab specific model information.\n\n        Args:\n            model_name: Name of the model from factory\n            model_config: Model configuration dictionary\n        \"\"\"\n        # Log model factory name\n        self.experiment.set_tag(self.run_id, \"astrolab/model_name\", model_name)\n\n        # Log model configuration\n        for key, value in model_config.items():\n            if self._is_loggable_value(value):\n                self.experiment.log_param(self.run_id, f\"astrolab/{key}\", value)\n\n    @rank_zero_only\n    def finalize(self, status: str = \"success\") -&gt; None:\n        \"\"\"\n        Finalize the experiment run.\n\n        Args:\n            status: Status of the run\n        \"\"\"\n        try:\n            if self.experiment:\n                mlflow.set_tag(\"status\", status)\n                mlflow.set_tag(\"ended_at\", datetime.now().isoformat())\n        except Exception as e:\n            logger.warning(f\"Could not finalize run: {e}\")\n\n        super().finalize(status)\n\n    # Helper methods\n\n    def _flatten_dict(self, d: Dict[str, Any], parent_key: str = \"\", sep: str = \"/\") -&gt; Dict[str, Any]:\n        \"\"\"Flatten nested dictionary.\"\"\"\n        items = []\n\n        for k, v in d.items():\n            new_key = f\"{parent_key}{sep}{k}\" if parent_key else k\n\n            if isinstance(v, dict):\n                items.extend(self._flatten_dict(v, new_key, sep=sep).items())\n            else:\n                items.append((new_key, v))\n\n        return dict(items)\n\n    def _is_loggable_value(self, value: Any) -&gt; bool:\n        \"\"\"Check if value can be logged to MLflow.\"\"\"\n        if isinstance(value, (str, int, float, bool)):\n            return True\n        if isinstance(value, (list, tuple)):\n            return len(str(value)) &lt; 500  # Avoid very long lists\n        return False\n\n    def _make_serializable(self, obj: Any) -&gt; Any:\n        \"\"\"Make object JSON serializable.\"\"\"\n        if isinstance(obj, dict):\n            return {k: self._make_serializable(v) for k, v in obj.items()}\n        elif isinstance(obj, (list, tuple)):\n            return [self._make_serializable(v) for v in obj]\n        elif hasattr(obj, \"to_dict\"):\n            return obj.to_dict()\n        elif hasattr(obj, \"__dict__\"):\n            return str(obj)\n        elif isinstance(obj, (torch.Tensor, torch.nn.Module)):\n            return str(obj)\n        else:\n            return obj\n</code></pre>"},{"location":"api/astro_lab.training/#astro_lab.training.LightningMLflowLogger.name","title":"name  <code>property</code>","text":"<pre><code>name: str\n</code></pre> <p>Return logger name.</p>"},{"location":"api/astro_lab.training/#astro_lab.training.LightningMLflowLogger.version","title":"version  <code>property</code>","text":"<pre><code>version: str\n</code></pre> <p>Return run id.</p>"},{"location":"api/astro_lab.training/#astro_lab.training.LightningMLflowLogger.finalize","title":"finalize","text":"<pre><code>finalize(status: str = 'success') -&gt; None\n</code></pre> <p>Finalize the experiment run.</p> <p>Parameters:</p> Name Type Description Default <code>status</code> <code>str</code> <p>Status of the run</p> <code>'success'</code> Source code in <code>src\\astro_lab\\training\\mlflow_logger.py</code> <pre><code>@rank_zero_only\ndef finalize(self, status: str = \"success\") -&gt; None:\n    \"\"\"\n    Finalize the experiment run.\n\n    Args:\n        status: Status of the run\n    \"\"\"\n    try:\n        if self.experiment:\n            mlflow.set_tag(\"status\", status)\n            mlflow.set_tag(\"ended_at\", datetime.now().isoformat())\n    except Exception as e:\n        logger.warning(f\"Could not finalize run: {e}\")\n\n    super().finalize(status)\n</code></pre>"},{"location":"api/astro_lab.training/#astro_lab.training.LightningMLflowLogger.log_astrolab_model_info","title":"log_astrolab_model_info","text":"<pre><code>log_astrolab_model_info(model_name: str, model_config: Dict[str, Any]) -&gt; None\n</code></pre> <p>Log AstroLab specific model information.</p> <p>Parameters:</p> Name Type Description Default <code>model_name</code> <code>str</code> <p>Name of the model from factory</p> required <code>model_config</code> <code>Dict[str, Any]</code> <p>Model configuration dictionary</p> required Source code in <code>src\\astro_lab\\training\\mlflow_logger.py</code> <pre><code>@rank_zero_only\ndef log_astrolab_model_info(self, model_name: str, model_config: Dict[str, Any]) -&gt; None:\n    \"\"\"\n    Log AstroLab specific model information.\n\n    Args:\n        model_name: Name of the model from factory\n        model_config: Model configuration dictionary\n    \"\"\"\n    # Log model factory name\n    self.experiment.set_tag(self.run_id, \"astrolab/model_name\", model_name)\n\n    # Log model configuration\n    for key, value in model_config.items():\n        if self._is_loggable_value(value):\n            self.experiment.log_param(self.run_id, f\"astrolab/{key}\", value)\n</code></pre>"},{"location":"api/astro_lab.training/#astro_lab.training.LightningMLflowLogger.log_configuration","title":"log_configuration","text":"<pre><code>log_configuration(config: Any, filename: str = 'config.yaml') -&gt; None\n</code></pre> <p>Log configuration as artifact.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>Any</code> <p>Configuration object</p> required <code>filename</code> <code>str</code> <p>Name for config file</p> <code>'config.yaml'</code> Source code in <code>src\\astro_lab\\training\\mlflow_logger.py</code> <pre><code>@rank_zero_only\ndef log_configuration(self, config: Any, filename: str = \"config.yaml\") -&gt; None:\n    \"\"\"\n    Log configuration as artifact.\n\n    Args:\n        config: Configuration object\n        filename: Name for config file\n    \"\"\"\n    # Convert to dict\n    if hasattr(config, \"model_dump\"):\n        config_dict = config.model_dump()\n    elif hasattr(config, \"to_dict\"):\n        config_dict = config.to_dict()\n    elif hasattr(config, \"__dict__\"):\n        config_dict = {k: v for k, v in config.__dict__.items() \n                      if not k.startswith(\"_\")}\n    else:\n        config_dict = config if isinstance(config, dict) else {\"config\": str(config)}\n\n    # Handle nested objects\n    config_dict = self._make_serializable(config_dict)\n\n    # Save based on file extension\n    config_path = Path(filename)\n\n    try:\n        if config_path.suffix in [\".yaml\", \".yml\"]:\n            import yaml\n            with open(config_path, \"w\") as f:\n                yaml.dump(config_dict, f, default_flow_style=False)\n        else:\n            with open(config_path, \"w\") as f:\n                json.dump(config_dict, f, indent=2, default=str)\n\n        # Log artifact\n        self.experiment.log_artifact(self.run_id, str(config_path))\n        config_path.unlink()\n\n    except Exception as e:\n        logger.warning(f\"Could not log configuration: {e}\")\n</code></pre>"},{"location":"api/astro_lab.training/#astro_lab.training.LightningMLflowLogger.log_hyperparams","title":"log_hyperparams","text":"<pre><code>log_hyperparams(params: Union[Dict[str, Any], Any]) -&gt; None\n</code></pre> <p>Log hyperparameters with enhanced handling for AstroLab models.</p> <p>Parameters:</p> Name Type Description Default <code>params</code> <code>Union[Dict[str, Any], Any]</code> <p>Parameters to log (dict or namespace)</p> required Source code in <code>src\\astro_lab\\training\\mlflow_logger.py</code> <pre><code>@rank_zero_only\ndef log_hyperparams(self, params: Union[Dict[str, Any], Any]) -&gt; None:\n    \"\"\"\n    Log hyperparameters with enhanced handling for AstroLab models.\n\n    Args:\n        params: Parameters to log (dict or namespace)\n    \"\"\"\n    # Convert to dict if needed\n    if hasattr(params, \"hparams\"):\n        # Lightning module with hparams\n        params_dict = dict(params.hparams)\n    elif hasattr(params, \"model_dump\"):\n        # Pydantic model\n        params_dict = params.model_dump()\n    elif hasattr(params, \"__dict__\"):\n        # Object with attributes\n        params_dict = {k: v for k, v in params.__dict__.items() \n                      if not k.startswith(\"_\")}\n    else:\n        params_dict = dict(params) if isinstance(params, dict) else {}\n\n    # Handle AstroLab specific parameters\n    if \"model_config\" in params_dict:\n        model_config = params_dict.pop(\"model_config\")\n        if hasattr(model_config, \"to_dict\"):\n            params_dict[\"model\"] = model_config.to_dict()\n        else:\n            params_dict[\"model\"] = str(model_config)\n\n    # Flatten nested parameters\n    flat_params = self._flatten_dict(params_dict)\n\n    # Filter out non-loggable values\n    loggable_params = {\n        k: v for k, v in flat_params.items() \n        if v is not None and self._is_loggable_value(v)\n    }\n\n    # Log via parent method\n    super().log_hyperparams(loggable_params)\n</code></pre>"},{"location":"api/astro_lab.training/#astro_lab.training.LightningMLflowLogger.log_metrics","title":"log_metrics","text":"<pre><code>log_metrics(metrics: Dict[str, float], step: Optional[int] = None) -&gt; None\n</code></pre> <p>Log metrics with validation.</p> <p>Parameters:</p> Name Type Description Default <code>metrics</code> <code>Dict[str, float]</code> <p>Dictionary of metrics</p> required <code>step</code> <code>Optional[int]</code> <p>Global step number</p> <code>None</code> Source code in <code>src\\astro_lab\\training\\mlflow_logger.py</code> <pre><code>@rank_zero_only\ndef log_metrics(self, metrics: Dict[str, float], step: Optional[int] = None) -&gt; None:\n    \"\"\"\n    Log metrics with validation.\n\n    Args:\n        metrics: Dictionary of metrics\n        step: Global step number\n    \"\"\"\n    # Filter out non-numeric values\n    numeric_metrics = {}\n    for k, v in metrics.items():\n        if isinstance(v, (int, float)) and not isinstance(v, bool):\n            numeric_metrics[k] = float(v)\n        elif hasattr(v, \"item\"):  # Tensor\n            numeric_metrics[k] = float(v.item())\n\n    # Log via parent method\n    super().log_metrics(numeric_metrics, step)\n</code></pre>"},{"location":"api/astro_lab.training/#astro_lab.training.LightningMLflowLogger.log_model_summary","title":"log_model_summary","text":"<pre><code>log_model_summary(model: Module, max_depth: int = 2) -&gt; None\n</code></pre> <p>Log model architecture summary.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Module</code> <p>PyTorch model</p> required <code>max_depth</code> <code>int</code> <p>Maximum depth for summary</p> <code>2</code> Source code in <code>src\\astro_lab\\training\\mlflow_logger.py</code> <pre><code>@rank_zero_only\ndef log_model_summary(self, model: torch.nn.Module, max_depth: int = 2) -&gt; None:\n    \"\"\"\n    Log model architecture summary.\n\n    Args:\n        model: PyTorch model\n        max_depth: Maximum depth for summary\n    \"\"\"\n    if self._logged_model_summary:\n        return\n\n    try:\n        from lightning.pytorch.utilities.model_summary import ModelSummary\n\n        # Generate summary\n        summary = ModelSummary(model, max_depth=max_depth)\n        summary_str = str(summary)\n\n        # Log as text artifact\n        self.log_text(summary_str, \"model_summary.txt\")\n\n        # Log key statistics as metrics\n        total_params = sum(p.numel() for p in model.parameters())\n        trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n\n        self.experiment.log_metric(self.run_id, \"model/total_params\", total_params)\n        self.experiment.log_metric(self.run_id, \"model/trainable_params\", trainable_params)\n        self.experiment.log_metric(self.run_id, \"model/size_mb\", total_params * 4 / 1024 / 1024)\n\n        # Log model type if it's an AstroLab model\n        model_type = type(model).__name__\n        self.experiment.set_tag(self.run_id, \"model_type\", model_type)\n\n        self._logged_model_summary = True\n\n    except Exception as e:\n        logger.warning(f\"Could not log model summary: {e}\")\n</code></pre>"},{"location":"api/astro_lab.training/#astro_lab.training.LightningMLflowLogger.log_text","title":"log_text","text":"<pre><code>log_text(text: str, filename: str) -&gt; None\n</code></pre> <p>Log text as artifact.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>Text content</p> required <code>filename</code> <code>str</code> <p>Filename for artifact</p> required Source code in <code>src\\astro_lab\\training\\mlflow_logger.py</code> <pre><code>@rank_zero_only\ndef log_text(self, text: str, filename: str) -&gt; None:\n    \"\"\"\n    Log text as artifact.\n\n    Args:\n        text: Text content\n        filename: Filename for artifact\n    \"\"\"\n    text_path = Path(filename)\n    text_path.write_text(text)\n    self.experiment.log_artifact(self.run_id, str(text_path))\n    text_path.unlink()\n</code></pre>"},{"location":"api/astro_lab.training/#astro_lab.training.MLflowModelCheckpoint","title":"MLflowModelCheckpoint","text":"<p>               Bases: <code>Callback</code></p> <p>Lightning callback to log best models to MLflow.</p> <p>This callback monitors a metric and saves the best model(s) to MLflow when improvements are detected.</p> <p>Methods:</p> Name Description <code>on_validation_end</code> <p>Check and save best model after validation.</p> Source code in <code>src\\astro_lab\\training\\mlflow_logger.py</code> <pre><code>class MLflowModelCheckpoint(Callback):\n    \"\"\"\n    Lightning callback to log best models to MLflow.\n\n    This callback monitors a metric and saves the best model(s) to MLflow\n    when improvements are detected.\n    \"\"\"\n\n    def __init__(\n        self,\n        monitor: str = \"val_loss\",\n        mode: str = \"min\",\n        save_top_k: int = 1,\n        artifact_path: str = \"best_model\",\n        registered_model_name: Optional[str] = None,\n    ):\n        \"\"\"\n        Initialize MLflow model checkpoint callback.\n\n        Args:\n            monitor: Metric to monitor\n            mode: 'min' or 'max'\n            save_top_k: Number of best models to save\n            artifact_path: Path in MLflow artifacts\n            registered_model_name: Name for model registry\n        \"\"\"\n        super().__init__()\n        self.monitor = monitor\n        self.mode = mode\n        self.save_top_k = save_top_k\n        self.artifact_path = artifact_path\n        self.registered_model_name = registered_model_name\n\n        self.best_score = float(\"inf\") if mode == \"min\" else float(\"-inf\")\n        self.best_scores = []\n\n    def on_validation_end(self, trainer, pl_module):\n        \"\"\"Check and save best model after validation.\"\"\"\n        if self.monitor not in trainer.callback_metrics:\n            logger.warning(f\"Metric '{self.monitor}' not found in callback metrics\")\n            return\n\n        current = trainer.callback_metrics[self.monitor].item()\n\n        # Check if this is a new best\n        is_best = (\n            (self.mode == \"min\" and current &lt; self.best_score) or\n            (self.mode == \"max\" and current &gt; self.best_score)\n        )\n\n        if is_best:\n            self.best_score = current\n\n            # Log model to MLflow\n            if hasattr(trainer.logger, \"experiment\"):\n                try:\n                    mlflow.pytorch.log_model(\n                        pl_module,\n                        artifact_path=self.artifact_path,\n                        registered_model_name=self.registered_model_name\n                    )\n\n                    # Log best score\n                    trainer.logger.log_metrics({\n                        f\"best_{self.monitor}\": current\n                    })\n\n                    # Log checkpoint info\n                    checkpoint_info = {\n                        \"epoch\": trainer.current_epoch,\n                        \"global_step\": trainer.global_step,\n                        \"metric\": self.monitor,\n                        \"score\": current,\n                        \"timestamp\": datetime.now().isoformat()\n                    }\n\n                    info_path = Path(\"best_model_info.json\")\n                    with open(info_path, \"w\") as f:\n                        json.dump(checkpoint_info, f, indent=2)\n\n                    mlflow.log_artifact(str(info_path))\n                    info_path.unlink()\n\n                    logger.info(f\"Saved best model with {self.monitor}={current:.4f}\")\n\n                except Exception as e:\n                    logger.error(f\"Failed to log model to MLflow: {e}\")\n</code></pre>"},{"location":"api/astro_lab.training/#astro_lab.training.MLflowModelCheckpoint.on_validation_end","title":"on_validation_end","text":"<pre><code>on_validation_end(trainer, pl_module)\n</code></pre> <p>Check and save best model after validation.</p> Source code in <code>src\\astro_lab\\training\\mlflow_logger.py</code> <pre><code>def on_validation_end(self, trainer, pl_module):\n    \"\"\"Check and save best model after validation.\"\"\"\n    if self.monitor not in trainer.callback_metrics:\n        logger.warning(f\"Metric '{self.monitor}' not found in callback metrics\")\n        return\n\n    current = trainer.callback_metrics[self.monitor].item()\n\n    # Check if this is a new best\n    is_best = (\n        (self.mode == \"min\" and current &lt; self.best_score) or\n        (self.mode == \"max\" and current &gt; self.best_score)\n    )\n\n    if is_best:\n        self.best_score = current\n\n        # Log model to MLflow\n        if hasattr(trainer.logger, \"experiment\"):\n            try:\n                mlflow.pytorch.log_model(\n                    pl_module,\n                    artifact_path=self.artifact_path,\n                    registered_model_name=self.registered_model_name\n                )\n\n                # Log best score\n                trainer.logger.log_metrics({\n                    f\"best_{self.monitor}\": current\n                })\n\n                # Log checkpoint info\n                checkpoint_info = {\n                    \"epoch\": trainer.current_epoch,\n                    \"global_step\": trainer.global_step,\n                    \"metric\": self.monitor,\n                    \"score\": current,\n                    \"timestamp\": datetime.now().isoformat()\n                }\n\n                info_path = Path(\"best_model_info.json\")\n                with open(info_path, \"w\") as f:\n                    json.dump(checkpoint_info, f, indent=2)\n\n                mlflow.log_artifact(str(info_path))\n                info_path.unlink()\n\n                logger.info(f\"Saved best model with {self.monitor}={current:.4f}\")\n\n            except Exception as e:\n                logger.error(f\"Failed to log model to MLflow: {e}\")\n</code></pre>"},{"location":"api/astro_lab.training/#astro_lab.training.TrainingConfig","title":"TrainingConfig  <code>dataclass</code>","text":"<p>Configuration for model training.</p> Source code in <code>src\\astro_lab\\training\\config.py</code> <pre><code>@dataclass\nclass TrainingConfig:\n    \"\"\"Configuration for model training.\"\"\"\n\n    # Basic settings\n    dataset: str = \"gaia\"\n    model_name: str = \"gaia_classifier\"\n\n    # Model configuration\n    model_config: Dict[str, Any] = field(default_factory=dict)\n\n    # Training settings\n    max_epochs: int = 100\n    batch_size: int = 64\n    learning_rate: float = 0.001\n    weight_decay: float = 0.01\n\n    # Hardware settings\n    devices: int = 1\n    accelerator: str = \"auto\"\n    precision: str = \"16-mixed\"\n\n    # MLflow settings\n    experiment_name: Optional[str] = None\n    run_name: Optional[str] = None\n\n    # Data settings\n    data_path: Union[str, Path] = \"./data\"\n    max_samples: Optional[int] = None\n\n    def __post_init__(self):\n        \"\"\"Post-initialization processing.\"\"\"\n        self.data_path = Path(self.data_path)\n        if self.experiment_name is None:\n            self.experiment_name = f\"{self.dataset}_{self.model_name}\"\n</code></pre>"},{"location":"api/astro_lab.training/#astro_lab.training.set_random_seed","title":"set_random_seed","text":"<pre><code>set_random_seed(seed: int = 42)\n</code></pre> <p>Set random seed for reproducibility across all libraries.</p> <p>Parameters:</p> Name Type Description Default <code>seed</code> <code>int</code> <p>Random seed value</p> <code>42</code> Source code in <code>src\\astro_lab\\training\\utils.py</code> <pre><code>def set_random_seed(seed: int = 42):\n    \"\"\"\n    Set random seed for reproducibility across all libraries.\n\n    Args:\n        seed: Random seed value\n    \"\"\"\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed(seed)\n        torch.cuda.manual_seed_all(seed)\n        # Ensure deterministic behavior on CUDA\n        torch.backends.cudnn.deterministic = True\n        torch.backends.cudnn.benchmark = False\n</code></pre>"},{"location":"api/astro_lab.training/#astro_lab.training.setup_logging","title":"setup_logging","text":"<pre><code>setup_logging(\n    level: str = \"INFO\", format: Optional[str] = None, filename: Optional[str] = None\n) -&gt; Logger\n</code></pre> <p>Setup logging configuration for training.</p> <p>Parameters:</p> Name Type Description Default <code>level</code> <code>str</code> <p>Logging level (DEBUG, INFO, WARNING, ERROR, CRITICAL)</p> <code>'INFO'</code> <code>format</code> <code>Optional[str]</code> <p>Custom log format string</p> <code>None</code> <code>filename</code> <code>Optional[str]</code> <p>Optional log file path</p> <code>None</code> <p>Returns:</p> Type Description <code>Logger</code> <p>Configured logger instance</p> Source code in <code>src\\astro_lab\\training\\utils.py</code> <pre><code>def setup_logging(\n    level: str = \"INFO\",\n    format: Optional[str] = None,\n    filename: Optional[str] = None\n) -&gt; logging.Logger:\n    \"\"\"\n    Setup logging configuration for training.\n\n    Args:\n        level: Logging level (DEBUG, INFO, WARNING, ERROR, CRITICAL)\n        format: Custom log format string\n        filename: Optional log file path\n\n    Returns:\n        Configured logger instance\n    \"\"\"\n    if format is None:\n        format = \"%(asctime)s - %(name)s - %(levelname)s - %(message)s\"\n\n    handlers = [logging.StreamHandler(sys.stdout)]\n    if filename:\n        handlers.append(logging.FileHandler(filename))\n\n    logging.basicConfig(\n        level=getattr(logging, level.upper()),\n        format=format,\n        handlers=handlers,\n        force=True  # Override any existing configuration\n    )\n\n    # Set specific loggers to appropriate levels\n    logging.getLogger(\"lightning\").setLevel(logging.INFO)\n    logging.getLogger(\"torch\").setLevel(logging.WARNING)\n\n    return logging.getLogger(\"astro_lab.training\")\n</code></pre>"},{"location":"api/astro_lab.training/#astro_lab.training.train_model","title":"train_model","text":"<pre><code>train_model(config: Dict[str, Any]) -&gt; bool\n</code></pre> <p>Convenience function to train a model with configuration.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>Dict[str, Any]</code> <p>Training configuration dictionary</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if training succeeded, False otherwise</p> Source code in <code>src\\astro_lab\\training\\astro_trainer.py</code> <pre><code>def train_model(config: Dict[str, Any]) -&gt; bool:\n    \"\"\"\n    Convenience function to train a model with configuration.\n\n    Args:\n        config: Training configuration dictionary\n\n    Returns:\n        True if training succeeded, False otherwise\n    \"\"\"\n    trainer = AstroTrainer(config)\n    return trainer.train()\n</code></pre>"},{"location":"api/astro_lab.ui/","title":"astro_lab.ui","text":""},{"location":"api/astro_lab.ui/#astro_lab.ui","title":"ui","text":""},{"location":"api/astro_lab.ui/#astro_lab.ui--astrolab-ui-module","title":"AstroLab UI Module","text":"<p>Clean marimo-based user interface for AstroLab. Integrated with the AstroLab widget system for advanced functionality.</p>"},{"location":"api/astro_lab.ui/#astro_lab.ui--quick-start","title":"Quick Start","text":"<pre><code>import marimo as mo\nfrom astro_lab.ui import create_astrolab_dashboard\n\n# Create full dashboard\ndashboard = create_astrolab_dashboard()\n\n# Or use individual components\nfrom astro_lab.ui import (\n    ui_config_loader,\n    ui_quick_setup,\n    ui_model_selector,\n    ui_graph_controls,\n    handle_component_actions,\n    WIDGETS_AVAILABLE\n)\n\nconfig_loader = ui_config_loader()\nquick_setup = ui_quick_setup()\n\n# Check if advanced widget features are available\nif WIDGETS_AVAILABLE:\n    graph_controls = ui_graph_controls()\n</code></pre>"},{"location":"api/astro_lab.ui/#astro_lab.ui--integration","title":"Integration","text":"<ul> <li>ConfigLoader: Uses real AstroLab configuration system</li> <li>data_config: Integrates with path management</li> <li>Model Configs: Works with actual model configurations</li> <li>Training Configs: Supports predefined training setups</li> <li>AstroLab Widgets: Advanced visualization and analysis (if available)</li> </ul> <p>Modules:</p> Name Description <code>components</code> <p>AstroLab UI Components - Marimo UI Components</p> <code>dashboard</code> <p>AstroLab Dashboard</p> <code>settings</code> <p>AstroLab UI Settings Integration</p> <p>Classes:</p> Name Description <code>AstroLabDashboard</code> <p>Complete AstroLab dashboard with integrated components.</p> <code>UIConfigManager</code> <p>UI interface for AstroLab's configuration system.</p> <p>Functions:</p> Name Description <code>create_analysis_dashboard</code> <p>Create an analysis-focused dashboard with widget integration.</p> <code>create_astrolab_dashboard</code> <p>Create the complete AstroLab dashboard.</p> <code>create_config_dashboard</code> <p>Create a configuration-focused dashboard.</p> <code>create_dashboard</code> <p>Create a dashboard of specified type.</p> <code>create_minimal_dashboard</code> <p>Create a minimal dashboard for quick analysis.</p> <code>create_widget_showcase</code> <p>Create a dashboard showcasing AstroLab widget capabilities.</p> <code>handle_component_actions</code> <p>Handle actions from UI components.</p> <code>handle_config_actions</code> <p>Handle configuration-related actions from UI components.</p> <code>ui_analysis_controls</code> <p>Analysis controls using AstroLab widget functionality.</p> <code>ui_config_loader</code> <p>Configuration loader UI component.</p> <code>ui_config_status</code> <p>Configuration status display.</p> <code>ui_data_controls</code> <p>Data loading and management controls.</p> <code>ui_data_paths</code> <p>Data paths configuration UI component.</p> <code>ui_experiment_manager</code> <p>Experiment management UI component.</p> <code>ui_graph_controls</code> <p>Graph analysis controls using AstroLab widget.</p> <code>ui_model_controls</code> <p>Model training controls.</p> <code>ui_model_selector</code> <p>Model selection UI component.</p> <code>ui_quick_actions</code> <p>Quick action buttons for common tasks.</p> <code>ui_quick_setup</code> <p>Quick setup component for common workflows.</p> <code>ui_survey_selector</code> <p>Survey selection and configuration UI component.</p> <code>ui_system_status</code> <p>System status and information.</p> <code>ui_training_selector</code> <p>Training configuration selector UI component.</p> <code>ui_visualization_controls</code> <p>Visualization controls using AstroLab widget backends.</p>"},{"location":"api/astro_lab.ui/#astro_lab.ui.AstroLabDashboard","title":"AstroLabDashboard","text":"<p>Complete AstroLab dashboard with integrated components.</p> <p>Methods:</p> Name Description <code>create_full_dashboard</code> <p>Create the complete dashboard layout.</p> <code>create_main_tabs</code> <p>Create main dashboard tabs.</p> <code>create_sidebar</code> <p>Create dashboard sidebar.</p> <code>create_welcome_screen</code> <p>Create welcome screen.</p> <code>handle_interactions</code> <p>Handle all dashboard interactions.</p> Source code in <code>src\\astro_lab\\ui\\dashboard.py</code> <pre><code>class AstroLabDashboard:\n    \"\"\"Complete AstroLab dashboard with integrated components.\"\"\"\n\n    def __init__(self):\n        \"\"\"Initialize the dashboard.\"\"\"\n        self.current_data = None\n        self.status_message = \"Ready\"\n\n    def create_main_tabs(self) -&gt; mo.ui.tabs:\n        \"\"\"Create main dashboard tabs.\"\"\"\n\n        # Data &amp; Analysis Tab\n        data_tab = mo.vstack(\n            [\n                mo.md(\"## \ud83d\udcca Data Management\"),\n                ui_data_controls(),\n                mo.md(\"## \ud83d\udd2c Analysis\"),\n                ui_analysis_controls(),\n                mo.md(\"## \ud83d\udcc8 Visualization\"),\n                ui_visualization_controls(),\n            ]\n        )\n\n        #  Analysis Tab (with graph functionality)\n        advanced_tab = mo.vstack(\n            [\n                mo.md(\"## \ud83d\udd78\ufe0f Graph Analysis\"),\n                ui_graph_controls(),\n                mo.md(\"## \ud83e\udd16 Model Training\"),\n                ui_model_controls(),\n                mo.md(\"## \ud83d\udcca System Status\"),\n                ui_system_status(),\n            ]\n        )\n\n        # Configuration Tab\n        config_tab = mo.vstack(\n            [\n                mo.md(\"## \ud83d\ude80 Quick Setup\"),\n                ui_quick_setup(),\n                mo.md(\"## \ud83d\udcc2 Configuration\"),\n                ui_config_loader(),\n                mo.md(\"## \ud83d\udcca Survey Selection\"),\n                ui_survey_selector(),\n                mo.md(\"## \ud83e\udd16 Model Selection\"),\n                ui_model_selector(),\n            ]\n        )\n\n        # Management Tab\n        management_tab = mo.vstack(\n            [\n                mo.md(\"## \ud83e\uddea Experiment Management\"),\n                ui_experiment_manager(),\n                mo.md(\"## \ud83d\udcc1 Data Paths\"),\n                ui_data_paths(),\n                mo.md(\"## \u2139\ufe0f Configuration Status\"),\n                ui_config_status(),\n            ]\n        )\n\n        return mo.ui.tabs(\n            {\n                \"\ud83d\ude80 Main\": data_tab,\n                \"\ud83d\udd2c \": advanced_tab,\n                \"\u2699\ufe0f Config\": config_tab,\n                \"\ud83d\udd27 Manage\": management_tab,\n            }\n        )\n\n    def create_sidebar(self) -&gt; mo.vstack:\n        \"\"\"Create dashboard sidebar.\"\"\"\n        widget_status = \"\ud83d\udfe2 Available\" if WIDGETS_AVAILABLE else \"\ud83d\udd34 Not Available\"\n\n        return mo.vstack(\n            [\n                mo.md(\"# \ud83c\udf1f AstroLab\"),\n                mo.md(\"* Astronomical Data Analysis*\"),\n                mo.md(\"---\"),\n                mo.md(\"### \u26a1 Quick Actions\"),\n                ui_quick_actions(),\n                mo.md(\"### \ud83d\udcca System Status\"),\n                ui_system_status(),\n                mo.md(f\"**AstroLab Widgets:** {widget_status}\"),\n                mo.md(\"---\"),\n                mo.md(\"### \ud83d\udcda Resources\"),\n                mo.md(\"\"\"\n- [Documentation](https://astro-lab.readthedocs.io)\n- [GitHub](https://github.com/astro-lab/astro-lab)\n- [Examples](examples/)\n            \"\"\"),\n            ]\n        )\n\n    def create_welcome_screen(self) -&gt; mo.vstack:\n        \"\"\"Create welcome screen.\"\"\"\n        features_text = \"\"\"\n## Modern Astronomical Data Analysis Platform\n\nAstroLab provides comprehensive tools for astronomical data analysis,\nmachine learning, and interactive visualization.\n\n### \ud83d\ude80 Getting Started\n\n1. **Configure**: Set up your experiment and data paths\n2. **Load Data**: Choose a survey and load astronomical data\n3. **Visualize**: Create interactive plots and visualizations\n4. **Analyze**: Run clustering, classification, and analysis\n5. **Model**: Train machine learning models on your data\n\n### \ud83d\udcca Supported Surveys\n- **Gaia**: European Space Agency's stellar survey\n- **SDSS**: Sloan Digital Sky Survey\n- **NSA**: NASA-Sloan Atlas\n- **LINEAR**: Linear Asteroid Survey\n- **TNG50**: IllustrisTNG simulation data\n\n### \ud83e\uddf0 Available Tools\n- Interactive plotting with Plotly, Matplotlib, Bokeh\"\"\"\n\n        if WIDGETS_AVAILABLE:\n            features_text += \"\"\"\n- ** 3D visualization** with Open3D, PyVista, Blender\n- **GPU-accelerated analysis** and clustering\n- **Graph-based analysis** with PyTorch Geometric\n- **High-performance neighbor finding**\"\"\"\n        else:\n            features_text += \"\"\"\n- Graph-based analysis and clustering\n- Neural networks and machine learning\"\"\"\n\n        features_text += \"\"\"\n- GPU acceleration support\n            \"\"\"\n\n        return mo.vstack(\n            [\n                mo.md(\"# \ud83c\udf1f Welcome to AstroLab\"),\n                mo.md(features_text),\n                mo.md(\"### \ud83c\udfaf Quick Start\"),\n                mo.hstack(\n                    [\n                        mo.ui.button(label=\"\ud83d\udcca Load Sample Data\"),\n                        mo.ui.button(label=\"\ud83c\udfa8 Create Plot\"),\n                        mo.ui.button(label=\"\ud83e\udd16 Train Model\"),\n                    ]\n                ),\n            ]\n        )\n\n    def create_full_dashboard(self) -&gt; mo.vstack:\n        \"\"\"Create the complete dashboard layout.\"\"\"\n        header = mo.md(\"# \ud83c\udf1f AstroLab Dashboard\")\n\n        welcome = self.create_welcome_screen()\n        main_tabs = self.create_main_tabs()\n        sidebar = self.create_sidebar()\n\n        # Main content area\n        main_content = mo.hstack(\n            [\n                mo.vstack([welcome, main_tabs]),\n                sidebar,\n            ]\n        )\n\n        return mo.vstack(\n            [\n                header,\n                mo.md(\"---\"),\n                main_content,\n            ]\n        )\n\n    def handle_interactions(self, components: Dict[str, Any]) -&gt; Optional[str]:\n        \"\"\"Handle all dashboard interactions.\"\"\"\n\n        # Handle component actions\n        component_result = handle_component_actions(components)\n        if component_result:\n            self.status_message = component_result\n            return component_result\n\n        # Handle config actions\n        config_result = handle_config_actions(components)\n        if config_result:\n            self.status_message = \"\u2705 Configuration action completed\"\n            return self.status_message\n\n        return None\n</code></pre>"},{"location":"api/astro_lab.ui/#astro_lab.ui.AstroLabDashboard.create_full_dashboard","title":"create_full_dashboard","text":"<pre><code>create_full_dashboard() -&gt; vstack\n</code></pre> <p>Create the complete dashboard layout.</p> Source code in <code>src\\astro_lab\\ui\\dashboard.py</code> <pre><code>def create_full_dashboard(self) -&gt; mo.vstack:\n    \"\"\"Create the complete dashboard layout.\"\"\"\n    header = mo.md(\"# \ud83c\udf1f AstroLab Dashboard\")\n\n    welcome = self.create_welcome_screen()\n    main_tabs = self.create_main_tabs()\n    sidebar = self.create_sidebar()\n\n    # Main content area\n    main_content = mo.hstack(\n        [\n            mo.vstack([welcome, main_tabs]),\n            sidebar,\n        ]\n    )\n\n    return mo.vstack(\n        [\n            header,\n            mo.md(\"---\"),\n            main_content,\n        ]\n    )\n</code></pre>"},{"location":"api/astro_lab.ui/#astro_lab.ui.AstroLabDashboard.create_main_tabs","title":"create_main_tabs","text":"<pre><code>create_main_tabs() -&gt; tabs\n</code></pre> <p>Create main dashboard tabs.</p> Source code in <code>src\\astro_lab\\ui\\dashboard.py</code> <pre><code>def create_main_tabs(self) -&gt; mo.ui.tabs:\n    \"\"\"Create main dashboard tabs.\"\"\"\n\n    # Data &amp; Analysis Tab\n    data_tab = mo.vstack(\n        [\n            mo.md(\"## \ud83d\udcca Data Management\"),\n            ui_data_controls(),\n            mo.md(\"## \ud83d\udd2c Analysis\"),\n            ui_analysis_controls(),\n            mo.md(\"## \ud83d\udcc8 Visualization\"),\n            ui_visualization_controls(),\n        ]\n    )\n\n    #  Analysis Tab (with graph functionality)\n    advanced_tab = mo.vstack(\n        [\n            mo.md(\"## \ud83d\udd78\ufe0f Graph Analysis\"),\n            ui_graph_controls(),\n            mo.md(\"## \ud83e\udd16 Model Training\"),\n            ui_model_controls(),\n            mo.md(\"## \ud83d\udcca System Status\"),\n            ui_system_status(),\n        ]\n    )\n\n    # Configuration Tab\n    config_tab = mo.vstack(\n        [\n            mo.md(\"## \ud83d\ude80 Quick Setup\"),\n            ui_quick_setup(),\n            mo.md(\"## \ud83d\udcc2 Configuration\"),\n            ui_config_loader(),\n            mo.md(\"## \ud83d\udcca Survey Selection\"),\n            ui_survey_selector(),\n            mo.md(\"## \ud83e\udd16 Model Selection\"),\n            ui_model_selector(),\n        ]\n    )\n\n    # Management Tab\n    management_tab = mo.vstack(\n        [\n            mo.md(\"## \ud83e\uddea Experiment Management\"),\n            ui_experiment_manager(),\n            mo.md(\"## \ud83d\udcc1 Data Paths\"),\n            ui_data_paths(),\n            mo.md(\"## \u2139\ufe0f Configuration Status\"),\n            ui_config_status(),\n        ]\n    )\n\n    return mo.ui.tabs(\n        {\n            \"\ud83d\ude80 Main\": data_tab,\n            \"\ud83d\udd2c \": advanced_tab,\n            \"\u2699\ufe0f Config\": config_tab,\n            \"\ud83d\udd27 Manage\": management_tab,\n        }\n    )\n</code></pre>"},{"location":"api/astro_lab.ui/#astro_lab.ui.AstroLabDashboard.create_sidebar","title":"create_sidebar","text":"<pre><code>create_sidebar() -&gt; vstack\n</code></pre> <p>Create dashboard sidebar.</p> Source code in <code>src\\astro_lab\\ui\\dashboard.py</code> <pre><code>    def create_sidebar(self) -&gt; mo.vstack:\n        \"\"\"Create dashboard sidebar.\"\"\"\n        widget_status = \"\ud83d\udfe2 Available\" if WIDGETS_AVAILABLE else \"\ud83d\udd34 Not Available\"\n\n        return mo.vstack(\n            [\n                mo.md(\"# \ud83c\udf1f AstroLab\"),\n                mo.md(\"* Astronomical Data Analysis*\"),\n                mo.md(\"---\"),\n                mo.md(\"### \u26a1 Quick Actions\"),\n                ui_quick_actions(),\n                mo.md(\"### \ud83d\udcca System Status\"),\n                ui_system_status(),\n                mo.md(f\"**AstroLab Widgets:** {widget_status}\"),\n                mo.md(\"---\"),\n                mo.md(\"### \ud83d\udcda Resources\"),\n                mo.md(\"\"\"\n- [Documentation](https://astro-lab.readthedocs.io)\n- [GitHub](https://github.com/astro-lab/astro-lab)\n- [Examples](examples/)\n            \"\"\"),\n            ]\n        )\n</code></pre>"},{"location":"api/astro_lab.ui/#astro_lab.ui.AstroLabDashboard.create_welcome_screen","title":"create_welcome_screen","text":"<pre><code>create_welcome_screen() -&gt; vstack\n</code></pre> <p>Create welcome screen.</p> Source code in <code>src\\astro_lab\\ui\\dashboard.py</code> <pre><code>    def create_welcome_screen(self) -&gt; mo.vstack:\n        \"\"\"Create welcome screen.\"\"\"\n        features_text = \"\"\"\n## Modern Astronomical Data Analysis Platform\n\nAstroLab provides comprehensive tools for astronomical data analysis,\nmachine learning, and interactive visualization.\n\n### \ud83d\ude80 Getting Started\n\n1. **Configure**: Set up your experiment and data paths\n2. **Load Data**: Choose a survey and load astronomical data\n3. **Visualize**: Create interactive plots and visualizations\n4. **Analyze**: Run clustering, classification, and analysis\n5. **Model**: Train machine learning models on your data\n\n### \ud83d\udcca Supported Surveys\n- **Gaia**: European Space Agency's stellar survey\n- **SDSS**: Sloan Digital Sky Survey\n- **NSA**: NASA-Sloan Atlas\n- **LINEAR**: Linear Asteroid Survey\n- **TNG50**: IllustrisTNG simulation data\n\n### \ud83e\uddf0 Available Tools\n- Interactive plotting with Plotly, Matplotlib, Bokeh\"\"\"\n\n        if WIDGETS_AVAILABLE:\n            features_text += \"\"\"\n- ** 3D visualization** with Open3D, PyVista, Blender\n- **GPU-accelerated analysis** and clustering\n- **Graph-based analysis** with PyTorch Geometric\n- **High-performance neighbor finding**\"\"\"\n        else:\n            features_text += \"\"\"\n- Graph-based analysis and clustering\n- Neural networks and machine learning\"\"\"\n\n        features_text += \"\"\"\n- GPU acceleration support\n            \"\"\"\n\n        return mo.vstack(\n            [\n                mo.md(\"# \ud83c\udf1f Welcome to AstroLab\"),\n                mo.md(features_text),\n                mo.md(\"### \ud83c\udfaf Quick Start\"),\n                mo.hstack(\n                    [\n                        mo.ui.button(label=\"\ud83d\udcca Load Sample Data\"),\n                        mo.ui.button(label=\"\ud83c\udfa8 Create Plot\"),\n                        mo.ui.button(label=\"\ud83e\udd16 Train Model\"),\n                    ]\n                ),\n            ]\n        )\n</code></pre>"},{"location":"api/astro_lab.ui/#astro_lab.ui.AstroLabDashboard.handle_interactions","title":"handle_interactions","text":"<pre><code>handle_interactions(components: Dict[str, Any]) -&gt; Optional[str]\n</code></pre> <p>Handle all dashboard interactions.</p> Source code in <code>src\\astro_lab\\ui\\dashboard.py</code> <pre><code>def handle_interactions(self, components: Dict[str, Any]) -&gt; Optional[str]:\n    \"\"\"Handle all dashboard interactions.\"\"\"\n\n    # Handle component actions\n    component_result = handle_component_actions(components)\n    if component_result:\n        self.status_message = component_result\n        return component_result\n\n    # Handle config actions\n    config_result = handle_config_actions(components)\n    if config_result:\n        self.status_message = \"\u2705 Configuration action completed\"\n        return self.status_message\n\n    return None\n</code></pre>"},{"location":"api/astro_lab.ui/#astro_lab.ui.UIConfigManager","title":"UIConfigManager","text":"<p>UI interface for AstroLab's configuration system.</p> <p>Methods:</p> Name Description <code>get_current_config</code> <p>Get currently loaded configuration.</p> <code>get_data_config</code> <p>Get data configuration from data_config.</p> <code>get_model_configs</code> <p>Get available model configurations.</p> <code>get_survey_info</code> <p>Get survey configuration and paths.</p> <code>get_training_configs</code> <p>Get available training configurations.</p> <code>load_config</code> <p>Load configuration using ConfigLoader.</p> <code>setup_experiment</code> <p>Setup experiment directories.</p> Source code in <code>src\\astro_lab\\ui\\settings.py</code> <pre><code>class UIConfigManager:\n    \"\"\"UI interface for AstroLab's configuration system.\"\"\"\n\n    def __init__(self):\n        \"\"\"Initialize the UI config manager.\"\"\"\n        self.config_loader = ConfigLoader()\n        self.current_config: Optional[Dict[str, Any]] = None\n        self.available_configs = self._discover_configs()\n\n    def _discover_configs(self) -&gt; Dict[str, List[str]]:\n        \"\"\"Discover available configuration files.\"\"\"\n        configs_dir = Path(\"configs\")\n\n        available = {\n            \"experiments\": [],\n            \"surveys\": [],\n            \"models\": list(MODEL_CONFIGS.keys()),\n            \"training\": list(PREDEFINED_TRAINING_CONFIGS.keys()),\n        }\n\n        if configs_dir.exists():\n            # Find experiment configs\n            for config_file in configs_dir.glob(\"*.yaml\"):\n                if config_file.name != \"default.yaml\":\n                    available[\"experiments\"].append(config_file.stem)\n\n            # Find survey configs\n            surveys_dir = configs_dir / \"surveys\"\n            if surveys_dir.exists():\n                for survey_file in surveys_dir.glob(\"*.yaml\"):\n                    available[\"surveys\"].append(survey_file.stem)\n\n        return available\n\n    def load_config(\n        self,\n        config_path: str = \"configs/default.yaml\",\n        experiment_name: Optional[str] = None,\n    ) -&gt; Dict[str, Any]:\n        \"\"\"Load configuration using ConfigLoader.\"\"\"\n        self.config_loader = ConfigLoader(config_path)\n        self.current_config = self.config_loader.load_config(experiment_name)\n        return self.current_config\n\n    def get_current_config(self) -&gt; Optional[Dict[str, Any]]:\n        \"\"\"Get currently loaded configuration.\"\"\"\n        return self.current_config\n\n    def get_data_config(self) -&gt; Dict[str, Any]:\n        \"\"\"Get data configuration from data_config.\"\"\"\n        return {\n            \"base_dir\": str(data_config.base_dir),\n            \"raw_dir\": str(data_config.raw_dir),\n            \"processed_dir\": str(data_config.processed_dir),\n            \"cache_dir\": str(data_config.cache_dir),\n            \"results_dir\": str(data_config.results_dir),\n            \"experiments_dir\": str(data_config.experiments_dir),\n        }\n\n    def get_model_configs(self) -&gt; Dict[str, Any]:\n        \"\"\"Get available model configurations.\"\"\"\n        return {name: config.to_dict() for name, config in MODEL_CONFIGS.items()}\n\n    def get_training_configs(self) -&gt; Dict[str, str]:\n        \"\"\"Get available training configurations.\"\"\"\n        return {\n            name: config.description or \"No description\"\n            for name, config in PREDEFINED_TRAINING_CONFIGS.items()\n        }\n\n    def setup_experiment(self, experiment_name: str) -&gt; Dict[str, Path]:\n        \"\"\"Setup experiment directories.\"\"\"\n        data_config.ensure_experiment_directories(experiment_name)\n        return data_config.get_experiment_paths(experiment_name)\n\n    def get_survey_info(self, survey: str) -&gt; Dict[str, Any]:\n        \"\"\"Get survey configuration and paths.\"\"\"\n        try:\n            survey_config = load_survey_config(survey)\n            survey_paths = get_survey_paths(survey)\n            return {\n                \"config\": survey_config,\n                \"paths\": {k: str(v) for k, v in survey_paths.items()},\n            }\n        except Exception as e:\n            return {\"error\": str(e)}\n</code></pre>"},{"location":"api/astro_lab.ui/#astro_lab.ui.UIConfigManager.get_current_config","title":"get_current_config","text":"<pre><code>get_current_config() -&gt; Optional[Dict[str, Any]]\n</code></pre> <p>Get currently loaded configuration.</p> Source code in <code>src\\astro_lab\\ui\\settings.py</code> <pre><code>def get_current_config(self) -&gt; Optional[Dict[str, Any]]:\n    \"\"\"Get currently loaded configuration.\"\"\"\n    return self.current_config\n</code></pre>"},{"location":"api/astro_lab.ui/#astro_lab.ui.UIConfigManager.get_data_config","title":"get_data_config","text":"<pre><code>get_data_config() -&gt; Dict[str, Any]\n</code></pre> <p>Get data configuration from data_config.</p> Source code in <code>src\\astro_lab\\ui\\settings.py</code> <pre><code>def get_data_config(self) -&gt; Dict[str, Any]:\n    \"\"\"Get data configuration from data_config.\"\"\"\n    return {\n        \"base_dir\": str(data_config.base_dir),\n        \"raw_dir\": str(data_config.raw_dir),\n        \"processed_dir\": str(data_config.processed_dir),\n        \"cache_dir\": str(data_config.cache_dir),\n        \"results_dir\": str(data_config.results_dir),\n        \"experiments_dir\": str(data_config.experiments_dir),\n    }\n</code></pre>"},{"location":"api/astro_lab.ui/#astro_lab.ui.UIConfigManager.get_model_configs","title":"get_model_configs","text":"<pre><code>get_model_configs() -&gt; Dict[str, Any]\n</code></pre> <p>Get available model configurations.</p> Source code in <code>src\\astro_lab\\ui\\settings.py</code> <pre><code>def get_model_configs(self) -&gt; Dict[str, Any]:\n    \"\"\"Get available model configurations.\"\"\"\n    return {name: config.to_dict() for name, config in MODEL_CONFIGS.items()}\n</code></pre>"},{"location":"api/astro_lab.ui/#astro_lab.ui.UIConfigManager.get_survey_info","title":"get_survey_info","text":"<pre><code>get_survey_info(survey: str) -&gt; Dict[str, Any]\n</code></pre> <p>Get survey configuration and paths.</p> Source code in <code>src\\astro_lab\\ui\\settings.py</code> <pre><code>def get_survey_info(self, survey: str) -&gt; Dict[str, Any]:\n    \"\"\"Get survey configuration and paths.\"\"\"\n    try:\n        survey_config = load_survey_config(survey)\n        survey_paths = get_survey_paths(survey)\n        return {\n            \"config\": survey_config,\n            \"paths\": {k: str(v) for k, v in survey_paths.items()},\n        }\n    except Exception as e:\n        return {\"error\": str(e)}\n</code></pre>"},{"location":"api/astro_lab.ui/#astro_lab.ui.UIConfigManager.get_training_configs","title":"get_training_configs","text":"<pre><code>get_training_configs() -&gt; Dict[str, str]\n</code></pre> <p>Get available training configurations.</p> Source code in <code>src\\astro_lab\\ui\\settings.py</code> <pre><code>def get_training_configs(self) -&gt; Dict[str, str]:\n    \"\"\"Get available training configurations.\"\"\"\n    return {\n        name: config.description or \"No description\"\n        for name, config in PREDEFINED_TRAINING_CONFIGS.items()\n    }\n</code></pre>"},{"location":"api/astro_lab.ui/#astro_lab.ui.UIConfigManager.load_config","title":"load_config","text":"<pre><code>load_config(\n    config_path: str = \"configs/default.yaml\", experiment_name: Optional[str] = None\n) -&gt; Dict[str, Any]\n</code></pre> <p>Load configuration using ConfigLoader.</p> Source code in <code>src\\astro_lab\\ui\\settings.py</code> <pre><code>def load_config(\n    self,\n    config_path: str = \"configs/default.yaml\",\n    experiment_name: Optional[str] = None,\n) -&gt; Dict[str, Any]:\n    \"\"\"Load configuration using ConfigLoader.\"\"\"\n    self.config_loader = ConfigLoader(config_path)\n    self.current_config = self.config_loader.load_config(experiment_name)\n    return self.current_config\n</code></pre>"},{"location":"api/astro_lab.ui/#astro_lab.ui.UIConfigManager.setup_experiment","title":"setup_experiment","text":"<pre><code>setup_experiment(experiment_name: str) -&gt; Dict[str, Path]\n</code></pre> <p>Setup experiment directories.</p> Source code in <code>src\\astro_lab\\ui\\settings.py</code> <pre><code>def setup_experiment(self, experiment_name: str) -&gt; Dict[str, Path]:\n    \"\"\"Setup experiment directories.\"\"\"\n    data_config.ensure_experiment_directories(experiment_name)\n    return data_config.get_experiment_paths(experiment_name)\n</code></pre>"},{"location":"api/astro_lab.ui/#astro_lab.ui.create_analysis_dashboard","title":"create_analysis_dashboard","text":"<pre><code>create_analysis_dashboard() -&gt; vstack\n</code></pre> <p>Create an analysis-focused dashboard with widget integration.</p> Source code in <code>src\\astro_lab\\ui\\dashboard.py</code> <pre><code>def create_analysis_dashboard() -&gt; mo.vstack:\n    \"\"\"Create an analysis-focused dashboard with widget integration.\"\"\"\n    return mo.vstack(\n        [\n            mo.md(\"# \ud83d\udd2c AstroLab Analysis Dashboard\"),\n            mo.hstack(\n                [\n                    mo.vstack(\n                        [\n                            mo.md(\"### \ud83d\udcca Data\"),\n                            ui_data_controls(),\n                            mo.md(\"### \ud83d\udd2c Analysis\"),\n                            ui_analysis_controls(),\n                        ]\n                    ),\n                    mo.vstack(\n                        [\n                            mo.md(\"### \ud83d\udcc8 Visualization\"),\n                            ui_visualization_controls(),\n                            mo.md(\"### \ud83d\udd78\ufe0f Graph Analysis\"),\n                            ui_graph_controls(),\n                        ]\n                    ),\n                ]\n            ),\n            mo.md(\"## \ud83e\udd16 Model Training\"),\n            ui_model_controls(),\n            mo.md(\"## \ud83d\udcca Status &amp; Actions\"),\n            mo.hstack(\n                [\n                    ui_system_status(),\n                    ui_quick_actions(),\n                ]\n            ),\n        ]\n    )\n</code></pre>"},{"location":"api/astro_lab.ui/#astro_lab.ui.create_astrolab_dashboard","title":"create_astrolab_dashboard","text":"<pre><code>create_astrolab_dashboard() -&gt; vstack\n</code></pre> <p>Create the complete AstroLab dashboard.</p> Source code in <code>src\\astro_lab\\ui\\dashboard.py</code> <pre><code>def create_astrolab_dashboard() -&gt; mo.vstack:\n    \"\"\"Create the complete AstroLab dashboard.\"\"\"\n    return dashboard.create_full_dashboard()\n</code></pre>"},{"location":"api/astro_lab.ui/#astro_lab.ui.create_config_dashboard","title":"create_config_dashboard","text":"<pre><code>create_config_dashboard() -&gt; vstack\n</code></pre> <p>Create a configuration-focused dashboard.</p> Source code in <code>src\\astro_lab\\ui\\dashboard.py</code> <pre><code>def create_config_dashboard() -&gt; mo.vstack:\n    \"\"\"Create a configuration-focused dashboard.\"\"\"\n    return mo.vstack(\n        [\n            mo.md(\"# \u2699\ufe0f AstroLab Configuration\"),\n            mo.ui.tabs(\n                {\n                    \"\ud83d\ude80 Quick\": ui_quick_setup(),\n                    \"\ud83d\udcc2 Config\": ui_config_loader(),\n                    \"\ud83d\udcca Survey\": ui_survey_selector(),\n                    \"\ud83e\udd16 Model\": ui_model_selector(),\n                    \"\ud83c\udfcb\ufe0f Training\": ui_training_selector(),\n                    \"\ud83e\uddea Experiment\": ui_experiment_manager(),\n                    \"\ud83d\udcc1 Paths\": ui_data_paths(),\n                    \"\u2139\ufe0f Status\": ui_config_status(),\n                }\n            ),\n        ]\n    )\n</code></pre>"},{"location":"api/astro_lab.ui/#astro_lab.ui.create_dashboard","title":"create_dashboard","text":"<pre><code>create_dashboard(dashboard_type: str = 'full') -&gt; Any\n</code></pre> <p>Create a dashboard of specified type.</p> <p>Parameters:</p> Name Type Description Default <code>dashboard_type</code> <code>str</code> <p>One of \"full\", \"minimal\", \"config\", \"analysis\", \"widgets\"</p> <code>'full'</code> <p>Returns:</p> Type Description <code>Any</code> <p>Marimo UI component</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If dashboard_type is not supported</p> Source code in <code>src\\astro_lab\\ui\\__init__.py</code> <pre><code>def create_dashboard(dashboard_type: str = \"full\") -&gt; Any:\n    \"\"\"\n    Create a dashboard of specified type.\n\n    Args:\n        dashboard_type: One of \"full\", \"minimal\", \"config\", \"analysis\", \"widgets\"\n\n    Returns:\n        Marimo UI component\n\n    Raises:\n        ValueError: If dashboard_type is not supported\n    \"\"\"\n    dashboard_factories = {\n        \"full\": create_astrolab_dashboard,\n        \"minimal\": create_minimal_dashboard,\n        \"config\": create_config_dashboard,\n        \"analysis\": create_analysis_dashboard,\n        \"widgets\": create_widget_showcase,\n    }\n\n    if dashboard_type not in dashboard_factories:\n        raise ValueError(\n            f\"Unknown dashboard type: {dashboard_type}. Supported: {list(dashboard_factories.keys())}\"\n        )\n\n    return dashboard_factories[dashboard_type]()\n</code></pre>"},{"location":"api/astro_lab.ui/#astro_lab.ui.create_minimal_dashboard","title":"create_minimal_dashboard","text":"<pre><code>create_minimal_dashboard() -&gt; vstack\n</code></pre> <p>Create a minimal dashboard for quick analysis.</p> Source code in <code>src\\astro_lab\\ui\\dashboard.py</code> <pre><code>def create_minimal_dashboard() -&gt; mo.vstack:\n    \"\"\"Create a minimal dashboard for quick analysis.\"\"\"\n    return mo.vstack(\n        [\n            mo.md(\"# \ud83c\udf1f AstroLab - Quick Analysis\"),\n            mo.md(\"## \ud83d\udcca Data Loading\"),\n            ui_data_controls(),\n            mo.md(\"## \ud83d\udcc8 Visualization\"),\n            ui_visualization_controls(),\n            mo.md(\"## \ud83d\udd2c Analysis\"),\n            ui_analysis_controls(),\n            mo.md(\"## \u26a1 Quick Actions\"),\n            ui_quick_actions(),\n        ]\n    )\n</code></pre>"},{"location":"api/astro_lab.ui/#astro_lab.ui.create_widget_showcase","title":"create_widget_showcase","text":"<pre><code>create_widget_showcase() -&gt; vstack\n</code></pre> <p>Create a dashboard showcasing AstroLab widget capabilities.</p> Source code in <code>src\\astro_lab\\ui\\dashboard.py</code> <pre><code>def create_widget_showcase() -&gt; mo.vstack:\n    \"\"\"Create a dashboard showcasing AstroLab widget capabilities.\"\"\"\n    if not WIDGETS_AVAILABLE:\n        return mo.vstack(\n            [\n                mo.md(\"# \u274c AstroLab Widgets Not Available\"),\n                mo.md(\n                    \"Please install the required dependencies for widget functionality.\"\n                ),\n            ]\n        )\n\n    return mo.vstack(\n        [\n            mo.md(\"# \ud83c\udf1f AstroLab Widget Showcase\"),\n            mo.md(\"*Demonstrating advanced visualization and analysis capabilities*\"),\n            mo.md(\"## \ud83c\udfa8  Visualization\"),\n            mo.md(\"**Backends:** Open3D, PyVista, Blender integration\"),\n            ui_visualization_controls(),\n            mo.md(\"## \ud83d\udd78\ufe0f Graph Analysis\"),\n            mo.md(\n                \"**Features:** GPU-accelerated neighbor finding, PyTorch Geometric integration\"\n            ),\n            ui_graph_controls(),\n            mo.md(\"## \ud83d\udd2c  Analysis\"),\n            mo.md(\n                \"**Capabilities:** GPU clustering, density analysis, structure analysis\"\n            ),\n            ui_analysis_controls(),\n            mo.md(\"## \ud83d\udcca System Information\"),\n            ui_system_status(),\n        ]\n    )\n</code></pre>"},{"location":"api/astro_lab.ui/#astro_lab.ui.handle_component_actions","title":"handle_component_actions","text":"<pre><code>handle_component_actions(components: Dict[str, dictionary]) -&gt; Optional[str]\n</code></pre> <p>Handle actions from UI components.</p> Source code in <code>src\\astro_lab\\ui\\components.py</code> <pre><code>def handle_component_actions(components: Dict[str, mo.ui.dictionary]) -&gt; Optional[str]:\n    \"\"\"Handle actions from UI components.\"\"\"\n    # This function would handle the actual logic for component actions\n    # For now, it's a placeholder\n    return \"Action handled\"\n</code></pre>"},{"location":"api/astro_lab.ui/#astro_lab.ui.handle_config_actions","title":"handle_config_actions","text":"<pre><code>handle_config_actions(components: Dict[str, dictionary]) -&gt; Optional[Dict[str, Any]]\n</code></pre> <p>Handle configuration-related actions from UI components.</p> Source code in <code>src\\astro_lab\\ui\\settings.py</code> <pre><code>def handle_config_actions(\n    components: Dict[str, mo.ui.dictionary],\n) -&gt; Optional[Dict[str, Any]]:\n    \"\"\"Handle configuration-related actions from UI components.\"\"\"\n\n    # Handle config loading\n    if \"config_loader\" in components:\n        loader_values = components[\"config_loader\"].value\n\n        if loader_values.get(\"load_button\"):\n            config_file = f\"configs/{loader_values.get('config_file', 'default.yaml')}\"\n            experiment_name = loader_values.get(\"experiment_name\")\n            experiment_name = str(experiment_name) if experiment_name else None\n\n            try:\n                config = ui_config.load_config(config_file, experiment_name)\n                print(f\"\u2705 Configuration loaded: {config_file}\")\n                if experiment_name:\n                    print(f\"\ud83e\uddea Experiment: {experiment_name}\")\n                return config\n            except Exception as e:\n                print(f\"\u274c Failed to load configuration: {e}\")\n                return None\n\n        if loader_values.get(\"create_experiment\"):\n            experiment_name = loader_values.get(\"experiment_name\")\n            if experiment_name:\n                experiment_name = str(experiment_name)\n                try:\n                    paths = ui_config.setup_experiment(experiment_name)\n                    print(f\"\u2705 Experiment created: {experiment_name}\")\n                    print(f\"\ud83d\udcc1 Paths: {paths}\")\n                except Exception as e:\n                    print(f\"\u274c Failed to create experiment: {e}\")\n\n    # Handle survey configuration\n    if \"survey_selector\" in components:\n        survey_values = components[\"survey_selector\"].value\n\n        if survey_values.get(\"load_survey_config\"):\n            survey = survey_values.get(\"survey\", \"gaia\")\n            survey = str(survey)\n            try:\n                survey_info = ui_config.get_survey_info(survey)\n                print(f\"\u2705 Survey config loaded: {survey}\")\n                return survey_info\n            except Exception as e:\n                print(f\"\u274c Failed to load survey config: {e}\")\n\n        if survey_values.get(\"setup_survey_dirs\"):\n            survey = survey_values.get(\"survey\", \"gaia\")\n            survey = str(survey)\n            try:\n                data_config.ensure_survey_directories(survey)\n                print(f\"\u2705 Survey directories created: {survey}\")\n            except Exception as e:\n                print(f\"\u274c Failed to create survey directories: {e}\")\n\n    # Handle model configuration\n    if \"model_selector\" in components:\n        model_values = components[\"model_selector\"].value\n\n        if model_values.get(\"load_model_config\"):\n            model_name = model_values.get(\"model_name\", \"gaia_classifier\")\n            model_name = str(model_name)\n            try:\n                model_config = get_predefined_config(model_name)\n                print(f\"\u2705 Model config loaded: {model_name}\")\n                print(f\"\ud83e\udd16 Details: {model_config.to_dict()}\")\n                return model_config.to_dict()\n            except Exception as e:\n                print(f\"\u274c Failed to load model config: {e}\")\n\n    # Handle data paths\n    if \"data_paths\" in components:\n        paths_values = components[\"data_paths\"].value\n\n        if paths_values.get(\"setup_dirs\"):\n            try:\n                data_config.setup_directories()\n                print(\"\u2705 Data directories created\")\n            except Exception as e:\n                print(f\"\u274c Failed to create data directories: {e}\")\n\n    return None\n</code></pre>"},{"location":"api/astro_lab.ui/#astro_lab.ui.ui_analysis_controls","title":"ui_analysis_controls","text":"<pre><code>ui_analysis_controls() -&gt; dictionary\n</code></pre> <p>Analysis controls using AstroLab widget functionality.</p> Source code in <code>src\\astro_lab\\ui\\components.py</code> <pre><code>def ui_analysis_controls() -&gt; mo.ui.dictionary:\n    \"\"\"Analysis controls using AstroLab widget functionality.\"\"\"\n    return mo.ui.dictionary(\n        {\n            \"method\": mo.ui.dropdown(\n                label=\"Analysis Method\",\n                options=[\"clustering\", \"density\", \"structure\", \"neighbors\"],\n                value=\"clustering\",\n            ),\n            \"clustering_algorithm\": mo.ui.dropdown(\n                label=\"Clustering Algorithm\",\n                options=[\"dbscan\", \"kmeans\", \"agglomerative\"],\n                value=\"dbscan\",\n            ),\n            \"k_neighbors\": mo.ui.slider(\n                label=\"K-Neighbors\",\n                start=3,\n                stop=50,\n                step=1,\n                value=10,\n            ),\n            \"eps\": mo.ui.number(\n                label=\"DBSCAN Eps\",\n                value=10.0,\n                start=0.1,\n                stop=100.0,\n                step=0.1,\n            ),\n            \"min_samples\": mo.ui.slider(\n                label=\"Min Samples\",\n                start=3,\n                stop=20,\n                step=1,\n                value=5,\n            ),\n            \"use_gpu\": mo.ui.checkbox(\n                label=\"Use GPU\",\n                value=torch.cuda.is_available(),\n            ),\n            \"run_analysis\": mo.ui.button(label=\"\ud83d\udd2c Run Analysis\"),\n        },\n        label=\"\ud83d\udd2c Analysis\",\n    )\n</code></pre>"},{"location":"api/astro_lab.ui/#astro_lab.ui.ui_config_loader","title":"ui_config_loader","text":"<pre><code>ui_config_loader() -&gt; dictionary\n</code></pre> <p>Configuration loader UI component.</p> Source code in <code>src\\astro_lab\\ui\\settings.py</code> <pre><code>def ui_config_loader() -&gt; mo.ui.dictionary:\n    \"\"\"Configuration loader UI component.\"\"\"\n    configs_dir = Path(\"configs\")\n    config_files = []\n\n    if configs_dir.exists():\n        config_files = [f.name for f in configs_dir.glob(\"*.yaml\")]\n\n    if not config_files:\n        config_files = [\"default.yaml\"]\n\n    return mo.ui.dictionary(\n        {\n            \"config_file\": mo.ui.dropdown(\n                label=\"Configuration File\",\n                options=config_files,\n                value=\"default.yaml\",\n            ),\n            \"experiment_name\": mo.ui.text(\n                label=\"Experiment Name\",\n                placeholder=\"e.g., gaia_stellar_classification\",\n            ),\n            \"load_button\": mo.ui.button(label=\"\ud83d\udcc2 Load Configuration\"),\n            \"create_experiment\": mo.ui.button(label=\"\ud83e\uddea Create Experiment\"),\n        },\n        label=\"\ud83d\udcc2 Configuration Loader\",\n    )\n</code></pre>"},{"location":"api/astro_lab.ui/#astro_lab.ui.ui_config_status","title":"ui_config_status","text":"<pre><code>ui_config_status() -&gt; dictionary\n</code></pre> <p>Configuration status display.</p> Source code in <code>src\\astro_lab\\ui\\settings.py</code> <pre><code>def ui_config_status() -&gt; mo.ui.dictionary:\n    \"\"\"Configuration status display.\"\"\"\n    current_config = ui_config.get_current_config()\n\n    status_text = \"No configuration loaded\"\n    if current_config:\n        exp_name = current_config.get(\"mlflow\", {}).get(\"experiment_name\", \"Unknown\")\n        status_text = f\"Loaded: {exp_name}\"\n\n    return mo.ui.dictionary(\n        {\n            \"status\": mo.ui.text(\n                label=\"Configuration Status\",\n                value=status_text,\n                disabled=True,\n            ),\n            \"config_details\": mo.ui.text_area(\n                label=\"Configuration Details\",\n                value=str(current_config) if current_config else \"No config loaded\",\n                disabled=True,\n            ),\n            \"refresh\": mo.ui.button(label=\"\ud83d\udd04 Refresh Status\"),\n        },\n        label=\"\u2139\ufe0f Configuration Status\",\n    )\n</code></pre>"},{"location":"api/astro_lab.ui/#astro_lab.ui.ui_data_controls","title":"ui_data_controls","text":"<pre><code>ui_data_controls() -&gt; dictionary\n</code></pre> <p>Data loading and management controls.</p> Source code in <code>src\\astro_lab\\ui\\components.py</code> <pre><code>def ui_data_controls() -&gt; mo.ui.dictionary:\n    \"\"\"Data loading and management controls.\"\"\"\n    return mo.ui.dictionary(\n        {\n            \"survey\": mo.ui.dropdown(\n                label=\"Survey\",\n                options=[\"gaia\", \"sdss\", \"nsa\", \"linear\", \"tng50\"],\n                value=\"gaia\",\n            ),\n            \"max_samples\": mo.ui.slider(\n                label=\"Max Samples\",\n                start=1000,\n                stop=100000,\n                step=1000,\n                value=25000,\n            ),\n            \"use_cache\": mo.ui.checkbox(\n                label=\"Use Cache\",\n                value=True,\n            ),\n            \"load_data\": mo.ui.button(label=\"\ud83d\udcca Load Data\"),\n            \"preview_data\": mo.ui.button(label=\"\ud83d\udc41\ufe0f Preview Data\"),\n        },\n        label=\"\ud83d\udcca Data Controls\",\n    )\n</code></pre>"},{"location":"api/astro_lab.ui/#astro_lab.ui.ui_data_paths","title":"ui_data_paths","text":"<pre><code>ui_data_paths() -&gt; dictionary\n</code></pre> <p>Data paths configuration UI component.</p> Source code in <code>src\\astro_lab\\ui\\settings.py</code> <pre><code>def ui_data_paths() -&gt; mo.ui.dictionary:\n    \"\"\"Data paths configuration UI component.\"\"\"\n    data_info = ui_config.get_data_config()\n\n    return mo.ui.dictionary(\n        {\n            \"base_dir\": mo.ui.text(\n                label=\"Base Data Directory\",\n                value=data_info[\"base_dir\"],\n                disabled=True,\n            ),\n            \"raw_dir\": mo.ui.text(\n                label=\"Raw Data Directory\",\n                value=data_info[\"raw_dir\"],\n                disabled=True,\n            ),\n            \"processed_dir\": mo.ui.text(\n                label=\"Processed Data Directory\",\n                value=data_info[\"processed_dir\"],\n                disabled=True,\n            ),\n            \"cache_dir\": mo.ui.text(\n                label=\"Cache Directory\",\n                value=data_info[\"cache_dir\"],\n                disabled=True,\n            ),\n            \"setup_dirs\": mo.ui.button(label=\"\ud83d\udcc1 Setup Directories\"),\n        },\n        label=\"\ud83d\udcc1 Data Paths\",\n    )\n</code></pre>"},{"location":"api/astro_lab.ui/#astro_lab.ui.ui_experiment_manager","title":"ui_experiment_manager","text":"<pre><code>ui_experiment_manager() -&gt; dictionary\n</code></pre> <p>Experiment management UI component.</p> Source code in <code>src\\astro_lab\\ui\\settings.py</code> <pre><code>def ui_experiment_manager() -&gt; mo.ui.dictionary:\n    \"\"\"Experiment management UI component.\"\"\"\n    return mo.ui.dictionary(\n        {\n            \"experiment_name\": mo.ui.text(\n                label=\"Experiment Name\",\n                placeholder=\"e.g., gaia_stellar_v2\",\n            ),\n            \"description\": mo.ui.text_area(\n                label=\"Description\",\n                placeholder=\"Experiment description...\",\n            ),\n            \"create_experiment\": mo.ui.button(label=\"\ud83e\uddea Create Experiment\"),\n            \"list_experiments\": mo.ui.button(label=\"\ud83d\udccb List Experiments\"),\n        },\n        label=\"\ud83e\uddea Experiment Management\",\n    )\n</code></pre>"},{"location":"api/astro_lab.ui/#astro_lab.ui.ui_graph_controls","title":"ui_graph_controls","text":"<pre><code>ui_graph_controls() -&gt; dictionary\n</code></pre> <p>Graph analysis controls using AstroLab widget.</p> Source code in <code>src\\astro_lab\\ui\\components.py</code> <pre><code>def ui_graph_controls() -&gt; mo.ui.dictionary:\n    \"\"\"Graph analysis controls using AstroLab widget.\"\"\"\n    return mo.ui.dictionary(\n        {\n            \"graph_type\": mo.ui.dropdown(\n                label=\"Graph Type\",\n                options=[\"spatial\", \"knn\", \"radius\", \"delaunay\"],\n                value=\"spatial\",\n            ),\n            \"k_neighbors\": mo.ui.slider(\n                label=\"K-Neighbors\",\n                start=3,\n                stop=50,\n                step=1,\n                value=10,\n            ),\n            \"radius\": mo.ui.number(\n                label=\"Radius\",\n                value=1.0,\n                start=0.1,\n                stop=10.0,\n                step=0.1,\n            ),\n            \"edge_weight\": mo.ui.dropdown(\n                label=\"Edge Weight\",\n                options=[\"distance\", \"inverse_distance\", \"none\"],\n                value=\"distance\",\n            ),\n            \"use_gpu\": mo.ui.checkbox(\n                label=\"Use GPU\",\n                value=torch.cuda.is_available(),\n            ),\n            \"create_graph\": mo.ui.button(label=\"\ud83d\udd78\ufe0f Create Graph\"),\n            \"analyze_graph\": mo.ui.button(label=\"\ud83d\udcca Analyze Graph\"),\n        },\n        label=\"\ud83d\udd78\ufe0f Graph Analysis\",\n    )\n</code></pre>"},{"location":"api/astro_lab.ui/#astro_lab.ui.ui_model_controls","title":"ui_model_controls","text":"<pre><code>ui_model_controls() -&gt; dictionary\n</code></pre> <p>Model training controls.</p> Source code in <code>src\\astro_lab\\ui\\components.py</code> <pre><code>def ui_model_controls() -&gt; mo.ui.dictionary:\n    \"\"\"Model training controls.\"\"\"\n    return mo.ui.dictionary(\n        {\n            \"model_type\": mo.ui.dropdown(\n                label=\"Model Type\",\n                options=[\"gaia_classifier\", \"survey_gnn\", \"point_cloud_gnn\"],\n                value=\"gaia_classifier\",\n            ),\n            \"task\": mo.ui.dropdown(\n                label=\"Task\",\n                options=[\"classification\", \"regression\", \"clustering\"],\n                value=\"classification\",\n            ),\n            \"batch_size\": mo.ui.slider(\n                label=\"Batch Size\",\n                start=8,\n                stop=128,\n                step=8,\n                value=32,\n            ),\n            \"epochs\": mo.ui.slider(\n                label=\"Epochs\",\n                start=1,\n                stop=100,\n                step=1,\n                value=10,\n            ),\n            \"learning_rate\": mo.ui.number(\n                label=\"Learning Rate\",\n                value=0.001,\n                start=0.0001,\n                stop=0.1,\n                step=0.0001,\n            ),\n            \"prepare_data\": mo.ui.button(label=\"\ud83d\udd27 Prepare Data for Model\"),\n            \"train_model\": mo.ui.button(label=\"\ud83c\udfcb\ufe0f Train Model\"),\n        },\n        label=\"\ud83e\udd16 Model Training\",\n    )\n</code></pre>"},{"location":"api/astro_lab.ui/#astro_lab.ui.ui_model_selector","title":"ui_model_selector","text":"<pre><code>ui_model_selector() -&gt; dictionary\n</code></pre> <p>Model selection UI component.</p> Source code in <code>src\\astro_lab\\ui\\settings.py</code> <pre><code>def ui_model_selector() -&gt; mo.ui.dictionary:\n    \"\"\"Model selection UI component.\"\"\"\n    available_models = ui_config.available_configs[\"models\"]\n\n    return mo.ui.dictionary(\n        {\n            \"model_name\": mo.ui.dropdown(\n                label=\"Model Configuration\",\n                options=available_models,\n                value=available_models[0] if available_models else \"gaia_classifier\",\n            ),\n            \"load_model_config\": mo.ui.button(label=\"\ud83e\udd16 Load Model Config\"),\n            \"show_model_details\": mo.ui.button(label=\"\u2139\ufe0f Show Details\"),\n        },\n        label=\"\ud83e\udd16 Model Configuration\",\n    )\n</code></pre>"},{"location":"api/astro_lab.ui/#astro_lab.ui.ui_quick_actions","title":"ui_quick_actions","text":"<pre><code>ui_quick_actions() -&gt; dictionary\n</code></pre> <p>Quick action buttons for common tasks.</p> Source code in <code>src\\astro_lab\\ui\\components.py</code> <pre><code>def ui_quick_actions() -&gt; mo.ui.dictionary:\n    \"\"\"Quick action buttons for common tasks.\"\"\"\n    return mo.ui.dictionary(\n        {\n            \"load_gaia\": mo.ui.button(label=\"\ud83c\udf1f Load Gaia\"),\n            \"load_sdss\": mo.ui.button(label=\"\ud83d\udd2d Load SDSS\"),\n            \"load_tng50\": mo.ui.button(label=\"\ud83c\udf0c Load TNG50\"),\n            \"create_3d_plot\": mo.ui.button(label=\"\ud83c\udfa8 Create 3D Plot\"),\n            \"run_clustering\": mo.ui.button(label=\"\ud83d\udd2c Run Clustering\"),\n            \"export_results\": mo.ui.button(label=\"\ud83d\udcbe Export Results\"),\n        },\n        label=\"\u26a1 Quick Actions\",\n    )\n</code></pre>"},{"location":"api/astro_lab.ui/#astro_lab.ui.ui_quick_setup","title":"ui_quick_setup","text":"<pre><code>ui_quick_setup() -&gt; dictionary\n</code></pre> <p>Quick setup component for common workflows.</p> Source code in <code>src\\astro_lab\\ui\\components.py</code> <pre><code>def ui_quick_setup() -&gt; mo.ui.dictionary:\n    \"\"\"Quick setup component for common workflows.\"\"\"\n    return mo.ui.dictionary(\n        {\n            \"experiment_name\": mo.ui.text(\n                label=\"Experiment Name\",\n                placeholder=\"e.g., gaia_stellar_v1\",\n            ),\n            \"survey\": mo.ui.dropdown(\n                label=\"Survey\",\n                options=[\"gaia\", \"sdss\", \"nsa\", \"linear\", \"tng50\"],\n                value=\"gaia\",\n            ),\n            \"model\": mo.ui.dropdown(\n                label=\"Model\",\n                options=[\"gaia_classifier\", \"survey_gnn\", \"point_cloud_gnn\"],\n                value=\"gaia_classifier\",\n            ),\n            \"quick_setup\": mo.ui.button(label=\"\ud83d\ude80 Quick Setup\"),\n        },\n        label=\"\ud83d\ude80 Quick Setup\",\n    )\n</code></pre>"},{"location":"api/astro_lab.ui/#astro_lab.ui.ui_survey_selector","title":"ui_survey_selector","text":"<pre><code>ui_survey_selector() -&gt; dictionary\n</code></pre> <p>Survey selection and configuration UI component.</p> Source code in <code>src\\astro_lab\\ui\\settings.py</code> <pre><code>def ui_survey_selector() -&gt; mo.ui.dictionary:\n    \"\"\"Survey selection and configuration UI component.\"\"\"\n    available_surveys = ui_config.available_configs[\"surveys\"]\n\n    if not available_surveys:\n        available_surveys = [\"gaia\", \"sdss\", \"nsa\", \"linear\", \"tng50\"]\n\n    return mo.ui.dictionary(\n        {\n            \"survey\": mo.ui.dropdown(\n                label=\"Survey\",\n                options=available_surveys,\n                value=available_surveys[0] if available_surveys else \"gaia\",\n            ),\n            \"load_survey_config\": mo.ui.button(label=\"\ud83d\udcca Load Survey Config\"),\n            \"setup_survey_dirs\": mo.ui.button(label=\"\ud83d\udcc1 Setup Survey Directories\"),\n        },\n        label=\"\ud83d\udcca Survey Configuration\",\n    )\n</code></pre>"},{"location":"api/astro_lab.ui/#astro_lab.ui.ui_system_status","title":"ui_system_status","text":"<pre><code>ui_system_status() -&gt; dictionary\n</code></pre> <p>System status and information.</p> Source code in <code>src\\astro_lab\\ui\\components.py</code> <pre><code>def ui_system_status() -&gt; mo.ui.dictionary:\n    \"\"\"System status and information.\"\"\"\n    # Get system information\n    device_info = \"CUDA\" if torch.cuda.is_available() else \"CPU\"\n    if torch.cuda.is_available():\n        device_info += f\" ({torch.cuda.get_device_name()})\"\n\n    return mo.ui.dictionary(\n        {\n            \"device\": mo.ui.text(\n                label=\"Device\",\n                value=device_info,\n                disabled=True,\n            ),\n            \"widgets_available\": mo.ui.text(\n                label=\"Widgets Available\",\n                value=\"Yes\" if WIDGETS_AVAILABLE else \"No\",\n                disabled=True,\n            ),\n            \"memory_usage\": mo.ui.text(\n                label=\"Memory Usage\",\n                value=\"Checking...\",\n                disabled=True,\n            ),\n            \"refresh_status\": mo.ui.button(label=\"\ud83d\udd04 Refresh Status\"),\n        },\n        label=\"\ud83d\udcbb System Status\",\n    )\n</code></pre>"},{"location":"api/astro_lab.ui/#astro_lab.ui.ui_training_selector","title":"ui_training_selector","text":"<pre><code>ui_training_selector() -&gt; dictionary\n</code></pre> <p>Training configuration selector UI component.</p> Source code in <code>src\\astro_lab\\ui\\settings.py</code> <pre><code>def ui_training_selector() -&gt; mo.ui.dictionary:\n    \"\"\"Training configuration selector UI component.\"\"\"\n    available_training = ui_config.available_configs[\"training\"]\n\n    return mo.ui.dictionary(\n        {\n            \"training_config\": mo.ui.dropdown(\n                label=\"Training Configuration\",\n                options=available_training,\n                value=available_training[0]\n                if available_training\n                else \"gaia_stellar_training\",\n            ),\n            \"load_training_config\": mo.ui.button(label=\"\ud83c\udfcb\ufe0f Load Training Config\"),\n            \"show_training_details\": mo.ui.button(label=\"\u2139\ufe0f Show Details\"),\n        },\n        label=\"\ud83c\udfcb\ufe0f Training Configuration\",\n    )\n</code></pre>"},{"location":"api/astro_lab.ui/#astro_lab.ui.ui_visualization_controls","title":"ui_visualization_controls","text":"<pre><code>ui_visualization_controls() -&gt; dictionary\n</code></pre> <p>Visualization controls using AstroLab widget backends.</p> Source code in <code>src\\astro_lab\\ui\\components.py</code> <pre><code>def ui_visualization_controls() -&gt; mo.ui.dictionary:\n    \"\"\"Visualization controls using AstroLab widget backends.\"\"\"\n    backend_options = [\"plotly\", \"matplotlib\", \"bokeh\"]\n\n    # Add widget backends if available\n    if WIDGETS_AVAILABLE:\n        backend_options.extend([\"open3d\", \"pyvista\", \"blender\"])\n\n    return mo.ui.dictionary(\n        {\n            \"backend\": mo.ui.dropdown(\n                label=\"Backend\",\n                options=backend_options,\n                value=\"plotly\",\n            ),\n            \"plot_type\": mo.ui.dropdown(\n                label=\"Plot Type\",\n                options=[\"scatter\", \"scatter_3d\", \"histogram\", \"heatmap\", \"density\"],\n                value=\"scatter\",\n            ),\n            \"max_points\": mo.ui.slider(\n                label=\"Max Points\",\n                start=1000,\n                stop=100000,\n                step=5000,\n                value=25000,\n            ),\n            \"interactive\": mo.ui.checkbox(\n                label=\"Interactive\",\n                value=True,\n            ),\n            \"enable_3d\": mo.ui.checkbox(\n                label=\"Enable 3D\",\n                value=True,\n            ),\n            \"create_plot\": mo.ui.button(label=\"\ud83d\udcc8 Create Plot\"),\n        },\n        label=\"\ud83c\udfa8 Visualization\",\n    )\n</code></pre>"},{"location":"api/astro_lab.widgets.bpy.advanced/","title":"astro_lab.widgets.bpy.advanced","text":""},{"location":"api/astro_lab.widgets.bpy.advanced/#astro_lab.widgets.bpy.advanced","title":"advanced","text":""},{"location":"api/astro_lab.widgets.bpy.advanced/#astro_lab.widgets.bpy.advanced--astrolab-blender-utils","title":"AstroLab Blender  Utils","text":"<p>Blender utilities for astronomical visualization. Includes materials, physics, volumetrics, and shaders.</p> <p>Modules:</p> Name Description <code>futuristic_materials</code> <p>Futuristic Materials for Astronomical Visualization</p> <code>geometry_nodes</code> <p>Blender 4.4 Procedural Astronomical Geometry Module</p> <code>physics</code> <p>Blender 4.4 Astronomical Physics Simulation Module</p> <code>post_processing</code> <p>Post-Processing for Astronomical Visualization</p> <code>shaders</code> <p>Astronomical Shaders for Blender 4.4</p> <code>volumetrics</code> <p>Blender 4.4 Volumetric Astronomical Rendering Module</p> <p>Classes:</p> Name Description <code>ArtisticFilters</code> <p>Artistic filters for astronomical visualization</p> <code>AstronomicalMaterials</code> <p>Materials for astronomical objects</p> <code>AstronomicalShaders</code> <p>Create scientifically accurate astronomical shaders</p> <code>FuturisticMaterials</code> <p>Create futuristic materials for astronomical visualization</p> <code>GravitationalSimulation</code> <p>N-body gravitational simulations</p> <code>MaterialPresets</code> <p>Preset material configurations</p> <code>OrbitalMechanics</code> <p>Create realistic orbital mechanics visualizations</p> <code>PhysicsShaders</code> <p>Shaders for astrophysical objects</p> <code>PostProcessingSuite</code> <p>post-processing for astronomical visualization</p> <code>ProceduralAstronomy</code> <p>Generate procedural astronomical structures</p> <code>VisualizationSuite</code> <p>Main interface for advanced astronomical visualization.</p> <code>VolumetricAstronomy</code> <p>Create volumetric astronomical phenomena</p> <code>VolumetricShaders</code> <p>Volumetric shaders for astronomical phenomena</p> <p>Functions:</p> Name Description <code>apply_material_preset</code> <p>Apply a material preset to an object.</p> <code>apply_visual_style</code> <p>Apply a high-level visual style preset to the current advanced scene.</p> <code>create_futuristic_scene</code> <p>Create a complete futuristic astronomical scene with all advanced features.</p> <code>create_galaxy_showcase</code> <p>Create showcase of different galaxy types.</p> <code>create_nebula_showcase</code> <p>Create showcase of different nebula types.</p> <code>create_stellar_system_showcase</code> <p>Create showcase stellar system with planets.</p> <code>initialize_advanced_scene</code> <p>Initialize advanced astronomical visualization scene.</p>"},{"location":"api/astro_lab.widgets.bpy.advanced/#astro_lab.widgets.bpy.advanced.ArtisticFilters","title":"ArtisticFilters","text":"<p>Artistic filters for astronomical visualization</p> <p>Methods:</p> Name Description <code>add_chromatic_aberration</code> <p>Add chromatic aberration effect.</p> <code>add_film_grain</code> <p>Add film grain effect.</p> Source code in <code>src\\astro_lab\\widgets\\bpy\\advanced\\post_processing.py</code> <pre><code>class ArtisticFilters:\n    \"\"\"Artistic filters for astronomical visualization\"\"\"\n\n    @staticmethod  # type: ignore\n    def add_film_grain(intensity: float = 0.1) -&gt; None:  # type: ignore\n        \"\"\"Add film grain effect.\"\"\"\n        scene = bpy.context.scene\n        tree = scene.node_tree\n\n        # Add noise texture\n        noise = tree.nodes.new(\"CompositorNodeTexNoise\")\n        noise.location = (600, 200)\n        noise.inputs[\"Scale\"].default_value = 100.0\n        noise.inputs[\"Detail\"].default_value = 2.0\n\n        # Mix with current image\n        mix = tree.nodes.new(\"CompositorNodeMixRGB\")\n        mix.location = (800, 0)\n        mix.blend_type = \"OVERLAY\"\n        mix.inputs[\"Fac\"].default_value = intensity\n\n        # Connect\n        current_input = PostProcessingSuite._get_current_image_input_static(scene)\n        if current_input:\n            tree.links.new(current_input, mix.inputs[\"Image1\"])\n            tree.links.new(noise.outputs[\"Color\"], mix.inputs[\"Image2\"])\n            PostProcessingSuite._update_composite_connection_static(\n                scene, mix.outputs[\"Image\"]\n            )\n\n    @staticmethod  # type: ignore\n    def add_chromatic_aberration(intensity: float = 0.02) -&gt; None:  # type: ignore\n        \"\"\"Add chromatic aberration effect.\"\"\"\n        scene = bpy.context.scene\n        tree = scene.node_tree\n\n        # Add lens distortion for red channel\n        lens_red = tree.nodes.new(\"CompositorNodeLensDist\")\n        lens_red.location = (600, 100)\n        lens_red.inputs[\"Distort\"].default_value = intensity\n\n        # Add lens distortion for blue channel\n        lens_blue = tree.nodes.new(\"CompositorNodeLensDist\")\n        lens_blue.location = (600, -100)\n        lens_blue.inputs[\"Distort\"].default_value = -intensity\n\n        # Separate RGB\n        separate = tree.nodes.new(\"CompositorNodeSepRGBA\")\n        separate.location = (400, 0)\n\n        # Combine RGB\n        combine = tree.nodes.new(\"CompositorNodeCombRGBA\")\n        combine.location = (800, 0)\n\n        # Connect\n        current_input = PostProcessingSuite._get_current_image_input_static(scene)\n        if current_input:\n            tree.links.new(current_input, separate.inputs[\"Image\"])\n            tree.links.new(separate.outputs[\"R\"], lens_red.inputs[\"Image\"])\n            tree.links.new(separate.outputs[\"B\"], lens_blue.inputs[\"Image\"])\n            tree.links.new(separate.outputs[\"G\"], combine.inputs[\"G\"])\n            tree.links.new(lens_red.outputs[\"Image\"], combine.inputs[\"R\"])\n            tree.links.new(lens_blue.outputs[\"Image\"], combine.inputs[\"B\"])\n            PostProcessingSuite._update_composite_connection_static(\n                scene, combine.outputs[\"Image\"]\n            )\n\n    @staticmethod  # type: ignore\n    def _get_current_image_input_static(scene):  # type: ignore\n        \"\"\"Static version of _get_current_image_input.\"\"\"\n        tree = scene.node_tree\n        composite = tree.nodes.get(\"Composite\")\n        if composite and composite.inputs[\"Image\"].links:\n            return composite.inputs[\"Image\"].links[0].from_socket\n        return None\n\n    @staticmethod  # type: ignore\n    def _update_composite_connection_static(scene, output_socket):  # type: ignore\n        \"\"\"Static version of _update_composite_connection.\"\"\"\n        tree = scene.node_tree\n        composite = tree.nodes.get(\"Composite\")\n        if composite:\n            if composite.inputs[\"Image\"].links:\n                tree.links.remove(composite.inputs[\"Image\"].links[0])\n            tree.links.new(output_socket, composite.inputs[\"Image\"])\n</code></pre>"},{"location":"api/astro_lab.widgets.bpy.advanced/#astro_lab.widgets.bpy.advanced.ArtisticFilters.add_chromatic_aberration","title":"add_chromatic_aberration  <code>staticmethod</code>","text":"<pre><code>add_chromatic_aberration(intensity: float = 0.02) -&gt; None\n</code></pre> <p>Add chromatic aberration effect.</p> Source code in <code>src\\astro_lab\\widgets\\bpy\\advanced\\post_processing.py</code> <pre><code>@staticmethod  # type: ignore\ndef add_chromatic_aberration(intensity: float = 0.02) -&gt; None:  # type: ignore\n    \"\"\"Add chromatic aberration effect.\"\"\"\n    scene = bpy.context.scene\n    tree = scene.node_tree\n\n    # Add lens distortion for red channel\n    lens_red = tree.nodes.new(\"CompositorNodeLensDist\")\n    lens_red.location = (600, 100)\n    lens_red.inputs[\"Distort\"].default_value = intensity\n\n    # Add lens distortion for blue channel\n    lens_blue = tree.nodes.new(\"CompositorNodeLensDist\")\n    lens_blue.location = (600, -100)\n    lens_blue.inputs[\"Distort\"].default_value = -intensity\n\n    # Separate RGB\n    separate = tree.nodes.new(\"CompositorNodeSepRGBA\")\n    separate.location = (400, 0)\n\n    # Combine RGB\n    combine = tree.nodes.new(\"CompositorNodeCombRGBA\")\n    combine.location = (800, 0)\n\n    # Connect\n    current_input = PostProcessingSuite._get_current_image_input_static(scene)\n    if current_input:\n        tree.links.new(current_input, separate.inputs[\"Image\"])\n        tree.links.new(separate.outputs[\"R\"], lens_red.inputs[\"Image\"])\n        tree.links.new(separate.outputs[\"B\"], lens_blue.inputs[\"Image\"])\n        tree.links.new(separate.outputs[\"G\"], combine.inputs[\"G\"])\n        tree.links.new(lens_red.outputs[\"Image\"], combine.inputs[\"R\"])\n        tree.links.new(lens_blue.outputs[\"Image\"], combine.inputs[\"B\"])\n        PostProcessingSuite._update_composite_connection_static(\n            scene, combine.outputs[\"Image\"]\n        )\n</code></pre>"},{"location":"api/astro_lab.widgets.bpy.advanced/#astro_lab.widgets.bpy.advanced.ArtisticFilters.add_film_grain","title":"add_film_grain  <code>staticmethod</code>","text":"<pre><code>add_film_grain(intensity: float = 0.1) -&gt; None\n</code></pre> <p>Add film grain effect.</p> Source code in <code>src\\astro_lab\\widgets\\bpy\\advanced\\post_processing.py</code> <pre><code>@staticmethod  # type: ignore\ndef add_film_grain(intensity: float = 0.1) -&gt; None:  # type: ignore\n    \"\"\"Add film grain effect.\"\"\"\n    scene = bpy.context.scene\n    tree = scene.node_tree\n\n    # Add noise texture\n    noise = tree.nodes.new(\"CompositorNodeTexNoise\")\n    noise.location = (600, 200)\n    noise.inputs[\"Scale\"].default_value = 100.0\n    noise.inputs[\"Detail\"].default_value = 2.0\n\n    # Mix with current image\n    mix = tree.nodes.new(\"CompositorNodeMixRGB\")\n    mix.location = (800, 0)\n    mix.blend_type = \"OVERLAY\"\n    mix.inputs[\"Fac\"].default_value = intensity\n\n    # Connect\n    current_input = PostProcessingSuite._get_current_image_input_static(scene)\n    if current_input:\n        tree.links.new(current_input, mix.inputs[\"Image1\"])\n        tree.links.new(noise.outputs[\"Color\"], mix.inputs[\"Image2\"])\n        PostProcessingSuite._update_composite_connection_static(\n            scene, mix.outputs[\"Image\"]\n        )\n</code></pre>"},{"location":"api/astro_lab.widgets.bpy.advanced/#astro_lab.widgets.bpy.advanced.AstronomicalMaterials","title":"AstronomicalMaterials","text":"<p>Materials for astronomical objects</p> <p>Methods:</p> Name Description <code>create_stellar_classification_material</code> <p>Create material based on stellar spectral classification.</p> Source code in <code>src\\astro_lab\\widgets\\bpy\\advanced\\geometry_nodes.py</code> <pre><code>class AstronomicalMaterials:\n    \"\"\"Materials for astronomical objects\"\"\"\n\n    @staticmethod  # type: ignore\n    def create_stellar_classification_material(\n        spectral_class: str = \"G\",\n    ) -&gt; bpy.types.Material:  # type: ignore\n        \"\"\"\n        Create material based on stellar spectral classification.\n\n        Args:\n            spectral_class: Stellar class (O, B, A, F, G, K, M)\n\n        Returns:\n            Created material\n        \"\"\"\n        mat = bpy.data.materials.new(name=f\"Star_{spectral_class}\")\n        mat.use_nodes = True\n        nodes = mat.node_tree.nodes\n        links = mat.node_tree.links\n\n        # Clear default nodes\n        nodes.clear()\n\n        # Add nodes\n        output = nodes.new(\"ShaderNodeOutputMaterial\")\n        emission = nodes.new(\"ShaderNodeEmission\")\n        blackbody = nodes.new(\"ShaderNodeBlackbody\")\n\n        # Stellar temperatures and properties\n        stellar_data = {\n            \"O\": {\"temp\": 30000, \"strength\": 50.0},\n            \"B\": {\"temp\": 20000, \"strength\": 20.0},\n            \"A\": {\"temp\": 8500, \"strength\": 10.0},\n            \"F\": {\"temp\": 6500, \"strength\": 5.0},\n            \"G\": {\"temp\": 5500, \"strength\": 2.0},\n            \"K\": {\"temp\": 4000, \"strength\": 1.0},\n            \"M\": {\"temp\": 3000, \"strength\": 0.3},\n        }\n\n        data = stellar_data.get(spectral_class, stellar_data[\"G\"])\n\n        # Set temperature\n        blackbody.inputs[\"Temperature\"].default_value = data[\"temp\"]\n        emission.inputs[\"Strength\"].default_value = data[\"strength\"]\n\n        # Position nodes\n        blackbody.location = (-200, 0)\n        emission.location = (0, 0)\n        output.location = (200, 0)\n\n        # Connect nodes\n        links.new(blackbody.outputs[\"Color\"], emission.inputs[\"Color\"])\n        links.new(emission.outputs[\"Emission\"], output.inputs[\"Surface\"])\n\n        return mat\n</code></pre>"},{"location":"api/astro_lab.widgets.bpy.advanced/#astro_lab.widgets.bpy.advanced.AstronomicalMaterials.create_stellar_classification_material","title":"create_stellar_classification_material  <code>staticmethod</code>","text":"<pre><code>create_stellar_classification_material(spectral_class: str = 'G') -&gt; Material\n</code></pre> <p>Create material based on stellar spectral classification.</p> <p>Parameters:</p> Name Type Description Default <code>spectral_class</code> <code>str</code> <p>Stellar class (O, B, A, F, G, K, M)</p> <code>'G'</code> <p>Returns:</p> Type Description <code>Material</code> <p>Created material</p> Source code in <code>src\\astro_lab\\widgets\\bpy\\advanced\\geometry_nodes.py</code> <pre><code>@staticmethod  # type: ignore\ndef create_stellar_classification_material(\n    spectral_class: str = \"G\",\n) -&gt; bpy.types.Material:  # type: ignore\n    \"\"\"\n    Create material based on stellar spectral classification.\n\n    Args:\n        spectral_class: Stellar class (O, B, A, F, G, K, M)\n\n    Returns:\n        Created material\n    \"\"\"\n    mat = bpy.data.materials.new(name=f\"Star_{spectral_class}\")\n    mat.use_nodes = True\n    nodes = mat.node_tree.nodes\n    links = mat.node_tree.links\n\n    # Clear default nodes\n    nodes.clear()\n\n    # Add nodes\n    output = nodes.new(\"ShaderNodeOutputMaterial\")\n    emission = nodes.new(\"ShaderNodeEmission\")\n    blackbody = nodes.new(\"ShaderNodeBlackbody\")\n\n    # Stellar temperatures and properties\n    stellar_data = {\n        \"O\": {\"temp\": 30000, \"strength\": 50.0},\n        \"B\": {\"temp\": 20000, \"strength\": 20.0},\n        \"A\": {\"temp\": 8500, \"strength\": 10.0},\n        \"F\": {\"temp\": 6500, \"strength\": 5.0},\n        \"G\": {\"temp\": 5500, \"strength\": 2.0},\n        \"K\": {\"temp\": 4000, \"strength\": 1.0},\n        \"M\": {\"temp\": 3000, \"strength\": 0.3},\n    }\n\n    data = stellar_data.get(spectral_class, stellar_data[\"G\"])\n\n    # Set temperature\n    blackbody.inputs[\"Temperature\"].default_value = data[\"temp\"]\n    emission.inputs[\"Strength\"].default_value = data[\"strength\"]\n\n    # Position nodes\n    blackbody.location = (-200, 0)\n    emission.location = (0, 0)\n    output.location = (200, 0)\n\n    # Connect nodes\n    links.new(blackbody.outputs[\"Color\"], emission.inputs[\"Color\"])\n    links.new(emission.outputs[\"Emission\"], output.inputs[\"Surface\"])\n\n    return mat\n</code></pre>"},{"location":"api/astro_lab.widgets.bpy.advanced/#astro_lab.widgets.bpy.advanced.AstronomicalShaders","title":"AstronomicalShaders","text":"<p>Create scientifically accurate astronomical shaders</p> <p>Methods:</p> Name Description <code>create_atmospheric_scattering_shader</code> <p>Create atmospheric scattering shader (Rayleigh + Mie).</p> <code>create_nebula_emission_shader</code> <p>Create nebula shader based on emission line spectra.</p> <code>create_planetary_surface_shader</code> <p>Create planetary surface shader based on composition.</p> <code>create_stellar_blackbody_shader</code> <p>Create physically accurate stellar shader based on blackbody radiation.</p> Source code in <code>src\\astro_lab\\widgets\\bpy\\advanced\\shaders.py</code> <pre><code>class AstronomicalShaders:\n    \"\"\"Create scientifically accurate astronomical shaders\"\"\"\n\n    @staticmethod  # type: ignore\n    def create_stellar_blackbody_shader(\n        temperature: float, luminosity: float = 1.0, stellar_class: Optional[str] = None\n    ) -&gt; bpy.types.Material:  # type: ignore\n        \"\"\"\n        Create physically accurate stellar shader based on blackbody radiation.\n\n        Args:\n            temperature: Stellar temperature in Kelvin\n            luminosity: Stellar luminosity relative to Sun\n            stellar_class: Optional spectral class (O, B, A, F, G, K, M)\n\n        Returns:\n            Created stellar material\n        \"\"\"\n        # Determine spectral class if not provided\n        if stellar_class is None:\n            stellar_class = AstronomicalShaders._classify_star_by_temperature(\n                temperature\n            )\n\n        mat = bpy.data.materials.new(name=f\"Star_{stellar_class}_{temperature}K\")\n        mat.use_nodes = True\n        nodes = mat.node_tree.nodes\n        links = mat.node_tree.links\n\n        # Clear default nodes\n        nodes.clear()\n\n        # Add nodes\n        output = nodes.new(\"ShaderNodeOutputMaterial\")\n        emission = nodes.new(\"ShaderNodeEmission\")\n        blackbody = nodes.new(\"ShaderNodeBlackbody\")\n\n        # Set temperature\n        blackbody.inputs[\"Temperature\"].default_value = temperature\n\n        # Calculate emission strength from luminosity\n        # Using Stefan-Boltzmann law approximation\n        strength = luminosity * 10.0  # Base strength scaling\n        emission.inputs[\"Strength\"].default_value = strength\n\n        # Add stellar surface variation\n        noise = nodes.new(\"ShaderNodeTexNoise\")\n        noise.inputs[\"Scale\"].default_value = 50.0\n        noise.inputs[\"Detail\"].default_value = 8.0\n\n        # Color variation for stellar activity\n        color_mix = nodes.new(\"ShaderNodeMixRGB\")\n        color_mix.blend_type = \"ADD\"\n        color_mix.inputs[\"Fac\"].default_value = 0.1\n\n        # Hot spots for stellar activity\n        math_node = nodes.new(\"ShaderNodeMath\")\n        math_node.operation = \"MULTIPLY\"\n        math_node.inputs[1].default_value = 1.2  # Activity scaling\n\n        # Position nodes\n        blackbody.location = (-400, 100)\n        noise.location = (-400, -200)\n        color_mix.location = (-200, 0)\n        math_node.location = (-200, -300)\n        emission.location = (0, 0)\n        output.location = (200, 0)\n\n        # Connect nodes\n        links.new(blackbody.outputs[\"Color\"], color_mix.inputs[\"Color1\"])\n        links.new(noise.outputs[\"Color\"], color_mix.inputs[\"Color2\"])\n        links.new(color_mix.outputs[\"Color\"], emission.inputs[\"Color\"])\n        links.new(noise.outputs[\"Fac\"], math_node.inputs[0])\n        links.new(math_node.outputs[\"Value\"], emission.inputs[\"Strength\"])\n        links.new(emission.outputs[\"Emission\"], output.inputs[\"Surface\"])\n\n        return mat\n\n    @staticmethod  # type: ignore\n    def create_planetary_surface_shader(\n        planet_type: str, composition: Optional[Dict[str, float]] = None\n    ) -&gt; bpy.types.Material:  # type: ignore\n        \"\"\"\n        Create planetary surface shader based on composition.\n\n        Args:\n            planet_type: Type of planet ('terrestrial', 'gas_giant', 'ice_giant', 'moon')\n            composition: Compositional percentages {'rock': 0.7, 'ice': 0.3, etc.}\n\n        Returns:\n            Created planetary material\n        \"\"\"\n        mat = bpy.data.materials.new(name=f\"Planet_{planet_type}\")\n        mat.use_nodes = True\n        nodes = mat.node_tree.nodes\n        links = mat.node_tree.links\n\n        # Clear default nodes\n        nodes.clear()\n\n        # Add nodes\n        output = nodes.new(\"ShaderNodeOutputMaterial\")\n        bsdf = nodes.new(\"ShaderNodeBsdfPrincipled\")\n\n        # Default compositions\n        if composition is None:\n            composition = AstronomicalShaders._get_default_composition(planet_type)\n\n        # Base color from composition\n        base_color = AstronomicalShaders._calculate_surface_color(composition)\n        bsdf.inputs[\"Base Color\"].default_value = (*base_color, 1.0)\n\n        # Surface properties based on planet type\n        if planet_type == \"terrestrial\":\n            bsdf.inputs[\"Roughness\"].default_value = 0.8\n            bsdf.inputs[\"Metallic\"].default_value = 0.1\n            bsdf.inputs[\"Specular\"].default_value = 0.3\n\n        elif planet_type == \"gas_giant\":\n            # Gas giants have cloudy, banded surfaces\n            bsdf.inputs[\"Roughness\"].default_value = 0.3\n            bsdf.inputs[\"Metallic\"].default_value = 0.0\n            bsdf.inputs[\"Specular\"].default_value = 0.8\n\n            # Add cloud bands\n            AstronomicalShaders._add_gas_giant_bands(nodes, links, bsdf)\n\n        elif planet_type == \"ice_giant\":\n            bsdf.inputs[\"Roughness\"].default_value = 0.1\n            bsdf.inputs[\"Metallic\"].default_value = 0.0\n            bsdf.inputs[\"Specular\"].default_value = 1.0\n            bsdf.inputs[\"Transmission\"].default_value = 0.3\n\n        else:  # moon\n            bsdf.inputs[\"Roughness\"].default_value = 0.9\n            bsdf.inputs[\"Metallic\"].default_value = 0.05\n            bsdf.inputs[\"Specular\"].default_value = 0.1\n\n        # Add surface texture variation\n        noise = nodes.new(\"ShaderNodeTexNoise\")\n        noise.inputs[\"Scale\"].default_value = 5.0\n        noise.inputs[\"Detail\"].default_value = 6.0\n\n        # Mix with base color\n        color_mix = nodes.new(\"ShaderNodeMixRGB\")\n        color_mix.blend_type = \"MULTIPLY\"\n        color_mix.inputs[\"Fac\"].default_value = 0.3\n\n        # Position nodes\n        noise.location = (-400, -200)\n        color_mix.location = (-200, 0)\n        bsdf.location = (0, 0)\n        output.location = (200, 0)\n\n        # Connect nodes\n        links.new(noise.outputs[\"Color\"], color_mix.inputs[\"Color2\"])\n        links.new(color_mix.outputs[\"Color\"], bsdf.inputs[\"Base Color\"])\n        links.new(bsdf.outputs[\"BSDF\"], output.inputs[\"Surface\"])\n\n        return mat\n\n    @staticmethod  # type: ignore\n    def create_nebula_emission_shader(\n        emission_lines: List[str], density_variation: float = 0.5\n    ) -&gt; bpy.types.Material:  # type: ignore\n        \"\"\"\n        Create nebula shader based on emission line spectra.\n\n        Args:\n            emission_lines: List of emission lines ['H_alpha', 'O_III', 'H_beta', etc.]\n            density_variation: Amount of density variation (0-1)\n\n        Returns:\n            Created nebula material\n        \"\"\"\n        mat = bpy.data.materials.new(name=f\"Nebula_{'_'.join(emission_lines)}\")\n        mat.use_nodes = True\n        nodes = mat.node_tree.nodes\n        links = mat.node_tree.links\n\n        # Clear default nodes\n        nodes.clear()\n\n        # Add nodes\n        output = nodes.new(\"ShaderNodeOutputMaterial\")\n        emission = nodes.new(\"ShaderNodeVolumeEmission\")\n\n        # Calculate combined color from emission lines\n        combined_color = AstronomicalShaders._combine_emission_lines(emission_lines)\n        emission.inputs[\"Color\"].default_value = (*combined_color, 1.0)\n\n        # Add density variation with multiple noise scales\n        noise1 = nodes.new(\"ShaderNodeTexNoise\")\n        noise1.inputs[\"Scale\"].default_value = 1.0\n        noise1.inputs[\"Detail\"].default_value = 8.0\n\n        noise2 = nodes.new(\"ShaderNodeTexNoise\")\n        noise2.inputs[\"Scale\"].default_value = 5.0\n        noise2.inputs[\"Detail\"].default_value = 4.0\n\n        # Combine noises for complex structure\n        multiply = nodes.new(\"ShaderNodeMath\")\n        multiply.operation = \"MULTIPLY\"\n\n        # Color ramp for density falloff\n        ramp = nodes.new(\"ShaderNodeValToRGB\")\n        ramp.color_ramp.elements[0].position = 0.1\n        ramp.color_ramp.elements[1].position = 0.9\n\n        # Emission strength variation\n        strength_variation = nodes.new(\"ShaderNodeMath\")\n        strength_variation.operation = \"MULTIPLY\"\n        strength_variation.inputs[1].default_value = 2.0  # Base strength\n\n        # Position nodes\n        noise1.location = (-600, 0)\n        noise2.location = (-600, -200)\n        multiply.location = (-400, -100)\n        ramp.location = (-200, 0)\n        strength_variation.location = (-200, -300)\n        emission.location = (0, 0)\n        output.location = (200, 0)\n\n        # Connect nodes\n        links.new(noise1.outputs[\"Fac\"], multiply.inputs[0])\n        links.new(noise2.outputs[\"Fac\"], multiply.inputs[1])\n        links.new(multiply.outputs[\"Value\"], ramp.inputs[\"Fac\"])\n        links.new(ramp.outputs[\"Color\"], emission.inputs[\"Density\"])\n        links.new(multiply.outputs[\"Value\"], strength_variation.inputs[0])\n        links.new(strength_variation.outputs[\"Value\"], emission.inputs[\"Strength\"])\n        links.new(emission.outputs[\"Volume\"], output.inputs[\"Volume\"])\n\n        return mat\n\n    @staticmethod  # type: ignore\n    def create_atmospheric_scattering_shader(\n        atmosphere_type: str, scale_height: float = 8.5\n    ) -&gt; bpy.types.Material:  # type: ignore\n        \"\"\"\n        Create atmospheric scattering shader (Rayleigh + Mie).\n\n        Args:\n            atmosphere_type: Type of atmosphere ('earth', 'mars', 'venus', 'titan')\n            scale_height: Atmospheric scale height in km\n\n        Returns:\n            Created atmospheric material\n        \"\"\"\n        mat = bpy.data.materials.new(name=f\"Atmosphere_{atmosphere_type}\")\n        mat.use_nodes = True\n        nodes = mat.node_tree.nodes\n        links = mat.node_tree.links\n\n        # Clear default nodes\n        nodes.clear()\n\n        # Add nodes\n        output = nodes.new(\"ShaderNodeOutputMaterial\")\n        scatter = nodes.new(\"ShaderNodeVolumeScatter\")\n\n        # Atmospheric parameters\n        atmo_params = AstronomicalShaders._get_atmospheric_parameters(atmosphere_type)\n\n        # Set scattering color (Rayleigh scattering favors blue)\n        scatter.inputs[\"Color\"].default_value = (*atmo_params[\"color\"], 1.0)\n\n        # Altitude-dependent density\n        geometry = nodes.new(\"GeometryNodeInputPosition\")\n        vector_length = nodes.new(\"ShaderNodeVectorMath\")\n        vector_length.operation = \"LENGTH\"\n\n        # Exponential density falloff\n        subtract = nodes.new(\"ShaderNodeMath\")\n        subtract.operation = \"SUBTRACT\"\n        subtract.inputs[0].default_value = atmo_params[\"radius\"]\n\n        divide = nodes.new(\"ShaderNodeMath\")\n        divide.operation = \"DIVIDE\"\n        divide.inputs[1].default_value = scale_height\n\n        power = nodes.new(\"ShaderNodeMath\")\n        power.operation = \"POWER\"\n        power.inputs[0].default_value = math.e\n\n        multiply_negative = nodes.new(\"ShaderNodeMath\")\n        multiply_negative.operation = \"MULTIPLY\"\n        multiply_negative.inputs[1].default_value = -1.0\n\n        # Base density\n        multiply_density = nodes.new(\"ShaderNodeMath\")\n        multiply_density.operation = \"MULTIPLY\"\n        multiply_density.inputs[1].default_value = atmo_params[\"density\"]\n\n        # Position nodes\n        geometry.location = (-600, -200)\n        vector_length.location = (-400, -200)\n        subtract.location = (-200, -200)\n        multiply_negative.location = (0, -300)\n        divide.location = (0, -200)\n        power.location = (200, -200)\n        multiply_density.location = (400, -100)\n        scatter.location = (600, 0)\n        output.location = (800, 0)\n\n        # Connect nodes\n        links.new(geometry.outputs[\"Position\"], vector_length.inputs[0])\n        links.new(vector_length.outputs[\"Value\"], subtract.inputs[1])\n        links.new(subtract.outputs[\"Value\"], divide.inputs[0])\n        links.new(divide.outputs[\"Value\"], multiply_negative.inputs[0])\n        links.new(multiply_negative.outputs[\"Value\"], power.inputs[1])\n        links.new(power.outputs[\"Value\"], multiply_density.inputs[0])\n        links.new(multiply_density.outputs[\"Value\"], scatter.inputs[\"Density\"])\n        links.new(scatter.outputs[\"Volume\"], output.inputs[\"Volume\"])\n\n        return mat\n\n    @staticmethod  # type: ignore\n    def _classify_star_by_temperature(temperature: float) -&gt; str:  # type: ignore\n        \"\"\"Classify star by temperature.\"\"\"\n        if temperature &gt;= 30000:\n            return \"O\"\n        elif temperature &gt;= 10000:\n            return \"B\"\n        elif temperature &gt;= 7500:\n            return \"A\"\n        elif temperature &gt;= 6000:\n            return \"F\"\n        elif temperature &gt;= 5200:\n            return \"G\"\n        elif temperature &gt;= 3700:\n            return \"K\"\n        else:\n            return \"M\"\n\n    @staticmethod  # type: ignore\n    def _get_default_composition(planet_type: str) -&gt; Dict[str, float]:  # type: ignore\n        \"\"\"Get default planetary composition.\"\"\"\n        compositions = {\n            \"terrestrial\": {\"rock\": 0.8, \"metal\": 0.15, \"ice\": 0.05},\n            \"gas_giant\": {\"hydrogen\": 0.75, \"helium\": 0.24, \"methane\": 0.01},\n            \"ice_giant\": {\"water\": 0.65, \"methane\": 0.25, \"ammonia\": 0.10},\n            \"moon\": {\"rock\": 0.9, \"metal\": 0.05, \"ice\": 0.05},\n        }\n        return compositions.get(planet_type, compositions[\"terrestrial\"])\n\n    @staticmethod  # type: ignore\n    def _calculate_surface_color(\n        composition: Dict[str, float],\n    ) -&gt; Tuple[float, float, float]:  # type: ignore\n        \"\"\"Calculate surface color from composition.\"\"\"\n        # Color mapping for different materials\n        material_colors = {\n            \"rock\": (0.4, 0.3, 0.2),\n            \"metal\": (0.6, 0.6, 0.5),\n            \"ice\": (0.8, 0.9, 1.0),\n            \"hydrogen\": (0.9, 0.8, 0.7),\n            \"helium\": (0.8, 0.8, 0.9),\n            \"methane\": (0.7, 0.9, 0.8),\n            \"water\": (0.2, 0.4, 0.8),\n            \"ammonia\": (0.9, 0.9, 0.7),\n        }\n\n        # Weighted average of colors\n        r = g = b = 0.0\n        for material, fraction in composition.items():\n            if material in material_colors:\n                color = material_colors[material]\n                r += color[0] * fraction\n                g += color[1] * fraction\n                b += color[2] * fraction\n\n        return (r, g, b)\n\n    @staticmethod  # type: ignore\n    def _combine_emission_lines(\n        emission_lines: List[str],\n    ) -&gt; Tuple[float, float, float]:  # type: ignore\n        \"\"\"Combine emission line colors.\"\"\"\n        # Emission line wavelengths and colors\n        line_colors = {\n            \"H_alpha\": (0.8, 0.2, 0.2),  # 656.3 nm - Red\n            \"H_beta\": (0.4, 0.6, 1.0),  # 486.1 nm - Blue\n            \"O_III\": (0.2, 0.8, 0.3),  # 500.7 nm - Green\n            \"N_II\": (0.8, 0.4, 0.2),  # 658.3 nm - Orange\n            \"S_II\": (0.6, 0.2, 0.4),  # 671.6 nm - Purple\n            \"He_II\": (0.4, 0.8, 1.0),  # 468.6 nm - Cyan\n        }\n\n        # Average the colors\n        r = g = b = 0.0\n        count = len(emission_lines)\n\n        for line in emission_lines:\n            if line in line_colors:\n                color = line_colors[line]\n                r += color[0]\n                g += color[1]\n                b += color[2]\n\n        if count &gt; 0:\n            return (r / count, g / count, b / count)\n        else:\n            return (1.0, 0.5, 0.3)  # Default nebula color\n\n    @staticmethod  # type: ignore\n    def _get_atmospheric_parameters(atmosphere_type: str) -&gt; Dict[str, Any]:  # type: ignore\n        \"\"\"Get atmospheric parameters for different worlds.\"\"\"\n        parameters = {\n            \"earth\": {\"color\": (0.3, 0.6, 1.0), \"density\": 0.1, \"radius\": 1.0},\n            \"mars\": {\"color\": (0.8, 0.5, 0.3), \"density\": 0.01, \"radius\": 0.53},\n            \"venus\": {\"color\": (0.9, 0.8, 0.6), \"density\": 0.9, \"radius\": 0.95},\n            \"titan\": {\"color\": (0.7, 0.6, 0.4), \"density\": 0.15, \"radius\": 0.4},\n        }\n        return parameters.get(atmosphere_type, parameters[\"earth\"])\n\n    @staticmethod  # type: ignore\n    def _add_gas_giant_bands(nodes, links, bsdf) -&gt; None:  # type: ignore\n        \"\"\"Add banded structure to gas giant shader.\"\"\"\n        # Coordinate system for bands\n        coords = nodes.new(\"ShaderNodeTexCoord\")\n        mapping = nodes.new(\"ShaderNodeMapping\")\n\n        # Scale Y coordinate for bands\n        mapping.inputs[\"Scale\"].default_value = (1.0, 10.0, 1.0)\n\n        # Wave texture for bands\n        wave = nodes.new(\"ShaderNodeTexWave\")\n        wave.wave_type = \"BANDS\"\n        wave.inputs[\"Scale\"].default_value = 2.0\n        wave.inputs[\"Distortion\"].default_value = 0.5\n\n        # Color ramp for band colors\n        ramp = nodes.new(\"ShaderNodeValToRGB\")\n        ramp.color_ramp.elements[0].color = (0.8, 0.6, 0.4, 1.0)  # Light band\n        ramp.color_ramp.elements[1].color = (0.4, 0.3, 0.2, 1.0)  # Dark band\n\n        # Position nodes\n        coords.location = (-800, -400)\n        mapping.location = (-600, -400)\n        wave.location = (-400, -400)\n        ramp.location = (-200, -400)\n\n        # Connect nodes\n        links.new(coords.outputs[\"Generated\"], mapping.inputs[\"Vector\"])\n        links.new(mapping.outputs[\"Vector\"], wave.inputs[\"Vector\"])\n        links.new(wave.outputs[\"Color\"], ramp.inputs[\"Fac\"])\n        links.new(ramp.outputs[\"Color\"], bsdf.inputs[\"Base Color\"])\n</code></pre>"},{"location":"api/astro_lab.widgets.bpy.advanced/#astro_lab.widgets.bpy.advanced.AstronomicalShaders.create_atmospheric_scattering_shader","title":"create_atmospheric_scattering_shader  <code>staticmethod</code>","text":"<pre><code>create_atmospheric_scattering_shader(\n    atmosphere_type: str, scale_height: float = 8.5\n) -&gt; Material\n</code></pre> <p>Create atmospheric scattering shader (Rayleigh + Mie).</p> <p>Parameters:</p> Name Type Description Default <code>atmosphere_type</code> <code>str</code> <p>Type of atmosphere ('earth', 'mars', 'venus', 'titan')</p> required <code>scale_height</code> <code>float</code> <p>Atmospheric scale height in km</p> <code>8.5</code> <p>Returns:</p> Type Description <code>Material</code> <p>Created atmospheric material</p> Source code in <code>src\\astro_lab\\widgets\\bpy\\advanced\\shaders.py</code> <pre><code>@staticmethod  # type: ignore\ndef create_atmospheric_scattering_shader(\n    atmosphere_type: str, scale_height: float = 8.5\n) -&gt; bpy.types.Material:  # type: ignore\n    \"\"\"\n    Create atmospheric scattering shader (Rayleigh + Mie).\n\n    Args:\n        atmosphere_type: Type of atmosphere ('earth', 'mars', 'venus', 'titan')\n        scale_height: Atmospheric scale height in km\n\n    Returns:\n        Created atmospheric material\n    \"\"\"\n    mat = bpy.data.materials.new(name=f\"Atmosphere_{atmosphere_type}\")\n    mat.use_nodes = True\n    nodes = mat.node_tree.nodes\n    links = mat.node_tree.links\n\n    # Clear default nodes\n    nodes.clear()\n\n    # Add nodes\n    output = nodes.new(\"ShaderNodeOutputMaterial\")\n    scatter = nodes.new(\"ShaderNodeVolumeScatter\")\n\n    # Atmospheric parameters\n    atmo_params = AstronomicalShaders._get_atmospheric_parameters(atmosphere_type)\n\n    # Set scattering color (Rayleigh scattering favors blue)\n    scatter.inputs[\"Color\"].default_value = (*atmo_params[\"color\"], 1.0)\n\n    # Altitude-dependent density\n    geometry = nodes.new(\"GeometryNodeInputPosition\")\n    vector_length = nodes.new(\"ShaderNodeVectorMath\")\n    vector_length.operation = \"LENGTH\"\n\n    # Exponential density falloff\n    subtract = nodes.new(\"ShaderNodeMath\")\n    subtract.operation = \"SUBTRACT\"\n    subtract.inputs[0].default_value = atmo_params[\"radius\"]\n\n    divide = nodes.new(\"ShaderNodeMath\")\n    divide.operation = \"DIVIDE\"\n    divide.inputs[1].default_value = scale_height\n\n    power = nodes.new(\"ShaderNodeMath\")\n    power.operation = \"POWER\"\n    power.inputs[0].default_value = math.e\n\n    multiply_negative = nodes.new(\"ShaderNodeMath\")\n    multiply_negative.operation = \"MULTIPLY\"\n    multiply_negative.inputs[1].default_value = -1.0\n\n    # Base density\n    multiply_density = nodes.new(\"ShaderNodeMath\")\n    multiply_density.operation = \"MULTIPLY\"\n    multiply_density.inputs[1].default_value = atmo_params[\"density\"]\n\n    # Position nodes\n    geometry.location = (-600, -200)\n    vector_length.location = (-400, -200)\n    subtract.location = (-200, -200)\n    multiply_negative.location = (0, -300)\n    divide.location = (0, -200)\n    power.location = (200, -200)\n    multiply_density.location = (400, -100)\n    scatter.location = (600, 0)\n    output.location = (800, 0)\n\n    # Connect nodes\n    links.new(geometry.outputs[\"Position\"], vector_length.inputs[0])\n    links.new(vector_length.outputs[\"Value\"], subtract.inputs[1])\n    links.new(subtract.outputs[\"Value\"], divide.inputs[0])\n    links.new(divide.outputs[\"Value\"], multiply_negative.inputs[0])\n    links.new(multiply_negative.outputs[\"Value\"], power.inputs[1])\n    links.new(power.outputs[\"Value\"], multiply_density.inputs[0])\n    links.new(multiply_density.outputs[\"Value\"], scatter.inputs[\"Density\"])\n    links.new(scatter.outputs[\"Volume\"], output.inputs[\"Volume\"])\n\n    return mat\n</code></pre>"},{"location":"api/astro_lab.widgets.bpy.advanced/#astro_lab.widgets.bpy.advanced.AstronomicalShaders.create_nebula_emission_shader","title":"create_nebula_emission_shader  <code>staticmethod</code>","text":"<pre><code>create_nebula_emission_shader(\n    emission_lines: List[str], density_variation: float = 0.5\n) -&gt; Material\n</code></pre> <p>Create nebula shader based on emission line spectra.</p> <p>Parameters:</p> Name Type Description Default <code>emission_lines</code> <code>List[str]</code> <p>List of emission lines ['H_alpha', 'O_III', 'H_beta', etc.]</p> required <code>density_variation</code> <code>float</code> <p>Amount of density variation (0-1)</p> <code>0.5</code> <p>Returns:</p> Type Description <code>Material</code> <p>Created nebula material</p> Source code in <code>src\\astro_lab\\widgets\\bpy\\advanced\\shaders.py</code> <pre><code>@staticmethod  # type: ignore\ndef create_nebula_emission_shader(\n    emission_lines: List[str], density_variation: float = 0.5\n) -&gt; bpy.types.Material:  # type: ignore\n    \"\"\"\n    Create nebula shader based on emission line spectra.\n\n    Args:\n        emission_lines: List of emission lines ['H_alpha', 'O_III', 'H_beta', etc.]\n        density_variation: Amount of density variation (0-1)\n\n    Returns:\n        Created nebula material\n    \"\"\"\n    mat = bpy.data.materials.new(name=f\"Nebula_{'_'.join(emission_lines)}\")\n    mat.use_nodes = True\n    nodes = mat.node_tree.nodes\n    links = mat.node_tree.links\n\n    # Clear default nodes\n    nodes.clear()\n\n    # Add nodes\n    output = nodes.new(\"ShaderNodeOutputMaterial\")\n    emission = nodes.new(\"ShaderNodeVolumeEmission\")\n\n    # Calculate combined color from emission lines\n    combined_color = AstronomicalShaders._combine_emission_lines(emission_lines)\n    emission.inputs[\"Color\"].default_value = (*combined_color, 1.0)\n\n    # Add density variation with multiple noise scales\n    noise1 = nodes.new(\"ShaderNodeTexNoise\")\n    noise1.inputs[\"Scale\"].default_value = 1.0\n    noise1.inputs[\"Detail\"].default_value = 8.0\n\n    noise2 = nodes.new(\"ShaderNodeTexNoise\")\n    noise2.inputs[\"Scale\"].default_value = 5.0\n    noise2.inputs[\"Detail\"].default_value = 4.0\n\n    # Combine noises for complex structure\n    multiply = nodes.new(\"ShaderNodeMath\")\n    multiply.operation = \"MULTIPLY\"\n\n    # Color ramp for density falloff\n    ramp = nodes.new(\"ShaderNodeValToRGB\")\n    ramp.color_ramp.elements[0].position = 0.1\n    ramp.color_ramp.elements[1].position = 0.9\n\n    # Emission strength variation\n    strength_variation = nodes.new(\"ShaderNodeMath\")\n    strength_variation.operation = \"MULTIPLY\"\n    strength_variation.inputs[1].default_value = 2.0  # Base strength\n\n    # Position nodes\n    noise1.location = (-600, 0)\n    noise2.location = (-600, -200)\n    multiply.location = (-400, -100)\n    ramp.location = (-200, 0)\n    strength_variation.location = (-200, -300)\n    emission.location = (0, 0)\n    output.location = (200, 0)\n\n    # Connect nodes\n    links.new(noise1.outputs[\"Fac\"], multiply.inputs[0])\n    links.new(noise2.outputs[\"Fac\"], multiply.inputs[1])\n    links.new(multiply.outputs[\"Value\"], ramp.inputs[\"Fac\"])\n    links.new(ramp.outputs[\"Color\"], emission.inputs[\"Density\"])\n    links.new(multiply.outputs[\"Value\"], strength_variation.inputs[0])\n    links.new(strength_variation.outputs[\"Value\"], emission.inputs[\"Strength\"])\n    links.new(emission.outputs[\"Volume\"], output.inputs[\"Volume\"])\n\n    return mat\n</code></pre>"},{"location":"api/astro_lab.widgets.bpy.advanced/#astro_lab.widgets.bpy.advanced.AstronomicalShaders.create_planetary_surface_shader","title":"create_planetary_surface_shader  <code>staticmethod</code>","text":"<pre><code>create_planetary_surface_shader(\n    planet_type: str, composition: Optional[Dict[str, float]] = None\n) -&gt; Material\n</code></pre> <p>Create planetary surface shader based on composition.</p> <p>Parameters:</p> Name Type Description Default <code>planet_type</code> <code>str</code> <p>Type of planet ('terrestrial', 'gas_giant', 'ice_giant', 'moon')</p> required <code>composition</code> <code>Optional[Dict[str, float]]</code> <p>Compositional percentages {'rock': 0.7, 'ice': 0.3, etc.}</p> <code>None</code> <p>Returns:</p> Type Description <code>Material</code> <p>Created planetary material</p> Source code in <code>src\\astro_lab\\widgets\\bpy\\advanced\\shaders.py</code> <pre><code>@staticmethod  # type: ignore\ndef create_planetary_surface_shader(\n    planet_type: str, composition: Optional[Dict[str, float]] = None\n) -&gt; bpy.types.Material:  # type: ignore\n    \"\"\"\n    Create planetary surface shader based on composition.\n\n    Args:\n        planet_type: Type of planet ('terrestrial', 'gas_giant', 'ice_giant', 'moon')\n        composition: Compositional percentages {'rock': 0.7, 'ice': 0.3, etc.}\n\n    Returns:\n        Created planetary material\n    \"\"\"\n    mat = bpy.data.materials.new(name=f\"Planet_{planet_type}\")\n    mat.use_nodes = True\n    nodes = mat.node_tree.nodes\n    links = mat.node_tree.links\n\n    # Clear default nodes\n    nodes.clear()\n\n    # Add nodes\n    output = nodes.new(\"ShaderNodeOutputMaterial\")\n    bsdf = nodes.new(\"ShaderNodeBsdfPrincipled\")\n\n    # Default compositions\n    if composition is None:\n        composition = AstronomicalShaders._get_default_composition(planet_type)\n\n    # Base color from composition\n    base_color = AstronomicalShaders._calculate_surface_color(composition)\n    bsdf.inputs[\"Base Color\"].default_value = (*base_color, 1.0)\n\n    # Surface properties based on planet type\n    if planet_type == \"terrestrial\":\n        bsdf.inputs[\"Roughness\"].default_value = 0.8\n        bsdf.inputs[\"Metallic\"].default_value = 0.1\n        bsdf.inputs[\"Specular\"].default_value = 0.3\n\n    elif planet_type == \"gas_giant\":\n        # Gas giants have cloudy, banded surfaces\n        bsdf.inputs[\"Roughness\"].default_value = 0.3\n        bsdf.inputs[\"Metallic\"].default_value = 0.0\n        bsdf.inputs[\"Specular\"].default_value = 0.8\n\n        # Add cloud bands\n        AstronomicalShaders._add_gas_giant_bands(nodes, links, bsdf)\n\n    elif planet_type == \"ice_giant\":\n        bsdf.inputs[\"Roughness\"].default_value = 0.1\n        bsdf.inputs[\"Metallic\"].default_value = 0.0\n        bsdf.inputs[\"Specular\"].default_value = 1.0\n        bsdf.inputs[\"Transmission\"].default_value = 0.3\n\n    else:  # moon\n        bsdf.inputs[\"Roughness\"].default_value = 0.9\n        bsdf.inputs[\"Metallic\"].default_value = 0.05\n        bsdf.inputs[\"Specular\"].default_value = 0.1\n\n    # Add surface texture variation\n    noise = nodes.new(\"ShaderNodeTexNoise\")\n    noise.inputs[\"Scale\"].default_value = 5.0\n    noise.inputs[\"Detail\"].default_value = 6.0\n\n    # Mix with base color\n    color_mix = nodes.new(\"ShaderNodeMixRGB\")\n    color_mix.blend_type = \"MULTIPLY\"\n    color_mix.inputs[\"Fac\"].default_value = 0.3\n\n    # Position nodes\n    noise.location = (-400, -200)\n    color_mix.location = (-200, 0)\n    bsdf.location = (0, 0)\n    output.location = (200, 0)\n\n    # Connect nodes\n    links.new(noise.outputs[\"Color\"], color_mix.inputs[\"Color2\"])\n    links.new(color_mix.outputs[\"Color\"], bsdf.inputs[\"Base Color\"])\n    links.new(bsdf.outputs[\"BSDF\"], output.inputs[\"Surface\"])\n\n    return mat\n</code></pre>"},{"location":"api/astro_lab.widgets.bpy.advanced/#astro_lab.widgets.bpy.advanced.AstronomicalShaders.create_stellar_blackbody_shader","title":"create_stellar_blackbody_shader  <code>staticmethod</code>","text":"<pre><code>create_stellar_blackbody_shader(\n    temperature: float, luminosity: float = 1.0, stellar_class: Optional[str] = None\n) -&gt; Material\n</code></pre> <p>Create physically accurate stellar shader based on blackbody radiation.</p> <p>Parameters:</p> Name Type Description Default <code>temperature</code> <code>float</code> <p>Stellar temperature in Kelvin</p> required <code>luminosity</code> <code>float</code> <p>Stellar luminosity relative to Sun</p> <code>1.0</code> <code>stellar_class</code> <code>Optional[str]</code> <p>Optional spectral class (O, B, A, F, G, K, M)</p> <code>None</code> <p>Returns:</p> Type Description <code>Material</code> <p>Created stellar material</p> Source code in <code>src\\astro_lab\\widgets\\bpy\\advanced\\shaders.py</code> <pre><code>@staticmethod  # type: ignore\ndef create_stellar_blackbody_shader(\n    temperature: float, luminosity: float = 1.0, stellar_class: Optional[str] = None\n) -&gt; bpy.types.Material:  # type: ignore\n    \"\"\"\n    Create physically accurate stellar shader based on blackbody radiation.\n\n    Args:\n        temperature: Stellar temperature in Kelvin\n        luminosity: Stellar luminosity relative to Sun\n        stellar_class: Optional spectral class (O, B, A, F, G, K, M)\n\n    Returns:\n        Created stellar material\n    \"\"\"\n    # Determine spectral class if not provided\n    if stellar_class is None:\n        stellar_class = AstronomicalShaders._classify_star_by_temperature(\n            temperature\n        )\n\n    mat = bpy.data.materials.new(name=f\"Star_{stellar_class}_{temperature}K\")\n    mat.use_nodes = True\n    nodes = mat.node_tree.nodes\n    links = mat.node_tree.links\n\n    # Clear default nodes\n    nodes.clear()\n\n    # Add nodes\n    output = nodes.new(\"ShaderNodeOutputMaterial\")\n    emission = nodes.new(\"ShaderNodeEmission\")\n    blackbody = nodes.new(\"ShaderNodeBlackbody\")\n\n    # Set temperature\n    blackbody.inputs[\"Temperature\"].default_value = temperature\n\n    # Calculate emission strength from luminosity\n    # Using Stefan-Boltzmann law approximation\n    strength = luminosity * 10.0  # Base strength scaling\n    emission.inputs[\"Strength\"].default_value = strength\n\n    # Add stellar surface variation\n    noise = nodes.new(\"ShaderNodeTexNoise\")\n    noise.inputs[\"Scale\"].default_value = 50.0\n    noise.inputs[\"Detail\"].default_value = 8.0\n\n    # Color variation for stellar activity\n    color_mix = nodes.new(\"ShaderNodeMixRGB\")\n    color_mix.blend_type = \"ADD\"\n    color_mix.inputs[\"Fac\"].default_value = 0.1\n\n    # Hot spots for stellar activity\n    math_node = nodes.new(\"ShaderNodeMath\")\n    math_node.operation = \"MULTIPLY\"\n    math_node.inputs[1].default_value = 1.2  # Activity scaling\n\n    # Position nodes\n    blackbody.location = (-400, 100)\n    noise.location = (-400, -200)\n    color_mix.location = (-200, 0)\n    math_node.location = (-200, -300)\n    emission.location = (0, 0)\n    output.location = (200, 0)\n\n    # Connect nodes\n    links.new(blackbody.outputs[\"Color\"], color_mix.inputs[\"Color1\"])\n    links.new(noise.outputs[\"Color\"], color_mix.inputs[\"Color2\"])\n    links.new(color_mix.outputs[\"Color\"], emission.inputs[\"Color\"])\n    links.new(noise.outputs[\"Fac\"], math_node.inputs[0])\n    links.new(math_node.outputs[\"Value\"], emission.inputs[\"Strength\"])\n    links.new(emission.outputs[\"Emission\"], output.inputs[\"Surface\"])\n\n    return mat\n</code></pre>"},{"location":"api/astro_lab.widgets.bpy.advanced/#astro_lab.widgets.bpy.advanced.FuturisticMaterials","title":"FuturisticMaterials","text":"<p>Create futuristic materials for astronomical visualization</p> <p>Methods:</p> Name Description <code>create_energy_field_material</code> <p>Create energy field material with pulsing effect.</p> <code>create_force_field_material</code> <p>Create force field material with ripple effect.</p> <code>create_glass_material</code> <p>Create realistic glass material.</p> <code>create_holographic_material</code> <p>Create holographic material with scanning effect.</p> <code>create_iridescent_material</code> <p>Create iridescent material with chromatic shift.</p> <code>create_metallic_material</code> <p>Create metallic material with optional anisotropy.</p> Source code in <code>src\\astro_lab\\widgets\\bpy\\advanced\\futuristic_materials.py</code> <pre><code>class FuturisticMaterials:\n    \"\"\"Create futuristic materials for astronomical visualization\"\"\"\n\n    @staticmethod\n    def create_iridescent_material(\n        base_color: Tuple[float, float, float] = (0.8, 0.2, 0.8),\n        iridescence_strength: float = 1.0,\n        iridescence_shift: float = 0.0,\n    ) -&gt; bpy.types.Material:  # type: ignore\n        \"\"\"\n        Create iridescent material with chromatic shift.\n\n        Args:\n            base_color: Base material color\n            iridescence_strength: Strength of iridescent effect\n            iridescence_shift: Color shift amount\n\n        Returns:\n            Created iridescent material\n        \"\"\"\n        mat = bpy.data.materials.new(name=\"IridescentMaterial\")\n        mat.use_nodes = True\n        nodes = mat.node_tree.nodes\n        links = mat.node_tree.links\n\n        # Clear default nodes\n        nodes.clear()\n\n        # Add nodes\n        output = nodes.new(\"ShaderNodeOutputMaterial\")\n        bsdf = nodes.new(\"ShaderNodeBsdfPrincipled\")\n\n        # Set base properties\n        bsdf.inputs[\"Base Color\"].default_value = (*base_color, 1.0)\n        bsdf.inputs[\"Metallic\"].default_value = 0.8\n        bsdf.inputs[\"Roughness\"].default_value = 0.1\n        bsdf.inputs[\"Specular\"].default_value = 1.0\n\n        # Add iridescence\n        bsdf.inputs[\"Iridescence\"].default_value = iridescence_strength\n        bsdf.inputs[\"Iridescence IOR\"].default_value = 1.3 + iridescence_shift\n\n        # Add fresnel for enhanced effect\n        fresnel = nodes.new(\"ShaderNodeFresnel\")\n        fresnel.inputs[\"IOR\"].default_value = 1.5\n\n        # Mix fresnel with base color\n        mix_rgb = nodes.new(\"ShaderNodeMixRGB\")\n        mix_rgb.blend_type = \"ADD\"\n        mix_rgb.inputs[\"Fac\"].default_value = 0.3\n\n        # Position nodes\n        fresnel.location = (-400, 200)\n        mix_rgb.location = (-200, 0)\n        bsdf.location = (0, 0)\n        output.location = (200, 0)\n\n        # Connect nodes\n        links.new(fresnel.outputs[\"Fac\"], mix_rgb.inputs[\"Fac\"])\n        links.new(mix_rgb.outputs[\"Color\"], bsdf.inputs[\"Base Color\"])\n        links.new(bsdf.outputs[\"BSDF\"], output.inputs[\"Surface\"])\n\n        return mat\n\n    @staticmethod\n    def create_glass_material(\n        color: Tuple[float, float, float] = (0.9, 0.95, 1.0),\n        transmission: float = 0.95,\n        ior: float = 1.45,\n        roughness: float = 0.0,\n    ) -&gt; bpy.types.Material:  # type: ignore\n        \"\"\"\n        Create realistic glass material.\n\n        Args:\n            color: Glass color\n            transmission: Transmission strength\n            ior: Index of refraction\n            roughness: Surface roughness\n\n        Returns:\n            Created glass material\n        \"\"\"\n        mat = bpy.data.materials.new(name=\"GlassMaterial\")\n        mat.use_nodes = True\n        nodes = mat.node_tree.nodes\n        links = mat.node_tree.links\n\n        # Clear default nodes\n        nodes.clear()\n\n        # Add nodes\n        output = nodes.new(\"ShaderNodeOutputMaterial\")\n        bsdf = nodes.new(\"ShaderNodeBsdfPrincipled\")\n\n        # Set glass properties\n        bsdf.inputs[\"Base Color\"].default_value = (*color, 1.0)\n        bsdf.inputs[\"Transmission\"].default_value = transmission\n        bsdf.inputs[\"IOR\"].default_value = ior\n        bsdf.inputs[\"Roughness\"].default_value = roughness\n        bsdf.inputs[\"Specular\"].default_value = 1.0\n\n        # Add subtle color variation\n        noise = nodes.new(\"ShaderNodeTexNoise\")\n        noise.inputs[\"Scale\"].default_value = 10.0\n        noise.inputs[\"Detail\"].default_value = 2.0\n\n        # Mix with base color\n        mix_rgb = nodes.new(\"ShaderNodeMixRGB\")\n        mix_rgb.blend_type = \"MULTIPLY\"\n        mix_rgb.inputs[\"Fac\"].default_value = 0.1\n\n        # Position nodes\n        noise.location = (-400, 0)\n        mix_rgb.location = (-200, 0)\n        bsdf.location = (0, 0)\n        output.location = (200, 0)\n\n        # Connect nodes\n        links.new(noise.outputs[\"Color\"], mix_rgb.inputs[\"Color2\"])\n        links.new(mix_rgb.outputs[\"Color\"], bsdf.inputs[\"Base Color\"])\n        links.new(bsdf.outputs[\"BSDF\"], output.inputs[\"Surface\"])\n\n        return mat\n\n    @staticmethod\n    def create_metallic_material(\n        color: Tuple[float, float, float] = (0.8, 0.8, 0.8),\n        metallic: float = 1.0,\n        roughness: float = 0.2,\n        anisotropy: float = 0.0,\n    ) -&gt; bpy.types.Material:  # type: ignore\n        \"\"\"\n        Create metallic material with optional anisotropy.\n\n        Args:\n            color: Metal color\n            metallic: Metallic strength\n            roughness: Surface roughness\n            anisotropy: Anisotropic reflection strength\n\n        Returns:\n            Created metallic material\n        \"\"\"\n        mat = bpy.data.materials.new(name=\"MetallicMaterial\")\n        mat.use_nodes = True\n        nodes = mat.node_tree.nodes\n        links = mat.node_tree.links\n\n        # Clear default nodes\n        nodes.clear()\n\n        # Add nodes\n        output = nodes.new(\"ShaderNodeOutputMaterial\")\n        bsdf = nodes.new(\"ShaderNodeBsdfPrincipled\")\n\n        # Set metallic properties\n        bsdf.inputs[\"Base Color\"].default_value = (*color, 1.0)\n        bsdf.inputs[\"Metallic\"].default_value = metallic\n        bsdf.inputs[\"Roughness\"].default_value = roughness\n        bsdf.inputs[\"Specular\"].default_value = 0.5\n\n        # Add anisotropy if specified\n        if anisotropy &gt; 0.0:\n            bsdf.inputs[\"Anisotropic\"].default_value = anisotropy\n            bsdf.inputs[\"Anisotropic Rotation\"].default_value = 0.0\n\n            # Add anisotropic texture for variation\n            tex_coord = nodes.new(\"ShaderNodeTexCoord\")\n            mapping = nodes.new(\"ShaderNodeMapping\")\n\n            # Position nodes\n            tex_coord.location = (-600, 0)\n            mapping.location = (-400, 0)\n            bsdf.location = (0, 0)\n            output.location = (200, 0)\n\n            # Connect nodes\n            links.new(tex_coord.outputs[\"Generated\"], mapping.inputs[\"Vector\"])\n            links.new(mapping.outputs[\"Vector\"], bsdf.inputs[\"Tangent\"])\n        else:\n            # Position nodes\n            bsdf.location = (0, 0)\n            output.location = (200, 0)\n\n        # Connect to output\n        links.new(bsdf.outputs[\"BSDF\"], output.inputs[\"Surface\"])\n\n        return mat\n\n    @staticmethod\n    def create_holographic_material(\n        base_color: Tuple[float, float, float] = (0.2, 0.8, 1.0),\n        hologram_strength: float = 1.0,\n        scan_speed: float = 1.0,\n    ) -&gt; bpy.types.Material:  # type: ignore\n        \"\"\"\n        Create holographic material with scanning effect.\n\n        Args:\n            base_color: Base hologram color\n            hologram_strength: Strength of holographic effect\n            scan_speed: Speed of scanning lines\n\n        Returns:\n            Created holographic material\n        \"\"\"\n        mat = bpy.data.materials.new(name=\"HolographicMaterial\")\n        mat.use_nodes = True\n        nodes = mat.node_tree.nodes\n        links = mat.node_tree.links\n\n        # Clear default nodes\n        nodes.clear()\n\n        # Add nodes\n        output = nodes.new(\"ShaderNodeOutputMaterial\")\n        emission = nodes.new(\"ShaderNodeEmission\")\n        transparent = nodes.new(\"ShaderNodeBsdfTransparent\")\n        mix_shader = nodes.new(\"ShaderNodeMixShader\")\n\n        # Create scanning line effect\n        tex_coord = nodes.new(\"ShaderNodeTexCoord\")\n        wave = nodes.new(\"ShaderNodeTexWave\")\n        wave.wave_type = \"SAW\"\n        wave.inputs[\"Scale\"].default_value = 50.0 * scan_speed\n        wave.inputs[\"Distortion\"].default_value = 0.5\n\n        # Color ramp for scan lines\n        ramp = nodes.new(\"ShaderNodeValToRGB\")\n        ramp.color_ramp.elements[0].position = 0.4\n        ramp.color_ramp.elements[1].position = 0.6\n\n        # Add noise for holographic interference\n        noise = nodes.new(\"ShaderNodeTexNoise\")\n        noise.inputs[\"Scale\"].default_value = 100.0\n        noise.inputs[\"Detail\"].default_value = 10.0\n\n        # Mix noise with scan lines\n        mix_noise = nodes.new(\"ShaderNodeMixRGB\")\n        mix_noise.blend_type = \"MULTIPLY\"\n        mix_noise.inputs[\"Fac\"].default_value = 0.3\n\n        # Position nodes\n        tex_coord.location = (-800, 0)\n        wave.location = (-600, 0)\n        ramp.location = (-400, 0)\n        noise.location = (-600, -200)\n        mix_noise.location = (-200, 0)\n        emission.location = (0, 100)\n        transparent.location = (0, -100)\n        mix_shader.location = (200, 0)\n        output.location = (400, 0)\n\n        # Connect nodes\n        links.new(tex_coord.outputs[\"Generated\"], wave.inputs[\"Vector\"])\n        links.new(wave.outputs[\"Color\"], ramp.inputs[\"Fac\"])\n        links.new(noise.outputs[\"Color\"], mix_noise.inputs[\"Color2\"])\n        links.new(ramp.outputs[\"Color\"], mix_noise.inputs[\"Color1\"])\n        links.new(mix_noise.outputs[\"Color\"], emission.inputs[\"Color\"])\n        links.new(emission.outputs[\"Emission\"], mix_shader.inputs[1])\n        links.new(transparent.outputs[\"BSDF\"], mix_shader.inputs[2])\n        links.new(mix_shader.outputs[\"Shader\"], output.inputs[\"Surface\"])\n\n        # Set emission strength\n        emission.inputs[\"Strength\"].default_value = hologram_strength * 2.0\n\n        # Set mix factor\n        mix_shader.inputs[\"Fac\"].default_value = 0.7\n\n        return mat\n\n    @staticmethod\n    def create_energy_field_material(\n        color: Tuple[float, float, float] = (0.2, 0.8, 1.0),\n        energy_strength: float = 2.0,\n        pulse_speed: float = 1.0,\n    ) -&gt; bpy.types.Material:  # type: ignore\n        \"\"\"\n        Create energy field material with pulsing effect.\n\n        Args:\n            color: Energy field color\n            energy_strength: Strength of energy emission\n            pulse_speed: Speed of pulsing effect\n\n        Returns:\n            Created energy field material\n        \"\"\"\n        mat = bpy.data.materials.new(name=\"EnergyFieldMaterial\")\n        mat.use_nodes = True\n        nodes = mat.node_tree.nodes\n        links = mat.node_tree.links\n\n        # Clear default nodes\n        nodes.clear()\n\n        # Add nodes\n        output = nodes.new(\"ShaderNodeOutputMaterial\")\n        emission = nodes.new(\"ShaderNodeEmission\")\n\n        # Create pulsing effect\n        sine = nodes.new(\"ShaderNodeMath\")\n        sine.operation = \"SINE\"\n        sine.inputs[1].default_value = pulse_speed\n\n        # Add time input for animation\n        time = nodes.new(\"ShaderNodeValue\")\n        time.outputs[0].default_value = 0.0  # Will be animated\n\n        # Scale and offset sine wave\n        scale = nodes.new(\"ShaderNodeMath\")\n        scale.operation = \"MULTIPLY\"\n        scale.inputs[1].default_value = 0.5\n\n        offset = nodes.new(\"ShaderNodeMath\")\n        offset.operation = \"ADD\"\n        offset.inputs[1].default_value = 0.5\n\n        # Add noise for energy variation\n        noise = nodes.new(\"ShaderNodeTexNoise\")\n        noise.inputs[\"Scale\"].default_value = 5.0\n        noise.inputs[\"Detail\"].default_value = 8.0\n\n        # Mix noise with pulse\n        mix = nodes.new(\"ShaderNodeMixRGB\")\n        mix.blend_type = \"MULTIPLY\"\n        mix.inputs[\"Fac\"].default_value = 0.3\n\n        # Position nodes\n        time.location = (-800, 0)\n        sine.location = (-600, 0)\n        scale.location = (-400, 0)\n        offset.location = (-200, 0)\n        noise.location = (-600, -200)\n        mix.location = (0, 0)\n        emission.location = (200, 0)\n        output.location = (400, 0)\n\n        # Connect nodes\n        links.new(time.outputs[0], sine.inputs[0])\n        links.new(sine.outputs[0], scale.inputs[0])\n        links.new(scale.outputs[0], offset.inputs[0])\n        links.new(noise.outputs[\"Color\"], mix.inputs[\"Color2\"])\n        links.new(offset.outputs[0], mix.inputs[\"Color1\"])\n        links.new(mix.outputs[\"Color\"], emission.inputs[\"Color\"])\n        links.new(emission.outputs[\"Emission\"], output.inputs[\"Surface\"])\n\n        # Set emission strength\n        emission.inputs[\"Strength\"].default_value = energy_strength\n\n        return mat\n\n    @staticmethod\n    def create_force_field_material(\n        color: Tuple[float, float, float] = (0.8, 0.2, 1.0),\n        field_strength: float = 1.5,\n        ripple_speed: float = 2.0,\n    ) -&gt; bpy.types.Material:  # type: ignore\n        \"\"\"\n        Create force field material with ripple effect.\n\n        Args:\n            color: Force field color\n            field_strength: Strength of field emission\n            ripple_speed: Speed of ripple waves\n\n        Returns:\n            Created force field material\n        \"\"\"\n        mat = bpy.data.materials.new(name=\"ForceFieldMaterial\")\n        mat.use_nodes = True\n        nodes = mat.node_tree.nodes\n        links = mat.node_tree.links\n\n        # Clear default nodes\n        nodes.clear()\n\n        # Add nodes\n        output = nodes.new(\"ShaderNodeOutputMaterial\")\n        emission = nodes.new(\"ShaderNodeEmission\")\n        transparent = nodes.new(\"ShaderNodeBsdfTransparent\")\n        mix_shader = nodes.new(\"ShaderNodeMixShader\")\n\n        # Create ripple effect\n        tex_coord = nodes.new(\"ShaderNodeTexCoord\")\n        wave = nodes.new(\"ShaderNodeTexWave\")\n        wave.wave_type = \"RINGS\"\n        wave.inputs[\"Scale\"].default_value = 20.0 * ripple_speed\n        wave.inputs[\"Distortion\"].default_value = 0.3\n\n        # Add time for animation\n        time = nodes.new(\"ShaderNodeValue\")\n        time.outputs[0].default_value = 0.0\n\n        # Mix time with coordinates\n        mix_time = nodes.new(\"ShaderNodeMixRGB\")\n        mix_time.blend_type = \"ADD\"\n        mix_time.inputs[\"Fac\"].default_value = 0.1\n\n        # Color ramp for ripple visibility\n        ramp = nodes.new(\"ShaderNodeValToRGB\")\n        ramp.color_ramp.elements[0].position = 0.3\n        ramp.color_ramp.elements[1].position = 0.7\n\n        # Add noise for field variation\n        noise = nodes.new(\"ShaderNodeTexNoise\")\n        noise.inputs[\"Scale\"].default_value = 10.0\n        noise.inputs[\"Detail\"].default_value = 6.0\n\n        # Mix noise with ripples\n        mix_noise = nodes.new(\"ShaderNodeMixRGB\")\n        mix_noise.blend_type = \"MULTIPLY\"\n        mix_noise.inputs[\"Fac\"].default_value = 0.4\n\n        # Position nodes\n        tex_coord.location = (-800, 0)\n        time.location = (-800, -200)\n        mix_time.location = (-600, 0)\n        wave.location = (-400, 0)\n        ramp.location = (-200, 0)\n        noise.location = (-600, -200)\n        mix_noise.location = (0, 0)\n        emission.location = (200, 100)\n        transparent.location = (200, -100)\n        mix_shader.location = (400, 0)\n        output.location = (600, 0)\n\n        # Connect nodes\n        links.new(tex_coord.outputs[\"Generated\"], mix_time.inputs[\"Color1\"])\n        links.new(time.outputs[0], mix_time.inputs[\"Color2\"])\n        links.new(mix_time.outputs[\"Color\"], wave.inputs[\"Vector\"])\n        links.new(wave.outputs[\"Color\"], ramp.inputs[\"Fac\"])\n        links.new(noise.outputs[\"Color\"], mix_noise.inputs[\"Color2\"])\n        links.new(ramp.outputs[\"Color\"], mix_noise.inputs[\"Color1\"])\n        links.new(mix_noise.outputs[\"Color\"], emission.inputs[\"Color\"])\n        links.new(emission.outputs[\"Emission\"], mix_shader.inputs[1])\n        links.new(transparent.outputs[\"BSDF\"], mix_shader.inputs[2])\n        links.new(mix_shader.outputs[\"Shader\"], output.inputs[\"Surface\"])\n\n        # Set emission strength\n        emission.inputs[\"Strength\"].default_value = field_strength\n\n        # Set mix factor\n        mix_shader.inputs[\"Fac\"].default_value = 0.6\n\n        return mat\n</code></pre>"},{"location":"api/astro_lab.widgets.bpy.advanced/#astro_lab.widgets.bpy.advanced.FuturisticMaterials.create_energy_field_material","title":"create_energy_field_material  <code>staticmethod</code>","text":"<pre><code>create_energy_field_material(\n    color: Tuple[float, float, float] = (0.2, 0.8, 1.0),\n    energy_strength: float = 2.0,\n    pulse_speed: float = 1.0,\n) -&gt; Material\n</code></pre> <p>Create energy field material with pulsing effect.</p> <p>Parameters:</p> Name Type Description Default <code>color</code> <code>Tuple[float, float, float]</code> <p>Energy field color</p> <code>(0.2, 0.8, 1.0)</code> <code>energy_strength</code> <code>float</code> <p>Strength of energy emission</p> <code>2.0</code> <code>pulse_speed</code> <code>float</code> <p>Speed of pulsing effect</p> <code>1.0</code> <p>Returns:</p> Type Description <code>Material</code> <p>Created energy field material</p> Source code in <code>src\\astro_lab\\widgets\\bpy\\advanced\\futuristic_materials.py</code> <pre><code>@staticmethod\ndef create_energy_field_material(\n    color: Tuple[float, float, float] = (0.2, 0.8, 1.0),\n    energy_strength: float = 2.0,\n    pulse_speed: float = 1.0,\n) -&gt; bpy.types.Material:  # type: ignore\n    \"\"\"\n    Create energy field material with pulsing effect.\n\n    Args:\n        color: Energy field color\n        energy_strength: Strength of energy emission\n        pulse_speed: Speed of pulsing effect\n\n    Returns:\n        Created energy field material\n    \"\"\"\n    mat = bpy.data.materials.new(name=\"EnergyFieldMaterial\")\n    mat.use_nodes = True\n    nodes = mat.node_tree.nodes\n    links = mat.node_tree.links\n\n    # Clear default nodes\n    nodes.clear()\n\n    # Add nodes\n    output = nodes.new(\"ShaderNodeOutputMaterial\")\n    emission = nodes.new(\"ShaderNodeEmission\")\n\n    # Create pulsing effect\n    sine = nodes.new(\"ShaderNodeMath\")\n    sine.operation = \"SINE\"\n    sine.inputs[1].default_value = pulse_speed\n\n    # Add time input for animation\n    time = nodes.new(\"ShaderNodeValue\")\n    time.outputs[0].default_value = 0.0  # Will be animated\n\n    # Scale and offset sine wave\n    scale = nodes.new(\"ShaderNodeMath\")\n    scale.operation = \"MULTIPLY\"\n    scale.inputs[1].default_value = 0.5\n\n    offset = nodes.new(\"ShaderNodeMath\")\n    offset.operation = \"ADD\"\n    offset.inputs[1].default_value = 0.5\n\n    # Add noise for energy variation\n    noise = nodes.new(\"ShaderNodeTexNoise\")\n    noise.inputs[\"Scale\"].default_value = 5.0\n    noise.inputs[\"Detail\"].default_value = 8.0\n\n    # Mix noise with pulse\n    mix = nodes.new(\"ShaderNodeMixRGB\")\n    mix.blend_type = \"MULTIPLY\"\n    mix.inputs[\"Fac\"].default_value = 0.3\n\n    # Position nodes\n    time.location = (-800, 0)\n    sine.location = (-600, 0)\n    scale.location = (-400, 0)\n    offset.location = (-200, 0)\n    noise.location = (-600, -200)\n    mix.location = (0, 0)\n    emission.location = (200, 0)\n    output.location = (400, 0)\n\n    # Connect nodes\n    links.new(time.outputs[0], sine.inputs[0])\n    links.new(sine.outputs[0], scale.inputs[0])\n    links.new(scale.outputs[0], offset.inputs[0])\n    links.new(noise.outputs[\"Color\"], mix.inputs[\"Color2\"])\n    links.new(offset.outputs[0], mix.inputs[\"Color1\"])\n    links.new(mix.outputs[\"Color\"], emission.inputs[\"Color\"])\n    links.new(emission.outputs[\"Emission\"], output.inputs[\"Surface\"])\n\n    # Set emission strength\n    emission.inputs[\"Strength\"].default_value = energy_strength\n\n    return mat\n</code></pre>"},{"location":"api/astro_lab.widgets.bpy.advanced/#astro_lab.widgets.bpy.advanced.FuturisticMaterials.create_force_field_material","title":"create_force_field_material  <code>staticmethod</code>","text":"<pre><code>create_force_field_material(\n    color: Tuple[float, float, float] = (0.8, 0.2, 1.0),\n    field_strength: float = 1.5,\n    ripple_speed: float = 2.0,\n) -&gt; Material\n</code></pre> <p>Create force field material with ripple effect.</p> <p>Parameters:</p> Name Type Description Default <code>color</code> <code>Tuple[float, float, float]</code> <p>Force field color</p> <code>(0.8, 0.2, 1.0)</code> <code>field_strength</code> <code>float</code> <p>Strength of field emission</p> <code>1.5</code> <code>ripple_speed</code> <code>float</code> <p>Speed of ripple waves</p> <code>2.0</code> <p>Returns:</p> Type Description <code>Material</code> <p>Created force field material</p> Source code in <code>src\\astro_lab\\widgets\\bpy\\advanced\\futuristic_materials.py</code> <pre><code>@staticmethod\ndef create_force_field_material(\n    color: Tuple[float, float, float] = (0.8, 0.2, 1.0),\n    field_strength: float = 1.5,\n    ripple_speed: float = 2.0,\n) -&gt; bpy.types.Material:  # type: ignore\n    \"\"\"\n    Create force field material with ripple effect.\n\n    Args:\n        color: Force field color\n        field_strength: Strength of field emission\n        ripple_speed: Speed of ripple waves\n\n    Returns:\n        Created force field material\n    \"\"\"\n    mat = bpy.data.materials.new(name=\"ForceFieldMaterial\")\n    mat.use_nodes = True\n    nodes = mat.node_tree.nodes\n    links = mat.node_tree.links\n\n    # Clear default nodes\n    nodes.clear()\n\n    # Add nodes\n    output = nodes.new(\"ShaderNodeOutputMaterial\")\n    emission = nodes.new(\"ShaderNodeEmission\")\n    transparent = nodes.new(\"ShaderNodeBsdfTransparent\")\n    mix_shader = nodes.new(\"ShaderNodeMixShader\")\n\n    # Create ripple effect\n    tex_coord = nodes.new(\"ShaderNodeTexCoord\")\n    wave = nodes.new(\"ShaderNodeTexWave\")\n    wave.wave_type = \"RINGS\"\n    wave.inputs[\"Scale\"].default_value = 20.0 * ripple_speed\n    wave.inputs[\"Distortion\"].default_value = 0.3\n\n    # Add time for animation\n    time = nodes.new(\"ShaderNodeValue\")\n    time.outputs[0].default_value = 0.0\n\n    # Mix time with coordinates\n    mix_time = nodes.new(\"ShaderNodeMixRGB\")\n    mix_time.blend_type = \"ADD\"\n    mix_time.inputs[\"Fac\"].default_value = 0.1\n\n    # Color ramp for ripple visibility\n    ramp = nodes.new(\"ShaderNodeValToRGB\")\n    ramp.color_ramp.elements[0].position = 0.3\n    ramp.color_ramp.elements[1].position = 0.7\n\n    # Add noise for field variation\n    noise = nodes.new(\"ShaderNodeTexNoise\")\n    noise.inputs[\"Scale\"].default_value = 10.0\n    noise.inputs[\"Detail\"].default_value = 6.0\n\n    # Mix noise with ripples\n    mix_noise = nodes.new(\"ShaderNodeMixRGB\")\n    mix_noise.blend_type = \"MULTIPLY\"\n    mix_noise.inputs[\"Fac\"].default_value = 0.4\n\n    # Position nodes\n    tex_coord.location = (-800, 0)\n    time.location = (-800, -200)\n    mix_time.location = (-600, 0)\n    wave.location = (-400, 0)\n    ramp.location = (-200, 0)\n    noise.location = (-600, -200)\n    mix_noise.location = (0, 0)\n    emission.location = (200, 100)\n    transparent.location = (200, -100)\n    mix_shader.location = (400, 0)\n    output.location = (600, 0)\n\n    # Connect nodes\n    links.new(tex_coord.outputs[\"Generated\"], mix_time.inputs[\"Color1\"])\n    links.new(time.outputs[0], mix_time.inputs[\"Color2\"])\n    links.new(mix_time.outputs[\"Color\"], wave.inputs[\"Vector\"])\n    links.new(wave.outputs[\"Color\"], ramp.inputs[\"Fac\"])\n    links.new(noise.outputs[\"Color\"], mix_noise.inputs[\"Color2\"])\n    links.new(ramp.outputs[\"Color\"], mix_noise.inputs[\"Color1\"])\n    links.new(mix_noise.outputs[\"Color\"], emission.inputs[\"Color\"])\n    links.new(emission.outputs[\"Emission\"], mix_shader.inputs[1])\n    links.new(transparent.outputs[\"BSDF\"], mix_shader.inputs[2])\n    links.new(mix_shader.outputs[\"Shader\"], output.inputs[\"Surface\"])\n\n    # Set emission strength\n    emission.inputs[\"Strength\"].default_value = field_strength\n\n    # Set mix factor\n    mix_shader.inputs[\"Fac\"].default_value = 0.6\n\n    return mat\n</code></pre>"},{"location":"api/astro_lab.widgets.bpy.advanced/#astro_lab.widgets.bpy.advanced.FuturisticMaterials.create_glass_material","title":"create_glass_material  <code>staticmethod</code>","text":"<pre><code>create_glass_material(\n    color: Tuple[float, float, float] = (0.9, 0.95, 1.0),\n    transmission: float = 0.95,\n    ior: float = 1.45,\n    roughness: float = 0.0,\n) -&gt; Material\n</code></pre> <p>Create realistic glass material.</p> <p>Parameters:</p> Name Type Description Default <code>color</code> <code>Tuple[float, float, float]</code> <p>Glass color</p> <code>(0.9, 0.95, 1.0)</code> <code>transmission</code> <code>float</code> <p>Transmission strength</p> <code>0.95</code> <code>ior</code> <code>float</code> <p>Index of refraction</p> <code>1.45</code> <code>roughness</code> <code>float</code> <p>Surface roughness</p> <code>0.0</code> <p>Returns:</p> Type Description <code>Material</code> <p>Created glass material</p> Source code in <code>src\\astro_lab\\widgets\\bpy\\advanced\\futuristic_materials.py</code> <pre><code>@staticmethod\ndef create_glass_material(\n    color: Tuple[float, float, float] = (0.9, 0.95, 1.0),\n    transmission: float = 0.95,\n    ior: float = 1.45,\n    roughness: float = 0.0,\n) -&gt; bpy.types.Material:  # type: ignore\n    \"\"\"\n    Create realistic glass material.\n\n    Args:\n        color: Glass color\n        transmission: Transmission strength\n        ior: Index of refraction\n        roughness: Surface roughness\n\n    Returns:\n        Created glass material\n    \"\"\"\n    mat = bpy.data.materials.new(name=\"GlassMaterial\")\n    mat.use_nodes = True\n    nodes = mat.node_tree.nodes\n    links = mat.node_tree.links\n\n    # Clear default nodes\n    nodes.clear()\n\n    # Add nodes\n    output = nodes.new(\"ShaderNodeOutputMaterial\")\n    bsdf = nodes.new(\"ShaderNodeBsdfPrincipled\")\n\n    # Set glass properties\n    bsdf.inputs[\"Base Color\"].default_value = (*color, 1.0)\n    bsdf.inputs[\"Transmission\"].default_value = transmission\n    bsdf.inputs[\"IOR\"].default_value = ior\n    bsdf.inputs[\"Roughness\"].default_value = roughness\n    bsdf.inputs[\"Specular\"].default_value = 1.0\n\n    # Add subtle color variation\n    noise = nodes.new(\"ShaderNodeTexNoise\")\n    noise.inputs[\"Scale\"].default_value = 10.0\n    noise.inputs[\"Detail\"].default_value = 2.0\n\n    # Mix with base color\n    mix_rgb = nodes.new(\"ShaderNodeMixRGB\")\n    mix_rgb.blend_type = \"MULTIPLY\"\n    mix_rgb.inputs[\"Fac\"].default_value = 0.1\n\n    # Position nodes\n    noise.location = (-400, 0)\n    mix_rgb.location = (-200, 0)\n    bsdf.location = (0, 0)\n    output.location = (200, 0)\n\n    # Connect nodes\n    links.new(noise.outputs[\"Color\"], mix_rgb.inputs[\"Color2\"])\n    links.new(mix_rgb.outputs[\"Color\"], bsdf.inputs[\"Base Color\"])\n    links.new(bsdf.outputs[\"BSDF\"], output.inputs[\"Surface\"])\n\n    return mat\n</code></pre>"},{"location":"api/astro_lab.widgets.bpy.advanced/#astro_lab.widgets.bpy.advanced.FuturisticMaterials.create_holographic_material","title":"create_holographic_material  <code>staticmethod</code>","text":"<pre><code>create_holographic_material(\n    base_color: Tuple[float, float, float] = (0.2, 0.8, 1.0),\n    hologram_strength: float = 1.0,\n    scan_speed: float = 1.0,\n) -&gt; Material\n</code></pre> <p>Create holographic material with scanning effect.</p> <p>Parameters:</p> Name Type Description Default <code>base_color</code> <code>Tuple[float, float, float]</code> <p>Base hologram color</p> <code>(0.2, 0.8, 1.0)</code> <code>hologram_strength</code> <code>float</code> <p>Strength of holographic effect</p> <code>1.0</code> <code>scan_speed</code> <code>float</code> <p>Speed of scanning lines</p> <code>1.0</code> <p>Returns:</p> Type Description <code>Material</code> <p>Created holographic material</p> Source code in <code>src\\astro_lab\\widgets\\bpy\\advanced\\futuristic_materials.py</code> <pre><code>@staticmethod\ndef create_holographic_material(\n    base_color: Tuple[float, float, float] = (0.2, 0.8, 1.0),\n    hologram_strength: float = 1.0,\n    scan_speed: float = 1.0,\n) -&gt; bpy.types.Material:  # type: ignore\n    \"\"\"\n    Create holographic material with scanning effect.\n\n    Args:\n        base_color: Base hologram color\n        hologram_strength: Strength of holographic effect\n        scan_speed: Speed of scanning lines\n\n    Returns:\n        Created holographic material\n    \"\"\"\n    mat = bpy.data.materials.new(name=\"HolographicMaterial\")\n    mat.use_nodes = True\n    nodes = mat.node_tree.nodes\n    links = mat.node_tree.links\n\n    # Clear default nodes\n    nodes.clear()\n\n    # Add nodes\n    output = nodes.new(\"ShaderNodeOutputMaterial\")\n    emission = nodes.new(\"ShaderNodeEmission\")\n    transparent = nodes.new(\"ShaderNodeBsdfTransparent\")\n    mix_shader = nodes.new(\"ShaderNodeMixShader\")\n\n    # Create scanning line effect\n    tex_coord = nodes.new(\"ShaderNodeTexCoord\")\n    wave = nodes.new(\"ShaderNodeTexWave\")\n    wave.wave_type = \"SAW\"\n    wave.inputs[\"Scale\"].default_value = 50.0 * scan_speed\n    wave.inputs[\"Distortion\"].default_value = 0.5\n\n    # Color ramp for scan lines\n    ramp = nodes.new(\"ShaderNodeValToRGB\")\n    ramp.color_ramp.elements[0].position = 0.4\n    ramp.color_ramp.elements[1].position = 0.6\n\n    # Add noise for holographic interference\n    noise = nodes.new(\"ShaderNodeTexNoise\")\n    noise.inputs[\"Scale\"].default_value = 100.0\n    noise.inputs[\"Detail\"].default_value = 10.0\n\n    # Mix noise with scan lines\n    mix_noise = nodes.new(\"ShaderNodeMixRGB\")\n    mix_noise.blend_type = \"MULTIPLY\"\n    mix_noise.inputs[\"Fac\"].default_value = 0.3\n\n    # Position nodes\n    tex_coord.location = (-800, 0)\n    wave.location = (-600, 0)\n    ramp.location = (-400, 0)\n    noise.location = (-600, -200)\n    mix_noise.location = (-200, 0)\n    emission.location = (0, 100)\n    transparent.location = (0, -100)\n    mix_shader.location = (200, 0)\n    output.location = (400, 0)\n\n    # Connect nodes\n    links.new(tex_coord.outputs[\"Generated\"], wave.inputs[\"Vector\"])\n    links.new(wave.outputs[\"Color\"], ramp.inputs[\"Fac\"])\n    links.new(noise.outputs[\"Color\"], mix_noise.inputs[\"Color2\"])\n    links.new(ramp.outputs[\"Color\"], mix_noise.inputs[\"Color1\"])\n    links.new(mix_noise.outputs[\"Color\"], emission.inputs[\"Color\"])\n    links.new(emission.outputs[\"Emission\"], mix_shader.inputs[1])\n    links.new(transparent.outputs[\"BSDF\"], mix_shader.inputs[2])\n    links.new(mix_shader.outputs[\"Shader\"], output.inputs[\"Surface\"])\n\n    # Set emission strength\n    emission.inputs[\"Strength\"].default_value = hologram_strength * 2.0\n\n    # Set mix factor\n    mix_shader.inputs[\"Fac\"].default_value = 0.7\n\n    return mat\n</code></pre>"},{"location":"api/astro_lab.widgets.bpy.advanced/#astro_lab.widgets.bpy.advanced.FuturisticMaterials.create_iridescent_material","title":"create_iridescent_material  <code>staticmethod</code>","text":"<pre><code>create_iridescent_material(\n    base_color: Tuple[float, float, float] = (0.8, 0.2, 0.8),\n    iridescence_strength: float = 1.0,\n    iridescence_shift: float = 0.0,\n) -&gt; Material\n</code></pre> <p>Create iridescent material with chromatic shift.</p> <p>Parameters:</p> Name Type Description Default <code>base_color</code> <code>Tuple[float, float, float]</code> <p>Base material color</p> <code>(0.8, 0.2, 0.8)</code> <code>iridescence_strength</code> <code>float</code> <p>Strength of iridescent effect</p> <code>1.0</code> <code>iridescence_shift</code> <code>float</code> <p>Color shift amount</p> <code>0.0</code> <p>Returns:</p> Type Description <code>Material</code> <p>Created iridescent material</p> Source code in <code>src\\astro_lab\\widgets\\bpy\\advanced\\futuristic_materials.py</code> <pre><code>@staticmethod\ndef create_iridescent_material(\n    base_color: Tuple[float, float, float] = (0.8, 0.2, 0.8),\n    iridescence_strength: float = 1.0,\n    iridescence_shift: float = 0.0,\n) -&gt; bpy.types.Material:  # type: ignore\n    \"\"\"\n    Create iridescent material with chromatic shift.\n\n    Args:\n        base_color: Base material color\n        iridescence_strength: Strength of iridescent effect\n        iridescence_shift: Color shift amount\n\n    Returns:\n        Created iridescent material\n    \"\"\"\n    mat = bpy.data.materials.new(name=\"IridescentMaterial\")\n    mat.use_nodes = True\n    nodes = mat.node_tree.nodes\n    links = mat.node_tree.links\n\n    # Clear default nodes\n    nodes.clear()\n\n    # Add nodes\n    output = nodes.new(\"ShaderNodeOutputMaterial\")\n    bsdf = nodes.new(\"ShaderNodeBsdfPrincipled\")\n\n    # Set base properties\n    bsdf.inputs[\"Base Color\"].default_value = (*base_color, 1.0)\n    bsdf.inputs[\"Metallic\"].default_value = 0.8\n    bsdf.inputs[\"Roughness\"].default_value = 0.1\n    bsdf.inputs[\"Specular\"].default_value = 1.0\n\n    # Add iridescence\n    bsdf.inputs[\"Iridescence\"].default_value = iridescence_strength\n    bsdf.inputs[\"Iridescence IOR\"].default_value = 1.3 + iridescence_shift\n\n    # Add fresnel for enhanced effect\n    fresnel = nodes.new(\"ShaderNodeFresnel\")\n    fresnel.inputs[\"IOR\"].default_value = 1.5\n\n    # Mix fresnel with base color\n    mix_rgb = nodes.new(\"ShaderNodeMixRGB\")\n    mix_rgb.blend_type = \"ADD\"\n    mix_rgb.inputs[\"Fac\"].default_value = 0.3\n\n    # Position nodes\n    fresnel.location = (-400, 200)\n    mix_rgb.location = (-200, 0)\n    bsdf.location = (0, 0)\n    output.location = (200, 0)\n\n    # Connect nodes\n    links.new(fresnel.outputs[\"Fac\"], mix_rgb.inputs[\"Fac\"])\n    links.new(mix_rgb.outputs[\"Color\"], bsdf.inputs[\"Base Color\"])\n    links.new(bsdf.outputs[\"BSDF\"], output.inputs[\"Surface\"])\n\n    return mat\n</code></pre>"},{"location":"api/astro_lab.widgets.bpy.advanced/#astro_lab.widgets.bpy.advanced.FuturisticMaterials.create_metallic_material","title":"create_metallic_material  <code>staticmethod</code>","text":"<pre><code>create_metallic_material(\n    color: Tuple[float, float, float] = (0.8, 0.8, 0.8),\n    metallic: float = 1.0,\n    roughness: float = 0.2,\n    anisotropy: float = 0.0,\n) -&gt; Material\n</code></pre> <p>Create metallic material with optional anisotropy.</p> <p>Parameters:</p> Name Type Description Default <code>color</code> <code>Tuple[float, float, float]</code> <p>Metal color</p> <code>(0.8, 0.8, 0.8)</code> <code>metallic</code> <code>float</code> <p>Metallic strength</p> <code>1.0</code> <code>roughness</code> <code>float</code> <p>Surface roughness</p> <code>0.2</code> <code>anisotropy</code> <code>float</code> <p>Anisotropic reflection strength</p> <code>0.0</code> <p>Returns:</p> Type Description <code>Material</code> <p>Created metallic material</p> Source code in <code>src\\astro_lab\\widgets\\bpy\\advanced\\futuristic_materials.py</code> <pre><code>@staticmethod\ndef create_metallic_material(\n    color: Tuple[float, float, float] = (0.8, 0.8, 0.8),\n    metallic: float = 1.0,\n    roughness: float = 0.2,\n    anisotropy: float = 0.0,\n) -&gt; bpy.types.Material:  # type: ignore\n    \"\"\"\n    Create metallic material with optional anisotropy.\n\n    Args:\n        color: Metal color\n        metallic: Metallic strength\n        roughness: Surface roughness\n        anisotropy: Anisotropic reflection strength\n\n    Returns:\n        Created metallic material\n    \"\"\"\n    mat = bpy.data.materials.new(name=\"MetallicMaterial\")\n    mat.use_nodes = True\n    nodes = mat.node_tree.nodes\n    links = mat.node_tree.links\n\n    # Clear default nodes\n    nodes.clear()\n\n    # Add nodes\n    output = nodes.new(\"ShaderNodeOutputMaterial\")\n    bsdf = nodes.new(\"ShaderNodeBsdfPrincipled\")\n\n    # Set metallic properties\n    bsdf.inputs[\"Base Color\"].default_value = (*color, 1.0)\n    bsdf.inputs[\"Metallic\"].default_value = metallic\n    bsdf.inputs[\"Roughness\"].default_value = roughness\n    bsdf.inputs[\"Specular\"].default_value = 0.5\n\n    # Add anisotropy if specified\n    if anisotropy &gt; 0.0:\n        bsdf.inputs[\"Anisotropic\"].default_value = anisotropy\n        bsdf.inputs[\"Anisotropic Rotation\"].default_value = 0.0\n\n        # Add anisotropic texture for variation\n        tex_coord = nodes.new(\"ShaderNodeTexCoord\")\n        mapping = nodes.new(\"ShaderNodeMapping\")\n\n        # Position nodes\n        tex_coord.location = (-600, 0)\n        mapping.location = (-400, 0)\n        bsdf.location = (0, 0)\n        output.location = (200, 0)\n\n        # Connect nodes\n        links.new(tex_coord.outputs[\"Generated\"], mapping.inputs[\"Vector\"])\n        links.new(mapping.outputs[\"Vector\"], bsdf.inputs[\"Tangent\"])\n    else:\n        # Position nodes\n        bsdf.location = (0, 0)\n        output.location = (200, 0)\n\n    # Connect to output\n    links.new(bsdf.outputs[\"BSDF\"], output.inputs[\"Surface\"])\n\n    return mat\n</code></pre>"},{"location":"api/astro_lab.widgets.bpy.advanced/#astro_lab.widgets.bpy.advanced.GravitationalSimulation","title":"GravitationalSimulation","text":"<p>N-body gravitational simulations</p> <p>Methods:</p> Name Description <code>create_binary_system</code> <p>Create gravitationally bound binary system.</p> <code>create_n_body_system</code> <p>Create N-body gravitational simulation.</p> Source code in <code>src\\astro_lab\\widgets\\bpy\\advanced\\physics.py</code> <pre><code>class GravitationalSimulation:\n    \"\"\"N-body gravitational simulations\"\"\"\n\n    @staticmethod  # type: ignore\n    def create_n_body_system(bodies: List[Dict[str, Any]]) -&gt; List[bpy.types.Object]:  # type: ignore\n        \"\"\"\n        Create N-body gravitational simulation.\n\n        Args:\n            bodies: List of body parameters with mass, position, velocity\n\n        Returns:\n            List of created body objects\n        \"\"\"\n        body_objects = []\n\n        for i, body_data in enumerate(bodies):\n            # Create body object\n            bpy.ops.mesh.primitive_uv_sphere_add(\n                radius=body_data.get(\"radius\", 0.1), location=body_data[\"position\"]\n            )\n            body_obj = bpy.context.active_object\n            body_obj.name = body_data.get(\"name\", f\"Body_{i}\")\n\n            # Store physics properties\n            body_obj[\"mass\"] = body_data[\"mass\"]\n            body_obj[\"velocity\"] = body_data.get(\"velocity\", Vector((0, 0, 0)))\n\n            # Add physics material\n            body_mat = PhysicsShaders.create_body_material(\n                body_data[\"mass\"], body_data.get(\"body_type\", \"planet\")\n            )\n            body_obj.data.materials.append(body_mat)\n\n            body_objects.append(body_obj)\n\n        return body_objects\n\n    @staticmethod  # type: ignore\n    def create_binary_system(\n        primary_mass: float, secondary_mass: float, separation: float\n    ) -&gt; Tuple[bpy.types.Object, bpy.types.Object]:  # type: ignore\n        \"\"\"\n        Create gravitationally bound binary system.\n\n        Args:\n            primary_mass: Mass of primary star\n            secondary_mass: Mass of secondary star\n            separation: Orbital separation\n\n        Returns:\n            Tuple of (primary, secondary) objects\n        \"\"\"\n        # Calculate center of mass\n        total_mass = primary_mass + secondary_mass\n        primary_distance = separation * secondary_mass / total_mass\n        secondary_distance = separation * primary_mass / total_mass\n\n        # Create primary star\n        bpy.ops.mesh.primitive_uv_sphere_add(\n            radius=math.pow(primary_mass, 1 / 3) * 0.2,\n            location=Vector((-primary_distance, 0, 0)),\n        )\n        primary = bpy.context.active_object\n        primary.name = \"PrimaryStar\"\n        primary[\"mass\"] = primary_mass\n\n        # Create secondary star\n        bpy.ops.mesh.primitive_uv_sphere_add(\n            radius=math.pow(secondary_mass, 1 / 3) * 0.2,\n            location=Vector((secondary_distance, 0, 0)),\n        )\n        secondary = bpy.context.active_object\n        secondary.name = \"SecondaryStar\"\n        secondary[\"mass\"] = secondary_mass\n\n        # Add stellar materials\n        primary_mat = PhysicsShaders.create_stellar_material(primary_mass)\n        secondary_mat = PhysicsShaders.create_stellar_material(secondary_mass)\n\n        primary.data.materials.append(primary_mat)\n        secondary.data.materials.append(secondary_mat)\n\n        return primary, secondary\n</code></pre>"},{"location":"api/astro_lab.widgets.bpy.advanced/#astro_lab.widgets.bpy.advanced.GravitationalSimulation.create_binary_system","title":"create_binary_system  <code>staticmethod</code>","text":"<pre><code>create_binary_system(\n    primary_mass: float, secondary_mass: float, separation: float\n) -&gt; Tuple[Object, Object]\n</code></pre> <p>Create gravitationally bound binary system.</p> <p>Parameters:</p> Name Type Description Default <code>primary_mass</code> <code>float</code> <p>Mass of primary star</p> required <code>secondary_mass</code> <code>float</code> <p>Mass of secondary star</p> required <code>separation</code> <code>float</code> <p>Orbital separation</p> required <p>Returns:</p> Type Description <code>Tuple[Object, Object]</code> <p>Tuple of (primary, secondary) objects</p> Source code in <code>src\\astro_lab\\widgets\\bpy\\advanced\\physics.py</code> <pre><code>@staticmethod  # type: ignore\ndef create_binary_system(\n    primary_mass: float, secondary_mass: float, separation: float\n) -&gt; Tuple[bpy.types.Object, bpy.types.Object]:  # type: ignore\n    \"\"\"\n    Create gravitationally bound binary system.\n\n    Args:\n        primary_mass: Mass of primary star\n        secondary_mass: Mass of secondary star\n        separation: Orbital separation\n\n    Returns:\n        Tuple of (primary, secondary) objects\n    \"\"\"\n    # Calculate center of mass\n    total_mass = primary_mass + secondary_mass\n    primary_distance = separation * secondary_mass / total_mass\n    secondary_distance = separation * primary_mass / total_mass\n\n    # Create primary star\n    bpy.ops.mesh.primitive_uv_sphere_add(\n        radius=math.pow(primary_mass, 1 / 3) * 0.2,\n        location=Vector((-primary_distance, 0, 0)),\n    )\n    primary = bpy.context.active_object\n    primary.name = \"PrimaryStar\"\n    primary[\"mass\"] = primary_mass\n\n    # Create secondary star\n    bpy.ops.mesh.primitive_uv_sphere_add(\n        radius=math.pow(secondary_mass, 1 / 3) * 0.2,\n        location=Vector((secondary_distance, 0, 0)),\n    )\n    secondary = bpy.context.active_object\n    secondary.name = \"SecondaryStar\"\n    secondary[\"mass\"] = secondary_mass\n\n    # Add stellar materials\n    primary_mat = PhysicsShaders.create_stellar_material(primary_mass)\n    secondary_mat = PhysicsShaders.create_stellar_material(secondary_mass)\n\n    primary.data.materials.append(primary_mat)\n    secondary.data.materials.append(secondary_mat)\n\n    return primary, secondary\n</code></pre>"},{"location":"api/astro_lab.widgets.bpy.advanced/#astro_lab.widgets.bpy.advanced.GravitationalSimulation.create_n_body_system","title":"create_n_body_system  <code>staticmethod</code>","text":"<pre><code>create_n_body_system(bodies: List[Dict[str, Any]]) -&gt; List[Object]\n</code></pre> <p>Create N-body gravitational simulation.</p> <p>Parameters:</p> Name Type Description Default <code>bodies</code> <code>List[Dict[str, Any]]</code> <p>List of body parameters with mass, position, velocity</p> required <p>Returns:</p> Type Description <code>List[Object]</code> <p>List of created body objects</p> Source code in <code>src\\astro_lab\\widgets\\bpy\\advanced\\physics.py</code> <pre><code>@staticmethod  # type: ignore\ndef create_n_body_system(bodies: List[Dict[str, Any]]) -&gt; List[bpy.types.Object]:  # type: ignore\n    \"\"\"\n    Create N-body gravitational simulation.\n\n    Args:\n        bodies: List of body parameters with mass, position, velocity\n\n    Returns:\n        List of created body objects\n    \"\"\"\n    body_objects = []\n\n    for i, body_data in enumerate(bodies):\n        # Create body object\n        bpy.ops.mesh.primitive_uv_sphere_add(\n            radius=body_data.get(\"radius\", 0.1), location=body_data[\"position\"]\n        )\n        body_obj = bpy.context.active_object\n        body_obj.name = body_data.get(\"name\", f\"Body_{i}\")\n\n        # Store physics properties\n        body_obj[\"mass\"] = body_data[\"mass\"]\n        body_obj[\"velocity\"] = body_data.get(\"velocity\", Vector((0, 0, 0)))\n\n        # Add physics material\n        body_mat = PhysicsShaders.create_body_material(\n            body_data[\"mass\"], body_data.get(\"body_type\", \"planet\")\n        )\n        body_obj.data.materials.append(body_mat)\n\n        body_objects.append(body_obj)\n\n    return body_objects\n</code></pre>"},{"location":"api/astro_lab.widgets.bpy.advanced/#astro_lab.widgets.bpy.advanced.MaterialPresets","title":"MaterialPresets","text":"<p>Preset material configurations</p> <p>Methods:</p> Name Description <code>crystal_glass_material</code> <p>Create crystal glass material preset.</p> <code>energy_purple_material</code> <p>Create energy purple material preset.</p> <code>golden_metallic_material</code> <p>Create golden metallic material preset.</p> <code>holographic_blue_material</code> <p>Create holographic blue material preset.</p> <code>luxury_teal_material</code> <p>Create luxury teal material preset.</p> Source code in <code>src\\astro_lab\\widgets\\bpy\\advanced\\futuristic_materials.py</code> <pre><code>class MaterialPresets:\n    \"\"\"Preset material configurations\"\"\"\n\n    @staticmethod\n    def luxury_teal_material() -&gt; bpy.types.Material:  # type: ignore\n        \"\"\"Create luxury teal material preset.\"\"\"\n        return FuturisticMaterials.create_iridescent_material(\n            base_color=(0.0, 0.8, 0.6), iridescence_strength=0.8, iridescence_shift=0.2\n        )\n\n    @staticmethod\n    def golden_metallic_material() -&gt; bpy.types.Material:  # type: ignore\n        \"\"\"Create golden metallic material preset.\"\"\"\n        return FuturisticMaterials.create_metallic_material(\n            color=(1.0, 0.8, 0.2), metallic=1.0, roughness=0.1, anisotropy=0.3\n        )\n\n    @staticmethod\n    def crystal_glass_material() -&gt; bpy.types.Material:  # type: ignore\n        \"\"\"Create crystal glass material preset.\"\"\"\n        return FuturisticMaterials.create_glass_material(\n            color=(0.95, 0.98, 1.0), transmission=0.98, ior=1.5, roughness=0.0\n        )\n\n    @staticmethod\n    def holographic_blue_material() -&gt; bpy.types.Material:  # type: ignore\n        \"\"\"Create holographic blue material preset.\"\"\"\n        return FuturisticMaterials.create_holographic_material(\n            base_color=(0.2, 0.6, 1.0), hologram_strength=1.2, scan_speed=1.5\n        )\n\n    @staticmethod\n    def energy_purple_material() -&gt; bpy.types.Material:  # type: ignore # noqa: F821\n        \"\"\"Create energy purple material preset.\"\"\"\n        return FuturisticMaterials.create_energy_field_material(\n            color=(0.6, 0.2, 1.0), energy_strength=2.5, pulse_speed=1.8\n        )\n</code></pre>"},{"location":"api/astro_lab.widgets.bpy.advanced/#astro_lab.widgets.bpy.advanced.MaterialPresets.crystal_glass_material","title":"crystal_glass_material  <code>staticmethod</code>","text":"<pre><code>crystal_glass_material() -&gt; Material\n</code></pre> <p>Create crystal glass material preset.</p> Source code in <code>src\\astro_lab\\widgets\\bpy\\advanced\\futuristic_materials.py</code> <pre><code>@staticmethod\ndef crystal_glass_material() -&gt; bpy.types.Material:  # type: ignore\n    \"\"\"Create crystal glass material preset.\"\"\"\n    return FuturisticMaterials.create_glass_material(\n        color=(0.95, 0.98, 1.0), transmission=0.98, ior=1.5, roughness=0.0\n    )\n</code></pre>"},{"location":"api/astro_lab.widgets.bpy.advanced/#astro_lab.widgets.bpy.advanced.MaterialPresets.energy_purple_material","title":"energy_purple_material  <code>staticmethod</code>","text":"<pre><code>energy_purple_material() -&gt; Material\n</code></pre> <p>Create energy purple material preset.</p> Source code in <code>src\\astro_lab\\widgets\\bpy\\advanced\\futuristic_materials.py</code> <pre><code>@staticmethod\ndef energy_purple_material() -&gt; bpy.types.Material:  # type: ignore # noqa: F821\n    \"\"\"Create energy purple material preset.\"\"\"\n    return FuturisticMaterials.create_energy_field_material(\n        color=(0.6, 0.2, 1.0), energy_strength=2.5, pulse_speed=1.8\n    )\n</code></pre>"},{"location":"api/astro_lab.widgets.bpy.advanced/#astro_lab.widgets.bpy.advanced.MaterialPresets.golden_metallic_material","title":"golden_metallic_material  <code>staticmethod</code>","text":"<pre><code>golden_metallic_material() -&gt; Material\n</code></pre> <p>Create golden metallic material preset.</p> Source code in <code>src\\astro_lab\\widgets\\bpy\\advanced\\futuristic_materials.py</code> <pre><code>@staticmethod\ndef golden_metallic_material() -&gt; bpy.types.Material:  # type: ignore\n    \"\"\"Create golden metallic material preset.\"\"\"\n    return FuturisticMaterials.create_metallic_material(\n        color=(1.0, 0.8, 0.2), metallic=1.0, roughness=0.1, anisotropy=0.3\n    )\n</code></pre>"},{"location":"api/astro_lab.widgets.bpy.advanced/#astro_lab.widgets.bpy.advanced.MaterialPresets.holographic_blue_material","title":"holographic_blue_material  <code>staticmethod</code>","text":"<pre><code>holographic_blue_material() -&gt; Material\n</code></pre> <p>Create holographic blue material preset.</p> Source code in <code>src\\astro_lab\\widgets\\bpy\\advanced\\futuristic_materials.py</code> <pre><code>@staticmethod\ndef holographic_blue_material() -&gt; bpy.types.Material:  # type: ignore\n    \"\"\"Create holographic blue material preset.\"\"\"\n    return FuturisticMaterials.create_holographic_material(\n        base_color=(0.2, 0.6, 1.0), hologram_strength=1.2, scan_speed=1.5\n    )\n</code></pre>"},{"location":"api/astro_lab.widgets.bpy.advanced/#astro_lab.widgets.bpy.advanced.MaterialPresets.luxury_teal_material","title":"luxury_teal_material  <code>staticmethod</code>","text":"<pre><code>luxury_teal_material() -&gt; Material\n</code></pre> <p>Create luxury teal material preset.</p> Source code in <code>src\\astro_lab\\widgets\\bpy\\advanced\\futuristic_materials.py</code> <pre><code>@staticmethod\ndef luxury_teal_material() -&gt; bpy.types.Material:  # type: ignore\n    \"\"\"Create luxury teal material preset.\"\"\"\n    return FuturisticMaterials.create_iridescent_material(\n        base_color=(0.0, 0.8, 0.6), iridescence_strength=0.8, iridescence_shift=0.2\n    )\n</code></pre>"},{"location":"api/astro_lab.widgets.bpy.advanced/#astro_lab.widgets.bpy.advanced.OrbitalMechanics","title":"OrbitalMechanics","text":"<p>Create realistic orbital mechanics visualizations</p> <p>Methods:</p> Name Description <code>create_orbital_system</code> <p>Create a system of orbiting objects with trails.</p> Source code in <code>src\\astro_lab\\widgets\\bpy\\advanced\\physics.py</code> <pre><code>class OrbitalMechanics:\n    \"\"\"Create realistic orbital mechanics visualizations\"\"\"\n\n    @staticmethod  # type: ignore\n    def create_orbital_system(\n        center_obj: bpy.types.Object, orbits: List[Dict[str, float]]\n    ) -&gt; List[bpy.types.Object]:  # type: ignore\n        \"\"\"\n        Create a system of orbiting objects with trails.\n\n        Args:\n            center_obj: Central object (star/planet)\n            orbits: List of orbit parameters\n\n        Returns:\n            List of created orbital objects\n        \"\"\"\n        orbital_objects = []\n\n        for i, orbit_params in enumerate(orbits):\n            # Create orbiting object\n            bpy.ops.mesh.primitive_uv_sphere_add()\n            orbit_obj = bpy.context.active_object\n            orbit_obj.name = f\"Orbiter_{i}\"\n            orbit_obj.scale = Vector([orbit_params.get(\"size\", 0.1)] * 3)\n\n            # Create orbit path\n            orbit_curve = OrbitalMechanics._create_elliptical_orbit(\n                radius=orbit_params[\"radius\"],\n                eccentricity=orbit_params.get(\"eccentricity\", 0),\n                center=center_obj.location,\n            )\n\n            # Add follow path constraint\n            constraint = orbit_obj.constraints.new(\"FOLLOW_PATH\")\n            constraint.target = orbit_curve\n            constraint.use_curve_follow = True\n\n            # Animate along path\n            orbit_curve.data.use_path = True\n            orbit_curve.data.path_duration = int(orbit_params[\"period\"])\n\n            # Add trail using geometry nodes\n            OrbitalMechanics._add_orbit_trail(orbit_obj)\n\n            # Apply inclination\n            if \"inclination\" in orbit_params:\n                orbit_curve.rotation_euler.x = math.radians(orbit_params[\"inclination\"])\n\n            orbital_objects.append(orbit_obj)\n\n        return orbital_objects\n\n    @staticmethod  # type: ignore\n    def _create_elliptical_orbit(\n        radius: float,\n        eccentricity: float = 0,\n        center: Vector = Vector((0, 0, 0)),\n        segments: int = 64,\n    ) -&gt; bpy.types.Object:  # type: ignore\n        \"\"\"Create an elliptical orbit curve.\"\"\"\n        # Create curve data\n        curve_data = bpy.data.curves.new(name=\"Orbit\", type=\"CURVE\")\n        curve_data.dimensions = \"3D\"\n\n        # Create spline\n        spline = curve_data.splines.new(\"NURBS\")\n        spline.points.add(segments - 1)\n\n        # Calculate ellipse points\n        semi_major = radius\n        semi_minor = radius * math.sqrt(1 - eccentricity**2)\n\n        for i in range(segments):\n            angle = 2 * math.pi * i / segments\n            x = semi_major * math.cos(angle)\n            y = semi_minor * math.sin(angle)\n            z = 0\n\n            point = spline.points[i]\n            point.co = (x + center.x, y + center.y, z + center.z, 1)\n\n        # Close the curve\n        spline.use_cyclic_u = True\n\n        # Create curve object\n        curve_obj = bpy.data.objects.new(\"Orbit\", curve_data)\n        bpy.context.collection.objects.link(curve_obj)\n\n        return curve_obj\n\n    @staticmethod  # type: ignore\n    def _add_orbit_trail(obj: bpy.types.Object) -&gt; bpy.types.NodeTree:  # type: ignore\n        \"\"\"Add a glowing trail to an orbiting object.\"\"\"\n        # Add geometry nodes modifier\n        modifier = obj.modifiers.new(name=\"OrbitTrail\", type=\"NODES\")\n        tree = bpy.data.node_groups.new(name=\"OrbitTrail\", type=\"GeometryNodeTree\")\n        modifier.node_group = tree\n\n        nodes = tree.nodes\n        links = tree.links\n\n        # Clear and setup\n        nodes.clear()\n        tree.interface.clear()\n\n        # Interface\n        tree.interface.new_socket(\n            name=\"Geometry\", in_out=\"INPUT\", socket_type=\"NodeSocketGeometry\"\n        )\n        tree.interface.new_socket(\n            name=\"Trail Length\", in_out=\"INPUT\", socket_type=\"NodeSocketInt\"\n        )\n        tree.interface.new_socket(\n            name=\"Geometry\", in_out=\"OUTPUT\", socket_type=\"NodeSocketGeometry\"\n        )\n\n        # Set defaults\n        modifier[\"Input_2\"] = 100  # Trail length\n\n        # Create nodes\n        input_node = nodes.new(\"NodeGroupInput\")\n        output_node = nodes.new(\"NodeGroupOutput\")\n\n        # Position nodes\n        input_node.location = (-200, 0)\n        output_node.location = (200, 0)\n\n        # Basic connection (simplified trail)\n        links.new(input_node.outputs[\"Geometry\"], output_node.inputs[\"Geometry\"])\n\n        return tree\n</code></pre>"},{"location":"api/astro_lab.widgets.bpy.advanced/#astro_lab.widgets.bpy.advanced.OrbitalMechanics.create_orbital_system","title":"create_orbital_system  <code>staticmethod</code>","text":"<pre><code>create_orbital_system(\n    center_obj: Object, orbits: List[Dict[str, float]]\n) -&gt; List[Object]\n</code></pre> <p>Create a system of orbiting objects with trails.</p> <p>Parameters:</p> Name Type Description Default <code>center_obj</code> <code>Object</code> <p>Central object (star/planet)</p> required <code>orbits</code> <code>List[Dict[str, float]]</code> <p>List of orbit parameters</p> required <p>Returns:</p> Type Description <code>List[Object]</code> <p>List of created orbital objects</p> Source code in <code>src\\astro_lab\\widgets\\bpy\\advanced\\physics.py</code> <pre><code>@staticmethod  # type: ignore\ndef create_orbital_system(\n    center_obj: bpy.types.Object, orbits: List[Dict[str, float]]\n) -&gt; List[bpy.types.Object]:  # type: ignore\n    \"\"\"\n    Create a system of orbiting objects with trails.\n\n    Args:\n        center_obj: Central object (star/planet)\n        orbits: List of orbit parameters\n\n    Returns:\n        List of created orbital objects\n    \"\"\"\n    orbital_objects = []\n\n    for i, orbit_params in enumerate(orbits):\n        # Create orbiting object\n        bpy.ops.mesh.primitive_uv_sphere_add()\n        orbit_obj = bpy.context.active_object\n        orbit_obj.name = f\"Orbiter_{i}\"\n        orbit_obj.scale = Vector([orbit_params.get(\"size\", 0.1)] * 3)\n\n        # Create orbit path\n        orbit_curve = OrbitalMechanics._create_elliptical_orbit(\n            radius=orbit_params[\"radius\"],\n            eccentricity=orbit_params.get(\"eccentricity\", 0),\n            center=center_obj.location,\n        )\n\n        # Add follow path constraint\n        constraint = orbit_obj.constraints.new(\"FOLLOW_PATH\")\n        constraint.target = orbit_curve\n        constraint.use_curve_follow = True\n\n        # Animate along path\n        orbit_curve.data.use_path = True\n        orbit_curve.data.path_duration = int(orbit_params[\"period\"])\n\n        # Add trail using geometry nodes\n        OrbitalMechanics._add_orbit_trail(orbit_obj)\n\n        # Apply inclination\n        if \"inclination\" in orbit_params:\n            orbit_curve.rotation_euler.x = math.radians(orbit_params[\"inclination\"])\n\n        orbital_objects.append(orbit_obj)\n\n    return orbital_objects\n</code></pre>"},{"location":"api/astro_lab.widgets.bpy.advanced/#astro_lab.widgets.bpy.advanced.PhysicsShaders","title":"PhysicsShaders","text":"<p>Shaders for astrophysical objects</p> <p>Methods:</p> Name Description <code>create_body_material</code> <p>Create material for astronomical body.</p> <code>create_stellar_material</code> <p>Create stellar material based on mass.</p> Source code in <code>src\\astro_lab\\widgets\\bpy\\advanced\\physics.py</code> <pre><code>class PhysicsShaders:\n    \"\"\"Shaders for astrophysical objects\"\"\"\n\n    @staticmethod  # type: ignore\n    def create_body_material(mass: float, body_type: str) -&gt; bpy.types.Material:  # type: ignore\n        \"\"\"Create material for astronomical body.\"\"\"\n        mat = bpy.data.materials.new(name=f\"{body_type.title()}Material\")\n        mat.use_nodes = True\n        nodes = mat.node_tree.nodes\n        links = mat.node_tree.links\n\n        # Clear default nodes\n        nodes.clear()\n\n        # Add nodes\n        output = nodes.new(\"ShaderNodeOutputMaterial\")\n\n        if body_type == \"star\":\n            # Emission shader for stars\n            emission = nodes.new(\"ShaderNodeEmission\")\n            blackbody = nodes.new(\"ShaderNodeBlackbody\")\n\n            # Temperature based on mass\n            temp = 5778 * math.pow(mass, 0.5)  # Simplified mass-temperature relation\n            blackbody.inputs[\"Temperature\"].default_value = temp\n            emission.inputs[\"Strength\"].default_value = mass * 10\n\n            links.new(blackbody.outputs[\"Color\"], emission.inputs[\"Color\"])\n            links.new(emission.outputs[\"Emission\"], output.inputs[\"Surface\"])\n\n        else:\n            # Principled BSDF for planets\n            bsdf = nodes.new(\"ShaderNodeBsdfPrincipled\")\n\n            if body_type == \"planet\":\n                color = (0.2, 0.5, 0.8, 1.0) if mass &gt; 0.001 else (0.8, 0.6, 0.4, 1.0)\n            else:\n                color = (0.3, 0.3, 0.3, 1.0)  # Asteroid\n\n            bsdf.inputs[\"Base Color\"].default_value = color\n            bsdf.inputs[\"Roughness\"].default_value = 0.8\n\n            links.new(bsdf.outputs[\"BSDF\"], output.inputs[\"Surface\"])\n\n        return mat\n\n    @staticmethod  # type: ignore\n    def create_stellar_material(mass: float) -&gt; bpy.types.Material:  # type: ignore\n        \"\"\"Create stellar material based on mass.\"\"\"\n        mat = bpy.data.materials.new(name=f\"Star_{mass:.1f}M\")\n        mat.use_nodes = True\n        nodes = mat.node_tree.nodes\n        links = mat.node_tree.links\n\n        # Clear default nodes\n        nodes.clear()\n\n        # Add nodes\n        output = nodes.new(\"ShaderNodeOutputMaterial\")\n        emission = nodes.new(\"ShaderNodeEmission\")\n        blackbody = nodes.new(\"ShaderNodeBlackbody\")\n\n        # Mass-temperature relation\n        if mass &gt; 16:\n            temp = 30000\n        elif mass &gt; 2.1:\n            temp = 20000\n        elif mass &gt; 1.4:\n            temp = 8500\n        elif mass &gt; 1.04:\n            temp = 6500\n        elif mass &gt; 0.8:\n            temp = 5500\n        elif mass &gt; 0.45:\n            temp = 4000\n        else:\n            temp = 3000\n\n        blackbody.inputs[\"Temperature\"].default_value = temp\n        emission.inputs[\"Strength\"].default_value = mass * 5\n\n        # Position nodes\n        blackbody.location = (-200, 0)\n        emission.location = (0, 0)\n        output.location = (200, 0)\n\n        # Connect nodes\n        links.new(blackbody.outputs[\"Color\"], emission.inputs[\"Color\"])\n        links.new(emission.outputs[\"Emission\"], output.inputs[\"Surface\"])\n\n        return mat\n</code></pre>"},{"location":"api/astro_lab.widgets.bpy.advanced/#astro_lab.widgets.bpy.advanced.PhysicsShaders.create_body_material","title":"create_body_material  <code>staticmethod</code>","text":"<pre><code>create_body_material(mass: float, body_type: str) -&gt; Material\n</code></pre> <p>Create material for astronomical body.</p> Source code in <code>src\\astro_lab\\widgets\\bpy\\advanced\\physics.py</code> <pre><code>@staticmethod  # type: ignore\ndef create_body_material(mass: float, body_type: str) -&gt; bpy.types.Material:  # type: ignore\n    \"\"\"Create material for astronomical body.\"\"\"\n    mat = bpy.data.materials.new(name=f\"{body_type.title()}Material\")\n    mat.use_nodes = True\n    nodes = mat.node_tree.nodes\n    links = mat.node_tree.links\n\n    # Clear default nodes\n    nodes.clear()\n\n    # Add nodes\n    output = nodes.new(\"ShaderNodeOutputMaterial\")\n\n    if body_type == \"star\":\n        # Emission shader for stars\n        emission = nodes.new(\"ShaderNodeEmission\")\n        blackbody = nodes.new(\"ShaderNodeBlackbody\")\n\n        # Temperature based on mass\n        temp = 5778 * math.pow(mass, 0.5)  # Simplified mass-temperature relation\n        blackbody.inputs[\"Temperature\"].default_value = temp\n        emission.inputs[\"Strength\"].default_value = mass * 10\n\n        links.new(blackbody.outputs[\"Color\"], emission.inputs[\"Color\"])\n        links.new(emission.outputs[\"Emission\"], output.inputs[\"Surface\"])\n\n    else:\n        # Principled BSDF for planets\n        bsdf = nodes.new(\"ShaderNodeBsdfPrincipled\")\n\n        if body_type == \"planet\":\n            color = (0.2, 0.5, 0.8, 1.0) if mass &gt; 0.001 else (0.8, 0.6, 0.4, 1.0)\n        else:\n            color = (0.3, 0.3, 0.3, 1.0)  # Asteroid\n\n        bsdf.inputs[\"Base Color\"].default_value = color\n        bsdf.inputs[\"Roughness\"].default_value = 0.8\n\n        links.new(bsdf.outputs[\"BSDF\"], output.inputs[\"Surface\"])\n\n    return mat\n</code></pre>"},{"location":"api/astro_lab.widgets.bpy.advanced/#astro_lab.widgets.bpy.advanced.PhysicsShaders.create_stellar_material","title":"create_stellar_material  <code>staticmethod</code>","text":"<pre><code>create_stellar_material(mass: float) -&gt; Material\n</code></pre> <p>Create stellar material based on mass.</p> Source code in <code>src\\astro_lab\\widgets\\bpy\\advanced\\physics.py</code> <pre><code>@staticmethod  # type: ignore\ndef create_stellar_material(mass: float) -&gt; bpy.types.Material:  # type: ignore\n    \"\"\"Create stellar material based on mass.\"\"\"\n    mat = bpy.data.materials.new(name=f\"Star_{mass:.1f}M\")\n    mat.use_nodes = True\n    nodes = mat.node_tree.nodes\n    links = mat.node_tree.links\n\n    # Clear default nodes\n    nodes.clear()\n\n    # Add nodes\n    output = nodes.new(\"ShaderNodeOutputMaterial\")\n    emission = nodes.new(\"ShaderNodeEmission\")\n    blackbody = nodes.new(\"ShaderNodeBlackbody\")\n\n    # Mass-temperature relation\n    if mass &gt; 16:\n        temp = 30000\n    elif mass &gt; 2.1:\n        temp = 20000\n    elif mass &gt; 1.4:\n        temp = 8500\n    elif mass &gt; 1.04:\n        temp = 6500\n    elif mass &gt; 0.8:\n        temp = 5500\n    elif mass &gt; 0.45:\n        temp = 4000\n    else:\n        temp = 3000\n\n    blackbody.inputs[\"Temperature\"].default_value = temp\n    emission.inputs[\"Strength\"].default_value = mass * 5\n\n    # Position nodes\n    blackbody.location = (-200, 0)\n    emission.location = (0, 0)\n    output.location = (200, 0)\n\n    # Connect nodes\n    links.new(blackbody.outputs[\"Color\"], emission.inputs[\"Color\"])\n    links.new(emission.outputs[\"Emission\"], output.inputs[\"Surface\"])\n\n    return mat\n</code></pre>"},{"location":"api/astro_lab.widgets.bpy.advanced/#astro_lab.widgets.bpy.advanced.PostProcessingSuite","title":"PostProcessingSuite","text":"<p>post-processing for astronomical visualization</p> <p>Methods:</p> Name Description <code>add_color_grading</code> <p>Add color grading for different moods.</p> <code>add_depth_of_field</code> <p>Add depth of field effect.</p> <code>add_lens_flare</code> <p>Add lens flare effect to the scene.</p> <code>add_motion_blur</code> <p>Add motion blur for dynamic scenes.</p> <code>add_star_glow</code> <p>Add glow effect specifically for stars.</p> <code>add_vignette</code> <p>Add vignette effect for artistic framing.</p> <code>apply_cinematic_preset</code> <p>Apply cinematic post-processing preset.</p> <code>apply_dramatic_preset</code> <p>Apply dramatic post-processing preset.</p> <code>apply_dreamy_preset</code> <p>Apply dreamy post-processing preset.</p> <code>setup_compositor</code> <p>Setup compositor for post-processing.</p> Source code in <code>src\\astro_lab\\widgets\\bpy\\advanced\\post_processing.py</code> <pre><code>class PostProcessingSuite:\n    \"\"\"post-processing for astronomical visualization\"\"\"\n\n    def __init__(self, scene_name: str = \"AstroScene\"):\n        self.scene_name = scene_name\n        self.scene = bpy.context.scene\n\n    def setup_compositor(self) -&gt; None:\n        \"\"\"Setup compositor for post-processing.\"\"\"\n        self.scene.use_nodes = True\n        tree = self.scene.node_tree\n\n        # Clear existing nodes\n        tree.nodes.clear()\n\n        # Add render layers\n        render_layers = tree.nodes.new(\"CompositorNodeRLayers\")\n        render_layers.location = (0, 0)\n\n        # Add composite output\n        composite = tree.nodes.new(\"CompositorNodeComposite\")\n        composite.location = (1200, 0)\n\n        # Connect basic render\n        tree.links.new(render_layers.outputs[\"Image\"], composite.inputs[\"Image\"])\n\n    def add_lens_flare(\n        self, flare_type: str = \"stellar\", intensity: float = 1.0\n    ) -&gt; None:\n        \"\"\"\n        Add lens flare effect to the scene.\n\n        Args:\n            flare_type: Type of flare ('stellar', 'nebula', 'galactic')\n            intensity: Flare intensity\n        \"\"\"\n        tree = self.scene.node_tree\n\n        # Get render layers\n        render_layers = tree.nodes.get(\"Render Layers\")\n        if not render_layers:\n            return\n\n        # Add lens distortion for flare\n        lens_distortion = tree.nodes.new(\"CompositorNodeLensDist\")\n        lens_distortion.location = (200, 0)\n        lens_distortion.inputs[\"Distort\"].default_value = 0.02 * intensity\n\n        # Add glow effect\n        glow = tree.nodes.new(\"CompositorNodeGlare\")\n        glow.location = (400, 0)\n        glow.glare_type = \"FOG_GLOW\"\n        glow.quality = \"HIGH\"\n        glow.size = 9\n        glow.mix = 0.3 * intensity\n\n        # Add color correction for flare type\n        color_correction = tree.nodes.new(\"CompositorNodeColorCorrection\")\n        color_correction.location = (600, 0)\n\n        if flare_type == \"stellar\":\n            color_correction.master_saturation = 1.2\n            color_correction.master_gain = 1.1\n        elif flare_type == \"nebula\":\n            color_correction.master_saturation = 1.5\n            color_correction.master_gain = 1.3\n        elif flare_type == \"galactic\":\n            color_correction.master_saturation = 0.8\n            color_correction.master_gain = 1.0\n\n        # Connect nodes\n        tree.links.new(render_layers.outputs[\"Image\"], lens_distortion.inputs[\"Image\"])\n        tree.links.new(lens_distortion.outputs[\"Image\"], glow.inputs[\"Image\"])\n        tree.links.new(glow.outputs[\"Image\"], color_correction.inputs[\"Image\"])\n\n        # Update composite connection\n        composite = tree.nodes.get(\"Composite\")\n        if composite:\n            tree.links.new(color_correction.outputs[\"Image\"], composite.inputs[\"Image\"])\n\n    def add_vignette(self, intensity: float = 0.3, radius: float = 0.8) -&gt; None:\n        \"\"\"\n        Add vignette effect for artistic framing.\n\n        Args:\n            intensity: Vignette darkness (0-1)\n            radius: Vignette radius (0-1)\n        \"\"\"\n        tree = self.scene.node_tree\n\n        # Get current image input\n        current_input = self._get_current_image_input()\n        if not current_input:\n            return\n\n        # Add vignette using radial gradient\n        radial = tree.nodes.new(\"CompositorNodeEllipseMask\")\n        radial.location = (800, 200)\n        radial.width = radius * 2\n        radial.height = radius * 2\n\n        # Invert mask for vignette\n        invert = tree.nodes.new(\"CompositorNodeInvert\")\n        invert.location = (1000, 200)\n\n        # Mix with original image\n        mix = tree.nodes.new(\"CompositorNodeMixRGB\")\n        mix.location = (1200, 0)\n        mix.blend_type = \"MULTIPLY\"\n        mix.inputs[\"Fac\"].default_value = intensity\n\n        # Connect nodes\n        tree.links.new(radial.outputs[\"Mask\"], invert.inputs[\"Color\"])\n        tree.links.new(current_input, mix.inputs[\"Image1\"])\n        tree.links.new(invert.outputs[\"Color\"], mix.inputs[\"Image2\"])\n\n        # Update composite connection\n        self._update_composite_connection(mix.outputs[\"Image\"])\n\n    def add_color_grading(self, style: str = \"cinematic\") -&gt; None:\n        \"\"\"\n        Add color grading for different moods.\n\n        Args:\n            style: Grading style ('cinematic', 'warm', 'cool', 'dramatic', 'dreamy')\n        \"\"\"\n        tree = self.scene.node_tree\n\n        # Get current image input\n        current_input = self._get_current_image_input()\n        if not current_input:\n            return\n\n        # Add color correction\n        color_correction = tree.nodes.new(\"CompositorNodeColorCorrection\")\n        color_correction.location = (1000, 0)\n\n        # Apply style-specific settings\n        if style == \"cinematic\":\n            color_correction.master_contrast = 1.2\n            color_correction.master_saturation = 0.9\n            color_correction.master_gain = 1.1\n            color_correction.master_lift = 0.05\n\n        elif style == \"warm\":\n            color_correction.master_saturation = 1.3\n            color_correction.master_gain = 1.2\n            color_correction.master_lift = 0.1\n            color_correction.master_gamma = 0.9\n\n        elif style == \"cool\":\n            color_correction.master_saturation = 1.1\n            color_correction.master_gain = 0.9\n            color_correction.master_lift = -0.05\n            color_correction.master_gamma = 1.1\n\n        elif style == \"dramatic\":\n            color_correction.master_contrast = 1.5\n            color_correction.master_saturation = 1.4\n            color_correction.master_gain = 1.3\n            color_correction.master_lift = 0.15\n\n        elif style == \"dreamy\":\n            color_correction.master_contrast = 0.8\n            color_correction.master_saturation = 1.2\n            color_correction.master_gain = 1.0\n            color_correction.master_lift = 0.05\n\n        # Connect nodes\n        tree.links.new(current_input, color_correction.inputs[\"Image\"])\n\n        # Update composite connection\n        self._update_composite_connection(color_correction.outputs[\"Image\"])\n\n    def add_motion_blur(self, samples: int = 32, shutter: float = 0.5) -&gt; None:\n        \"\"\"\n        Add motion blur for dynamic scenes.\n\n        Args:\n            samples: Motion blur samples\n            shutter: Shutter time (0-1)\n        \"\"\"\n        # Enable motion blur in render settings\n        self.scene.render.use_motion_blur = True\n        self.scene.render.motion_blur_shutter = shutter\n\n        # Set motion blur samples\n        if hasattr(self.scene.render, \"motion_blur_samples\"):\n            self.scene.render.motion_blur_samples = samples\n\n    def add_depth_of_field(\n        self, focus_distance: float = 10.0, f_stop: float = 2.8\n    ) -&gt; None:\n        \"\"\"\n        Add depth of field effect.\n\n        Args:\n            focus_distance: Focus distance\n            f_stop: Aperture f-stop\n        \"\"\"\n        # Get active camera\n        camera = self.scene.camera\n        if not camera:\n            return\n\n        # Enable depth of field\n        camera.data.dof.use_dof = True\n        camera.data.dof.focus_distance = focus_distance\n        camera.data.dof.aperture_fstop = f_stop\n\n    def add_star_glow(self, glow_intensity: float = 1.0, glow_size: int = 9) -&gt; None:\n        \"\"\"\n        Add glow effect specifically for stars.\n\n        Args:\n            glow_intensity: Glow intensity\n            glow_size: Glow size\n        \"\"\"\n        tree = self.scene.node_tree\n\n        # Get current image input\n        current_input = self._get_current_image_input()\n        if not current_input:\n            return\n\n        # Add glow effect\n        glow = tree.nodes.new(\"CompositorNodeGlare\")\n        glow.location = (800, 0)\n        glow.glare_type = \"STREAKS\"\n        glow.quality = \"HIGH\"\n        glow.size = glow_size\n        glow.mix = 0.4 * glow_intensity\n        glow.angle_offset = 0.0\n\n        # Add second glow for cross pattern\n        glow2 = tree.nodes.new(\"CompositorNodeGlare\")\n        glow2.location = (1000, 0)\n        glow2.glare_type = \"STREAKS\"\n        glow2.quality = \"HIGH\"\n        glow2.size = glow_size // 2\n        glow2.mix = 0.2 * glow_intensity\n        glow2.angle_offset = 1.5708  # 90 degrees\n\n        # Mix glows\n        mix = tree.nodes.new(\"CompositorNodeMixRGB\")\n        mix.location = (1200, 0)\n        mix.blend_type = \"ADD\"\n        mix.inputs[\"Fac\"].default_value = 1.0\n\n        # Connect nodes\n        tree.links.new(current_input, glow.inputs[\"Image\"])\n        tree.links.new(current_input, glow2.inputs[\"Image\"])\n        tree.links.new(glow.outputs[\"Image\"], mix.inputs[\"Image1\"])\n        tree.links.new(glow2.outputs[\"Image\"], mix.inputs[\"Image2\"])\n\n        # Update composite connection\n        self._update_composite_connection(mix.outputs[\"Image\"])\n\n    def apply_cinematic_preset(self) -&gt; None:\n        \"\"\"Apply cinematic post-processing preset.\"\"\"\n        self.setup_compositor()\n        self.add_lens_flare(\"stellar\", 0.8)\n        self.add_vignette(0.4, 0.7)\n        self.add_color_grading(\"cinematic\")\n        self.add_star_glow(1.2, 11)\n        self.add_depth_of_field(15.0, 2.0)\n\n    def apply_dramatic_preset(self) -&gt; None:\n        \"\"\"Apply dramatic post-processing preset.\"\"\"\n        self.setup_compositor()\n        self.add_lens_flare(\"nebula\", 1.2)\n        self.add_vignette(0.6, 0.6)\n        self.add_color_grading(\"dramatic\")\n        self.add_star_glow(1.5, 13)\n        self.add_depth_of_field(10.0, 1.4)\n\n    def apply_dreamy_preset(self) -&gt; None:\n        \"\"\"Apply dreamy post-processing preset.\"\"\"\n        self.setup_compositor()\n        self.add_lens_flare(\"galactic\", 0.6)\n        self.add_vignette(0.2, 0.8)\n        self.add_color_grading(\"dreamy\")\n        self.add_star_glow(0.8, 7)\n        self.add_depth_of_field(20.0, 4.0)\n\n    def _get_current_image_input(self):\n        \"\"\"Get current image input for compositor.\"\"\"\n        tree = self.scene.node_tree\n        composite = tree.nodes.get(\"Composite\")\n        if composite and composite.inputs[\"Image\"].links:\n            return composite.inputs[\"Image\"].links[0].from_socket\n        return None\n\n    def _update_composite_connection(self, output_socket):\n        \"\"\"Update composite node connection.\"\"\"\n        tree = self.scene.node_tree\n        composite = tree.nodes.get(\"Composite\")\n        if composite:\n            # Remove existing connection\n            if composite.inputs[\"Image\"].links:\n                tree.links.remove(composite.inputs[\"Image\"].links[0])\n            # Add new connection\n            tree.links.new(output_socket, composite.inputs[\"Image\"])\n</code></pre>"},{"location":"api/astro_lab.widgets.bpy.advanced/#astro_lab.widgets.bpy.advanced.PostProcessingSuite.add_color_grading","title":"add_color_grading","text":"<pre><code>add_color_grading(style: str = 'cinematic') -&gt; None\n</code></pre> <p>Add color grading for different moods.</p> <p>Parameters:</p> Name Type Description Default <code>style</code> <code>str</code> <p>Grading style ('cinematic', 'warm', 'cool', 'dramatic', 'dreamy')</p> <code>'cinematic'</code> Source code in <code>src\\astro_lab\\widgets\\bpy\\advanced\\post_processing.py</code> <pre><code>def add_color_grading(self, style: str = \"cinematic\") -&gt; None:\n    \"\"\"\n    Add color grading for different moods.\n\n    Args:\n        style: Grading style ('cinematic', 'warm', 'cool', 'dramatic', 'dreamy')\n    \"\"\"\n    tree = self.scene.node_tree\n\n    # Get current image input\n    current_input = self._get_current_image_input()\n    if not current_input:\n        return\n\n    # Add color correction\n    color_correction = tree.nodes.new(\"CompositorNodeColorCorrection\")\n    color_correction.location = (1000, 0)\n\n    # Apply style-specific settings\n    if style == \"cinematic\":\n        color_correction.master_contrast = 1.2\n        color_correction.master_saturation = 0.9\n        color_correction.master_gain = 1.1\n        color_correction.master_lift = 0.05\n\n    elif style == \"warm\":\n        color_correction.master_saturation = 1.3\n        color_correction.master_gain = 1.2\n        color_correction.master_lift = 0.1\n        color_correction.master_gamma = 0.9\n\n    elif style == \"cool\":\n        color_correction.master_saturation = 1.1\n        color_correction.master_gain = 0.9\n        color_correction.master_lift = -0.05\n        color_correction.master_gamma = 1.1\n\n    elif style == \"dramatic\":\n        color_correction.master_contrast = 1.5\n        color_correction.master_saturation = 1.4\n        color_correction.master_gain = 1.3\n        color_correction.master_lift = 0.15\n\n    elif style == \"dreamy\":\n        color_correction.master_contrast = 0.8\n        color_correction.master_saturation = 1.2\n        color_correction.master_gain = 1.0\n        color_correction.master_lift = 0.05\n\n    # Connect nodes\n    tree.links.new(current_input, color_correction.inputs[\"Image\"])\n\n    # Update composite connection\n    self._update_composite_connection(color_correction.outputs[\"Image\"])\n</code></pre>"},{"location":"api/astro_lab.widgets.bpy.advanced/#astro_lab.widgets.bpy.advanced.PostProcessingSuite.add_depth_of_field","title":"add_depth_of_field","text":"<pre><code>add_depth_of_field(focus_distance: float = 10.0, f_stop: float = 2.8) -&gt; None\n</code></pre> <p>Add depth of field effect.</p> <p>Parameters:</p> Name Type Description Default <code>focus_distance</code> <code>float</code> <p>Focus distance</p> <code>10.0</code> <code>f_stop</code> <code>float</code> <p>Aperture f-stop</p> <code>2.8</code> Source code in <code>src\\astro_lab\\widgets\\bpy\\advanced\\post_processing.py</code> <pre><code>def add_depth_of_field(\n    self, focus_distance: float = 10.0, f_stop: float = 2.8\n) -&gt; None:\n    \"\"\"\n    Add depth of field effect.\n\n    Args:\n        focus_distance: Focus distance\n        f_stop: Aperture f-stop\n    \"\"\"\n    # Get active camera\n    camera = self.scene.camera\n    if not camera:\n        return\n\n    # Enable depth of field\n    camera.data.dof.use_dof = True\n    camera.data.dof.focus_distance = focus_distance\n    camera.data.dof.aperture_fstop = f_stop\n</code></pre>"},{"location":"api/astro_lab.widgets.bpy.advanced/#astro_lab.widgets.bpy.advanced.PostProcessingSuite.add_lens_flare","title":"add_lens_flare","text":"<pre><code>add_lens_flare(flare_type: str = 'stellar', intensity: float = 1.0) -&gt; None\n</code></pre> <p>Add lens flare effect to the scene.</p> <p>Parameters:</p> Name Type Description Default <code>flare_type</code> <code>str</code> <p>Type of flare ('stellar', 'nebula', 'galactic')</p> <code>'stellar'</code> <code>intensity</code> <code>float</code> <p>Flare intensity</p> <code>1.0</code> Source code in <code>src\\astro_lab\\widgets\\bpy\\advanced\\post_processing.py</code> <pre><code>def add_lens_flare(\n    self, flare_type: str = \"stellar\", intensity: float = 1.0\n) -&gt; None:\n    \"\"\"\n    Add lens flare effect to the scene.\n\n    Args:\n        flare_type: Type of flare ('stellar', 'nebula', 'galactic')\n        intensity: Flare intensity\n    \"\"\"\n    tree = self.scene.node_tree\n\n    # Get render layers\n    render_layers = tree.nodes.get(\"Render Layers\")\n    if not render_layers:\n        return\n\n    # Add lens distortion for flare\n    lens_distortion = tree.nodes.new(\"CompositorNodeLensDist\")\n    lens_distortion.location = (200, 0)\n    lens_distortion.inputs[\"Distort\"].default_value = 0.02 * intensity\n\n    # Add glow effect\n    glow = tree.nodes.new(\"CompositorNodeGlare\")\n    glow.location = (400, 0)\n    glow.glare_type = \"FOG_GLOW\"\n    glow.quality = \"HIGH\"\n    glow.size = 9\n    glow.mix = 0.3 * intensity\n\n    # Add color correction for flare type\n    color_correction = tree.nodes.new(\"CompositorNodeColorCorrection\")\n    color_correction.location = (600, 0)\n\n    if flare_type == \"stellar\":\n        color_correction.master_saturation = 1.2\n        color_correction.master_gain = 1.1\n    elif flare_type == \"nebula\":\n        color_correction.master_saturation = 1.5\n        color_correction.master_gain = 1.3\n    elif flare_type == \"galactic\":\n        color_correction.master_saturation = 0.8\n        color_correction.master_gain = 1.0\n\n    # Connect nodes\n    tree.links.new(render_layers.outputs[\"Image\"], lens_distortion.inputs[\"Image\"])\n    tree.links.new(lens_distortion.outputs[\"Image\"], glow.inputs[\"Image\"])\n    tree.links.new(glow.outputs[\"Image\"], color_correction.inputs[\"Image\"])\n\n    # Update composite connection\n    composite = tree.nodes.get(\"Composite\")\n    if composite:\n        tree.links.new(color_correction.outputs[\"Image\"], composite.inputs[\"Image\"])\n</code></pre>"},{"location":"api/astro_lab.widgets.bpy.advanced/#astro_lab.widgets.bpy.advanced.PostProcessingSuite.add_motion_blur","title":"add_motion_blur","text":"<pre><code>add_motion_blur(samples: int = 32, shutter: float = 0.5) -&gt; None\n</code></pre> <p>Add motion blur for dynamic scenes.</p> <p>Parameters:</p> Name Type Description Default <code>samples</code> <code>int</code> <p>Motion blur samples</p> <code>32</code> <code>shutter</code> <code>float</code> <p>Shutter time (0-1)</p> <code>0.5</code> Source code in <code>src\\astro_lab\\widgets\\bpy\\advanced\\post_processing.py</code> <pre><code>def add_motion_blur(self, samples: int = 32, shutter: float = 0.5) -&gt; None:\n    \"\"\"\n    Add motion blur for dynamic scenes.\n\n    Args:\n        samples: Motion blur samples\n        shutter: Shutter time (0-1)\n    \"\"\"\n    # Enable motion blur in render settings\n    self.scene.render.use_motion_blur = True\n    self.scene.render.motion_blur_shutter = shutter\n\n    # Set motion blur samples\n    if hasattr(self.scene.render, \"motion_blur_samples\"):\n        self.scene.render.motion_blur_samples = samples\n</code></pre>"},{"location":"api/astro_lab.widgets.bpy.advanced/#astro_lab.widgets.bpy.advanced.PostProcessingSuite.add_star_glow","title":"add_star_glow","text":"<pre><code>add_star_glow(glow_intensity: float = 1.0, glow_size: int = 9) -&gt; None\n</code></pre> <p>Add glow effect specifically for stars.</p> <p>Parameters:</p> Name Type Description Default <code>glow_intensity</code> <code>float</code> <p>Glow intensity</p> <code>1.0</code> <code>glow_size</code> <code>int</code> <p>Glow size</p> <code>9</code> Source code in <code>src\\astro_lab\\widgets\\bpy\\advanced\\post_processing.py</code> <pre><code>def add_star_glow(self, glow_intensity: float = 1.0, glow_size: int = 9) -&gt; None:\n    \"\"\"\n    Add glow effect specifically for stars.\n\n    Args:\n        glow_intensity: Glow intensity\n        glow_size: Glow size\n    \"\"\"\n    tree = self.scene.node_tree\n\n    # Get current image input\n    current_input = self._get_current_image_input()\n    if not current_input:\n        return\n\n    # Add glow effect\n    glow = tree.nodes.new(\"CompositorNodeGlare\")\n    glow.location = (800, 0)\n    glow.glare_type = \"STREAKS\"\n    glow.quality = \"HIGH\"\n    glow.size = glow_size\n    glow.mix = 0.4 * glow_intensity\n    glow.angle_offset = 0.0\n\n    # Add second glow for cross pattern\n    glow2 = tree.nodes.new(\"CompositorNodeGlare\")\n    glow2.location = (1000, 0)\n    glow2.glare_type = \"STREAKS\"\n    glow2.quality = \"HIGH\"\n    glow2.size = glow_size // 2\n    glow2.mix = 0.2 * glow_intensity\n    glow2.angle_offset = 1.5708  # 90 degrees\n\n    # Mix glows\n    mix = tree.nodes.new(\"CompositorNodeMixRGB\")\n    mix.location = (1200, 0)\n    mix.blend_type = \"ADD\"\n    mix.inputs[\"Fac\"].default_value = 1.0\n\n    # Connect nodes\n    tree.links.new(current_input, glow.inputs[\"Image\"])\n    tree.links.new(current_input, glow2.inputs[\"Image\"])\n    tree.links.new(glow.outputs[\"Image\"], mix.inputs[\"Image1\"])\n    tree.links.new(glow2.outputs[\"Image\"], mix.inputs[\"Image2\"])\n\n    # Update composite connection\n    self._update_composite_connection(mix.outputs[\"Image\"])\n</code></pre>"},{"location":"api/astro_lab.widgets.bpy.advanced/#astro_lab.widgets.bpy.advanced.PostProcessingSuite.add_vignette","title":"add_vignette","text":"<pre><code>add_vignette(intensity: float = 0.3, radius: float = 0.8) -&gt; None\n</code></pre> <p>Add vignette effect for artistic framing.</p> <p>Parameters:</p> Name Type Description Default <code>intensity</code> <code>float</code> <p>Vignette darkness (0-1)</p> <code>0.3</code> <code>radius</code> <code>float</code> <p>Vignette radius (0-1)</p> <code>0.8</code> Source code in <code>src\\astro_lab\\widgets\\bpy\\advanced\\post_processing.py</code> <pre><code>def add_vignette(self, intensity: float = 0.3, radius: float = 0.8) -&gt; None:\n    \"\"\"\n    Add vignette effect for artistic framing.\n\n    Args:\n        intensity: Vignette darkness (0-1)\n        radius: Vignette radius (0-1)\n    \"\"\"\n    tree = self.scene.node_tree\n\n    # Get current image input\n    current_input = self._get_current_image_input()\n    if not current_input:\n        return\n\n    # Add vignette using radial gradient\n    radial = tree.nodes.new(\"CompositorNodeEllipseMask\")\n    radial.location = (800, 200)\n    radial.width = radius * 2\n    radial.height = radius * 2\n\n    # Invert mask for vignette\n    invert = tree.nodes.new(\"CompositorNodeInvert\")\n    invert.location = (1000, 200)\n\n    # Mix with original image\n    mix = tree.nodes.new(\"CompositorNodeMixRGB\")\n    mix.location = (1200, 0)\n    mix.blend_type = \"MULTIPLY\"\n    mix.inputs[\"Fac\"].default_value = intensity\n\n    # Connect nodes\n    tree.links.new(radial.outputs[\"Mask\"], invert.inputs[\"Color\"])\n    tree.links.new(current_input, mix.inputs[\"Image1\"])\n    tree.links.new(invert.outputs[\"Color\"], mix.inputs[\"Image2\"])\n\n    # Update composite connection\n    self._update_composite_connection(mix.outputs[\"Image\"])\n</code></pre>"},{"location":"api/astro_lab.widgets.bpy.advanced/#astro_lab.widgets.bpy.advanced.PostProcessingSuite.apply_cinematic_preset","title":"apply_cinematic_preset","text":"<pre><code>apply_cinematic_preset() -&gt; None\n</code></pre> <p>Apply cinematic post-processing preset.</p> Source code in <code>src\\astro_lab\\widgets\\bpy\\advanced\\post_processing.py</code> <pre><code>def apply_cinematic_preset(self) -&gt; None:\n    \"\"\"Apply cinematic post-processing preset.\"\"\"\n    self.setup_compositor()\n    self.add_lens_flare(\"stellar\", 0.8)\n    self.add_vignette(0.4, 0.7)\n    self.add_color_grading(\"cinematic\")\n    self.add_star_glow(1.2, 11)\n    self.add_depth_of_field(15.0, 2.0)\n</code></pre>"},{"location":"api/astro_lab.widgets.bpy.advanced/#astro_lab.widgets.bpy.advanced.PostProcessingSuite.apply_dramatic_preset","title":"apply_dramatic_preset","text":"<pre><code>apply_dramatic_preset() -&gt; None\n</code></pre> <p>Apply dramatic post-processing preset.</p> Source code in <code>src\\astro_lab\\widgets\\bpy\\advanced\\post_processing.py</code> <pre><code>def apply_dramatic_preset(self) -&gt; None:\n    \"\"\"Apply dramatic post-processing preset.\"\"\"\n    self.setup_compositor()\n    self.add_lens_flare(\"nebula\", 1.2)\n    self.add_vignette(0.6, 0.6)\n    self.add_color_grading(\"dramatic\")\n    self.add_star_glow(1.5, 13)\n    self.add_depth_of_field(10.0, 1.4)\n</code></pre>"},{"location":"api/astro_lab.widgets.bpy.advanced/#astro_lab.widgets.bpy.advanced.PostProcessingSuite.apply_dreamy_preset","title":"apply_dreamy_preset","text":"<pre><code>apply_dreamy_preset() -&gt; None\n</code></pre> <p>Apply dreamy post-processing preset.</p> Source code in <code>src\\astro_lab\\widgets\\bpy\\advanced\\post_processing.py</code> <pre><code>def apply_dreamy_preset(self) -&gt; None:\n    \"\"\"Apply dreamy post-processing preset.\"\"\"\n    self.setup_compositor()\n    self.add_lens_flare(\"galactic\", 0.6)\n    self.add_vignette(0.2, 0.8)\n    self.add_color_grading(\"dreamy\")\n    self.add_star_glow(0.8, 7)\n    self.add_depth_of_field(20.0, 4.0)\n</code></pre>"},{"location":"api/astro_lab.widgets.bpy.advanced/#astro_lab.widgets.bpy.advanced.PostProcessingSuite.setup_compositor","title":"setup_compositor","text":"<pre><code>setup_compositor() -&gt; None\n</code></pre> <p>Setup compositor for post-processing.</p> Source code in <code>src\\astro_lab\\widgets\\bpy\\advanced\\post_processing.py</code> <pre><code>def setup_compositor(self) -&gt; None:\n    \"\"\"Setup compositor for post-processing.\"\"\"\n    self.scene.use_nodes = True\n    tree = self.scene.node_tree\n\n    # Clear existing nodes\n    tree.nodes.clear()\n\n    # Add render layers\n    render_layers = tree.nodes.new(\"CompositorNodeRLayers\")\n    render_layers.location = (0, 0)\n\n    # Add composite output\n    composite = tree.nodes.new(\"CompositorNodeComposite\")\n    composite.location = (1200, 0)\n\n    # Connect basic render\n    tree.links.new(render_layers.outputs[\"Image\"], composite.inputs[\"Image\"])\n</code></pre>"},{"location":"api/astro_lab.widgets.bpy.advanced/#astro_lab.widgets.bpy.advanced.ProceduralAstronomy","title":"ProceduralAstronomy","text":"<p>Generate procedural astronomical structures</p> <p>Methods:</p> Name Description <code>create_galaxy_structure</code> <p>Create a procedural galaxy structure.</p> <code>create_hr_diagram_3d</code> <p>Create a 3D Hertzsprung-Russell diagram visualization.</p> Source code in <code>src\\astro_lab\\widgets\\bpy\\advanced\\geometry_nodes.py</code> <pre><code>class ProceduralAstronomy:\n    \"\"\"Generate procedural astronomical structures\"\"\"\n\n    @staticmethod  # type: ignore\n    def create_hr_diagram_3d(\n        stellar_data: List[Dict[str, float]], scale_factor: float = 1.0\n    ) -&gt; bpy.types.Object:  # type: ignore\n        \"\"\"\n        Create a 3D Hertzsprung-Russell diagram visualization.\n\n        Args:\n            stellar_data: List of stellar parameters\n            scale_factor: Scale factor for the diagram\n\n        Returns:\n            Created HR diagram object\n        \"\"\"\n        # Create mesh for star positions\n        mesh = bpy.data.meshes.new(\"HR_Diagram\")\n        obj = bpy.data.objects.new(\"HR_Diagram\", mesh)\n        bpy.context.collection.objects.link(obj)\n\n        verts = []\n\n        # Temperature-color mapping\n        temp_colors = {\n            \"O\": (0.6, 0.7, 1.0),\n            \"B\": (0.7, 0.8, 1.0),\n            \"A\": (1.0, 1.0, 1.0),\n            \"F\": (1.0, 1.0, 0.9),\n            \"G\": (1.0, 0.9, 0.7),\n            \"K\": (1.0, 0.7, 0.4),\n            \"M\": (1.0, 0.4, 0.2),\n        }\n\n        for star in stellar_data:\n            # Position in HR diagram space\n            x = -math.log10(star.get(\"temperature\", 5778)) * scale_factor\n            y = math.log10(star.get(\"luminosity\", 1.0)) * scale_factor\n            z = math.log10(star.get(\"mass\", 1.0)) * scale_factor * 0.5\n\n            verts.append((x, y, z))\n\n        # Create mesh from vertices\n        mesh.from_pydata(verts, [], [])\n        mesh.update()\n\n        # Add geometry nodes for stellar rendering\n        ProceduralAstronomy._add_stellar_instances(obj)\n\n        return obj\n\n    @staticmethod  # type: ignore\n    def create_galaxy_structure(\n        center: Vector = Vector((0, 0, 0)),\n        galaxy_type: str = \"spiral\",\n        num_stars: int = 50000,\n        radius: float = 20.0,\n    ) -&gt; bpy.types.Object:  # type: ignore\n        \"\"\"\n        Create a procedural galaxy structure.\n\n        Args:\n            center: Galaxy center position\n            galaxy_type: Type of galaxy ('spiral', 'elliptical', 'irregular')\n            num_stars: Number of stars to generate\n            radius: Galaxy radius\n\n        Returns:\n            Created galaxy object\n        \"\"\"\n        # Create base object\n        bpy.ops.mesh.primitive_plane_add(location=center)\n        galaxy_obj = bpy.context.active_object\n        galaxy_obj.name = f\"{galaxy_type.title()}Galaxy\"\n\n        # Remove default mesh\n        bpy.ops.object.mode_set(mode=\"EDIT\")\n        bpy.ops.mesh.delete(type=\"VERT\")\n        bpy.ops.object.mode_set(mode=\"OBJECT\")\n\n        if galaxy_type == \"spiral\":\n            tree = ProceduralAstronomy._create_spiral_galaxy_nodes(\n                num_stars, radius, num_arms=4\n            )\n        elif galaxy_type == \"elliptical\":\n            tree = ProceduralAstronomy._create_elliptical_galaxy_nodes(\n                num_stars, radius\n            )\n        else:  # irregular\n            tree = ProceduralAstronomy._create_irregular_galaxy_nodes(num_stars, radius)\n\n        # Add geometry nodes modifier\n        modifier = galaxy_obj.modifiers.new(name=\"GalaxyStructure\", type=\"NODES\")\n        modifier.node_group = tree\n\n        return galaxy_obj\n\n    @staticmethod  # type: ignore\n    def _add_stellar_instances(obj: bpy.types.Object) -&gt; None:  # type: ignore\n        \"\"\"Add instanced stars with spectral colors.\"\"\"\n        # Add geometry nodes modifier\n        modifier = obj.modifiers.new(name=\"StellarInstances\", type=\"NODES\")\n        tree = bpy.data.node_groups.new(\n            name=\"StellarInstances\", type=\"GeometryNodeTree\"\n        )\n        modifier.node_group = tree\n\n        nodes = tree.nodes\n        links = tree.links\n\n        # Clear and setup interface\n        nodes.clear()\n        tree.interface.clear()\n\n        # Create interface\n        tree.interface.new_socket(\n            name=\"Geometry\", in_out=\"INPUT\", socket_type=\"NodeSocketGeometry\"\n        )\n        tree.interface.new_socket(\n            name=\"Star Size\", in_out=\"INPUT\", socket_type=\"NodeSocketFloat\"\n        )\n        tree.interface.new_socket(\n            name=\"Geometry\", in_out=\"OUTPUT\", socket_type=\"NodeSocketGeometry\"\n        )\n\n        # Set defaults\n        modifier[\"Input_2\"] = 0.1  # Star size\n\n        # Create nodes\n        input_node = nodes.new(\"NodeGroupInput\")\n        output_node = nodes.new(\"NodeGroupOutput\")\n\n        # Create star geometry\n        ico_sphere = nodes.new(\"GeometryNodeMeshIcoSphere\")\n        ico_sphere.inputs[\"Subdivisions\"].default_value = 2\n\n        # Instance on points\n        instance = nodes.new(\"GeometryNodeInstanceOnPoints\")\n\n        # Random size variation\n        random_size = nodes.new(\"FunctionNodeRandomValue\")\n        random_size.data_type = \"FLOAT\"\n        random_size.inputs[\"Min\"].default_value = 0.5\n        random_size.inputs[\"Max\"].default_value = 2.0\n\n        # Position nodes\n        input_node.location = (-400, 0)\n        ico_sphere.location = (-200, -200)\n        random_size.location = (-200, -100)\n        instance.location = (0, 0)\n        output_node.location = (200, 0)\n\n        # Connect nodes\n        links.new(input_node.outputs[\"Geometry\"], instance.inputs[\"Points\"])\n        links.new(input_node.outputs[\"Star Size\"], ico_sphere.inputs[\"Radius\"])\n        links.new(ico_sphere.outputs[\"Mesh\"], instance.inputs[\"Instance\"])\n        links.new(random_size.outputs[\"Value\"], instance.inputs[\"Scale\"])\n        links.new(instance.outputs[\"Instances\"], output_node.inputs[\"Geometry\"])\n\n    @staticmethod  # type: ignore\n    def _create_spiral_galaxy_nodes(\n        num_stars: int, radius: float, num_arms: int = 4\n    ) -&gt; bpy.types.NodeTree:  # type: ignore\n        \"\"\"Create node tree for spiral galaxy generation.\"\"\"\n        tree = bpy.data.node_groups.new(name=\"SpiralGalaxy\", type=\"GeometryNodeTree\")\n        nodes = tree.nodes\n        links = tree.links\n\n        # Clear and setup interface\n        nodes.clear()\n        tree.interface.clear()\n\n        # Create interface\n        tree.interface.new_socket(\n            name=\"Geometry\", in_out=\"INPUT\", socket_type=\"NodeSocketGeometry\"\n        )\n        tree.interface.new_socket(\n            name=\"Star Count\", in_out=\"INPUT\", socket_type=\"NodeSocketInt\"\n        )\n        tree.interface.new_socket(\n            name=\"Galaxy Radius\", in_out=\"INPUT\", socket_type=\"NodeSocketFloat\"\n        )\n        tree.interface.new_socket(\n            name=\"Geometry\", in_out=\"OUTPUT\", socket_type=\"NodeSocketGeometry\"\n        )\n\n        # Create nodes\n        input_node = nodes.new(\"NodeGroupInput\")\n        output_node = nodes.new(\"NodeGroupOutput\")\n\n        # Create base disk\n        cylinder = nodes.new(\"GeometryNodeMeshCylinder\")\n        cylinder.fill_type = \"NGON\"\n        cylinder.inputs[\"Depth\"].default_value = 0.1\n\n        # Distribute points for stars\n        distribute = nodes.new(\"GeometryNodeDistributePointsOnFaces\")\n\n        # Instance stars\n        ico_sphere = nodes.new(\"GeometryNodeMeshIcoSphere\")\n        ico_sphere.inputs[\"Subdivisions\"].default_value = 1\n        ico_sphere.inputs[\"Radius\"].default_value = 0.02\n\n        instance = nodes.new(\"GeometryNodeInstanceOnPoints\")\n\n        # Position nodes\n        input_node.location = (-800, 0)\n        cylinder.location = (-600, 0)\n        distribute.location = (-400, 0)\n        ico_sphere.location = (-200, 200)\n        instance.location = (0, 0)\n        output_node.location = (200, 0)\n\n        # Connect basic structure\n        links.new(input_node.outputs[\"Galaxy Radius\"], cylinder.inputs[\"Radius\"])\n        links.new(cylinder.outputs[\"Mesh\"], distribute.inputs[\"Mesh\"])\n        links.new(input_node.outputs[\"Star Count\"], distribute.inputs[\"Density\"])\n\n        # Final instancing\n        links.new(distribute.outputs[\"Points\"], instance.inputs[\"Points\"])\n        links.new(ico_sphere.outputs[\"Mesh\"], instance.inputs[\"Instance\"])\n        links.new(instance.outputs[\"Instances\"], output_node.inputs[\"Geometry\"])\n\n        return tree\n\n    @staticmethod  # type: ignore\n    def _create_elliptical_galaxy_nodes(\n        num_stars: int, radius: float\n    ) -&gt; bpy.types.NodeTree:  # type: ignore\n        \"\"\"Create node tree for elliptical galaxy generation.\"\"\"\n        tree = bpy.data.node_groups.new(\n            name=\"EllipticalGalaxy\", type=\"GeometryNodeTree\"\n        )\n        nodes = tree.nodes\n        links = tree.links\n\n        # Clear and setup\n        nodes.clear()\n        tree.interface.clear()\n\n        # Create interface\n        tree.interface.new_socket(\n            name=\"Geometry\", in_out=\"INPUT\", socket_type=\"NodeSocketGeometry\"\n        )\n        tree.interface.new_socket(\n            name=\"Star Count\", in_out=\"INPUT\", socket_type=\"NodeSocketInt\"\n        )\n        tree.interface.new_socket(\n            name=\"Galaxy Radius\", in_out=\"INPUT\", socket_type=\"NodeSocketFloat\"\n        )\n        tree.interface.new_socket(\n            name=\"Geometry\", in_out=\"OUTPUT\", socket_type=\"NodeSocketGeometry\"\n        )\n\n        # Create nodes\n        input_node = nodes.new(\"NodeGroupInput\")\n        output_node = nodes.new(\"NodeGroupOutput\")\n\n        # Create ellipsoid\n        uv_sphere = nodes.new(\"GeometryNodeMeshUVSphere\")\n\n        # Scale to elliptical shape\n        transform = nodes.new(\"GeometryNodeTransform\")\n        transform.inputs[\"Scale\"].default_value = (1.0, 0.7, 0.5)\n\n        # Distribute points\n        distribute = nodes.new(\"GeometryNodeDistributePointsOnFaces\")\n\n        # Instance stars\n        ico_sphere = nodes.new(\"GeometryNodeMeshIcoSphere\")\n        ico_sphere.inputs[\"Radius\"].default_value = 0.015\n\n        instance = nodes.new(\"GeometryNodeInstanceOnPoints\")\n\n        # Position nodes\n        input_node.location = (-600, 0)\n        uv_sphere.location = (-400, 0)\n        transform.location = (-200, 0)\n        distribute.location = (0, 0)\n        ico_sphere.location = (0, 200)\n        instance.location = (200, 0)\n        output_node.location = (400, 0)\n\n        # Connect nodes\n        links.new(input_node.outputs[\"Galaxy Radius\"], uv_sphere.inputs[\"Radius\"])\n        links.new(uv_sphere.outputs[\"Mesh\"], transform.inputs[\"Geometry\"])\n        links.new(transform.outputs[\"Geometry\"], distribute.inputs[\"Mesh\"])\n        links.new(input_node.outputs[\"Star Count\"], distribute.inputs[\"Density\"])\n        links.new(distribute.outputs[\"Points\"], instance.inputs[\"Points\"])\n        links.new(ico_sphere.outputs[\"Mesh\"], instance.inputs[\"Instance\"])\n        links.new(instance.outputs[\"Instances\"], output_node.inputs[\"Geometry\"])\n\n        return tree\n\n    @staticmethod  # type: ignore\n    def _create_irregular_galaxy_nodes(\n        num_stars: int, radius: float\n    ) -&gt; bpy.types.NodeTree:  # type: ignore\n        \"\"\"Create node tree for irregular galaxy generation.\"\"\"\n        tree = bpy.data.node_groups.new(name=\"IrregularGalaxy\", type=\"GeometryNodeTree\")\n        nodes = tree.nodes\n        links = tree.links\n\n        # Clear and setup\n        nodes.clear()\n        tree.interface.clear()\n\n        # Create interface\n        tree.interface.new_socket(\n            name=\"Geometry\", in_out=\"INPUT\", socket_type=\"NodeSocketGeometry\"\n        )\n        tree.interface.new_socket(\n            name=\"Star Count\", in_out=\"INPUT\", socket_type=\"NodeSocketInt\"\n        )\n        tree.interface.new_socket(\n            name=\"Galaxy Radius\", in_out=\"INPUT\", socket_type=\"NodeSocketFloat\"\n        )\n        tree.interface.new_socket(\n            name=\"Geometry\", in_out=\"OUTPUT\", socket_type=\"NodeSocketGeometry\"\n        )\n\n        # Create nodes\n        input_node = nodes.new(\"NodeGroupInput\")\n        output_node = nodes.new(\"NodeGroupOutput\")\n\n        # Create irregular shape with noise\n        ico_sphere = nodes.new(\"GeometryNodeMeshIcoSphere\")\n        ico_sphere.inputs[\"Subdivisions\"].default_value = 3\n\n        # Distribute points\n        distribute = nodes.new(\"GeometryNodeDistributePointsOnFaces\")\n\n        # Instance stars\n        star_sphere = nodes.new(\"GeometryNodeMeshIcoSphere\")\n        star_sphere.inputs[\"Radius\"].default_value = 0.02\n\n        instance = nodes.new(\"GeometryNodeInstanceOnPoints\")\n\n        # Position nodes\n        input_node.location = (-600, 0)\n        ico_sphere.location = (-400, 0)\n        distribute.location = (-200, 0)\n        star_sphere.location = (-200, 200)\n        instance.location = (0, 0)\n        output_node.location = (200, 0)\n\n        # Connect nodes\n        links.new(input_node.outputs[\"Galaxy Radius\"], ico_sphere.inputs[\"Radius\"])\n        links.new(ico_sphere.outputs[\"Mesh\"], distribute.inputs[\"Mesh\"])\n        links.new(input_node.outputs[\"Star Count\"], distribute.inputs[\"Density\"])\n        links.new(distribute.outputs[\"Points\"], instance.inputs[\"Points\"])\n        links.new(star_sphere.outputs[\"Mesh\"], instance.inputs[\"Instance\"])\n        links.new(instance.outputs[\"Instances\"], output_node.inputs[\"Geometry\"])\n\n        return tree\n</code></pre>"},{"location":"api/astro_lab.widgets.bpy.advanced/#astro_lab.widgets.bpy.advanced.ProceduralAstronomy.create_galaxy_structure","title":"create_galaxy_structure  <code>staticmethod</code>","text":"<pre><code>create_galaxy_structure(\n    center: Vector = Vector((0, 0, 0)),\n    galaxy_type: str = \"spiral\",\n    num_stars: int = 50000,\n    radius: float = 20.0,\n) -&gt; Object\n</code></pre> <p>Create a procedural galaxy structure.</p> <p>Parameters:</p> Name Type Description Default <code>center</code> <code>Vector</code> <p>Galaxy center position</p> <code>Vector((0, 0, 0))</code> <code>galaxy_type</code> <code>str</code> <p>Type of galaxy ('spiral', 'elliptical', 'irregular')</p> <code>'spiral'</code> <code>num_stars</code> <code>int</code> <p>Number of stars to generate</p> <code>50000</code> <code>radius</code> <code>float</code> <p>Galaxy radius</p> <code>20.0</code> <p>Returns:</p> Type Description <code>Object</code> <p>Created galaxy object</p> Source code in <code>src\\astro_lab\\widgets\\bpy\\advanced\\geometry_nodes.py</code> <pre><code>@staticmethod  # type: ignore\ndef create_galaxy_structure(\n    center: Vector = Vector((0, 0, 0)),\n    galaxy_type: str = \"spiral\",\n    num_stars: int = 50000,\n    radius: float = 20.0,\n) -&gt; bpy.types.Object:  # type: ignore\n    \"\"\"\n    Create a procedural galaxy structure.\n\n    Args:\n        center: Galaxy center position\n        galaxy_type: Type of galaxy ('spiral', 'elliptical', 'irregular')\n        num_stars: Number of stars to generate\n        radius: Galaxy radius\n\n    Returns:\n        Created galaxy object\n    \"\"\"\n    # Create base object\n    bpy.ops.mesh.primitive_plane_add(location=center)\n    galaxy_obj = bpy.context.active_object\n    galaxy_obj.name = f\"{galaxy_type.title()}Galaxy\"\n\n    # Remove default mesh\n    bpy.ops.object.mode_set(mode=\"EDIT\")\n    bpy.ops.mesh.delete(type=\"VERT\")\n    bpy.ops.object.mode_set(mode=\"OBJECT\")\n\n    if galaxy_type == \"spiral\":\n        tree = ProceduralAstronomy._create_spiral_galaxy_nodes(\n            num_stars, radius, num_arms=4\n        )\n    elif galaxy_type == \"elliptical\":\n        tree = ProceduralAstronomy._create_elliptical_galaxy_nodes(\n            num_stars, radius\n        )\n    else:  # irregular\n        tree = ProceduralAstronomy._create_irregular_galaxy_nodes(num_stars, radius)\n\n    # Add geometry nodes modifier\n    modifier = galaxy_obj.modifiers.new(name=\"GalaxyStructure\", type=\"NODES\")\n    modifier.node_group = tree\n\n    return galaxy_obj\n</code></pre>"},{"location":"api/astro_lab.widgets.bpy.advanced/#astro_lab.widgets.bpy.advanced.ProceduralAstronomy.create_hr_diagram_3d","title":"create_hr_diagram_3d  <code>staticmethod</code>","text":"<pre><code>create_hr_diagram_3d(\n    stellar_data: List[Dict[str, float]], scale_factor: float = 1.0\n) -&gt; Object\n</code></pre> <p>Create a 3D Hertzsprung-Russell diagram visualization.</p> <p>Parameters:</p> Name Type Description Default <code>stellar_data</code> <code>List[Dict[str, float]]</code> <p>List of stellar parameters</p> required <code>scale_factor</code> <code>float</code> <p>Scale factor for the diagram</p> <code>1.0</code> <p>Returns:</p> Type Description <code>Object</code> <p>Created HR diagram object</p> Source code in <code>src\\astro_lab\\widgets\\bpy\\advanced\\geometry_nodes.py</code> <pre><code>@staticmethod  # type: ignore\ndef create_hr_diagram_3d(\n    stellar_data: List[Dict[str, float]], scale_factor: float = 1.0\n) -&gt; bpy.types.Object:  # type: ignore\n    \"\"\"\n    Create a 3D Hertzsprung-Russell diagram visualization.\n\n    Args:\n        stellar_data: List of stellar parameters\n        scale_factor: Scale factor for the diagram\n\n    Returns:\n        Created HR diagram object\n    \"\"\"\n    # Create mesh for star positions\n    mesh = bpy.data.meshes.new(\"HR_Diagram\")\n    obj = bpy.data.objects.new(\"HR_Diagram\", mesh)\n    bpy.context.collection.objects.link(obj)\n\n    verts = []\n\n    # Temperature-color mapping\n    temp_colors = {\n        \"O\": (0.6, 0.7, 1.0),\n        \"B\": (0.7, 0.8, 1.0),\n        \"A\": (1.0, 1.0, 1.0),\n        \"F\": (1.0, 1.0, 0.9),\n        \"G\": (1.0, 0.9, 0.7),\n        \"K\": (1.0, 0.7, 0.4),\n        \"M\": (1.0, 0.4, 0.2),\n    }\n\n    for star in stellar_data:\n        # Position in HR diagram space\n        x = -math.log10(star.get(\"temperature\", 5778)) * scale_factor\n        y = math.log10(star.get(\"luminosity\", 1.0)) * scale_factor\n        z = math.log10(star.get(\"mass\", 1.0)) * scale_factor * 0.5\n\n        verts.append((x, y, z))\n\n    # Create mesh from vertices\n    mesh.from_pydata(verts, [], [])\n    mesh.update()\n\n    # Add geometry nodes for stellar rendering\n    ProceduralAstronomy._add_stellar_instances(obj)\n\n    return obj\n</code></pre>"},{"location":"api/astro_lab.widgets.bpy.advanced/#astro_lab.widgets.bpy.advanced.VisualizationSuite","title":"VisualizationSuite","text":"<p>Main interface for advanced astronomical visualization.</p> <p>Combines all advanced Blender capabilities into a unified system for creating scientific astronomical visualizations.</p> <p>Methods:</p> Name Description <code>create_binary_star_system</code> <p>Create gravitationally bound binary star system.</p> <code>create_emission_nebula_complex</code> <p>Create complex emission nebula with realistic volumetrics.</p> <code>create_hr_diagram_3d</code> <p>Create 3D Hertzsprung-Russell diagram.</p> <code>create_orbital_system</code> <p>Create realistic orbital system with physics.</p> <code>create_procedural_galaxy</code> <p>Create procedural galaxy using Geometry Nodes.</p> <code>render_scene</code> <p>Render the advanced astronomical scene.</p> <code>setup_advanced_lighting</code> <p>Setup advanced lighting for astronomical scenes.</p> <code>setup_cinematic_camera</code> <p>Setup cinematic camera for astronomical visualization.</p> Source code in <code>src\\astro_lab\\widgets\\bpy\\advanced\\__init__.py</code> <pre><code>class VisualizationSuite:\n    \"\"\"\n    Main interface for advanced astronomical visualization.\n\n    Combines all advanced Blender capabilities into a unified system\n    for creating scientific astronomical visualizations.\n    \"\"\"\n\n    def __init__(self, scene_name: str = \"Astro\"):\n        self.scene_name = scene_name\n        self.scene_objects = {}\n\n    def _initialize_scene(self) -&gt; None:\n        \"\"\"Initialize advanced scene with optimal settings.\"\"\"\n        if not bpy:\n            print(\"Blender not available\")\n            return\n\n        # Set render engine to EEVEE Next\n        bpy.context.scene.render.engine = \"BLENDER_EEVEE_NEXT\"\n\n        # Configure EEVEE Next for astronomy\n        scene = bpy.context.scene\n        if hasattr(scene.eevee, \"use_bloom\"):\n            scene.eevee.use_bloom = True\n            scene.eevee.bloom_intensity = 0.8\n            scene.eevee.bloom_radius = 6.5\n\n        # Enable volumetrics\n        if hasattr(scene.eevee, \"volumetric_tile_size\"):\n            scene.eevee.volumetric_tile_size = \"8\"\n            scene.eevee.volumetric_samples = 64\n            scene.eevee.volumetric_start = 0.1\n            scene.eevee.volumetric_end = 1000.0\n\n        # Color management for space scenes\n        scene.view_settings.view_transform = \"Filmic\"\n        scene.view_settings.look = \"High Contrast\"\n\n        # World settings for deep space\n        world = bpy.data.worlds.new(\"AstroWorld\")\n        world.use_nodes = True\n        world_nodes = world.node_tree.nodes\n        world_nodes.clear()\n\n        # Background shader for space\n        background = world_nodes.new(\"ShaderNodeBackground\")\n        background.inputs[\"Color\"].default_value = (0.01, 0.01, 0.02, 1.0)  # Deep space\n        background.inputs[\"Strength\"].default_value = 0.1\n\n        output = world_nodes.new(\"ShaderNodeOutputWorld\")\n        world.node_tree.links.new(\n            background.outputs[\"Background\"], output.inputs[\"Surface\"]\n        )\n\n        bpy.context.scene.world = world\n\n        print(f\" scene '{self.scene_name}' initialized with EEVEE Next\")\n\n    def create_procedural_galaxy(\n        self,\n        galaxy_type: str = \"spiral\",\n        position: Optional[Any] = None,\n        num_stars: int = 50000,\n        radius: float = 20.0,\n    ) -&gt; Optional[Any]:\n        \"\"\"\n        Create procedural galaxy using Geometry Nodes.\n\n        Args:\n            galaxy_type: Type of galaxy ('spiral', 'elliptical', 'irregular')\n            position: Galaxy center position\n            num_stars: Number of stars to generate\n            radius: Galaxy radius\n\n        Returns:\n            Created galaxy object\n        \"\"\"\n        if position is None and Vector:\n            position = Vector((0, 0, 0))\n\n        galaxy = ProceduralAstronomy.create_galaxy_structure(\n            center=position, galaxy_type=galaxy_type, num_stars=num_stars, radius=radius\n        )\n\n        self.scene_objects[f\"galaxy_{galaxy_type}\"] = galaxy\n\n        # Add appropriate stellar material\n        if galaxy_type == \"spiral\":\n            material = AstronomicalMaterials.create_stellar_classification_material(\"B\")\n        elif galaxy_type == \"elliptical\":\n            material = AstronomicalMaterials.create_stellar_classification_material(\"K\")\n        else:  # irregular\n            material = AstronomicalMaterials.create_stellar_classification_material(\"M\")\n\n        galaxy.data.materials.append(material)\n\n        print(f\"Created {galaxy_type} galaxy with {num_stars} stars\")\n        return galaxy\n\n    def create_emission_nebula_complex(\n        self,\n        center: Optional[Any] = None,\n        nebula_type: str = \"h_alpha\",\n        size: float = 15.0,\n    ) -&gt; Optional[Any]:\n        \"\"\"\n        Create complex emission nebula with realistic volumetrics.\n\n        Args:\n            center: Nebula center position\n            nebula_type: Type of emission ('h_alpha', 'oxygen', 'planetary')\n            size: Nebula size\n\n        Returns:\n            Created nebula object\n        \"\"\"\n        if center is None and Vector:\n            center = Vector((0, 0, 0))\n\n        nebula = VolumetricAstronomy.create_emission_nebula(\n            center=center, size=size, nebula_type=nebula_type, density=0.2\n        )\n\n        self.scene_objects[f\"nebula_{nebula_type}\"] = nebula\n\n        print(f\"Created {nebula_type} emission nebula\")\n        return nebula\n\n    def create_orbital_system(\n        self, star_mass: float = 1.0, planet_data: Optional[List[Dict[str, Any]]] = None\n    ) -&gt; List[Any]:\n        \"\"\"\n        Create realistic orbital system with physics.\n\n        Args:\n            star_mass: Central star mass in solar masses\n            planet_data: List of planet parameters\n\n        Returns:\n            List of created objects [star, ...planets]\n        \"\"\"\n        # Create central star\n        bpy.ops.mesh.primitive_uv_sphere_add(location=(0, 0, 0))\n        star = bpy.context.active_object\n        star.name = \"CentralStar\"\n        star.scale = Vector(\n            [\n                math.pow(star_mass, 1 / 3) * 2,\n                math.pow(star_mass, 1 / 3) * 2,\n                math.pow(star_mass, 1 / 3) * 2,\n            ]\n        )\n\n        # Add stellar material\n        stellar_material = AstronomicalShaders.create_stellar_blackbody_shader(\n            temperature=5778 * math.pow(star_mass, 0.5),\n            luminosity=star_mass,\n            stellar_class=None,\n        )\n        star.data.materials.append(stellar_material)\n\n        # Default planet data if none provided\n        if planet_data is None:\n            planet_data = [\n                {\"radius\": 5, \"period\": 100, \"size\": 0.3, \"type\": \"terrestrial\"},\n                {\"radius\": 8, \"period\": 200, \"size\": 0.5, \"type\": \"terrestrial\"},\n                {\"radius\": 15, \"period\": 500, \"size\": 2.0, \"type\": \"gas_giant\"},\n                {\"radius\": 25, \"period\": 1000, \"size\": 1.5, \"type\": \"ice_giant\"},\n            ]\n\n        # Create orbital system\n        orbits = [\n            {\"radius\": p[\"radius\"], \"period\": p[\"period\"], \"size\": p[\"size\"]}\n            for p in planet_data\n        ]\n\n        planets = OrbitalMechanics.create_orbital_system(star, orbits)\n\n        # Add planetary materials\n        for i, (planet, data) in enumerate(zip(planets, planet_data)):\n            planet_material = AstronomicalShaders.create_planetary_surface_shader(\n                planet_type=data[\"type\"]\n            )\n            planet.data.materials.append(planet_material)\n\n            # Add atmosphere for suitable planets\n            if data[\"type\"] in [\"terrestrial\", \"gas_giant\", \"ice_giant\"]:\n                atmosphere_type = (\n                    \"earth\" if data[\"type\"] == \"terrestrial\" else data[\"type\"]\n                )\n                VolumetricAstronomy.create_planetary_atmosphere(\n                    planet_obj=planet,\n                    atmosphere_type=atmosphere_type,\n                    thickness=0.3 if data[\"type\"] == \"terrestrial\" else 0.1,\n                )\n\n        system_objects = [star] + planets\n        self.scene_objects[\"orbital_system\"] = system_objects\n\n        print(f\"Created orbital system with {len(planets)} planets\")\n        return system_objects\n\n    def create_binary_star_system(\n        self,\n        primary_mass: float = 2.0,\n        secondary_mass: float = 0.8,\n        separation: float = 10.0,\n    ) -&gt; Tuple[Any, Any]:\n        \"\"\"\n        Create gravitationally bound binary star system.\n\n        Args:\n            primary_mass: Primary star mass in solar masses\n            secondary_mass: Secondary star mass in solar masses\n            separation: Orbital separation in AU\n\n        Returns:\n            Tuple of (primary_star, secondary_star)\n        \"\"\"\n        primary, secondary = GravitationalSimulation.create_binary_system(\n            primary_mass=primary_mass,\n            secondary_mass=secondary_mass,\n            separation=separation,\n        )\n\n        # Add stellar winds\n        for star in [primary, secondary]:\n            VolumetricAstronomy.create_stellar_wind(\n                star_obj=star, wind_speed=500.0, mass_loss_rate=1e-6, wind_radius=8.0\n            )\n\n        self.scene_objects[\"binary_system\"] = [primary, secondary]\n\n        print(f\"Created binary system: {primary_mass}M\u2609 + {secondary_mass}M\u2609\")\n        return primary, secondary\n\n    def create_hr_diagram_3d(\n        self,\n        stellar_data: Optional[List[Dict[str, float]]] = None,\n        scale_factor: float = 2.0,\n    ) -&gt; Optional[Any]:\n        \"\"\"\n        Create 3D Hertzsprung-Russell diagram.\n\n        Args:\n            stellar_data: List of stellar parameters\n            scale_factor: Scale factor for the diagram\n\n        Returns:\n            Created HR diagram object\n        \"\"\"\n        # Generate sample data if none provided\n        if stellar_data is None:\n            stellar_data = []\n\n            for i in range(200):\n                temp = random.uniform(3000, 30000)\n                luminosity = math.pow(temp / 5778, 3.5) + random.uniform(-0.5, 0.5)\n                mass = math.pow(temp / 5778, 0.7)\n\n                stellar_data.append(\n                    {\"temperature\": temp, \"luminosity\": luminosity, \"mass\": mass}\n                )\n\n        hr_diagram = ProceduralAstronomy.create_hr_diagram_3d(\n            stellar_data=stellar_data, scale_factor=scale_factor\n        )\n\n        self.scene_objects[\"hr_diagram\"] = hr_diagram\n\n        print(f\"Created 3D HR diagram with {len(stellar_data)} stars\")\n        return hr_diagram\n\n    def setup_cinematic_camera(\n        self,\n        target: Vector = Vector((0, 0, 0)),\n        distance: float = 30.0,\n        height: float = 10.0,\n    ) -&gt; Any:\n        \"\"\"\n        Setup cinematic camera for astronomical visualization.\n\n        Args:\n            target: Camera target position\n            distance: Distance from target\n            height: Height above target plane\n\n        Returns:\n            Created camera object\n        \"\"\"\n        # Calculate camera position\n        angle = math.radians(45)  # 45-degree angle\n        camera_pos = Vector(\n            (\n                target.x + distance * math.cos(angle),\n                target.y - distance * math.sin(angle),\n                target.z + height,\n            )\n        )\n\n        # Create camera\n        bpy.ops.object.camera_add(location=camera_pos)\n        camera = bpy.context.active_object\n        camera.name = \"AstroCinematicCamera\"\n\n        # Point at target\n        direction = target - camera_pos\n        camera.rotation_euler = direction.to_track_quat(\"-Z\", \"Y\").to_euler()\n\n        # Set camera properties\n        camera.data.lens_unit = \"FOV\"\n        camera.data.angle = math.radians(35)  # 35mm equivalent\n        camera.data.clip_start = 0.1\n        camera.data.clip_end = 10000.0\n        camera.data.dof.use_dof = True\n        camera.data.dof.focus_distance = distance\n\n        # Set as active camera\n        bpy.context.scene.camera = camera\n\n        self.scene_objects[\"camera\"] = camera\n\n        print(f\"Setup cinematic camera at distance {distance}\")\n        return camera\n\n    def setup_advanced_lighting(self, preset: str = \"deep_space\") -&gt; List[Any]:\n        \"\"\"\n        Setup advanced lighting for astronomical scenes.\n\n        Args:\n            preset: Lighting preset ('deep_space', 'nebula', 'planetary')\n\n        Returns:\n            List of created light objects\n        \"\"\"\n        lights = []\n\n        if preset == \"deep_space\":\n            # Ambient starlight\n            bpy.ops.object.light_add(type=\"SUN\", location=(50, 50, 50))\n            ambient = bpy.context.active_object\n            ambient.name = \"StarfieldAmbient\"\n            ambient.data.energy = 0.1\n            ambient.data.color = (0.8, 0.9, 1.0)\n            lights.append(ambient)\n\n            # Key celestial light\n            bpy.ops.object.light_add(type=\"SUN\", location=(-30, -30, 40))\n            key_light = bpy.context.active_object\n            key_light.name = \"CelestialKey\"\n            key_light.data.energy = 2.0\n            key_light.data.color = (1.0, 0.9, 0.8)\n            lights.append(key_light)\n\n        elif preset == \"nebula\":\n            # Warm nebula illumination\n            bpy.ops.object.light_add(type=\"AREA\", location=(20, 0, 20))\n            nebula_light = bpy.context.active_object\n            nebula_light.name = \"NebulaIllumination\"\n            nebula_light.data.energy = 100.0\n            nebula_light.data.color = (1.0, 0.6, 0.4)\n            nebula_light.data.size = 10.0\n            lights.append(nebula_light)\n\n        else:  # planetary\n            # Solar illumination\n            bpy.ops.object.light_add(type=\"SUN\", location=(100, 0, 50))\n            solar = bpy.context.active_object\n            solar.name = \"SolarIllumination\"\n            solar.data.energy = 5.0\n            solar.data.color = (1.0, 0.95, 0.9)\n            lights.append(solar)\n\n        self.scene_objects[\"lights\"] = lights\n\n        print(f\"Setup {preset} lighting with {len(lights)} lights\")\n        return lights\n\n    def render_scene(\n        self,\n        output_path: str,\n        resolution: Tuple[int, int] = (1920, 1080),\n        samples: int = 128,\n    ) -&gt; bool:\n        \"\"\"\n        Render the advanced astronomical scene.\n\n        Args:\n            output_path: Output file path\n            resolution: Render resolution (width, height)\n            samples: Number of samples for quality\n\n        Returns:\n            True if successful\n        \"\"\"\n        # Convert to absolute path relative to current working directory\n        if not os.path.isabs(output_path):\n            output_path = str(Path.cwd() / output_path)\n\n        # Ensure directory exists\n        output_dir = Path(output_path).parent\n        output_dir.mkdir(parents=True, exist_ok=True)\n\n        # Set render settings\n        scene = bpy.context.scene\n        scene.render.resolution_x = resolution[0]\n        scene.render.resolution_y = resolution[1]\n        scene.render.filepath = output_path\n\n        if hasattr(scene.eevee, \"taa_render_samples\"):\n            scene.eevee.taa_render_samples = samples\n\n        # Render\n        try:\n            bpy.ops.render.render(write_still=True)\n            print(f\"Rendered scene to {output_path}\")\n            return True\n        except Exception as e:\n            print(f\"Render failed: {e}\")\n            return False\n</code></pre>"},{"location":"api/astro_lab.widgets.bpy.advanced/#astro_lab.widgets.bpy.advanced.VisualizationSuite.create_binary_star_system","title":"create_binary_star_system","text":"<pre><code>create_binary_star_system(\n    primary_mass: float = 2.0, secondary_mass: float = 0.8, separation: float = 10.0\n) -&gt; Tuple[Any, Any]\n</code></pre> <p>Create gravitationally bound binary star system.</p> <p>Parameters:</p> Name Type Description Default <code>primary_mass</code> <code>float</code> <p>Primary star mass in solar masses</p> <code>2.0</code> <code>secondary_mass</code> <code>float</code> <p>Secondary star mass in solar masses</p> <code>0.8</code> <code>separation</code> <code>float</code> <p>Orbital separation in AU</p> <code>10.0</code> <p>Returns:</p> Type Description <code>Tuple[Any, Any]</code> <p>Tuple of (primary_star, secondary_star)</p> Source code in <code>src\\astro_lab\\widgets\\bpy\\advanced\\__init__.py</code> <pre><code>def create_binary_star_system(\n    self,\n    primary_mass: float = 2.0,\n    secondary_mass: float = 0.8,\n    separation: float = 10.0,\n) -&gt; Tuple[Any, Any]:\n    \"\"\"\n    Create gravitationally bound binary star system.\n\n    Args:\n        primary_mass: Primary star mass in solar masses\n        secondary_mass: Secondary star mass in solar masses\n        separation: Orbital separation in AU\n\n    Returns:\n        Tuple of (primary_star, secondary_star)\n    \"\"\"\n    primary, secondary = GravitationalSimulation.create_binary_system(\n        primary_mass=primary_mass,\n        secondary_mass=secondary_mass,\n        separation=separation,\n    )\n\n    # Add stellar winds\n    for star in [primary, secondary]:\n        VolumetricAstronomy.create_stellar_wind(\n            star_obj=star, wind_speed=500.0, mass_loss_rate=1e-6, wind_radius=8.0\n        )\n\n    self.scene_objects[\"binary_system\"] = [primary, secondary]\n\n    print(f\"Created binary system: {primary_mass}M\u2609 + {secondary_mass}M\u2609\")\n    return primary, secondary\n</code></pre>"},{"location":"api/astro_lab.widgets.bpy.advanced/#astro_lab.widgets.bpy.advanced.VisualizationSuite.create_emission_nebula_complex","title":"create_emission_nebula_complex","text":"<pre><code>create_emission_nebula_complex(\n    center: Optional[Any] = None, nebula_type: str = \"h_alpha\", size: float = 15.0\n) -&gt; Optional[Any]\n</code></pre> <p>Create complex emission nebula with realistic volumetrics.</p> <p>Parameters:</p> Name Type Description Default <code>center</code> <code>Optional[Any]</code> <p>Nebula center position</p> <code>None</code> <code>nebula_type</code> <code>str</code> <p>Type of emission ('h_alpha', 'oxygen', 'planetary')</p> <code>'h_alpha'</code> <code>size</code> <code>float</code> <p>Nebula size</p> <code>15.0</code> <p>Returns:</p> Type Description <code>Optional[Any]</code> <p>Created nebula object</p> Source code in <code>src\\astro_lab\\widgets\\bpy\\advanced\\__init__.py</code> <pre><code>def create_emission_nebula_complex(\n    self,\n    center: Optional[Any] = None,\n    nebula_type: str = \"h_alpha\",\n    size: float = 15.0,\n) -&gt; Optional[Any]:\n    \"\"\"\n    Create complex emission nebula with realistic volumetrics.\n\n    Args:\n        center: Nebula center position\n        nebula_type: Type of emission ('h_alpha', 'oxygen', 'planetary')\n        size: Nebula size\n\n    Returns:\n        Created nebula object\n    \"\"\"\n    if center is None and Vector:\n        center = Vector((0, 0, 0))\n\n    nebula = VolumetricAstronomy.create_emission_nebula(\n        center=center, size=size, nebula_type=nebula_type, density=0.2\n    )\n\n    self.scene_objects[f\"nebula_{nebula_type}\"] = nebula\n\n    print(f\"Created {nebula_type} emission nebula\")\n    return nebula\n</code></pre>"},{"location":"api/astro_lab.widgets.bpy.advanced/#astro_lab.widgets.bpy.advanced.VisualizationSuite.create_hr_diagram_3d","title":"create_hr_diagram_3d","text":"<pre><code>create_hr_diagram_3d(\n    stellar_data: Optional[List[Dict[str, float]]] = None, scale_factor: float = 2.0\n) -&gt; Optional[Any]\n</code></pre> <p>Create 3D Hertzsprung-Russell diagram.</p> <p>Parameters:</p> Name Type Description Default <code>stellar_data</code> <code>Optional[List[Dict[str, float]]]</code> <p>List of stellar parameters</p> <code>None</code> <code>scale_factor</code> <code>float</code> <p>Scale factor for the diagram</p> <code>2.0</code> <p>Returns:</p> Type Description <code>Optional[Any]</code> <p>Created HR diagram object</p> Source code in <code>src\\astro_lab\\widgets\\bpy\\advanced\\__init__.py</code> <pre><code>def create_hr_diagram_3d(\n    self,\n    stellar_data: Optional[List[Dict[str, float]]] = None,\n    scale_factor: float = 2.0,\n) -&gt; Optional[Any]:\n    \"\"\"\n    Create 3D Hertzsprung-Russell diagram.\n\n    Args:\n        stellar_data: List of stellar parameters\n        scale_factor: Scale factor for the diagram\n\n    Returns:\n        Created HR diagram object\n    \"\"\"\n    # Generate sample data if none provided\n    if stellar_data is None:\n        stellar_data = []\n\n        for i in range(200):\n            temp = random.uniform(3000, 30000)\n            luminosity = math.pow(temp / 5778, 3.5) + random.uniform(-0.5, 0.5)\n            mass = math.pow(temp / 5778, 0.7)\n\n            stellar_data.append(\n                {\"temperature\": temp, \"luminosity\": luminosity, \"mass\": mass}\n            )\n\n    hr_diagram = ProceduralAstronomy.create_hr_diagram_3d(\n        stellar_data=stellar_data, scale_factor=scale_factor\n    )\n\n    self.scene_objects[\"hr_diagram\"] = hr_diagram\n\n    print(f\"Created 3D HR diagram with {len(stellar_data)} stars\")\n    return hr_diagram\n</code></pre>"},{"location":"api/astro_lab.widgets.bpy.advanced/#astro_lab.widgets.bpy.advanced.VisualizationSuite.create_orbital_system","title":"create_orbital_system","text":"<pre><code>create_orbital_system(\n    star_mass: float = 1.0, planet_data: Optional[List[Dict[str, Any]]] = None\n) -&gt; List[Any]\n</code></pre> <p>Create realistic orbital system with physics.</p> <p>Parameters:</p> Name Type Description Default <code>star_mass</code> <code>float</code> <p>Central star mass in solar masses</p> <code>1.0</code> <code>planet_data</code> <code>Optional[List[Dict[str, Any]]]</code> <p>List of planet parameters</p> <code>None</code> <p>Returns:</p> Type Description <code>List[Any]</code> <p>List of created objects [star, ...planets]</p> Source code in <code>src\\astro_lab\\widgets\\bpy\\advanced\\__init__.py</code> <pre><code>def create_orbital_system(\n    self, star_mass: float = 1.0, planet_data: Optional[List[Dict[str, Any]]] = None\n) -&gt; List[Any]:\n    \"\"\"\n    Create realistic orbital system with physics.\n\n    Args:\n        star_mass: Central star mass in solar masses\n        planet_data: List of planet parameters\n\n    Returns:\n        List of created objects [star, ...planets]\n    \"\"\"\n    # Create central star\n    bpy.ops.mesh.primitive_uv_sphere_add(location=(0, 0, 0))\n    star = bpy.context.active_object\n    star.name = \"CentralStar\"\n    star.scale = Vector(\n        [\n            math.pow(star_mass, 1 / 3) * 2,\n            math.pow(star_mass, 1 / 3) * 2,\n            math.pow(star_mass, 1 / 3) * 2,\n        ]\n    )\n\n    # Add stellar material\n    stellar_material = AstronomicalShaders.create_stellar_blackbody_shader(\n        temperature=5778 * math.pow(star_mass, 0.5),\n        luminosity=star_mass,\n        stellar_class=None,\n    )\n    star.data.materials.append(stellar_material)\n\n    # Default planet data if none provided\n    if planet_data is None:\n        planet_data = [\n            {\"radius\": 5, \"period\": 100, \"size\": 0.3, \"type\": \"terrestrial\"},\n            {\"radius\": 8, \"period\": 200, \"size\": 0.5, \"type\": \"terrestrial\"},\n            {\"radius\": 15, \"period\": 500, \"size\": 2.0, \"type\": \"gas_giant\"},\n            {\"radius\": 25, \"period\": 1000, \"size\": 1.5, \"type\": \"ice_giant\"},\n        ]\n\n    # Create orbital system\n    orbits = [\n        {\"radius\": p[\"radius\"], \"period\": p[\"period\"], \"size\": p[\"size\"]}\n        for p in planet_data\n    ]\n\n    planets = OrbitalMechanics.create_orbital_system(star, orbits)\n\n    # Add planetary materials\n    for i, (planet, data) in enumerate(zip(planets, planet_data)):\n        planet_material = AstronomicalShaders.create_planetary_surface_shader(\n            planet_type=data[\"type\"]\n        )\n        planet.data.materials.append(planet_material)\n\n        # Add atmosphere for suitable planets\n        if data[\"type\"] in [\"terrestrial\", \"gas_giant\", \"ice_giant\"]:\n            atmosphere_type = (\n                \"earth\" if data[\"type\"] == \"terrestrial\" else data[\"type\"]\n            )\n            VolumetricAstronomy.create_planetary_atmosphere(\n                planet_obj=planet,\n                atmosphere_type=atmosphere_type,\n                thickness=0.3 if data[\"type\"] == \"terrestrial\" else 0.1,\n            )\n\n    system_objects = [star] + planets\n    self.scene_objects[\"orbital_system\"] = system_objects\n\n    print(f\"Created orbital system with {len(planets)} planets\")\n    return system_objects\n</code></pre>"},{"location":"api/astro_lab.widgets.bpy.advanced/#astro_lab.widgets.bpy.advanced.VisualizationSuite.create_procedural_galaxy","title":"create_procedural_galaxy","text":"<pre><code>create_procedural_galaxy(\n    galaxy_type: str = \"spiral\",\n    position: Optional[Any] = None,\n    num_stars: int = 50000,\n    radius: float = 20.0,\n) -&gt; Optional[Any]\n</code></pre> <p>Create procedural galaxy using Geometry Nodes.</p> <p>Parameters:</p> Name Type Description Default <code>galaxy_type</code> <code>str</code> <p>Type of galaxy ('spiral', 'elliptical', 'irregular')</p> <code>'spiral'</code> <code>position</code> <code>Optional[Any]</code> <p>Galaxy center position</p> <code>None</code> <code>num_stars</code> <code>int</code> <p>Number of stars to generate</p> <code>50000</code> <code>radius</code> <code>float</code> <p>Galaxy radius</p> <code>20.0</code> <p>Returns:</p> Type Description <code>Optional[Any]</code> <p>Created galaxy object</p> Source code in <code>src\\astro_lab\\widgets\\bpy\\advanced\\__init__.py</code> <pre><code>def create_procedural_galaxy(\n    self,\n    galaxy_type: str = \"spiral\",\n    position: Optional[Any] = None,\n    num_stars: int = 50000,\n    radius: float = 20.0,\n) -&gt; Optional[Any]:\n    \"\"\"\n    Create procedural galaxy using Geometry Nodes.\n\n    Args:\n        galaxy_type: Type of galaxy ('spiral', 'elliptical', 'irregular')\n        position: Galaxy center position\n        num_stars: Number of stars to generate\n        radius: Galaxy radius\n\n    Returns:\n        Created galaxy object\n    \"\"\"\n    if position is None and Vector:\n        position = Vector((0, 0, 0))\n\n    galaxy = ProceduralAstronomy.create_galaxy_structure(\n        center=position, galaxy_type=galaxy_type, num_stars=num_stars, radius=radius\n    )\n\n    self.scene_objects[f\"galaxy_{galaxy_type}\"] = galaxy\n\n    # Add appropriate stellar material\n    if galaxy_type == \"spiral\":\n        material = AstronomicalMaterials.create_stellar_classification_material(\"B\")\n    elif galaxy_type == \"elliptical\":\n        material = AstronomicalMaterials.create_stellar_classification_material(\"K\")\n    else:  # irregular\n        material = AstronomicalMaterials.create_stellar_classification_material(\"M\")\n\n    galaxy.data.materials.append(material)\n\n    print(f\"Created {galaxy_type} galaxy with {num_stars} stars\")\n    return galaxy\n</code></pre>"},{"location":"api/astro_lab.widgets.bpy.advanced/#astro_lab.widgets.bpy.advanced.VisualizationSuite.render_scene","title":"render_scene","text":"<pre><code>render_scene(\n    output_path: str, resolution: Tuple[int, int] = (1920, 1080), samples: int = 128\n) -&gt; bool\n</code></pre> <p>Render the advanced astronomical scene.</p> <p>Parameters:</p> Name Type Description Default <code>output_path</code> <code>str</code> <p>Output file path</p> required <code>resolution</code> <code>Tuple[int, int]</code> <p>Render resolution (width, height)</p> <code>(1920, 1080)</code> <code>samples</code> <code>int</code> <p>Number of samples for quality</p> <code>128</code> <p>Returns:</p> Type Description <code>bool</code> <p>True if successful</p> Source code in <code>src\\astro_lab\\widgets\\bpy\\advanced\\__init__.py</code> <pre><code>def render_scene(\n    self,\n    output_path: str,\n    resolution: Tuple[int, int] = (1920, 1080),\n    samples: int = 128,\n) -&gt; bool:\n    \"\"\"\n    Render the advanced astronomical scene.\n\n    Args:\n        output_path: Output file path\n        resolution: Render resolution (width, height)\n        samples: Number of samples for quality\n\n    Returns:\n        True if successful\n    \"\"\"\n    # Convert to absolute path relative to current working directory\n    if not os.path.isabs(output_path):\n        output_path = str(Path.cwd() / output_path)\n\n    # Ensure directory exists\n    output_dir = Path(output_path).parent\n    output_dir.mkdir(parents=True, exist_ok=True)\n\n    # Set render settings\n    scene = bpy.context.scene\n    scene.render.resolution_x = resolution[0]\n    scene.render.resolution_y = resolution[1]\n    scene.render.filepath = output_path\n\n    if hasattr(scene.eevee, \"taa_render_samples\"):\n        scene.eevee.taa_render_samples = samples\n\n    # Render\n    try:\n        bpy.ops.render.render(write_still=True)\n        print(f\"Rendered scene to {output_path}\")\n        return True\n    except Exception as e:\n        print(f\"Render failed: {e}\")\n        return False\n</code></pre>"},{"location":"api/astro_lab.widgets.bpy.advanced/#astro_lab.widgets.bpy.advanced.VisualizationSuite.setup_advanced_lighting","title":"setup_advanced_lighting","text":"<pre><code>setup_advanced_lighting(preset: str = 'deep_space') -&gt; List[Any]\n</code></pre> <p>Setup advanced lighting for astronomical scenes.</p> <p>Parameters:</p> Name Type Description Default <code>preset</code> <code>str</code> <p>Lighting preset ('deep_space', 'nebula', 'planetary')</p> <code>'deep_space'</code> <p>Returns:</p> Type Description <code>List[Any]</code> <p>List of created light objects</p> Source code in <code>src\\astro_lab\\widgets\\bpy\\advanced\\__init__.py</code> <pre><code>def setup_advanced_lighting(self, preset: str = \"deep_space\") -&gt; List[Any]:\n    \"\"\"\n    Setup advanced lighting for astronomical scenes.\n\n    Args:\n        preset: Lighting preset ('deep_space', 'nebula', 'planetary')\n\n    Returns:\n        List of created light objects\n    \"\"\"\n    lights = []\n\n    if preset == \"deep_space\":\n        # Ambient starlight\n        bpy.ops.object.light_add(type=\"SUN\", location=(50, 50, 50))\n        ambient = bpy.context.active_object\n        ambient.name = \"StarfieldAmbient\"\n        ambient.data.energy = 0.1\n        ambient.data.color = (0.8, 0.9, 1.0)\n        lights.append(ambient)\n\n        # Key celestial light\n        bpy.ops.object.light_add(type=\"SUN\", location=(-30, -30, 40))\n        key_light = bpy.context.active_object\n        key_light.name = \"CelestialKey\"\n        key_light.data.energy = 2.0\n        key_light.data.color = (1.0, 0.9, 0.8)\n        lights.append(key_light)\n\n    elif preset == \"nebula\":\n        # Warm nebula illumination\n        bpy.ops.object.light_add(type=\"AREA\", location=(20, 0, 20))\n        nebula_light = bpy.context.active_object\n        nebula_light.name = \"NebulaIllumination\"\n        nebula_light.data.energy = 100.0\n        nebula_light.data.color = (1.0, 0.6, 0.4)\n        nebula_light.data.size = 10.0\n        lights.append(nebula_light)\n\n    else:  # planetary\n        # Solar illumination\n        bpy.ops.object.light_add(type=\"SUN\", location=(100, 0, 50))\n        solar = bpy.context.active_object\n        solar.name = \"SolarIllumination\"\n        solar.data.energy = 5.0\n        solar.data.color = (1.0, 0.95, 0.9)\n        lights.append(solar)\n\n    self.scene_objects[\"lights\"] = lights\n\n    print(f\"Setup {preset} lighting with {len(lights)} lights\")\n    return lights\n</code></pre>"},{"location":"api/astro_lab.widgets.bpy.advanced/#astro_lab.widgets.bpy.advanced.VisualizationSuite.setup_cinematic_camera","title":"setup_cinematic_camera","text":"<pre><code>setup_cinematic_camera(\n    target: Vector = Vector((0, 0, 0)), distance: float = 30.0, height: float = 10.0\n) -&gt; Any\n</code></pre> <p>Setup cinematic camera for astronomical visualization.</p> <p>Parameters:</p> Name Type Description Default <code>target</code> <code>Vector</code> <p>Camera target position</p> <code>Vector((0, 0, 0))</code> <code>distance</code> <code>float</code> <p>Distance from target</p> <code>30.0</code> <code>height</code> <code>float</code> <p>Height above target plane</p> <code>10.0</code> <p>Returns:</p> Type Description <code>Any</code> <p>Created camera object</p> Source code in <code>src\\astro_lab\\widgets\\bpy\\advanced\\__init__.py</code> <pre><code>def setup_cinematic_camera(\n    self,\n    target: Vector = Vector((0, 0, 0)),\n    distance: float = 30.0,\n    height: float = 10.0,\n) -&gt; Any:\n    \"\"\"\n    Setup cinematic camera for astronomical visualization.\n\n    Args:\n        target: Camera target position\n        distance: Distance from target\n        height: Height above target plane\n\n    Returns:\n        Created camera object\n    \"\"\"\n    # Calculate camera position\n    angle = math.radians(45)  # 45-degree angle\n    camera_pos = Vector(\n        (\n            target.x + distance * math.cos(angle),\n            target.y - distance * math.sin(angle),\n            target.z + height,\n        )\n    )\n\n    # Create camera\n    bpy.ops.object.camera_add(location=camera_pos)\n    camera = bpy.context.active_object\n    camera.name = \"AstroCinematicCamera\"\n\n    # Point at target\n    direction = target - camera_pos\n    camera.rotation_euler = direction.to_track_quat(\"-Z\", \"Y\").to_euler()\n\n    # Set camera properties\n    camera.data.lens_unit = \"FOV\"\n    camera.data.angle = math.radians(35)  # 35mm equivalent\n    camera.data.clip_start = 0.1\n    camera.data.clip_end = 10000.0\n    camera.data.dof.use_dof = True\n    camera.data.dof.focus_distance = distance\n\n    # Set as active camera\n    bpy.context.scene.camera = camera\n\n    self.scene_objects[\"camera\"] = camera\n\n    print(f\"Setup cinematic camera at distance {distance}\")\n    return camera\n</code></pre>"},{"location":"api/astro_lab.widgets.bpy.advanced/#astro_lab.widgets.bpy.advanced.VolumetricAstronomy","title":"VolumetricAstronomy","text":"<p>Create volumetric astronomical phenomena</p> <p>Methods:</p> Name Description <code>create_emission_nebula</code> <p>Create an emission nebula with realistic structure.</p> <code>create_galactic_dust_lane</code> <p>Create galactic dust lane with absorption and scattering.</p> <code>create_planetary_atmosphere</code> <p>Create layered planetary atmosphere with scattering.</p> <code>create_stellar_wind</code> <p>Create stellar wind visualization around a star.</p> Source code in <code>src\\astro_lab\\widgets\\bpy\\advanced\\volumetrics.py</code> <pre><code>class VolumetricAstronomy:\n    \"\"\"Create volumetric astronomical phenomena\"\"\"\n\n    @staticmethod  # type: ignore\n    def create_emission_nebula(\n        center: Vector = Vector((0, 0, 0)),\n        size: float = 10.0,\n        nebula_type: str = \"h_alpha\",\n        density: float = 0.1,\n    ) -&gt; bpy.types.Object:  # type: ignore\n        \"\"\"\n        Create an emission nebula with realistic structure.\n\n        Args:\n            center: Nebula center position\n            size: Nebula size\n            nebula_type: Type ('h_alpha', 'oxygen', 'planetary', 'supernova')\n            density: Base density value\n\n        Returns:\n            Created nebula object\n        \"\"\"\n        # Create volume container\n        bpy.ops.mesh.primitive_cube_add(location=center, size=size)\n        nebula_obj = bpy.context.active_object\n        nebula_obj.name = f\"{nebula_type.title()}Nebula\"\n\n        # Apply volumetric material\n        nebula_mat = VolumetricShaders.create_emission_nebula_material(nebula_type)\n        nebula_obj.data.materials.append(nebula_mat)\n\n        # Add geometry nodes for structure\n        VolumetricAstronomy._add_nebula_structure_nodes(nebula_obj, density)\n\n        return nebula_obj\n\n    @staticmethod  # type: ignore\n    def create_stellar_wind(\n        star_obj: bpy.types.Object,\n        wind_speed: float = 500.0,\n        mass_loss_rate: float = 1e-6,\n        wind_radius: float = 5.0,\n    ) -&gt; bpy.types.Object:  # type: ignore # noqa: F821\n        \"\"\"\n        Create stellar wind visualization around a star.\n\n        Args:\n            star_obj: Star object to create wind around\n            wind_speed: Wind velocity in km/s\n            mass_loss_rate: Mass loss rate in solar masses per year\n            wind_radius: Maximum wind radius\n\n        Returns:\n            Created stellar wind object\n        \"\"\"\n        # Create wind sphere\n        bpy.ops.mesh.primitive_uv_sphere_add(\n            location=star_obj.location, radius=wind_radius\n        )\n        wind_obj = bpy.context.active_object\n        wind_obj.name = f\"{star_obj.name}_StellarWind\"\n\n        # Apply stellar wind material\n        wind_mat = VolumetricShaders.create_stellar_wind_material(wind_speed)\n        wind_obj.data.materials.append(wind_mat)\n\n        # Add particle system for wind dynamics\n        VolumetricAstronomy._add_wind_particles(wind_obj, wind_speed, mass_loss_rate)\n\n        return wind_obj\n\n    @staticmethod  # type: ignore\n    def create_planetary_atmosphere(\n        planet_obj: bpy.types.Object,\n        atmosphere_type: str = \"earth_like\",\n        thickness: float = 0.5,\n    ) -&gt; bpy.types.Object:  # type: ignore # noqa: F821\n        \"\"\"\n        Create layered planetary atmosphere with scattering.\n\n        Args:\n            planet_obj: Planet object\n            atmosphere_type: Type ('earth_like', 'mars_like', 'venus_like', 'gas_giant')\n            thickness: Atmosphere thickness relative to planet radius\n\n        Returns:\n            Created atmosphere object\n        \"\"\"\n        # Get planet scale\n        planet_radius = max(planet_obj.scale)\n        atmo_radius = planet_radius * (1.0 + thickness)\n\n        # Create atmosphere sphere\n        bpy.ops.mesh.primitive_uv_sphere_add(\n            location=planet_obj.location, radius=atmo_radius\n        )\n        atmo_obj = bpy.context.active_object\n        atmo_obj.name = f\"{planet_obj.name}_Atmosphere\"\n\n        # Apply atmospheric material\n        atmo_mat = VolumetricShaders.create_atmospheric_material(atmosphere_type)\n        atmo_obj.data.materials.append(atmo_mat)\n\n        return atmo_obj\n\n    @staticmethod  # type: ignore\n    def create_galactic_dust_lane(\n        start_pos: Vector,\n        end_pos: Vector,\n        width: float = 2.0,\n        dust_density: float = 0.05,\n    ) -&gt; bpy.types.Object:  # type: ignore # noqa: F821\n        \"\"\"\n        Create galactic dust lane with absorption and scattering.\n\n        Args:\n            start_pos: Start position of dust lane\n            end_pos: End position of dust lane\n            width: Width of the dust lane\n            dust_density: Dust density\n\n        Returns:\n            Created dust lane object\n        \"\"\"\n        # Calculate dust lane parameters\n        direction = end_pos - start_pos\n        length = direction.length\n        center = (start_pos + end_pos) / 2\n\n        # Create dust lane geometry\n        bpy.ops.mesh.primitive_cube_add(location=center)\n        dust_obj = bpy.context.active_object\n        dust_obj.name = \"GalacticDustLane\"\n\n        # Scale and orient\n        dust_obj.scale = Vector((length, width, width * 0.5))\n        dust_obj.rotation_euler = direction.to_track_quat(\"X\", \"Z\").to_euler()\n\n        # Apply dust material\n        dust_mat = VolumetricShaders.create_dust_lane_material(dust_density)\n        dust_obj.data.materials.append(dust_mat)\n\n        return dust_obj\n\n    @staticmethod  # type: ignore\n    def _add_nebula_structure_nodes(obj: bpy.types.Object, density: float) -&gt; None:  # type: ignore # noqa: F821\n        \"\"\"Add geometry nodes for nebula structure.\"\"\"\n        # Add geometry nodes modifier\n        modifier = obj.modifiers.new(name=\"NebulaStructure\", type=\"NODES\")\n        tree = bpy.data.node_groups.new(name=\"NebulaStructure\", type=\"GeometryNodeTree\")\n        modifier.node_group = tree\n\n        nodes = tree.nodes\n        links = tree.links\n\n        # Clear and setup\n        nodes.clear()\n        tree.interface.clear()\n\n        # Create interface\n        tree.interface.new_socket(\n            name=\"Geometry\", in_out=\"INPUT\", socket_type=\"NodeSocketGeometry\"\n        )\n        tree.interface.new_socket(\n            name=\"Density\", in_out=\"INPUT\", socket_type=\"NodeSocketFloat\"\n        )\n        tree.interface.new_socket(\n            name=\"Turbulence\", in_out=\"INPUT\", socket_type=\"NodeSocketFloat\"\n        )\n        tree.interface.new_socket(\n            name=\"Geometry\", in_out=\"OUTPUT\", socket_type=\"NodeSocketGeometry\"\n        )\n\n        # Set defaults\n        modifier[\"Input_2\"] = density\n        modifier[\"Input_3\"] = 2.0  # Turbulence\n\n        # Create nodes (simplified for space)\n        input_node = nodes.new(\"NodeGroupInput\")\n        output_node = nodes.new(\"NodeGroupOutput\")\n\n        # Position nodes\n        input_node.location = (-200, 0)\n        output_node.location = (200, 0)\n\n        # Connect\n        links.new(input_node.outputs[\"Geometry\"], output_node.inputs[\"Geometry\"])\n\n    @staticmethod  # type: ignore\n    def _add_wind_particles(\n        obj: bpy.types.Object, wind_speed: float, mass_loss_rate: float\n    ) -&gt; None:  # type: ignore\n        \"\"\"Add particle system for stellar wind.\"\"\"\n        # Add particle system\n        particle_settings = bpy.data.particles.new(\"StellarWindParticles\")\n        particle_modifier = obj.modifiers.new(\"StellarWind\", \"PARTICLE_SYSTEM\")\n        particle_modifier.particle_system.settings = particle_settings\n\n        # Configure particles\n        particle_settings.type = \"EMITTER\"\n        particle_settings.count = int(mass_loss_rate * 1e6)  # Scale appropriately\n        particle_settings.emit_from = \"VOLUME\"\n        particle_settings.normal_factor = wind_speed / 100.0\n        particle_settings.lifetime = 100\n        particle_settings.render_type = \"NONE\"  # Volume only\n</code></pre>"},{"location":"api/astro_lab.widgets.bpy.advanced/#astro_lab.widgets.bpy.advanced.VolumetricAstronomy.create_emission_nebula","title":"create_emission_nebula  <code>staticmethod</code>","text":"<pre><code>create_emission_nebula(\n    center: Vector = Vector((0, 0, 0)),\n    size: float = 10.0,\n    nebula_type: str = \"h_alpha\",\n    density: float = 0.1,\n) -&gt; Object\n</code></pre> <p>Create an emission nebula with realistic structure.</p> <p>Parameters:</p> Name Type Description Default <code>center</code> <code>Vector</code> <p>Nebula center position</p> <code>Vector((0, 0, 0))</code> <code>size</code> <code>float</code> <p>Nebula size</p> <code>10.0</code> <code>nebula_type</code> <code>str</code> <p>Type ('h_alpha', 'oxygen', 'planetary', 'supernova')</p> <code>'h_alpha'</code> <code>density</code> <code>float</code> <p>Base density value</p> <code>0.1</code> <p>Returns:</p> Type Description <code>Object</code> <p>Created nebula object</p> Source code in <code>src\\astro_lab\\widgets\\bpy\\advanced\\volumetrics.py</code> <pre><code>@staticmethod  # type: ignore\ndef create_emission_nebula(\n    center: Vector = Vector((0, 0, 0)),\n    size: float = 10.0,\n    nebula_type: str = \"h_alpha\",\n    density: float = 0.1,\n) -&gt; bpy.types.Object:  # type: ignore\n    \"\"\"\n    Create an emission nebula with realistic structure.\n\n    Args:\n        center: Nebula center position\n        size: Nebula size\n        nebula_type: Type ('h_alpha', 'oxygen', 'planetary', 'supernova')\n        density: Base density value\n\n    Returns:\n        Created nebula object\n    \"\"\"\n    # Create volume container\n    bpy.ops.mesh.primitive_cube_add(location=center, size=size)\n    nebula_obj = bpy.context.active_object\n    nebula_obj.name = f\"{nebula_type.title()}Nebula\"\n\n    # Apply volumetric material\n    nebula_mat = VolumetricShaders.create_emission_nebula_material(nebula_type)\n    nebula_obj.data.materials.append(nebula_mat)\n\n    # Add geometry nodes for structure\n    VolumetricAstronomy._add_nebula_structure_nodes(nebula_obj, density)\n\n    return nebula_obj\n</code></pre>"},{"location":"api/astro_lab.widgets.bpy.advanced/#astro_lab.widgets.bpy.advanced.VolumetricAstronomy.create_galactic_dust_lane","title":"create_galactic_dust_lane  <code>staticmethod</code>","text":"<pre><code>create_galactic_dust_lane(\n    start_pos: Vector, end_pos: Vector, width: float = 2.0, dust_density: float = 0.05\n) -&gt; Object\n</code></pre> <p>Create galactic dust lane with absorption and scattering.</p> <p>Parameters:</p> Name Type Description Default <code>start_pos</code> <code>Vector</code> <p>Start position of dust lane</p> required <code>end_pos</code> <code>Vector</code> <p>End position of dust lane</p> required <code>width</code> <code>float</code> <p>Width of the dust lane</p> <code>2.0</code> <code>dust_density</code> <code>float</code> <p>Dust density</p> <code>0.05</code> <p>Returns:</p> Type Description <code>Object</code> <p>Created dust lane object</p> Source code in <code>src\\astro_lab\\widgets\\bpy\\advanced\\volumetrics.py</code> <pre><code>@staticmethod  # type: ignore\ndef create_galactic_dust_lane(\n    start_pos: Vector,\n    end_pos: Vector,\n    width: float = 2.0,\n    dust_density: float = 0.05,\n) -&gt; bpy.types.Object:  # type: ignore # noqa: F821\n    \"\"\"\n    Create galactic dust lane with absorption and scattering.\n\n    Args:\n        start_pos: Start position of dust lane\n        end_pos: End position of dust lane\n        width: Width of the dust lane\n        dust_density: Dust density\n\n    Returns:\n        Created dust lane object\n    \"\"\"\n    # Calculate dust lane parameters\n    direction = end_pos - start_pos\n    length = direction.length\n    center = (start_pos + end_pos) / 2\n\n    # Create dust lane geometry\n    bpy.ops.mesh.primitive_cube_add(location=center)\n    dust_obj = bpy.context.active_object\n    dust_obj.name = \"GalacticDustLane\"\n\n    # Scale and orient\n    dust_obj.scale = Vector((length, width, width * 0.5))\n    dust_obj.rotation_euler = direction.to_track_quat(\"X\", \"Z\").to_euler()\n\n    # Apply dust material\n    dust_mat = VolumetricShaders.create_dust_lane_material(dust_density)\n    dust_obj.data.materials.append(dust_mat)\n\n    return dust_obj\n</code></pre>"},{"location":"api/astro_lab.widgets.bpy.advanced/#astro_lab.widgets.bpy.advanced.VolumetricAstronomy.create_planetary_atmosphere","title":"create_planetary_atmosphere  <code>staticmethod</code>","text":"<pre><code>create_planetary_atmosphere(\n    planet_obj: Object, atmosphere_type: str = \"earth_like\", thickness: float = 0.5\n) -&gt; Object\n</code></pre> <p>Create layered planetary atmosphere with scattering.</p> <p>Parameters:</p> Name Type Description Default <code>planet_obj</code> <code>Object</code> <p>Planet object</p> required <code>atmosphere_type</code> <code>str</code> <p>Type ('earth_like', 'mars_like', 'venus_like', 'gas_giant')</p> <code>'earth_like'</code> <code>thickness</code> <code>float</code> <p>Atmosphere thickness relative to planet radius</p> <code>0.5</code> <p>Returns:</p> Type Description <code>Object</code> <p>Created atmosphere object</p> Source code in <code>src\\astro_lab\\widgets\\bpy\\advanced\\volumetrics.py</code> <pre><code>@staticmethod  # type: ignore\ndef create_planetary_atmosphere(\n    planet_obj: bpy.types.Object,\n    atmosphere_type: str = \"earth_like\",\n    thickness: float = 0.5,\n) -&gt; bpy.types.Object:  # type: ignore # noqa: F821\n    \"\"\"\n    Create layered planetary atmosphere with scattering.\n\n    Args:\n        planet_obj: Planet object\n        atmosphere_type: Type ('earth_like', 'mars_like', 'venus_like', 'gas_giant')\n        thickness: Atmosphere thickness relative to planet radius\n\n    Returns:\n        Created atmosphere object\n    \"\"\"\n    # Get planet scale\n    planet_radius = max(planet_obj.scale)\n    atmo_radius = planet_radius * (1.0 + thickness)\n\n    # Create atmosphere sphere\n    bpy.ops.mesh.primitive_uv_sphere_add(\n        location=planet_obj.location, radius=atmo_radius\n    )\n    atmo_obj = bpy.context.active_object\n    atmo_obj.name = f\"{planet_obj.name}_Atmosphere\"\n\n    # Apply atmospheric material\n    atmo_mat = VolumetricShaders.create_atmospheric_material(atmosphere_type)\n    atmo_obj.data.materials.append(atmo_mat)\n\n    return atmo_obj\n</code></pre>"},{"location":"api/astro_lab.widgets.bpy.advanced/#astro_lab.widgets.bpy.advanced.VolumetricAstronomy.create_stellar_wind","title":"create_stellar_wind  <code>staticmethod</code>","text":"<pre><code>create_stellar_wind(\n    star_obj: Object,\n    wind_speed: float = 500.0,\n    mass_loss_rate: float = 1e-06,\n    wind_radius: float = 5.0,\n) -&gt; Object\n</code></pre> <p>Create stellar wind visualization around a star.</p> <p>Parameters:</p> Name Type Description Default <code>star_obj</code> <code>Object</code> <p>Star object to create wind around</p> required <code>wind_speed</code> <code>float</code> <p>Wind velocity in km/s</p> <code>500.0</code> <code>mass_loss_rate</code> <code>float</code> <p>Mass loss rate in solar masses per year</p> <code>1e-06</code> <code>wind_radius</code> <code>float</code> <p>Maximum wind radius</p> <code>5.0</code> <p>Returns:</p> Type Description <code>Object</code> <p>Created stellar wind object</p> Source code in <code>src\\astro_lab\\widgets\\bpy\\advanced\\volumetrics.py</code> <pre><code>@staticmethod  # type: ignore\ndef create_stellar_wind(\n    star_obj: bpy.types.Object,\n    wind_speed: float = 500.0,\n    mass_loss_rate: float = 1e-6,\n    wind_radius: float = 5.0,\n) -&gt; bpy.types.Object:  # type: ignore # noqa: F821\n    \"\"\"\n    Create stellar wind visualization around a star.\n\n    Args:\n        star_obj: Star object to create wind around\n        wind_speed: Wind velocity in km/s\n        mass_loss_rate: Mass loss rate in solar masses per year\n        wind_radius: Maximum wind radius\n\n    Returns:\n        Created stellar wind object\n    \"\"\"\n    # Create wind sphere\n    bpy.ops.mesh.primitive_uv_sphere_add(\n        location=star_obj.location, radius=wind_radius\n    )\n    wind_obj = bpy.context.active_object\n    wind_obj.name = f\"{star_obj.name}_StellarWind\"\n\n    # Apply stellar wind material\n    wind_mat = VolumetricShaders.create_stellar_wind_material(wind_speed)\n    wind_obj.data.materials.append(wind_mat)\n\n    # Add particle system for wind dynamics\n    VolumetricAstronomy._add_wind_particles(wind_obj, wind_speed, mass_loss_rate)\n\n    return wind_obj\n</code></pre>"},{"location":"api/astro_lab.widgets.bpy.advanced/#astro_lab.widgets.bpy.advanced.VolumetricShaders","title":"VolumetricShaders","text":"<p>Volumetric shaders for astronomical phenomena</p> <p>Methods:</p> Name Description <code>create_atmospheric_material</code> <p>Create planetary atmosphere material with Rayleigh scattering.</p> <code>create_dust_lane_material</code> <p>Create galactic dust lane material with absorption.</p> <code>create_emission_nebula_material</code> <p>Create emission nebula material based on spectral lines.</p> <code>create_stellar_wind_material</code> <p>Create stellar wind material with velocity-based opacity.</p> Source code in <code>src\\astro_lab\\widgets\\bpy\\advanced\\volumetrics.py</code> <pre><code>class VolumetricShaders:\n    \"\"\"Volumetric shaders for astronomical phenomena\"\"\"\n\n    @staticmethod  # type: ignore\n    def create_emission_nebula_material(nebula_type: str) -&gt; bpy.types.Material:  # type: ignore # noqa: F821\n        \"\"\"\n        Create emission nebula material based on spectral lines.\n\n        Args:\n            nebula_type: Type of emission nebula\n\n        Returns:\n            Created material\n        \"\"\"\n        mat = bpy.data.materials.new(name=f\"Nebula_{nebula_type}\")\n        mat.use_nodes = True\n        nodes = mat.node_tree.nodes\n        links = mat.node_tree.links\n\n        # Clear default nodes\n        nodes.clear()\n\n        # Add nodes\n        output = nodes.new(\"ShaderNodeOutputMaterial\")\n        emission = nodes.new(\"ShaderNodeVolumeEmission\")\n\n        # Spectral line colors\n        spectral_colors = {\n            \"h_alpha\": (0.8, 0.2, 0.2, 1.0),  # Red - Hydrogen alpha\n            \"oxygen\": (0.2, 0.8, 0.3, 1.0),  # Green - Oxygen III\n            \"planetary\": (0.3, 0.6, 1.0, 1.0),  # Blue-green mix\n            \"supernova\": (1.0, 0.6, 0.2, 1.0),  # Orange-yellow\n        }\n\n        color = spectral_colors.get(nebula_type, spectral_colors[\"h_alpha\"])\n        emission.inputs[\"Color\"].default_value = color\n\n        # Add noise for structure\n        noise1 = nodes.new(\"ShaderNodeTexNoise\")\n        noise1.inputs[\"Scale\"].default_value = 2.0\n        noise1.inputs[\"Detail\"].default_value = 8.0\n\n        noise2 = nodes.new(\"ShaderNodeTexNoise\")\n        noise2.inputs[\"Scale\"].default_value = 0.5\n        noise2.inputs[\"Detail\"].default_value = 4.0\n\n        # Combine noises\n        multiply = nodes.new(\"ShaderNodeMath\")\n        multiply.operation = \"MULTIPLY\"\n\n        # Color ramp for density variation\n        ramp = nodes.new(\"ShaderNodeValToRGB\")\n        ramp.color_ramp.elements[0].position = 0.2\n        ramp.color_ramp.elements[1].position = 0.8\n\n        # Position nodes\n        noise1.location = (-600, 0)\n        noise2.location = (-600, -200)\n        multiply.location = (-400, -100)\n        ramp.location = (-200, 0)\n        emission.location = (0, 0)\n        output.location = (200, 0)\n\n        # Connect nodes\n        links.new(noise1.outputs[\"Fac\"], multiply.inputs[0])\n        links.new(noise2.outputs[\"Fac\"], multiply.inputs[1])\n        links.new(multiply.outputs[\"Value\"], ramp.inputs[\"Fac\"])\n        links.new(ramp.outputs[\"Color\"], emission.inputs[\"Density\"])\n        links.new(emission.outputs[\"Volume\"], output.inputs[\"Volume\"])\n\n        # Set emission strength\n        emission.inputs[\"Strength\"].default_value = 2.0\n\n        return mat\n\n    @staticmethod  # type: ignore\n    def create_stellar_wind_material(wind_speed: float) -&gt; bpy.types.Material:  # type: ignore # noqa: F821\n        \"\"\"\n        Create stellar wind material with velocity-based opacity.\n\n        Args:\n            wind_speed: Wind velocity in km/s\n\n        Returns:\n            Created material\n        \"\"\"\n        mat = bpy.data.materials.new(name=\"StellarWind\")\n        mat.use_nodes = True\n        nodes = mat.node_tree.nodes\n        links = mat.node_tree.links\n\n        # Clear default nodes\n        nodes.clear()\n\n        # Add nodes\n        output = nodes.new(\"ShaderNodeOutputMaterial\")\n        scatter = nodes.new(\"ShaderNodeVolumeScatter\")\n\n        # Wind color based on temperature\n        if wind_speed &gt; 1000:  # Fast wind - hot\n            color = (0.8, 0.6, 1.0, 1.0)  # Blue-white\n        elif wind_speed &gt; 300:  # Medium wind\n            color = (1.0, 0.8, 0.6, 1.0)  # Yellow-white\n        else:  # Slow wind - cool\n            color = (1.0, 0.6, 0.4, 1.0)  # Orange\n\n        scatter.inputs[\"Color\"].default_value = color\n\n        # Radial falloff from center\n        geometry = nodes.new(\"GeometryNodeInputPosition\")\n        vector_length = nodes.new(\"ShaderNodeVectorMath\")\n        vector_length.operation = \"LENGTH\"\n\n        # Density falloff\n        divide = nodes.new(\"ShaderNodeMath\")\n        divide.operation = \"DIVIDE\"\n        divide.inputs[1].default_value = 5.0  # Falloff rate\n\n        power = nodes.new(\"ShaderNodeMath\")\n        power.operation = \"POWER\"\n        power.inputs[1].default_value = 2.0  # Square falloff\n\n        subtract = nodes.new(\"ShaderNodeMath\")\n        subtract.operation = \"SUBTRACT\"\n        subtract.inputs[0].default_value = 1.0\n\n        # Position nodes\n        geometry.location = (-400, -200)\n        vector_length.location = (-200, -200)\n        divide.location = (0, -200)\n        power.location = (200, -200)\n        subtract.location = (400, -200)\n        scatter.location = (600, 0)\n        output.location = (800, 0)\n\n        # Connect nodes\n        links.new(geometry.outputs[\"Position\"], vector_length.inputs[0])\n        links.new(vector_length.outputs[\"Value\"], divide.inputs[0])\n        links.new(divide.outputs[\"Value\"], power.inputs[0])\n        links.new(power.outputs[\"Value\"], subtract.inputs[1])\n        links.new(subtract.outputs[\"Value\"], scatter.inputs[\"Density\"])\n        links.new(scatter.outputs[\"Volume\"], output.inputs[\"Volume\"])\n\n        return mat\n\n    @staticmethod  # type: ignore\n    def create_atmospheric_material(atmosphere_type: str) -&gt; bpy.types.Material:  # type: ignore # noqa: F821\n        \"\"\"\n        Create planetary atmosphere material with Rayleigh scattering.\n\n        Args:\n            atmosphere_type: Type of atmosphere\n\n        Returns:\n            Created material\n        \"\"\"\n        mat = bpy.data.materials.new(name=f\"Atmosphere_{atmosphere_type}\")\n        mat.use_nodes = True\n        nodes = mat.node_tree.nodes\n        links = mat.node_tree.links\n\n        # Clear default nodes\n        nodes.clear()\n\n        # Add nodes\n        output = nodes.new(\"ShaderNodeOutputMaterial\")\n        scatter = nodes.new(\"ShaderNodeVolumeScatter\")\n\n        # Atmospheric compositions\n        atmosphere_colors = {\n            \"earth_like\": (0.3, 0.6, 1.0, 1.0),  # Blue - Rayleigh scattering\n            \"mars_like\": (0.8, 0.5, 0.3, 1.0),  # Dusty orange\n            \"venus_like\": (0.9, 0.8, 0.6, 1.0),  # Thick yellow\n            \"gas_giant\": (0.7, 0.4, 0.2, 1.0),  # Banded colors\n        }\n\n        color = atmosphere_colors.get(atmosphere_type, atmosphere_colors[\"earth_like\"])\n        scatter.inputs[\"Color\"].default_value = color\n\n        # Density based on altitude (distance from center)\n        geometry = nodes.new(\"GeometryNodeInputPosition\")\n        vector_length = nodes.new(\"ShaderNodeVectorMath\")\n        vector_length.operation = \"LENGTH\"\n\n        # Exponential atmospheric falloff\n        subtract = nodes.new(\"ShaderNodeMath\")\n        subtract.operation = \"SUBTRACT\"\n        subtract.inputs[0].default_value = 2.0  # Atmosphere radius\n\n        multiply = nodes.new(\"ShaderNodeMath\")\n        multiply.operation = \"MULTIPLY\"\n        multiply.inputs[1].default_value = -2.0  # Scale height\n\n        power = nodes.new(\"ShaderNodeMath\")\n        power.operation = \"POWER\"\n        power.inputs[0].default_value = math.e\n\n        # Position nodes\n        geometry.location = (-400, -200)\n        vector_length.location = (-200, -200)\n        subtract.location = (0, -200)\n        multiply.location = (200, -200)\n        power.location = (400, -200)\n        scatter.location = (600, 0)\n        output.location = (800, 0)\n\n        # Connect nodes\n        links.new(geometry.outputs[\"Position\"], vector_length.inputs[0])\n        links.new(vector_length.outputs[\"Value\"], subtract.inputs[1])\n        links.new(subtract.outputs[\"Value\"], multiply.inputs[0])\n        links.new(multiply.outputs[\"Value\"], power.inputs[1])\n        links.new(power.outputs[\"Value\"], scatter.inputs[\"Density\"])\n        links.new(scatter.outputs[\"Volume\"], output.inputs[\"Volume\"])\n\n        return mat\n\n    @staticmethod  # type: ignore\n    def create_dust_lane_material(dust_density: float) -&gt; bpy.types.Material:  # type: ignore # noqa: F821\n        \"\"\"\n        Create galactic dust lane material with absorption.\n\n        Args:\n            dust_density: Dust density\n\n        Returns:\n            Created material\n        \"\"\"\n        mat = bpy.data.materials.new(name=\"GalacticDust\")\n        mat.use_nodes = True\n        nodes = mat.node_tree.nodes\n        links = mat.node_tree.links\n\n        # Clear default nodes\n        nodes.clear()\n\n        # Add nodes\n        output = nodes.new(\"ShaderNodeOutputMaterial\")\n        absorption = nodes.new(\"ShaderNodeVolumeAbsorption\")\n\n        # Dust is brownish and absorbs blue light more\n        absorption.inputs[\"Color\"].default_value = (0.3, 0.2, 0.1, 1.0)\n        absorption.inputs[\"Density\"].default_value = dust_density\n\n        # Add some scattering for realism\n        scatter = nodes.new(\"ShaderNodeVolumeScatter\")\n        scatter.inputs[\"Color\"].default_value = (0.4, 0.3, 0.2, 1.0)\n        scatter.inputs[\"Density\"].default_value = dust_density * 0.1\n\n        # Mix absorption and scattering\n        mix = nodes.new(\"ShaderNodeMixShader\")\n        mix.inputs[\"Fac\"].default_value = 0.9  # Mostly absorption\n\n        # Position nodes\n        absorption.location = (-200, 100)\n        scatter.location = (-200, -100)\n        mix.location = (0, 0)\n        output.location = (200, 0)\n\n        # Connect nodes\n        links.new(absorption.outputs[\"Volume\"], mix.inputs[1])\n        links.new(scatter.outputs[\"Volume\"], mix.inputs[2])\n        links.new(mix.outputs[\"Shader\"], output.inputs[\"Volume\"])\n\n        return mat\n</code></pre>"},{"location":"api/astro_lab.widgets.bpy.advanced/#astro_lab.widgets.bpy.advanced.VolumetricShaders.create_atmospheric_material","title":"create_atmospheric_material  <code>staticmethod</code>","text":"<pre><code>create_atmospheric_material(atmosphere_type: str) -&gt; Material\n</code></pre> <p>Create planetary atmosphere material with Rayleigh scattering.</p> <p>Parameters:</p> Name Type Description Default <code>atmosphere_type</code> <code>str</code> <p>Type of atmosphere</p> required <p>Returns:</p> Type Description <code>Material</code> <p>Created material</p> Source code in <code>src\\astro_lab\\widgets\\bpy\\advanced\\volumetrics.py</code> <pre><code>@staticmethod  # type: ignore\ndef create_atmospheric_material(atmosphere_type: str) -&gt; bpy.types.Material:  # type: ignore # noqa: F821\n    \"\"\"\n    Create planetary atmosphere material with Rayleigh scattering.\n\n    Args:\n        atmosphere_type: Type of atmosphere\n\n    Returns:\n        Created material\n    \"\"\"\n    mat = bpy.data.materials.new(name=f\"Atmosphere_{atmosphere_type}\")\n    mat.use_nodes = True\n    nodes = mat.node_tree.nodes\n    links = mat.node_tree.links\n\n    # Clear default nodes\n    nodes.clear()\n\n    # Add nodes\n    output = nodes.new(\"ShaderNodeOutputMaterial\")\n    scatter = nodes.new(\"ShaderNodeVolumeScatter\")\n\n    # Atmospheric compositions\n    atmosphere_colors = {\n        \"earth_like\": (0.3, 0.6, 1.0, 1.0),  # Blue - Rayleigh scattering\n        \"mars_like\": (0.8, 0.5, 0.3, 1.0),  # Dusty orange\n        \"venus_like\": (0.9, 0.8, 0.6, 1.0),  # Thick yellow\n        \"gas_giant\": (0.7, 0.4, 0.2, 1.0),  # Banded colors\n    }\n\n    color = atmosphere_colors.get(atmosphere_type, atmosphere_colors[\"earth_like\"])\n    scatter.inputs[\"Color\"].default_value = color\n\n    # Density based on altitude (distance from center)\n    geometry = nodes.new(\"GeometryNodeInputPosition\")\n    vector_length = nodes.new(\"ShaderNodeVectorMath\")\n    vector_length.operation = \"LENGTH\"\n\n    # Exponential atmospheric falloff\n    subtract = nodes.new(\"ShaderNodeMath\")\n    subtract.operation = \"SUBTRACT\"\n    subtract.inputs[0].default_value = 2.0  # Atmosphere radius\n\n    multiply = nodes.new(\"ShaderNodeMath\")\n    multiply.operation = \"MULTIPLY\"\n    multiply.inputs[1].default_value = -2.0  # Scale height\n\n    power = nodes.new(\"ShaderNodeMath\")\n    power.operation = \"POWER\"\n    power.inputs[0].default_value = math.e\n\n    # Position nodes\n    geometry.location = (-400, -200)\n    vector_length.location = (-200, -200)\n    subtract.location = (0, -200)\n    multiply.location = (200, -200)\n    power.location = (400, -200)\n    scatter.location = (600, 0)\n    output.location = (800, 0)\n\n    # Connect nodes\n    links.new(geometry.outputs[\"Position\"], vector_length.inputs[0])\n    links.new(vector_length.outputs[\"Value\"], subtract.inputs[1])\n    links.new(subtract.outputs[\"Value\"], multiply.inputs[0])\n    links.new(multiply.outputs[\"Value\"], power.inputs[1])\n    links.new(power.outputs[\"Value\"], scatter.inputs[\"Density\"])\n    links.new(scatter.outputs[\"Volume\"], output.inputs[\"Volume\"])\n\n    return mat\n</code></pre>"},{"location":"api/astro_lab.widgets.bpy.advanced/#astro_lab.widgets.bpy.advanced.VolumetricShaders.create_dust_lane_material","title":"create_dust_lane_material  <code>staticmethod</code>","text":"<pre><code>create_dust_lane_material(dust_density: float) -&gt; Material\n</code></pre> <p>Create galactic dust lane material with absorption.</p> <p>Parameters:</p> Name Type Description Default <code>dust_density</code> <code>float</code> <p>Dust density</p> required <p>Returns:</p> Type Description <code>Material</code> <p>Created material</p> Source code in <code>src\\astro_lab\\widgets\\bpy\\advanced\\volumetrics.py</code> <pre><code>@staticmethod  # type: ignore\ndef create_dust_lane_material(dust_density: float) -&gt; bpy.types.Material:  # type: ignore # noqa: F821\n    \"\"\"\n    Create galactic dust lane material with absorption.\n\n    Args:\n        dust_density: Dust density\n\n    Returns:\n        Created material\n    \"\"\"\n    mat = bpy.data.materials.new(name=\"GalacticDust\")\n    mat.use_nodes = True\n    nodes = mat.node_tree.nodes\n    links = mat.node_tree.links\n\n    # Clear default nodes\n    nodes.clear()\n\n    # Add nodes\n    output = nodes.new(\"ShaderNodeOutputMaterial\")\n    absorption = nodes.new(\"ShaderNodeVolumeAbsorption\")\n\n    # Dust is brownish and absorbs blue light more\n    absorption.inputs[\"Color\"].default_value = (0.3, 0.2, 0.1, 1.0)\n    absorption.inputs[\"Density\"].default_value = dust_density\n\n    # Add some scattering for realism\n    scatter = nodes.new(\"ShaderNodeVolumeScatter\")\n    scatter.inputs[\"Color\"].default_value = (0.4, 0.3, 0.2, 1.0)\n    scatter.inputs[\"Density\"].default_value = dust_density * 0.1\n\n    # Mix absorption and scattering\n    mix = nodes.new(\"ShaderNodeMixShader\")\n    mix.inputs[\"Fac\"].default_value = 0.9  # Mostly absorption\n\n    # Position nodes\n    absorption.location = (-200, 100)\n    scatter.location = (-200, -100)\n    mix.location = (0, 0)\n    output.location = (200, 0)\n\n    # Connect nodes\n    links.new(absorption.outputs[\"Volume\"], mix.inputs[1])\n    links.new(scatter.outputs[\"Volume\"], mix.inputs[2])\n    links.new(mix.outputs[\"Shader\"], output.inputs[\"Volume\"])\n\n    return mat\n</code></pre>"},{"location":"api/astro_lab.widgets.bpy.advanced/#astro_lab.widgets.bpy.advanced.VolumetricShaders.create_emission_nebula_material","title":"create_emission_nebula_material  <code>staticmethod</code>","text":"<pre><code>create_emission_nebula_material(nebula_type: str) -&gt; Material\n</code></pre> <p>Create emission nebula material based on spectral lines.</p> <p>Parameters:</p> Name Type Description Default <code>nebula_type</code> <code>str</code> <p>Type of emission nebula</p> required <p>Returns:</p> Type Description <code>Material</code> <p>Created material</p> Source code in <code>src\\astro_lab\\widgets\\bpy\\advanced\\volumetrics.py</code> <pre><code>@staticmethod  # type: ignore\ndef create_emission_nebula_material(nebula_type: str) -&gt; bpy.types.Material:  # type: ignore # noqa: F821\n    \"\"\"\n    Create emission nebula material based on spectral lines.\n\n    Args:\n        nebula_type: Type of emission nebula\n\n    Returns:\n        Created material\n    \"\"\"\n    mat = bpy.data.materials.new(name=f\"Nebula_{nebula_type}\")\n    mat.use_nodes = True\n    nodes = mat.node_tree.nodes\n    links = mat.node_tree.links\n\n    # Clear default nodes\n    nodes.clear()\n\n    # Add nodes\n    output = nodes.new(\"ShaderNodeOutputMaterial\")\n    emission = nodes.new(\"ShaderNodeVolumeEmission\")\n\n    # Spectral line colors\n    spectral_colors = {\n        \"h_alpha\": (0.8, 0.2, 0.2, 1.0),  # Red - Hydrogen alpha\n        \"oxygen\": (0.2, 0.8, 0.3, 1.0),  # Green - Oxygen III\n        \"planetary\": (0.3, 0.6, 1.0, 1.0),  # Blue-green mix\n        \"supernova\": (1.0, 0.6, 0.2, 1.0),  # Orange-yellow\n    }\n\n    color = spectral_colors.get(nebula_type, spectral_colors[\"h_alpha\"])\n    emission.inputs[\"Color\"].default_value = color\n\n    # Add noise for structure\n    noise1 = nodes.new(\"ShaderNodeTexNoise\")\n    noise1.inputs[\"Scale\"].default_value = 2.0\n    noise1.inputs[\"Detail\"].default_value = 8.0\n\n    noise2 = nodes.new(\"ShaderNodeTexNoise\")\n    noise2.inputs[\"Scale\"].default_value = 0.5\n    noise2.inputs[\"Detail\"].default_value = 4.0\n\n    # Combine noises\n    multiply = nodes.new(\"ShaderNodeMath\")\n    multiply.operation = \"MULTIPLY\"\n\n    # Color ramp for density variation\n    ramp = nodes.new(\"ShaderNodeValToRGB\")\n    ramp.color_ramp.elements[0].position = 0.2\n    ramp.color_ramp.elements[1].position = 0.8\n\n    # Position nodes\n    noise1.location = (-600, 0)\n    noise2.location = (-600, -200)\n    multiply.location = (-400, -100)\n    ramp.location = (-200, 0)\n    emission.location = (0, 0)\n    output.location = (200, 0)\n\n    # Connect nodes\n    links.new(noise1.outputs[\"Fac\"], multiply.inputs[0])\n    links.new(noise2.outputs[\"Fac\"], multiply.inputs[1])\n    links.new(multiply.outputs[\"Value\"], ramp.inputs[\"Fac\"])\n    links.new(ramp.outputs[\"Color\"], emission.inputs[\"Density\"])\n    links.new(emission.outputs[\"Volume\"], output.inputs[\"Volume\"])\n\n    # Set emission strength\n    emission.inputs[\"Strength\"].default_value = 2.0\n\n    return mat\n</code></pre>"},{"location":"api/astro_lab.widgets.bpy.advanced/#astro_lab.widgets.bpy.advanced.VolumetricShaders.create_stellar_wind_material","title":"create_stellar_wind_material  <code>staticmethod</code>","text":"<pre><code>create_stellar_wind_material(wind_speed: float) -&gt; Material\n</code></pre> <p>Create stellar wind material with velocity-based opacity.</p> <p>Parameters:</p> Name Type Description Default <code>wind_speed</code> <code>float</code> <p>Wind velocity in km/s</p> required <p>Returns:</p> Type Description <code>Material</code> <p>Created material</p> Source code in <code>src\\astro_lab\\widgets\\bpy\\advanced\\volumetrics.py</code> <pre><code>@staticmethod  # type: ignore\ndef create_stellar_wind_material(wind_speed: float) -&gt; bpy.types.Material:  # type: ignore # noqa: F821\n    \"\"\"\n    Create stellar wind material with velocity-based opacity.\n\n    Args:\n        wind_speed: Wind velocity in km/s\n\n    Returns:\n        Created material\n    \"\"\"\n    mat = bpy.data.materials.new(name=\"StellarWind\")\n    mat.use_nodes = True\n    nodes = mat.node_tree.nodes\n    links = mat.node_tree.links\n\n    # Clear default nodes\n    nodes.clear()\n\n    # Add nodes\n    output = nodes.new(\"ShaderNodeOutputMaterial\")\n    scatter = nodes.new(\"ShaderNodeVolumeScatter\")\n\n    # Wind color based on temperature\n    if wind_speed &gt; 1000:  # Fast wind - hot\n        color = (0.8, 0.6, 1.0, 1.0)  # Blue-white\n    elif wind_speed &gt; 300:  # Medium wind\n        color = (1.0, 0.8, 0.6, 1.0)  # Yellow-white\n    else:  # Slow wind - cool\n        color = (1.0, 0.6, 0.4, 1.0)  # Orange\n\n    scatter.inputs[\"Color\"].default_value = color\n\n    # Radial falloff from center\n    geometry = nodes.new(\"GeometryNodeInputPosition\")\n    vector_length = nodes.new(\"ShaderNodeVectorMath\")\n    vector_length.operation = \"LENGTH\"\n\n    # Density falloff\n    divide = nodes.new(\"ShaderNodeMath\")\n    divide.operation = \"DIVIDE\"\n    divide.inputs[1].default_value = 5.0  # Falloff rate\n\n    power = nodes.new(\"ShaderNodeMath\")\n    power.operation = \"POWER\"\n    power.inputs[1].default_value = 2.0  # Square falloff\n\n    subtract = nodes.new(\"ShaderNodeMath\")\n    subtract.operation = \"SUBTRACT\"\n    subtract.inputs[0].default_value = 1.0\n\n    # Position nodes\n    geometry.location = (-400, -200)\n    vector_length.location = (-200, -200)\n    divide.location = (0, -200)\n    power.location = (200, -200)\n    subtract.location = (400, -200)\n    scatter.location = (600, 0)\n    output.location = (800, 0)\n\n    # Connect nodes\n    links.new(geometry.outputs[\"Position\"], vector_length.inputs[0])\n    links.new(vector_length.outputs[\"Value\"], divide.inputs[0])\n    links.new(divide.outputs[\"Value\"], power.inputs[0])\n    links.new(power.outputs[\"Value\"], subtract.inputs[1])\n    links.new(subtract.outputs[\"Value\"], scatter.inputs[\"Density\"])\n    links.new(scatter.outputs[\"Volume\"], output.inputs[\"Volume\"])\n\n    return mat\n</code></pre>"},{"location":"api/astro_lab.widgets.bpy.advanced/#astro_lab.widgets.bpy.advanced.apply_material_preset","title":"apply_material_preset","text":"<pre><code>apply_material_preset(\n    object_name: str, material_preset: str = \"luxury_teal\"\n) -&gt; Material\n</code></pre> <p>Apply a material preset to an object.</p> <p>Parameters:</p> Name Type Description Default <code>object_name</code> <code>str</code> <p>Name of the object</p> required <code>material_preset</code> <code>str</code> <p>Material preset name</p> <code>'luxury_teal'</code> <p>Returns:</p> Type Description <code>Material</code> <p>Applied material</p> Source code in <code>src\\astro_lab\\widgets\\bpy\\advanced\\__init__.py</code> <pre><code>def apply_material_preset(\n    object_name: str,\n    material_preset: str = \"luxury_teal\",\n) -&gt; bpy.types.Material:\n    \"\"\"\n    Apply a material preset to an object.\n\n    Args:\n        object_name: Name of the object\n        material_preset: Material preset name\n\n    Returns:\n        Applied material\n    \"\"\"\n    obj = bpy.data.objects.get(object_name)\n    if not obj:\n        print(f\"Object {object_name} not found\")\n        return None\n\n    # Get material based on preset\n    if material_preset == \"luxury_teal\":\n        material = MaterialPresets.luxury_teal_material()\n    elif material_preset == \"golden_metallic\":\n        material = MaterialPresets.golden_metallic_material()\n    elif material_preset == \"crystal_glass\":\n        material = MaterialPresets.crystal_glass_material()\n    elif material_preset == \"holographic_blue\":\n        material = MaterialPresets.holographic_blue_material()\n    elif material_preset == \"energy_purple\":\n        material = MaterialPresets.energy_purple_material()\n    else:\n        print(f\"Unknown material preset: {material_preset}\")\n        return None\n\n    # Apply material to object\n    if obj.data.materials:\n        obj.data.materials[0] = material\n    else:\n        obj.data.materials.append(material)\n\n    return material\n</code></pre>"},{"location":"api/astro_lab.widgets.bpy.advanced/#astro_lab.widgets.bpy.advanced.apply_visual_style","title":"apply_visual_style","text":"<pre><code>apply_visual_style(suite: VisualizationSuite, style: str = 'luxury_teal') -&gt; None\n</code></pre> <p>Apply a high-level visual style preset to the current advanced scene. Styles: 'luxury_teal', 'autumn', 'iridescent', 'cinematic_closeup', 'dreamy_pastel'</p> Source code in <code>src\\astro_lab\\widgets\\bpy\\advanced\\__init__.py</code> <pre><code>def apply_visual_style(suite: VisualizationSuite, style: str = \"luxury_teal\") -&gt; None:\n    \"\"\"\n    Apply a high-level visual style preset to the current advanced scene.\n    Styles: 'luxury_teal', 'autumn', 'iridescent', 'cinematic_closeup', 'dreamy_pastel'\n    \"\"\"\n    import bpy\n    from mathutils import Vector\n\n    scene = bpy.context.scene\n\n    # Default: luxury_teal (deep teal, high contrast, bloom, dramatic light)\n    if style == \"luxury_teal\":\n        suite._initialize_scene()\n        suite.setup_cinematic_camera(Vector((0, 0, 0)), 40.0, 12.0)\n        suite.setup_advanced_lighting(\"deep_space\")\n        # Set world color to deep teal\n        world = scene.world\n        if world and world.use_nodes:\n            bg = world.node_tree.nodes.get(\"Background\")\n            if bg:\n                bg.inputs[\"Color\"].default_value = (0.02, 0.15, 0.18, 1.0)\n        # Bloom\n        if hasattr(scene.eevee, \"use_bloom\"):\n            scene.eevee.use_bloom = True\n            scene.eevee.bloom_intensity = 1.2\n            scene.eevee.bloom_radius = 7.0\n        # High contrast\n        scene.view_settings.look = \"High Contrast\"\n        scene.view_settings.view_transform = \"Filmic\"\n\n        # Apply post-processing\n        post_processing = PostProcessingSuite()\n        post_processing.apply_cinematic_preset()\n\n    elif style == \"autumn\":\n        suite._initialize_scene()\n        suite.setup_cinematic_camera(Vector((0, 0, 0)), 35.0, 10.0)\n        suite.setup_advanced_lighting(\"nebula\")\n        # Warm autumn world color\n        world = scene.world\n        if world and world.use_nodes:\n            bg = world.node_tree.nodes.get(\"Background\")\n            if bg:\n                bg.inputs[\"Color\"].default_value = (0.18, 0.10, 0.04, 1.0)\n        if hasattr(scene.eevee, \"use_bloom\"):\n            scene.eevee.use_bloom = True\n            scene.eevee.bloom_intensity = 1.0\n            scene.eevee.bloom_radius = 6.0\n        scene.view_settings.look = \"Medium High Contrast\"\n        scene.view_settings.view_transform = \"Filmic\"\n\n        # Apply post-processing\n        post_processing = PostProcessingSuite()\n        post_processing.add_color_grading(\"warm\")\n        post_processing.add_vignette(0.3, 0.8)\n\n    elif style == \"iridescent\":\n        suite._initialize_scene()\n        suite.setup_cinematic_camera(Vector((0, 0, 0)), 45.0, 15.0)\n        suite.setup_advanced_lighting(\"deep_space\")\n        # Iridescent world color\n        world = scene.world\n        if world and world.use_nodes:\n            bg = world.node_tree.nodes.get(\"Background\")\n            if bg:\n                bg.inputs[\"Color\"].default_value = (0.10, 0.12, 0.18, 1.0)\n        if hasattr(scene.eevee, \"use_bloom\"):\n            scene.eevee.use_bloom = True\n            scene.eevee.bloom_intensity = 1.5\n            scene.eevee.bloom_radius = 8.0\n        scene.view_settings.look = \"Very High Contrast\"\n        scene.view_settings.view_transform = \"Filmic\"\n\n        # Apply post-processing\n        post_processing = PostProcessingSuite()\n        post_processing.add_lens_flare(\"stellar\", 1.0)\n        post_processing.add_star_glow(1.5, 12)\n\n    elif style == \"cinematic_closeup\":\n        suite._initialize_scene()\n        suite.setup_cinematic_camera(Vector((0, 0, 0)), 10.0, 2.0)\n        suite.setup_advanced_lighting(\"deep_space\")\n        # Moody world color\n        world = scene.world\n        if world and world.use_nodes:\n            bg = world.node_tree.nodes.get(\"Background\")\n            if bg:\n                bg.inputs[\"Color\"].default_value = (0.01, 0.01, 0.01, 1.0)\n        if hasattr(scene.eevee, \"use_bloom\"):\n            scene.eevee.use_bloom = True\n            scene.eevee.bloom_intensity = 1.8\n            scene.eevee.bloom_radius = 9.0\n        scene.view_settings.look = \"Very High Contrast\"\n        scene.view_settings.view_transform = \"Filmic\"\n\n        # Apply post-processing\n        post_processing = PostProcessingSuite()\n        post_processing.apply_dramatic_preset()\n\n    elif style == \"dreamy_pastel\":\n        suite._initialize_scene()\n        suite.setup_cinematic_camera(Vector((0, 0, 0)), 30.0, 8.0)\n        suite.setup_advanced_lighting(\"nebula\")\n        # Pastelliger world color\n        world = scene.world\n        if world and world.use_nodes:\n            bg = world.node_tree.nodes.get(\"Background\")\n            if bg:\n                bg.inputs[\"Color\"].default_value = (0.95, 0.90, 0.98, 1.0)\n        if hasattr(scene.eevee, \"use_bloom\"):\n            scene.eevee.use_bloom = True\n            scene.eevee.bloom_intensity = 0.7\n            scene.eevee.bloom_radius = 5.0\n        scene.view_settings.look = \"Low Contrast\"\n        scene.view_settings.view_transform = \"Filmic\"\n\n        # Apply post-processing\n        post_processing = PostProcessingSuite()\n        post_processing.apply_dreamy_preset()\n\n    else:\n        print(f\"Unknown style preset: {style}. Using default luxury_teal.\")\n        apply_visual_style(suite, \"luxury_teal\")\n\n    print(f\"Applied visual style: {style}\")\n</code></pre>"},{"location":"api/astro_lab.widgets.bpy.advanced/#astro_lab.widgets.bpy.advanced.create_futuristic_scene","title":"create_futuristic_scene","text":"<pre><code>create_futuristic_scene(\n    scene_name: str = \"FuturisticAstroScene\",\n    style: str = \"luxury_teal\",\n    use_post_processing: bool = True,\n) -&gt; VisualizationSuite\n</code></pre> <p>Create a complete futuristic astronomical scene with all advanced features.</p> <p>Parameters:</p> Name Type Description Default <code>scene_name</code> <code>str</code> <p>Name of the scene</p> <code>'FuturisticAstroScene'</code> <code>style</code> <code>str</code> <p>Visual style preset</p> <code>'luxury_teal'</code> <code>use_post_processing</code> <code>bool</code> <p>Whether to apply post-processing effects</p> <code>True</code> <p>Returns:</p> Type Description <code>VisualizationSuite</code> <p>Configured advanced visualization suite</p> Source code in <code>src\\astro_lab\\widgets\\bpy\\advanced\\__init__.py</code> <pre><code>def create_futuristic_scene(\n    scene_name: str = \"FuturisticAstroScene\",\n    style: str = \"luxury_teal\",\n    use_post_processing: bool = True,\n) -&gt; VisualizationSuite:\n    \"\"\"\n    Create a complete futuristic astronomical scene with all advanced features.\n\n    Args:\n        scene_name: Name of the scene\n        style: Visual style preset\n        use_post_processing: Whether to apply post-processing effects\n\n    Returns:\n        Configured advanced visualization suite\n    \"\"\"\n    # Create advanced suite\n    suite = VisualizationSuite(scene_name)\n\n    # Apply visual style\n    apply_visual_style(suite, style)\n\n    # Apply post-processing if requested\n    if use_post_processing:\n        post_processing = PostProcessingSuite(scene_name)\n        if style == \"luxury_teal\":\n            post_processing.apply_cinematic_preset()\n        elif style == \"dramatic\":\n            post_processing.apply_dramatic_preset()\n        elif style == \"dreamy\":\n            post_processing.apply_dreamy_preset()\n\n    return suite\n</code></pre>"},{"location":"api/astro_lab.widgets.bpy.advanced/#astro_lab.widgets.bpy.advanced.create_galaxy_showcase","title":"create_galaxy_showcase","text":"<pre><code>create_galaxy_showcase() -&gt; VisualizationSuite\n</code></pre> <p>Create showcase of different galaxy types.</p> Source code in <code>src\\astro_lab\\widgets\\bpy\\advanced\\__init__.py</code> <pre><code>def create_galaxy_showcase() -&gt; VisualizationSuite:\n    \"\"\"Create showcase of different galaxy types.\"\"\"\n    suite = initialize_advanced_scene(\"high\")\n\n    # Create different galaxy types\n    suite.create_procedural_galaxy(\"spiral\", Vector((-20, 0, 0)), 30000, 15.0)\n    suite.create_procedural_galaxy(\"elliptical\", Vector((0, 0, 0)), 25000, 12.0)\n    suite.create_procedural_galaxy(\"irregular\", Vector((20, 0, 0)), 15000, 8.0)\n\n    # Setup camera and lighting\n    suite.setup_cinematic_camera(Vector((0, 0, 0)), 50.0, 15.0)\n    suite.setup_advanced_lighting(\"deep_space\")\n\n    return suite\n</code></pre>"},{"location":"api/astro_lab.widgets.bpy.advanced/#astro_lab.widgets.bpy.advanced.create_nebula_showcase","title":"create_nebula_showcase","text":"<pre><code>create_nebula_showcase() -&gt; VisualizationSuite\n</code></pre> <p>Create showcase of different nebula types.</p> Source code in <code>src\\astro_lab\\widgets\\bpy\\advanced\\__init__.py</code> <pre><code>def create_nebula_showcase() -&gt; VisualizationSuite:\n    \"\"\"Create showcase of different nebula types.\"\"\"\n    suite = initialize_advanced_scene(\"high\")\n\n    # Create different nebula types\n    suite.create_emission_nebula_complex(Vector((-15, 0, 0)), \"h_alpha\", 12.0)\n    suite.create_emission_nebula_complex(Vector((0, 0, 0)), \"oxygen\", 10.0)\n    suite.create_emission_nebula_complex(Vector((15, 0, 0)), \"planetary\", 8.0)\n\n    # Setup camera and lighting\n    suite.setup_cinematic_camera(Vector((0, 0, 0)), 30.0, 8.0)\n    suite.setup_advanced_lighting(\"nebula\")\n\n    return suite\n</code></pre>"},{"location":"api/astro_lab.widgets.bpy.advanced/#astro_lab.widgets.bpy.advanced.create_stellar_system_showcase","title":"create_stellar_system_showcase","text":"<pre><code>create_stellar_system_showcase() -&gt; VisualizationSuite\n</code></pre> <p>Create showcase stellar system with planets.</p> Source code in <code>src\\astro_lab\\widgets\\bpy\\advanced\\__init__.py</code> <pre><code>def create_stellar_system_showcase() -&gt; VisualizationSuite:\n    \"\"\"Create showcase stellar system with planets.\"\"\"\n    suite = initialize_advanced_scene(\"high\")\n\n    # Create orbital system\n    suite.create_orbital_system(star_mass=1.2)\n\n    # Add binary companion\n    suite.create_binary_star_system(2.0, 0.8, 15.0)\n\n    # Setup camera and lighting\n    suite.setup_cinematic_camera(Vector((0, 0, 0)), 40.0, 12.0)\n    suite.setup_advanced_lighting(\"planetary\")\n\n    return suite\n</code></pre>"},{"location":"api/astro_lab.widgets.bpy.advanced/#astro_lab.widgets.bpy.advanced.initialize_advanced_scene","title":"initialize_advanced_scene","text":"<pre><code>initialize_advanced_scene(quality_preset: str = 'high') -&gt; VisualizationSuite\n</code></pre> <p>Initialize advanced astronomical visualization scene.</p> <p>Parameters:</p> Name Type Description Default <code>quality_preset</code> <code>str</code> <p>Quality preset ('low', 'medium', 'high', 'cinematic')</p> <code>'high'</code> <p>Returns:</p> Type Description <code>VisualizationSuite</code> <p>Configured VisualizationSuite instance</p> Source code in <code>src\\astro_lab\\widgets\\bpy\\advanced\\__init__.py</code> <pre><code>def initialize_advanced_scene(\n    quality_preset: str = \"high\",\n) -&gt; VisualizationSuite:\n    \"\"\"\n    Initialize advanced astronomical visualization scene.\n\n    Args:\n        quality_preset: Quality preset ('low', 'medium', 'high', 'cinematic')\n\n    Returns:\n        Configured VisualizationSuite instance\n    \"\"\"\n    # Clear existing scene\n    bpy.ops.object.select_all(action=\"SELECT\")\n    bpy.ops.object.delete(use_global=False)\n\n    # Create suite\n    suite = VisualizationSuite(\"Astro\")\n\n    # Configure quality settings\n    scene = bpy.context.scene\n    if quality_preset == \"low\":\n        if hasattr(scene.eevee, \"taa_render_samples\"):\n            scene.eevee.taa_render_samples = 32\n        scene.render.resolution_x = 1280\n        scene.render.resolution_y = 720\n    elif quality_preset == \"medium\":\n        if hasattr(scene.eevee, \"taa_render_samples\"):\n            scene.eevee.taa_render_samples = 64\n        scene.render.resolution_x = 1920\n        scene.render.resolution_y = 1080\n    elif quality_preset == \"high\":\n        if hasattr(scene.eevee, \"taa_render_samples\"):\n            scene.eevee.taa_render_samples = 128\n        scene.render.resolution_x = 2560\n        scene.render.resolution_y = 1440\n    else:  # cinematic\n        if hasattr(scene.eevee, \"taa_render_samples\"):\n            scene.eevee.taa_render_samples = 256\n        scene.render.resolution_x = 3840\n        scene.render.resolution_y = 2160\n\n    print(f\" scene initialized with {quality_preset} quality\")\n    return suite\n</code></pre>"},{"location":"api/astro_lab.widgets.bpy/","title":"astro_lab.widgets.bpy","text":""},{"location":"api/astro_lab.widgets.bpy/#astro_lab.widgets.bpy","title":"bpy","text":""},{"location":"api/astro_lab.widgets.bpy/#astro_lab.widgets.bpy--astrolab-bpy-blender-python-api-integration","title":"AstroLab BPY (Blender Python API) Integration","text":"<p>Provides clean, memory-managed access to Blender Python API with ONLY BPY (no bmesh).</p> <p>Modules:</p> Name Description <code>advanced</code> <p>AstroLab Blender  Utils</p> <code>core</code> <p>Core Blender utilities for astronomical data visualization.</p> <code>grease_pencil_2d</code> <p>Professional 2D Grease Pencil plotting for astronomical data.</p> <code>grease_pencil_3d</code> <p>3D Grease Pencil plotting for astronomical data.</p> <code>live_tensor_bridge</code> <p>Live Tensor Bridge for Blender - DISABLED</p> <code>numpy_compat</code> <p>NumPy 2.x Compatibility Layer for Blender Integration</p> <code>operators</code> <p>Astro-Lab (AL) Operator Registry &amp; API</p> <p>Classes:</p> Name Description <code>AstroLabApi</code> <p>The main programmatic API for Astro-Lab's Blender functionalities.</p> <p>Functions:</p> Name Description <code>blender_memory_context</code> <p>Context manager for Blender operations with proper memory cleanup.</p> <code>bpy_object_context</code> <p>Context manager for BPY mesh operations - NO BMESH.</p> <code>get_advanced</code> <p>Get advanced module with memory management.</p> <code>get_core</code> <p>Get core module with memory management.</p> <code>get_grease_pencil_2d</code> <p>Get grease pencil 2D module with memory management.</p> <code>get_grease_pencil_3d</code> <p>Get grease pencil 3D module with memory management.</p>"},{"location":"api/astro_lab.widgets.bpy/#astro_lab.widgets.bpy.AstroLabApi","title":"AstroLabApi","text":"<p>The main programmatic API for Astro-Lab's Blender functionalities. This class provides direct access to all visualization and simulation tools.</p> Source code in <code>src\\astro_lab\\widgets\\bpy\\operators.py</code> <pre><code>class AstroLabApi:\n    \"\"\"\n    The main programmatic API for Astro-Lab's Blender functionalities.\n    This class provides direct access to all visualization and simulation tools.\n    \"\"\"\n\n    def __init__(self):\n        if bpy is None:\n            raise RuntimeError(\n                \"Blender is not available. AstroLabApi cannot be initialized.\"\n            )\n\n        # Live Tensor-Socket Bridge\n        self.live_bridge = live_tensor_bridge\n\n        #  Visualization Suite\n        self.advanced = b3d_adv.VisualizationSuite()\n\n        # Core functionalities\n        self.core = {\n            \"create_camera\": create_camera,\n            \"create_light\": create_light,\n            \"create_material\": create_material,\n            \"reset_scene\": reset_scene,\n            \"normalize_scene\": normalize_scene,\n            \"create_cosmic_grid\": create_cosmic_grid,\n            \"create_text_legend\": create_text_legend,\n        }\n\n        # 2D and 3D Plotters\n        self.plot_2d = GreasePencil2DPlotter()\n        self.plot_3d = GreasePencil3DPlotter()\n\n    def __repr__(self):\n        return \"AstroLabApi(live_bridge, advanced, core, plot_2d, plot_3d)\"\n</code></pre>"},{"location":"api/astro_lab.widgets.bpy/#astro_lab.widgets.bpy.blender_memory_context","title":"blender_memory_context","text":"<pre><code>blender_memory_context()\n</code></pre> <p>Context manager for Blender operations with proper memory cleanup. ONLY uses BPY - no bmesh imports to avoid memory leaks.</p> Source code in <code>src\\astro_lab\\widgets\\bpy\\__init__.py</code> <pre><code>@contextmanager\ndef blender_memory_context():\n    \"\"\"\n    Context manager for Blender operations with proper memory cleanup.\n    ONLY uses BPY - no bmesh imports to avoid memory leaks.\n    \"\"\"\n    if bpy is None:\n        raise ImportError(\"Blender (bpy) not available\")\n\n    try:\n        # Setup: Clear any orphaned data blocks\n        try:\n            bpy.ops.outliner.orphans_purge(  # type: ignore\n                do_local_ids=True, do_linked_ids=True, do_recursive=True\n            )\n        except:\n            pass  # Ignore if purge fails\n        yield bpy\n    finally:\n        # Cleanup: Purge orphaned data and force garbage collection\n        try:\n            bpy.ops.outliner.orphans_purge(  # type: ignore\n                do_local_ids=True, do_linked_ids=True, do_recursive=True\n            )\n        except:\n            pass  # Ignore if purge fails\n        gc.collect()\n</code></pre>"},{"location":"api/astro_lab.widgets.bpy/#astro_lab.widgets.bpy.bpy_object_context","title":"bpy_object_context","text":"<pre><code>bpy_object_context(mesh_obj)\n</code></pre> <p>Context manager for BPY mesh operations - NO BMESH. Only uses pure BPY API for mesh manipulation.</p> Source code in <code>src\\astro_lab\\widgets\\bpy\\__init__.py</code> <pre><code>def bpy_object_context(mesh_obj):\n    \"\"\"\n    Context manager for BPY mesh operations - NO BMESH.\n    Only uses pure BPY API for mesh manipulation.\n    \"\"\"\n\n    @contextmanager\n    def _context():\n        if bpy is None:\n            raise ImportError(\"Blender (bpy) not available\")\n\n        # Store original selection state\n        original_active = bpy.context.view_layer.objects.active  # type: ignore\n        original_selected = [obj for obj in bpy.context.selected_objects]  # type: ignore\n\n        try:\n            # Setup: Make target object active and selected\n            bpy.ops.object.select_all(action=\"DESELECT\")  # type: ignore\n            bpy.context.view_layer.objects.active = mesh_obj  # type: ignore\n            mesh_obj.select_set(True)\n\n            yield mesh_obj\n\n        finally:\n            # Cleanup: Restore original selection\n            try:\n                bpy.ops.object.select_all(action=\"DESELECT\")  # type: ignore\n                for obj in original_selected:\n                    obj.select_set(True)\n                if original_active:\n                    bpy.context.view_layer.objects.active = original_active  # type: ignore\n            except:\n                pass  # Ignore if restoration fails\n            gc.collect()\n\n    return _context()\n</code></pre>"},{"location":"api/astro_lab.widgets.bpy/#astro_lab.widgets.bpy.get_advanced","title":"get_advanced","text":"<pre><code>get_advanced()\n</code></pre> <p>Get advanced module with memory management.</p> Source code in <code>src\\astro_lab\\widgets\\bpy\\__init__.py</code> <pre><code>def get_advanced():\n    \"\"\"Get advanced module with memory management.\"\"\"\n    from . import advanced\n\n    gc.collect()  # Clean up after import\n    return advanced\n</code></pre>"},{"location":"api/astro_lab.widgets.bpy/#astro_lab.widgets.bpy.get_core","title":"get_core","text":"<pre><code>get_core()\n</code></pre> <p>Get core module with memory management.</p> Source code in <code>src\\astro_lab\\widgets\\bpy\\__init__.py</code> <pre><code>def get_core():\n    \"\"\"Get core module with memory management.\"\"\"\n    from . import core\n\n    gc.collect()  # Clean up after import\n    return core\n</code></pre>"},{"location":"api/astro_lab.widgets.bpy/#astro_lab.widgets.bpy.get_grease_pencil_2d","title":"get_grease_pencil_2d","text":"<pre><code>get_grease_pencil_2d()\n</code></pre> <p>Get grease pencil 2D module with memory management.</p> Source code in <code>src\\astro_lab\\widgets\\bpy\\__init__.py</code> <pre><code>def get_grease_pencil_2d():\n    \"\"\"Get grease pencil 2D module with memory management.\"\"\"\n    from . import grease_pencil_2d\n\n    gc.collect()  # Clean up after import\n    return grease_pencil_2d\n</code></pre>"},{"location":"api/astro_lab.widgets.bpy/#astro_lab.widgets.bpy.get_grease_pencil_3d","title":"get_grease_pencil_3d","text":"<pre><code>get_grease_pencil_3d()\n</code></pre> <p>Get grease pencil 3D module with memory management.</p> Source code in <code>src\\astro_lab\\widgets\\bpy\\__init__.py</code> <pre><code>def get_grease_pencil_3d():\n    \"\"\"Get grease pencil 3D module with memory management.\"\"\"\n    from . import grease_pencil_3d\n\n    gc.collect()  # Clean up after import\n    return grease_pencil_3d\n</code></pre>"},{"location":"api/astro_lab.widgets.bpy/#astro_lab.widgets.bpy.register","title":"register","text":"<pre><code>register()\n</code></pre> <p>Register all Blender modules for Astro-Lab.</p> Source code in <code>src\\astro_lab\\widgets\\bpy\\__init__.py</code> <pre><code>def register():\n    \"\"\"Register all Blender modules for Astro-Lab.\"\"\"\n    if al_register:\n        al_register()\n        gc.collect()  # Clean up after registration\n</code></pre>"},{"location":"api/astro_lab.widgets.bpy/#astro_lab.widgets.bpy.unregister","title":"unregister","text":"<pre><code>unregister()\n</code></pre> <p>Unregister all Blender modules for Astro-Lab.</p> Source code in <code>src\\astro_lab\\widgets\\bpy\\__init__.py</code> <pre><code>def unregister():\n    \"\"\"Unregister all Blender modules for Astro-Lab.\"\"\"\n    if al_unregister:\n        al_unregister()\n        gc.collect()  # Clean up after unregistration\n</code></pre>"},{"location":"api/astro_lab.widgets/","title":"astro_lab.widgets","text":""},{"location":"api/astro_lab.widgets/#astro_lab.widgets","title":"widgets","text":"<p>AstroLab Widgets - Visualization and analysis widgets for astronomical data.</p> <p>Modules:</p> Name Description <code>astro_lab</code> <p>AstroLab Widget - Main interface for astronomical data visualization and analysis</p> <code>bpy</code> <p>AstroLab BPY (Blender Python API) Integration</p> <code>cosmograph_bridge</code> <p>Cosmograph bridge for astronomical data visualization.</p> <code>graph</code> <p>Simplified graph utilities for astronomical data.</p> <code>plotly_bridge</code> <p>Plotly Bridge for AstroLab visualization.</p> <code>tensor_bridge</code> <p>AstroLab Tensor Bridge - Zero-Copy Data Transfer</p> <code>tng50</code> <p>TNG50 Data Visualizer</p> <p>Classes:</p> Name Description <code>AstroLabWidget</code> <p>Main widget for astronomical data visualization and analysis.</p> <code>CosmographBridge</code> <p>Bridge class for creating Cosmograph visualizations from AstroLab data.</p> <code>TNG50Visualizer</code> <p>Direct TNG50 visualization from processed .pt files.</p> <p>Functions:</p> Name Description <code>cluster_and_analyze</code> <p>Perform clustering and return comprehensive analysis.</p> <code>create_plotly_visualization</code> <p>Create Plotly visualization for SurveyTensor.</p>"},{"location":"api/astro_lab.widgets/#astro_lab.widgets.AstroLabWidget","title":"AstroLabWidget","text":"<p>Main widget for astronomical data visualization and analysis.</p> <p>Uses local visualization modules that were moved from utils.viz.</p> <p>Methods:</p> Name Description <code>cluster_data</code> <p>Cluster data using local graph module.</p> <code>create_graph</code> <p>Create PyTorch Geometric graph - delegates to data.graphs module.</p> <code>plot</code> <p>Visualize astronomical data using local visualization modules.</p> Source code in <code>src\\astro_lab\\widgets\\astro_lab.py</code> <pre><code>class AstroLabWidget:\n    \"\"\"\n    Main widget for astronomical data visualization and analysis.\n\n    Uses local visualization modules that were moved from utils.viz.\n    \"\"\"\n\n    def __init__(self, **kwargs: Any) -&gt; None:\n        \"\"\"Initialize the widget.\"\"\"\n        self._setup_blender_api()\n        self.backend = kwargs.get(\"backend\", \"auto\")\n        logger.info(\"AstroLabWidget initialized with local visualization modules.\")\n\n    def _setup_blender_api(self) -&gt; None:\n        \"\"\"Set up Blender API access if available.\"\"\"\n        if bpy is None or AstroLabApi is None:\n            self.al = None\n            self.ops = None\n            self.data = None\n            self.context = None\n            self.scene = None\n            logger.warning(\"Blender not available - API access disabled.\")\n            return\n\n        try:\n            self.al = AstroLabApi()\n            self.ops = getattr(bpy, \"ops\", None)\n            self.data = getattr(bpy, \"data\", None)\n            self.context = getattr(bpy, \"context\", None)\n            self.scene = getattr(self.context, \"scene\", None) if self.context else None\n            logger.info(\"Blender API connected.\")\n        except Exception as e:\n            self.al = None\n            self.ops = None\n            self.data = None\n            self.context = None\n            self.scene = None\n            logger.error(f\"Failed to connect Blender API: {e}\")\n\n    def plot(\n        self,\n        data: Any,\n        plot_type: str = \"scatter_3d\",\n        backend: str = \"auto\",\n        max_points: int = 100_000,\n        **config: Any,\n    ) -&gt; Any:\n        \"\"\"\n        Visualize astronomical data using local visualization modules.\n\n        Args:\n            data: SurveyTensorDict or Data object\n            plot_type: Type of plot\n            backend: Visualization backend ('auto', 'plotly')\n            max_points: Maximum points to plot\n            **config: Additional configuration\n\n        Returns:\n            Visualization object\n        \"\"\"\n        survey_tensor = self._extract_survey_tensor(data)\n\n        # Use local plotly_bridge module\n        if backend in [\"auto\", \"plotly\"]:\n            return create_plotly_visualization(\n                survey_tensor, plot_type=plot_type, **config\n            )\n        else:\n            # For other backends, just log for now\n            coords = survey_tensor[\"spatial\"][\"coordinates\"]\n            logger.info(f\"Would visualize {len(coords)} points with {backend} backend\")\n            return survey_tensor\n\n    def create_graph(\n        self,\n        data: Any,\n        method: str = \"knn\",\n        k: int = 10,\n        radius: Optional[float] = None,\n        **kwargs: Any,\n    ) -&gt; Data:\n        \"\"\"\n        Create PyTorch Geometric graph - delegates to data.graphs module.\n\n        Args:\n            data: SurveyTensorDict or compatible data\n            method: Graph creation method ('knn', 'radius', 'astronomical')\n            k: Number of neighbors for KNN\n            radius: Radius for radius graphs\n            **kwargs: Additional parameters\n\n        Returns:\n            PyTorch Geometric Data object\n        \"\"\"\n        survey_tensor = self._extract_survey_tensor(data)\n\n        # Delegate to data.graphs module\n        if method == \"knn\":\n            return create_knn_graph(survey_tensor, k_neighbors=k, **kwargs)\n        elif method == \"radius\":\n            if radius is None:\n                raise ValueError(\"Radius must be specified for radius graphs\")\n            return create_radius_graph(survey_tensor, radius=radius, **kwargs)\n        elif method == \"astronomical\":\n            return create_astronomical_graph(survey_tensor, k_neighbors=k, **kwargs)\n        else:\n            raise ValueError(f\"Unknown graph method: {method}\")\n\n    def cluster_data(\n        self,\n        data: Any,\n        algorithm: str = \"dbscan\",\n        eps: float = 10.0,\n        min_samples: int = 5,\n        **kwargs: Any,\n    ) -&gt; Any:\n        \"\"\"\n        Cluster data using local graph module.\n\n        Args:\n            data: SurveyTensorDict or compatible data\n            algorithm: Clustering algorithm\n            eps: DBSCAN epsilon parameter\n            min_samples: DBSCAN minimum samples\n            **kwargs: Additional parameters\n\n        Returns:\n            Clustering results\n        \"\"\"\n        survey_tensor = self._extract_survey_tensor(data)\n        coords = survey_tensor[\"spatial\"][\"coordinates\"]\n\n        # Use local graph module\n        return cluster_and_analyze(\n            coords, algorithm=algorithm, eps=eps, min_samples=min_samples, **kwargs\n        )\n\n    def _extract_survey_tensor(self, data: Any) -&gt; SurveyTensorDict:\n        \"\"\"Extract SurveyTensorDict from various input formats.\"\"\"\n        if isinstance(data, SurveyTensorDict):\n            return data\n        elif isinstance(data, Data):\n            # Convert PyG Data to SurveyTensorDict\n            from astro_lab.tensors.factories import create_generic_survey\n\n            if hasattr(data, \"pos\") and data.pos is not None:\n                coords = data.pos\n            elif hasattr(data, \"x\") and data.x.shape[-1] &gt;= 3:\n                coords = data.x[:, :3]  # Use first 3 dimensions as coordinates\n            else:\n                raise ValueError(\"Cannot extract coordinates from Data object\")\n\n            # Create dummy magnitudes if not available\n            magnitudes = torch.zeros(coords.shape[0], 1)\n\n            return create_generic_survey(\n                coordinates=coords,\n                magnitudes=magnitudes,\n                bands=[\"dummy\"],\n                survey_name=\"converted\",\n            )\n        else:\n            raise TypeError(f\"Unsupported data type: {type(data)}\")\n</code></pre>"},{"location":"api/astro_lab.widgets/#astro_lab.widgets.AstroLabWidget.cluster_data","title":"cluster_data","text":"<pre><code>cluster_data(\n    data: Any,\n    algorithm: str = \"dbscan\",\n    eps: float = 10.0,\n    min_samples: int = 5,\n    **kwargs: Any\n) -&gt; Any\n</code></pre> <p>Cluster data using local graph module.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Any</code> <p>SurveyTensorDict or compatible data</p> required <code>algorithm</code> <code>str</code> <p>Clustering algorithm</p> <code>'dbscan'</code> <code>eps</code> <code>float</code> <p>DBSCAN epsilon parameter</p> <code>10.0</code> <code>min_samples</code> <code>int</code> <p>DBSCAN minimum samples</p> <code>5</code> <code>**kwargs</code> <code>Any</code> <p>Additional parameters</p> <code>{}</code> <p>Returns:</p> Type Description <code>Any</code> <p>Clustering results</p> Source code in <code>src\\astro_lab\\widgets\\astro_lab.py</code> <pre><code>def cluster_data(\n    self,\n    data: Any,\n    algorithm: str = \"dbscan\",\n    eps: float = 10.0,\n    min_samples: int = 5,\n    **kwargs: Any,\n) -&gt; Any:\n    \"\"\"\n    Cluster data using local graph module.\n\n    Args:\n        data: SurveyTensorDict or compatible data\n        algorithm: Clustering algorithm\n        eps: DBSCAN epsilon parameter\n        min_samples: DBSCAN minimum samples\n        **kwargs: Additional parameters\n\n    Returns:\n        Clustering results\n    \"\"\"\n    survey_tensor = self._extract_survey_tensor(data)\n    coords = survey_tensor[\"spatial\"][\"coordinates\"]\n\n    # Use local graph module\n    return cluster_and_analyze(\n        coords, algorithm=algorithm, eps=eps, min_samples=min_samples, **kwargs\n    )\n</code></pre>"},{"location":"api/astro_lab.widgets/#astro_lab.widgets.AstroLabWidget.create_graph","title":"create_graph","text":"<pre><code>create_graph(\n    data: Any,\n    method: str = \"knn\",\n    k: int = 10,\n    radius: Optional[float] = None,\n    **kwargs: Any\n) -&gt; Data\n</code></pre> <p>Create PyTorch Geometric graph - delegates to data.graphs module.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Any</code> <p>SurveyTensorDict or compatible data</p> required <code>method</code> <code>str</code> <p>Graph creation method ('knn', 'radius', 'astronomical')</p> <code>'knn'</code> <code>k</code> <code>int</code> <p>Number of neighbors for KNN</p> <code>10</code> <code>radius</code> <code>Optional[float]</code> <p>Radius for radius graphs</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Additional parameters</p> <code>{}</code> <p>Returns:</p> Type Description <code>Data</code> <p>PyTorch Geometric Data object</p> Source code in <code>src\\astro_lab\\widgets\\astro_lab.py</code> <pre><code>def create_graph(\n    self,\n    data: Any,\n    method: str = \"knn\",\n    k: int = 10,\n    radius: Optional[float] = None,\n    **kwargs: Any,\n) -&gt; Data:\n    \"\"\"\n    Create PyTorch Geometric graph - delegates to data.graphs module.\n\n    Args:\n        data: SurveyTensorDict or compatible data\n        method: Graph creation method ('knn', 'radius', 'astronomical')\n        k: Number of neighbors for KNN\n        radius: Radius for radius graphs\n        **kwargs: Additional parameters\n\n    Returns:\n        PyTorch Geometric Data object\n    \"\"\"\n    survey_tensor = self._extract_survey_tensor(data)\n\n    # Delegate to data.graphs module\n    if method == \"knn\":\n        return create_knn_graph(survey_tensor, k_neighbors=k, **kwargs)\n    elif method == \"radius\":\n        if radius is None:\n            raise ValueError(\"Radius must be specified for radius graphs\")\n        return create_radius_graph(survey_tensor, radius=radius, **kwargs)\n    elif method == \"astronomical\":\n        return create_astronomical_graph(survey_tensor, k_neighbors=k, **kwargs)\n    else:\n        raise ValueError(f\"Unknown graph method: {method}\")\n</code></pre>"},{"location":"api/astro_lab.widgets/#astro_lab.widgets.AstroLabWidget.plot","title":"plot","text":"<pre><code>plot(\n    data: Any,\n    plot_type: str = \"scatter_3d\",\n    backend: str = \"auto\",\n    max_points: int = 100000,\n    **config: Any\n) -&gt; Any\n</code></pre> <p>Visualize astronomical data using local visualization modules.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Any</code> <p>SurveyTensorDict or Data object</p> required <code>plot_type</code> <code>str</code> <p>Type of plot</p> <code>'scatter_3d'</code> <code>backend</code> <code>str</code> <p>Visualization backend ('auto', 'plotly')</p> <code>'auto'</code> <code>max_points</code> <code>int</code> <p>Maximum points to plot</p> <code>100000</code> <code>**config</code> <code>Any</code> <p>Additional configuration</p> <code>{}</code> <p>Returns:</p> Type Description <code>Any</code> <p>Visualization object</p> Source code in <code>src\\astro_lab\\widgets\\astro_lab.py</code> <pre><code>def plot(\n    self,\n    data: Any,\n    plot_type: str = \"scatter_3d\",\n    backend: str = \"auto\",\n    max_points: int = 100_000,\n    **config: Any,\n) -&gt; Any:\n    \"\"\"\n    Visualize astronomical data using local visualization modules.\n\n    Args:\n        data: SurveyTensorDict or Data object\n        plot_type: Type of plot\n        backend: Visualization backend ('auto', 'plotly')\n        max_points: Maximum points to plot\n        **config: Additional configuration\n\n    Returns:\n        Visualization object\n    \"\"\"\n    survey_tensor = self._extract_survey_tensor(data)\n\n    # Use local plotly_bridge module\n    if backend in [\"auto\", \"plotly\"]:\n        return create_plotly_visualization(\n            survey_tensor, plot_type=plot_type, **config\n        )\n    else:\n        # For other backends, just log for now\n        coords = survey_tensor[\"spatial\"][\"coordinates\"]\n        logger.info(f\"Would visualize {len(coords)} points with {backend} backend\")\n        return survey_tensor\n</code></pre>"},{"location":"api/astro_lab.widgets/#astro_lab.widgets.CosmographBridge","title":"CosmographBridge","text":"<p>Bridge class for creating Cosmograph visualizations from AstroLab data.</p> <p>Provides methods to convert various data sources to Cosmograph format: - Spatial3DTensor objects - Survey data dictionaries - Raw coordinate arrays - Polars DataFrames</p> <p>Methods:</p> Name Description <code>from_coordinates</code> <p>Create Cosmograph visualization from coordinate array.</p> <code>from_cosmic_web_results</code> <p>Create Cosmograph visualization from cosmic web analysis results.</p> <code>from_polars_dataframe</code> <p>Create Cosmograph visualization directly from Polars DataFrame.</p> <code>from_spatial_tensor</code> <p>Create Cosmograph visualization from Spatial3DTensor.</p> <code>from_survey_data</code> <p>Create Cosmograph visualization from survey data.</p> Source code in <code>src\\astro_lab\\widgets\\cosmograph_bridge.py</code> <pre><code>class CosmographBridge:\n    \"\"\"\n    Bridge class for creating Cosmograph visualizations from AstroLab data.\n\n    Provides methods to convert various data sources to Cosmograph format:\n    - Spatial3DTensor objects\n    - Survey data dictionaries\n    - Raw coordinate arrays\n    - Polars DataFrames\n    \"\"\"\n\n    def __init__(self):\n        \"\"\"Initialize CosmographBridge with default configuration.\"\"\"\n        self.default_config = {\n            \"width\": 800,\n            \"height\": 600,\n            \"background_color\": \"#000000\",\n            \"point_size\": 3,\n            \"link_width\": 1,\n            \"camera_distance\": 100,\n            \"camera_rotation\": [0, 0, 0],\n        }\n\n    def from_spatial_tensor(\n        self,\n        tensor,\n        radius: float = 5.0,\n        point_color: str = \"#ffd700\",\n        link_color: str = \"#666666\",\n        **kwargs,\n    ) -&gt; Any:\n        \"\"\"\n        Create Cosmograph visualization from Spatial3DTensor.\n\n        Args:\n            tensor: AstroLab Spatial3DTensor\n            radius: Radius for neighbor graph creation (default: 5.0)\n            point_color: Color for points (default: gold)\n            link_color: Color for links (default: gray)\n            **kwargs: Additional Cosmograph parameters\n\n        Returns:\n            Cosmograph widget\n        \"\"\"\n        # Extract coordinates\n        coords = tensor.cartesian.cpu().numpy()\n\n        # Create neighbor graph\n        edges = (\n            tensor._create_radius_graph(tensor.cartesian, radius=radius)\n            .t()\n            .cpu()\n            .numpy()\n        )\n\n        # Create Polars DataFrames\n        points_df = pl.DataFrame(\n            {\n                \"id\": [f\"node_{i}\" for i in range(len(coords))],\n                \"x\": coords[:, 0],\n                \"y\": coords[:, 1],\n                \"z\": coords[:, 2],\n                \"distance\": np.linalg.norm(coords, axis=1),\n            }\n        )\n\n        links_df = pl.DataFrame(\n            {\n                \"source\": [f\"node_{edges[i, 0]}\" for i in range(len(edges))],\n                \"target\": [f\"node_{edges[i, 1]}\" for i in range(len(edges))],\n                \"distance\": np.random.uniform(1.0, radius, len(edges)),\n            }\n        )\n\n        # Convert to pandas for Cosmograph (required by cosmograph)\n        points_pandas = points_df.to_pandas()\n        links_pandas = links_df.to_pandas()\n\n        # Merge configs\n        config = {**self.default_config, **kwargs}\n\n        # Remove point_color from config to avoid duplicate parameter\n        config.pop(\"point_color\", None)\n\n        # Use link_width_by and link_width_scale instead of link_width_range\n        return cosmo(\n            points=points_pandas,\n            links=links_pandas,\n            point_id_by=\"id\",\n            point_x_by=\"x\",\n            point_y_by=\"y\",\n            point_color=point_color,\n            point_size_range=[2, 6],\n            link_source_by=\"source\",\n            link_target_by=\"target\",\n            link_color=link_color,\n            link_width_by=\"distance\",\n            link_width_scale=1.0,\n            **config,\n        )\n\n    def from_cosmic_web_results(\n        self, results: Dict[str, Any], survey_name: str, radius: float = 5.0, **kwargs\n    ) -&gt; Any:\n        \"\"\"\n        Create Cosmograph visualization from cosmic web analysis results.\n\n        Args:\n            results: Results from create_cosmic_web_loader\n            survey_name: Name of survey (gaia, sdss, nsa, tng50, etc.)\n            radius: Radius for neighbor graph creation\n            **kwargs: Additional Cosmograph parameters\n\n        Returns:\n            Cosmograph widget\n        \"\"\"\n        # Extract coordinates from cosmic web results\n        if \"coordinates\" in results:\n            coords = np.array(results[\"coordinates\"])\n        elif \"spatial_tensor\" in results:\n            # If spatial_tensor is available, use it\n            tensor = results[\"spatial_tensor\"]\n            coords = tensor.cartesian.cpu().numpy()\n        else:\n            raise ValueError(\"No coordinates found in cosmic web results\")\n\n        # Set colors based on survey type\n        color_map = {\n            \"gaia\": \"#ffd700\",  # Gold for stars\n            \"sdss\": \"#4a90e2\",  # Blue for galaxies\n            \"nsa\": \"#e24a4a\",  # Red for NSA\n            \"tng50\": \"#00ff00\",  # Green for simulation\n            \"tng\": \"#00ff00\",  # Green for simulation\n            \"linear\": \"#ff8800\",  # Orange for asteroids\n            \"exoplanet\": \"#ff00ff\",  # Magenta for exoplanets\n        }\n\n        point_color = color_map.get(survey_name, \"#ffffff\")\n\n        return self.from_coordinates(\n            coords, radius=radius, point_color=point_color, **kwargs\n        )\n\n    def from_survey_data(\n        self, data: Dict[str, Any], survey_name: str, radius: float = 5.0, **kwargs\n    ) -&gt; Any:\n        \"\"\"\n        Create Cosmograph visualization from survey data.\n\n        Args:\n            data: Survey data dictionary with 'spatial_tensor' key\n            survey_name: Name of survey (gaia, sdss, nsa, tng50)\n            radius: Radius for neighbor graph creation\n            **kwargs: Additional Cosmograph parameters\n\n        Returns:\n            Cosmograph widget\n        \"\"\"\n        # Check if this is cosmic web results\n        if \"coordinates\" in data or \"spatial_tensor\" in data:\n            return self.from_cosmic_web_results(data, survey_name, radius, **kwargs)\n\n        # Fallback to old method\n        if \"spatial_tensor\" not in data:\n            raise ValueError(\"Survey data must contain 'spatial_tensor' key\")\n\n        # Set colors based on survey type\n        color_map = {\n            \"gaia\": \"#ffd700\",  # Gold for stars\n            \"sdss\": \"#4a90e2\",  # Blue for galaxies\n            \"nsa\": \"#e24a4a\",  # Red for NSA\n            \"tng50\": \"#00ff00\",  # Green for simulation\n        }\n\n        point_color = color_map.get(survey_name, \"#ffffff\")\n\n        return self.from_spatial_tensor(\n            data[\"spatial_tensor\"], radius=radius, point_color=point_color, **kwargs\n        )\n\n    def from_coordinates(\n        self,\n        coords: np.ndarray,\n        edges: Optional[np.ndarray] = None,\n        radius: float = 5.0,\n        **kwargs,\n    ) -&gt; Any:\n        \"\"\"\n        Create Cosmograph visualization from coordinate array.\n\n        Args:\n            coords: Coordinate array [N, 3]\n            edges: Optional edge array [M, 2]\n            radius: Radius for neighbor graph creation\n            **kwargs: Additional Cosmograph parameters\n\n        Returns:\n            Cosmograph widget\n        \"\"\"\n        if edges is None:\n            # Create simple neighbor graph using GPU acceleration\n            # Convert to PyTorch tensor and move to GPU\n            device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n            coords_tensor = torch.tensor(coords, dtype=torch.float32, device=device)\n\n            # Create k-NN graph on GPU\n            from torch_geometric.nn import knn_graph\n\n            edge_index = knn_graph(\n                coords_tensor,\n                k=5,  # 5 nearest neighbors\n                batch=None,\n                loop=False,  # No self-loops\n            )\n\n            # Move back to CPU and convert to numpy\n            edge_index = edge_index.cpu().numpy()\n\n            # Calculate distances and filter by radius\n            edge_list = []\n            for i in range(edge_index.shape[1]):\n                src, tgt = edge_index[:, i]\n                dist = np.linalg.norm(coords[src] - coords[tgt])\n                if dist &lt;= radius:\n                    edge_list.append([src, tgt])\n\n            edges = np.array(edge_list, dtype=int)\n\n        # Extract coordinates explicitly\n        x_coords = coords[:, 0].tolist()\n        y_coords = coords[:, 1].tolist()\n        z_coords = coords[:, 2].tolist()\n\n        # Create Polars DataFrames\n        points_df = pl.DataFrame(\n            {\n                \"id\": [f\"point_{i}\" for i in range(len(coords))],\n                \"x\": x_coords,\n                \"y\": y_coords,\n                \"z\": z_coords,\n            }\n        )\n\n        if edges.size &gt; 0:\n            sources = [f\"point_{src}\" for src in edges[:, 0]]\n            targets = [f\"point_{tgt}\" for tgt in edges[:, 1]]\n        else:\n            sources = []\n            targets = []\n        links_df = pl.DataFrame({\"source\": sources, \"target\": targets})\n\n        # Convert to pandas for Cosmograph (required by cosmograph)\n        points_pandas = points_df.to_pandas()\n        links_pandas = links_df.to_pandas()\n\n        # Merge configs\n        config = {**self.default_config, **kwargs}\n\n        # Extract point_color from kwargs or use default\n        point_color = kwargs.get(\"point_color\", \"#ffffff\")\n        link_color = kwargs.get(\"link_color\", \"#666666\")\n\n        # Remove these from config to avoid duplicate parameters\n        config.pop(\"point_color\", None)\n        config.pop(\"link_color\", None)\n\n        return cosmo(\n            points=points_pandas,\n            links=links_pandas,\n            point_id_by=\"id\",\n            point_x_by=\"x\",\n            point_y_by=\"y\",\n            point_color=point_color,\n            point_size_range=[2, 6],\n            link_source_by=\"source\",\n            link_target_by=\"target\",\n            link_color=link_color,\n            link_width_scale=1.0,\n            **config,\n        )\n\n    def from_polars_dataframe(\n        self,\n        df: pl.DataFrame,\n        x_col: str = \"x\",\n        y_col: str = \"y\",\n        z_col: str = \"z\",\n        id_col: Optional[str] = None,\n        radius: float = 5.0,\n        **kwargs,\n    ) -&gt; Any:\n        \"\"\"\n        Create Cosmograph visualization directly from Polars DataFrame.\n\n        Args:\n            df: Polars DataFrame with coordinate columns\n            x_col: Column name for x coordinates\n            y_col: Column name for y coordinates\n            z_col: Column name for z coordinates\n            id_col: Column name for point IDs (optional)\n            radius: Radius for neighbor graph creation\n            **kwargs: Additional Cosmograph parameters\n\n        Returns:\n            Cosmograph widget\n        \"\"\"\n        # Extract coordinates\n        coords = df.select([x_col, y_col, z_col]).to_numpy()\n\n        # Create IDs if not provided\n        if id_col is None or id_col not in df.columns:\n            ids = [f\"point_{i}\" for i in range(len(df))]\n        else:\n            ids = df[id_col].to_list()\n\n        # Create neighbor graph using GPU acceleration\n        # Convert to PyTorch tensor and move to GPU\n        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n        coords_tensor = torch.tensor(coords, dtype=torch.float32, device=device)\n\n        # Create k-NN graph on GPU\n        from torch_geometric.nn import knn_graph\n\n        edge_index = knn_graph(\n            coords_tensor,\n            k=5,  # 5 nearest neighbors\n            batch=None,\n            loop=False,  # No self-loops\n        )\n\n        # Move back to CPU and convert to numpy\n        edge_index = edge_index.cpu().numpy()\n\n        # Calculate distances and filter by radius\n        edge_list = []\n        for i in range(edge_index.shape[1]):\n            src, tgt = edge_index[:, i]\n            dist = np.linalg.norm(coords[src] - coords[tgt])\n            if dist &lt;= radius:\n                edge_list.append([src, tgt])\n\n        edges = np.array(edge_list, dtype=int)\n\n        # Extract coordinates explicitly\n        x_coords = coords[:, 0].tolist()\n        y_coords = coords[:, 1].tolist()\n        z_coords = coords[:, 2].tolist()\n\n        # Create Polars DataFrames\n        points_df = pl.DataFrame(\n            {\"id\": ids, \"x\": x_coords, \"y\": y_coords, \"z\": z_coords}\n        )\n\n        if edges.size &gt; 0:\n            sources = [ids[src] for src in edges[:, 0]]\n            targets = [ids[tgt] for tgt in edges[:, 1]]\n        else:\n            sources = []\n            targets = []\n        links_df = pl.DataFrame({\"source\": sources, \"target\": targets})\n\n        # Convert to pandas for Cosmograph\n        points_pandas = points_df.to_pandas()\n        links_pandas = links_df.to_pandas()\n\n        # Merge configs\n        config = {**self.default_config, **kwargs}\n\n        # Extract colors from kwargs or use defaults\n        point_color = kwargs.get(\"point_color\", \"#ffffff\")\n        link_color = kwargs.get(\"link_color\", \"#666666\")\n\n        # Remove these from config to avoid duplicate parameters\n        config.pop(\"point_color\", None)\n        config.pop(\"link_color\", None)\n\n        return cosmo(\n            points=points_pandas,\n            links=links_pandas,\n            point_id_by=\"id\",\n            point_x_by=\"x\",\n            point_y_by=\"y\",\n            point_color=point_color,\n            point_size_range=[2, 6],\n            link_source_by=\"source\",\n            link_target_by=\"target\",\n            link_color=link_color,\n            link_width_scale=1.0,\n            **config,\n        )\n</code></pre>"},{"location":"api/astro_lab.widgets/#astro_lab.widgets.CosmographBridge.from_coordinates","title":"from_coordinates","text":"<pre><code>from_coordinates(\n    coords: ndarray, edges: Optional[ndarray] = None, radius: float = 5.0, **kwargs\n) -&gt; Any\n</code></pre> <p>Create Cosmograph visualization from coordinate array.</p> <p>Parameters:</p> Name Type Description Default <code>coords</code> <code>ndarray</code> <p>Coordinate array [N, 3]</p> required <code>edges</code> <code>Optional[ndarray]</code> <p>Optional edge array [M, 2]</p> <code>None</code> <code>radius</code> <code>float</code> <p>Radius for neighbor graph creation</p> <code>5.0</code> <code>**kwargs</code> <p>Additional Cosmograph parameters</p> <code>{}</code> <p>Returns:</p> Type Description <code>Any</code> <p>Cosmograph widget</p> Source code in <code>src\\astro_lab\\widgets\\cosmograph_bridge.py</code> <pre><code>def from_coordinates(\n    self,\n    coords: np.ndarray,\n    edges: Optional[np.ndarray] = None,\n    radius: float = 5.0,\n    **kwargs,\n) -&gt; Any:\n    \"\"\"\n    Create Cosmograph visualization from coordinate array.\n\n    Args:\n        coords: Coordinate array [N, 3]\n        edges: Optional edge array [M, 2]\n        radius: Radius for neighbor graph creation\n        **kwargs: Additional Cosmograph parameters\n\n    Returns:\n        Cosmograph widget\n    \"\"\"\n    if edges is None:\n        # Create simple neighbor graph using GPU acceleration\n        # Convert to PyTorch tensor and move to GPU\n        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n        coords_tensor = torch.tensor(coords, dtype=torch.float32, device=device)\n\n        # Create k-NN graph on GPU\n        from torch_geometric.nn import knn_graph\n\n        edge_index = knn_graph(\n            coords_tensor,\n            k=5,  # 5 nearest neighbors\n            batch=None,\n            loop=False,  # No self-loops\n        )\n\n        # Move back to CPU and convert to numpy\n        edge_index = edge_index.cpu().numpy()\n\n        # Calculate distances and filter by radius\n        edge_list = []\n        for i in range(edge_index.shape[1]):\n            src, tgt = edge_index[:, i]\n            dist = np.linalg.norm(coords[src] - coords[tgt])\n            if dist &lt;= radius:\n                edge_list.append([src, tgt])\n\n        edges = np.array(edge_list, dtype=int)\n\n    # Extract coordinates explicitly\n    x_coords = coords[:, 0].tolist()\n    y_coords = coords[:, 1].tolist()\n    z_coords = coords[:, 2].tolist()\n\n    # Create Polars DataFrames\n    points_df = pl.DataFrame(\n        {\n            \"id\": [f\"point_{i}\" for i in range(len(coords))],\n            \"x\": x_coords,\n            \"y\": y_coords,\n            \"z\": z_coords,\n        }\n    )\n\n    if edges.size &gt; 0:\n        sources = [f\"point_{src}\" for src in edges[:, 0]]\n        targets = [f\"point_{tgt}\" for tgt in edges[:, 1]]\n    else:\n        sources = []\n        targets = []\n    links_df = pl.DataFrame({\"source\": sources, \"target\": targets})\n\n    # Convert to pandas for Cosmograph (required by cosmograph)\n    points_pandas = points_df.to_pandas()\n    links_pandas = links_df.to_pandas()\n\n    # Merge configs\n    config = {**self.default_config, **kwargs}\n\n    # Extract point_color from kwargs or use default\n    point_color = kwargs.get(\"point_color\", \"#ffffff\")\n    link_color = kwargs.get(\"link_color\", \"#666666\")\n\n    # Remove these from config to avoid duplicate parameters\n    config.pop(\"point_color\", None)\n    config.pop(\"link_color\", None)\n\n    return cosmo(\n        points=points_pandas,\n        links=links_pandas,\n        point_id_by=\"id\",\n        point_x_by=\"x\",\n        point_y_by=\"y\",\n        point_color=point_color,\n        point_size_range=[2, 6],\n        link_source_by=\"source\",\n        link_target_by=\"target\",\n        link_color=link_color,\n        link_width_scale=1.0,\n        **config,\n    )\n</code></pre>"},{"location":"api/astro_lab.widgets/#astro_lab.widgets.CosmographBridge.from_cosmic_web_results","title":"from_cosmic_web_results","text":"<pre><code>from_cosmic_web_results(\n    results: Dict[str, Any], survey_name: str, radius: float = 5.0, **kwargs\n) -&gt; Any\n</code></pre> <p>Create Cosmograph visualization from cosmic web analysis results.</p> <p>Parameters:</p> Name Type Description Default <code>results</code> <code>Dict[str, Any]</code> <p>Results from create_cosmic_web_loader</p> required <code>survey_name</code> <code>str</code> <p>Name of survey (gaia, sdss, nsa, tng50, etc.)</p> required <code>radius</code> <code>float</code> <p>Radius for neighbor graph creation</p> <code>5.0</code> <code>**kwargs</code> <p>Additional Cosmograph parameters</p> <code>{}</code> <p>Returns:</p> Type Description <code>Any</code> <p>Cosmograph widget</p> Source code in <code>src\\astro_lab\\widgets\\cosmograph_bridge.py</code> <pre><code>def from_cosmic_web_results(\n    self, results: Dict[str, Any], survey_name: str, radius: float = 5.0, **kwargs\n) -&gt; Any:\n    \"\"\"\n    Create Cosmograph visualization from cosmic web analysis results.\n\n    Args:\n        results: Results from create_cosmic_web_loader\n        survey_name: Name of survey (gaia, sdss, nsa, tng50, etc.)\n        radius: Radius for neighbor graph creation\n        **kwargs: Additional Cosmograph parameters\n\n    Returns:\n        Cosmograph widget\n    \"\"\"\n    # Extract coordinates from cosmic web results\n    if \"coordinates\" in results:\n        coords = np.array(results[\"coordinates\"])\n    elif \"spatial_tensor\" in results:\n        # If spatial_tensor is available, use it\n        tensor = results[\"spatial_tensor\"]\n        coords = tensor.cartesian.cpu().numpy()\n    else:\n        raise ValueError(\"No coordinates found in cosmic web results\")\n\n    # Set colors based on survey type\n    color_map = {\n        \"gaia\": \"#ffd700\",  # Gold for stars\n        \"sdss\": \"#4a90e2\",  # Blue for galaxies\n        \"nsa\": \"#e24a4a\",  # Red for NSA\n        \"tng50\": \"#00ff00\",  # Green for simulation\n        \"tng\": \"#00ff00\",  # Green for simulation\n        \"linear\": \"#ff8800\",  # Orange for asteroids\n        \"exoplanet\": \"#ff00ff\",  # Magenta for exoplanets\n    }\n\n    point_color = color_map.get(survey_name, \"#ffffff\")\n\n    return self.from_coordinates(\n        coords, radius=radius, point_color=point_color, **kwargs\n    )\n</code></pre>"},{"location":"api/astro_lab.widgets/#astro_lab.widgets.CosmographBridge.from_polars_dataframe","title":"from_polars_dataframe","text":"<pre><code>from_polars_dataframe(\n    df: DataFrame,\n    x_col: str = \"x\",\n    y_col: str = \"y\",\n    z_col: str = \"z\",\n    id_col: Optional[str] = None,\n    radius: float = 5.0,\n    **kwargs\n) -&gt; Any\n</code></pre> <p>Create Cosmograph visualization directly from Polars DataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>Polars DataFrame with coordinate columns</p> required <code>x_col</code> <code>str</code> <p>Column name for x coordinates</p> <code>'x'</code> <code>y_col</code> <code>str</code> <p>Column name for y coordinates</p> <code>'y'</code> <code>z_col</code> <code>str</code> <p>Column name for z coordinates</p> <code>'z'</code> <code>id_col</code> <code>Optional[str]</code> <p>Column name for point IDs (optional)</p> <code>None</code> <code>radius</code> <code>float</code> <p>Radius for neighbor graph creation</p> <code>5.0</code> <code>**kwargs</code> <p>Additional Cosmograph parameters</p> <code>{}</code> <p>Returns:</p> Type Description <code>Any</code> <p>Cosmograph widget</p> Source code in <code>src\\astro_lab\\widgets\\cosmograph_bridge.py</code> <pre><code>def from_polars_dataframe(\n    self,\n    df: pl.DataFrame,\n    x_col: str = \"x\",\n    y_col: str = \"y\",\n    z_col: str = \"z\",\n    id_col: Optional[str] = None,\n    radius: float = 5.0,\n    **kwargs,\n) -&gt; Any:\n    \"\"\"\n    Create Cosmograph visualization directly from Polars DataFrame.\n\n    Args:\n        df: Polars DataFrame with coordinate columns\n        x_col: Column name for x coordinates\n        y_col: Column name for y coordinates\n        z_col: Column name for z coordinates\n        id_col: Column name for point IDs (optional)\n        radius: Radius for neighbor graph creation\n        **kwargs: Additional Cosmograph parameters\n\n    Returns:\n        Cosmograph widget\n    \"\"\"\n    # Extract coordinates\n    coords = df.select([x_col, y_col, z_col]).to_numpy()\n\n    # Create IDs if not provided\n    if id_col is None or id_col not in df.columns:\n        ids = [f\"point_{i}\" for i in range(len(df))]\n    else:\n        ids = df[id_col].to_list()\n\n    # Create neighbor graph using GPU acceleration\n    # Convert to PyTorch tensor and move to GPU\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    coords_tensor = torch.tensor(coords, dtype=torch.float32, device=device)\n\n    # Create k-NN graph on GPU\n    from torch_geometric.nn import knn_graph\n\n    edge_index = knn_graph(\n        coords_tensor,\n        k=5,  # 5 nearest neighbors\n        batch=None,\n        loop=False,  # No self-loops\n    )\n\n    # Move back to CPU and convert to numpy\n    edge_index = edge_index.cpu().numpy()\n\n    # Calculate distances and filter by radius\n    edge_list = []\n    for i in range(edge_index.shape[1]):\n        src, tgt = edge_index[:, i]\n        dist = np.linalg.norm(coords[src] - coords[tgt])\n        if dist &lt;= radius:\n            edge_list.append([src, tgt])\n\n    edges = np.array(edge_list, dtype=int)\n\n    # Extract coordinates explicitly\n    x_coords = coords[:, 0].tolist()\n    y_coords = coords[:, 1].tolist()\n    z_coords = coords[:, 2].tolist()\n\n    # Create Polars DataFrames\n    points_df = pl.DataFrame(\n        {\"id\": ids, \"x\": x_coords, \"y\": y_coords, \"z\": z_coords}\n    )\n\n    if edges.size &gt; 0:\n        sources = [ids[src] for src in edges[:, 0]]\n        targets = [ids[tgt] for tgt in edges[:, 1]]\n    else:\n        sources = []\n        targets = []\n    links_df = pl.DataFrame({\"source\": sources, \"target\": targets})\n\n    # Convert to pandas for Cosmograph\n    points_pandas = points_df.to_pandas()\n    links_pandas = links_df.to_pandas()\n\n    # Merge configs\n    config = {**self.default_config, **kwargs}\n\n    # Extract colors from kwargs or use defaults\n    point_color = kwargs.get(\"point_color\", \"#ffffff\")\n    link_color = kwargs.get(\"link_color\", \"#666666\")\n\n    # Remove these from config to avoid duplicate parameters\n    config.pop(\"point_color\", None)\n    config.pop(\"link_color\", None)\n\n    return cosmo(\n        points=points_pandas,\n        links=links_pandas,\n        point_id_by=\"id\",\n        point_x_by=\"x\",\n        point_y_by=\"y\",\n        point_color=point_color,\n        point_size_range=[2, 6],\n        link_source_by=\"source\",\n        link_target_by=\"target\",\n        link_color=link_color,\n        link_width_scale=1.0,\n        **config,\n    )\n</code></pre>"},{"location":"api/astro_lab.widgets/#astro_lab.widgets.CosmographBridge.from_spatial_tensor","title":"from_spatial_tensor","text":"<pre><code>from_spatial_tensor(\n    tensor,\n    radius: float = 5.0,\n    point_color: str = \"#ffd700\",\n    link_color: str = \"#666666\",\n    **kwargs\n) -&gt; Any\n</code></pre> <p>Create Cosmograph visualization from Spatial3DTensor.</p> <p>Parameters:</p> Name Type Description Default <code>tensor</code> <p>AstroLab Spatial3DTensor</p> required <code>radius</code> <code>float</code> <p>Radius for neighbor graph creation (default: 5.0)</p> <code>5.0</code> <code>point_color</code> <code>str</code> <p>Color for points (default: gold)</p> <code>'#ffd700'</code> <code>link_color</code> <code>str</code> <p>Color for links (default: gray)</p> <code>'#666666'</code> <code>**kwargs</code> <p>Additional Cosmograph parameters</p> <code>{}</code> <p>Returns:</p> Type Description <code>Any</code> <p>Cosmograph widget</p> Source code in <code>src\\astro_lab\\widgets\\cosmograph_bridge.py</code> <pre><code>def from_spatial_tensor(\n    self,\n    tensor,\n    radius: float = 5.0,\n    point_color: str = \"#ffd700\",\n    link_color: str = \"#666666\",\n    **kwargs,\n) -&gt; Any:\n    \"\"\"\n    Create Cosmograph visualization from Spatial3DTensor.\n\n    Args:\n        tensor: AstroLab Spatial3DTensor\n        radius: Radius for neighbor graph creation (default: 5.0)\n        point_color: Color for points (default: gold)\n        link_color: Color for links (default: gray)\n        **kwargs: Additional Cosmograph parameters\n\n    Returns:\n        Cosmograph widget\n    \"\"\"\n    # Extract coordinates\n    coords = tensor.cartesian.cpu().numpy()\n\n    # Create neighbor graph\n    edges = (\n        tensor._create_radius_graph(tensor.cartesian, radius=radius)\n        .t()\n        .cpu()\n        .numpy()\n    )\n\n    # Create Polars DataFrames\n    points_df = pl.DataFrame(\n        {\n            \"id\": [f\"node_{i}\" for i in range(len(coords))],\n            \"x\": coords[:, 0],\n            \"y\": coords[:, 1],\n            \"z\": coords[:, 2],\n            \"distance\": np.linalg.norm(coords, axis=1),\n        }\n    )\n\n    links_df = pl.DataFrame(\n        {\n            \"source\": [f\"node_{edges[i, 0]}\" for i in range(len(edges))],\n            \"target\": [f\"node_{edges[i, 1]}\" for i in range(len(edges))],\n            \"distance\": np.random.uniform(1.0, radius, len(edges)),\n        }\n    )\n\n    # Convert to pandas for Cosmograph (required by cosmograph)\n    points_pandas = points_df.to_pandas()\n    links_pandas = links_df.to_pandas()\n\n    # Merge configs\n    config = {**self.default_config, **kwargs}\n\n    # Remove point_color from config to avoid duplicate parameter\n    config.pop(\"point_color\", None)\n\n    # Use link_width_by and link_width_scale instead of link_width_range\n    return cosmo(\n        points=points_pandas,\n        links=links_pandas,\n        point_id_by=\"id\",\n        point_x_by=\"x\",\n        point_y_by=\"y\",\n        point_color=point_color,\n        point_size_range=[2, 6],\n        link_source_by=\"source\",\n        link_target_by=\"target\",\n        link_color=link_color,\n        link_width_by=\"distance\",\n        link_width_scale=1.0,\n        **config,\n    )\n</code></pre>"},{"location":"api/astro_lab.widgets/#astro_lab.widgets.CosmographBridge.from_survey_data","title":"from_survey_data","text":"<pre><code>from_survey_data(\n    data: Dict[str, Any], survey_name: str, radius: float = 5.0, **kwargs\n) -&gt; Any\n</code></pre> <p>Create Cosmograph visualization from survey data.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Dict[str, Any]</code> <p>Survey data dictionary with 'spatial_tensor' key</p> required <code>survey_name</code> <code>str</code> <p>Name of survey (gaia, sdss, nsa, tng50)</p> required <code>radius</code> <code>float</code> <p>Radius for neighbor graph creation</p> <code>5.0</code> <code>**kwargs</code> <p>Additional Cosmograph parameters</p> <code>{}</code> <p>Returns:</p> Type Description <code>Any</code> <p>Cosmograph widget</p> Source code in <code>src\\astro_lab\\widgets\\cosmograph_bridge.py</code> <pre><code>def from_survey_data(\n    self, data: Dict[str, Any], survey_name: str, radius: float = 5.0, **kwargs\n) -&gt; Any:\n    \"\"\"\n    Create Cosmograph visualization from survey data.\n\n    Args:\n        data: Survey data dictionary with 'spatial_tensor' key\n        survey_name: Name of survey (gaia, sdss, nsa, tng50)\n        radius: Radius for neighbor graph creation\n        **kwargs: Additional Cosmograph parameters\n\n    Returns:\n        Cosmograph widget\n    \"\"\"\n    # Check if this is cosmic web results\n    if \"coordinates\" in data or \"spatial_tensor\" in data:\n        return self.from_cosmic_web_results(data, survey_name, radius, **kwargs)\n\n    # Fallback to old method\n    if \"spatial_tensor\" not in data:\n        raise ValueError(\"Survey data must contain 'spatial_tensor' key\")\n\n    # Set colors based on survey type\n    color_map = {\n        \"gaia\": \"#ffd700\",  # Gold for stars\n        \"sdss\": \"#4a90e2\",  # Blue for galaxies\n        \"nsa\": \"#e24a4a\",  # Red for NSA\n        \"tng50\": \"#00ff00\",  # Green for simulation\n    }\n\n    point_color = color_map.get(survey_name, \"#ffffff\")\n\n    return self.from_spatial_tensor(\n        data[\"spatial_tensor\"], radius=radius, point_color=point_color, **kwargs\n    )\n</code></pre>"},{"location":"api/astro_lab.widgets/#astro_lab.widgets.TNG50Visualizer","title":"TNG50Visualizer","text":"<p>Direct TNG50 visualization from processed .pt files.</p> <p>No redundant data loading or processing - uses existing graph data.</p> <p>Methods:</p> Name Description <code>list_available_graphs</code> <p>List all available TNG50 graph files.</p> <code>load_tng50_graph</code> <p>Load TNG50 graph data from processed .pt file.</p> <code>quick_visualization</code> <p>Quick visualization of TNG50 data.</p> <code>to_blender_objects</code> <p>Convert TNG50 graph to Blender objects.</p> <code>to_pyvista_mesh</code> <p>Convert TNG50 graph to PyVista mesh for 3D visualization.</p> Source code in <code>src\\astro_lab\\widgets\\tng50.py</code> <pre><code>class TNG50Visualizer:\n    \"\"\"\n    Direct TNG50 visualization from processed .pt files.\n\n    No redundant data loading or processing - uses existing graph data.\n    \"\"\"\n\n    def __init__(self, data_dir: Optional[Path] = None):\n        \"\"\"\n        Initialize TNG50 visualizer.\n\n        Args:\n            data_dir: Directory containing processed TNG50 graphs\n        \"\"\"\n        self.data_dir = data_dir or Path(\"data/processed/tng50_graphs\")\n\n        logger.info(\"\ud83c\udf0c TNG50Visualizer initialized\")\n        logger.info(f\"   Data directory: {self.data_dir}\")\n        logger.info(f\"   PyVista: {'\u2705' if pv is not None else '\u274c'}\")\n        logger.info(f\"   Blender: {'\u2705' if bpy is not None else '\u274c'}\")\n\n    def list_available_graphs(self) -&gt; Dict[str, List[str]]:\n        \"\"\"\n        List all available TNG50 graph files.\n\n        Returns:\n            Dictionary with particle types and their available files\n        \"\"\"\n        available = {}\n\n        if not self.data_dir.exists():\n            logger.warning(f\"Data directory not found: {self.data_dir}\")\n            return available\n\n        for particle_dir in self.data_dir.iterdir():\n            if particle_dir.is_dir():\n                processed_dir = particle_dir / \"processed\"\n                if processed_dir.exists():\n                    pt_files = list(processed_dir.glob(\"*.pt\"))\n                    # Filter out metadata files\n                    graph_files = [f.name for f in pt_files if \"graph\" in f.name]\n                    if graph_files:\n                        available[particle_dir.name] = graph_files\n\n        return available\n\n    def load_tng50_graph(\n        self,\n        particle_type: str = \"gas\",\n        snapshot: str = \"snap_099.0\",\n        radius: float = 1.0,\n        max_particles: int = 1000,\n    ) -&gt; Dict[str, Any]:\n        \"\"\"\n        Load TNG50 graph data from processed .pt file.\n\n        Args:\n            particle_type: Particle type (\"gas\", \"stars\", \"black_holes\")\n            snapshot: Snapshot identifier\n            radius: Graph radius used during processing\n            max_particles: Max particles used during processing\n\n        Returns:\n            Dictionary with graph data\n        \"\"\"\n        # Construct filename\n        filename = f\"tng50_graph_{snapshot}_parttype0_r{radius:.1f}_n{max_particles}.pt\"\n        pt_file = self.data_dir / particle_type / \"processed\" / filename\n\n        if not pt_file.exists():\n            # Try to find any matching file\n            available = self.list_available_graphs()\n            if particle_type in available:\n                logger.warning(\n                    f\"Exact file not found, using: {available[particle_type][0]}\"\n                )\n                pt_file = (\n                    self.data_dir\n                    / particle_type\n                    / \"processed\"\n                    / available[particle_type][0]\n                )\n            else:\n                raise FileNotFoundError(\n                    f\"No TNG50 graph data found for {particle_type}\"\n                )\n\n        logger.info(f\"\ud83d\udcc2 Loading TNG50 graph: {pt_file.name}\")\n\n        # Load PyG InMemoryDataset format\n        loaded_data = torch.load(pt_file, map_location=\"cpu\", weights_only=False)\n\n        if isinstance(loaded_data, tuple) and len(loaded_data) &gt;= 1:\n            data_dict = loaded_data[0]\n\n            # Extract key data\n            result = {\n                \"positions\": data_dict[\"pos\"].numpy(),  # [N, 3]\n                \"features\": data_dict[\"x\"].numpy(),  # [N, feature_dim]\n                \"edge_index\": data_dict[\"edge_index\"].numpy(),  # [2, num_edges]\n                \"edge_weights\": data_dict[\"edge_attr\"].numpy(),  # [num_edges, 1]\n                \"num_particles\": data_dict[\"num_particles\"],\n                \"particle_type\": data_dict[\"particle_type\"],\n                \"feature_names\": data_dict[\"feature_names\"],\n                \"snapshot_file\": data_dict[\"snapshot_file\"],\n                \"file_path\": str(pt_file),\n            }\n\n            logger.info(f\"\u2705 Loaded {result['num_particles']:,} particles\")\n            logger.info(f\"   Features: {result['feature_names']}\")\n            logger.info(f\"   Edges: {result['edge_index'].shape[1]:,}\")\n\n            return result\n\n        else:\n            raise ValueError(f\"Unexpected data format in {pt_file}\")\n\n    def to_pyvista_mesh(\n        self,\n        graph_data: Dict[str, Any],\n        point_size: float = 5.0,\n        color_by: str = \"mass\",\n        include_edges: bool = False,\n    ) -&gt; \"pv.PolyData\":\n        \"\"\"\n        Convert TNG50 graph to PyVista mesh for 3D visualization.\n\n        Args:\n            graph_data: Graph data from load_tng50_graph()\n            point_size: Point size for rendering\n            color_by: Feature to use for coloring\n            include_edges: Whether to include graph edges\n\n        Returns:\n            PyVista PolyData mesh\n        \"\"\"\n        if pv is None:\n            raise ImportError(\"PyVista not available\")\n\n        positions = graph_data[\"positions\"]\n        features = graph_data[\"features\"]\n        feature_names = graph_data[\"feature_names\"]\n\n        logger.info(f\"\ud83d\udd27 Converting to PyVista mesh: {len(positions):,} points\")\n\n        # Create point cloud\n        mesh = pv.PolyData(positions)\n\n        # Add features as point data\n        for i, name in enumerate(feature_names):\n            mesh.point_data[name] = features[:, i]\n\n        # Add particle indices\n        mesh.point_data[\"particle_id\"] = np.arange(len(positions))\n\n        # Set default coloring\n        if color_by in feature_names:\n            mesh.set_active_scalars(color_by)\n            logger.info(f\"   Coloring by: {color_by}\")\n\n        # Add edges if requested\n        if include_edges:\n            edge_index = graph_data[\"edge_index\"]\n            edge_weights = graph_data[\"edge_weights\"]\n\n            # Create lines between connected particles\n            lines = []\n            for i in range(edge_index.shape[1]):\n                start_idx = edge_index[0, i]\n                end_idx = edge_index[1, i]\n                lines.extend([2, start_idx, end_idx])  # PyVista line format\n\n            mesh.lines = np.array(lines)\n            mesh.line_data[\"edge_weight\"] = edge_weights.flatten()\n\n            logger.info(f\"   Added {edge_index.shape[1]:,} edges\")\n\n        return mesh\n\n    def to_blender_objects(\n        self,\n        graph_data: Dict[str, Any],\n        object_name: str = \"TNG50_particles\",\n        use_instancing: bool = True,\n    ) -&gt; List[Any]:\n        \"\"\"\n        Convert TNG50 graph to Blender objects.\n\n        Args:\n            graph_data: Graph data from load_tng50_graph()\n            object_name: Base name for Blender objects\n            use_instancing: Use instancing for better performance\n\n        Returns:\n            List of created Blender objects\n        \"\"\"\n        if bpy is None:\n            raise ImportError(\"Blender not available\")\n\n        positions = graph_data[\"positions\"]\n        features = graph_data[\"features\"]\n\n        logger.info(f\"\ud83c\udfa8 Converting to Blender: {len(positions):,} particles\")\n\n        # Create or get base mesh (sphere for particles)\n        if use_instancing:\n            # Create base sphere\n            import bpy  # Import locally after availability check\n\n            bpy.ops.mesh.primitive_uv_sphere_add(radius=1.0, location=(0, 0, 0))\n            base_sphere = bpy.context.active_object\n            base_sphere.name = f\"{object_name}_base\"\n\n            # Create collection for instances\n            collection_name = f\"{object_name}_collection\"\n            collection = bpy.data.collections.new(collection_name)\n            bpy.context.scene.collection.children.link(collection)\n\n            objects = [base_sphere]\n\n            # Use geometry nodes for instancing (more efficient)\n            # For now, create simple instances\n            for i, pos in enumerate(positions[:100]):  # Limit for performance\n                bpy.ops.object.duplicate()\n                instance = bpy.context.active_object\n                instance.location = pos\n                instance.name = f\"{object_name}_particle_{i}\"\n\n                # Scale by mass if available\n                if len(features) &gt; 0:\n                    mass = features[i, 0]  # Assume first feature is mass\n                    scale = np.log10(mass * 1000) if mass &gt; 0 else 1.0\n                    instance.scale = (scale, scale, scale)\n\n                collection.objects.link(instance)\n                objects.append(instance)\n\n            logger.info(f\"   Created {len(objects) - 1} particle instances\")\n\n        else:\n            # Create single mesh with all particles\n            import bmesh\n\n            bm = bmesh.new()\n\n            for i, pos in enumerate(positions):\n                # Add vertex at particle position\n                vert = bm.verts.new(pos)\n\n                # Could add more sophisticated geometry here\n\n            # Create mesh\n            mesh = bpy.data.meshes.new(object_name)\n            bm.to_mesh(mesh)\n            bm.free()\n\n            # Create object\n            obj = bpy.data.objects.new(object_name, mesh)\n            bpy.context.scene.collection.objects.link(obj)\n\n            objects = [obj]\n            logger.info(\"   Created single mesh object\")\n\n        return objects\n\n    def quick_visualization(\n        self, particle_type: str = \"gas\", method: str = \"pyvista\", **kwargs\n    ) -&gt; Any:\n        \"\"\"\n        Quick visualization of TNG50 data.\n\n        Args:\n            particle_type: Particle type to visualize\n            method: Visualization method (\"pyvista\" or \"blender\")\n            **kwargs: Additional arguments\n\n        Returns:\n            Visualization object (PyVista mesh or Blender objects)\n        \"\"\"\n        logger.info(f\"\ud83d\ude80 Quick TNG50 visualization: {particle_type} via {method}\")\n\n        # Load data\n        graph_data = self.load_tng50_graph(particle_type=particle_type)\n\n        # Convert to visualization format\n        if method == \"pyvista\":\n            result = self.to_pyvista_mesh(graph_data, **kwargs)\n\n            # Auto-show if possible\n            try:\n                result.plot(\n                    scalars=graph_data[\"feature_names\"][0],\n                    point_size=kwargs.get(\"point_size\", 5.0),\n                    title=f\"TNG50 {particle_type.title()} Particles\",\n                )\n            except Exception as e:\n                logger.warning(f\"Could not auto-plot: {e}\")\n\n            return result\n\n        elif method == \"blender\":\n            return self.to_blender_objects(graph_data, **kwargs)\n\n        else:\n            raise ValueError(f\"Unknown method: {method}\")\n</code></pre>"},{"location":"api/astro_lab.widgets/#astro_lab.widgets.TNG50Visualizer.list_available_graphs","title":"list_available_graphs","text":"<pre><code>list_available_graphs() -&gt; Dict[str, List[str]]\n</code></pre> <p>List all available TNG50 graph files.</p> <p>Returns:</p> Type Description <code>Dict[str, List[str]]</code> <p>Dictionary with particle types and their available files</p> Source code in <code>src\\astro_lab\\widgets\\tng50.py</code> <pre><code>def list_available_graphs(self) -&gt; Dict[str, List[str]]:\n    \"\"\"\n    List all available TNG50 graph files.\n\n    Returns:\n        Dictionary with particle types and their available files\n    \"\"\"\n    available = {}\n\n    if not self.data_dir.exists():\n        logger.warning(f\"Data directory not found: {self.data_dir}\")\n        return available\n\n    for particle_dir in self.data_dir.iterdir():\n        if particle_dir.is_dir():\n            processed_dir = particle_dir / \"processed\"\n            if processed_dir.exists():\n                pt_files = list(processed_dir.glob(\"*.pt\"))\n                # Filter out metadata files\n                graph_files = [f.name for f in pt_files if \"graph\" in f.name]\n                if graph_files:\n                    available[particle_dir.name] = graph_files\n\n    return available\n</code></pre>"},{"location":"api/astro_lab.widgets/#astro_lab.widgets.TNG50Visualizer.load_tng50_graph","title":"load_tng50_graph","text":"<pre><code>load_tng50_graph(\n    particle_type: str = \"gas\",\n    snapshot: str = \"snap_099.0\",\n    radius: float = 1.0,\n    max_particles: int = 1000,\n) -&gt; Dict[str, Any]\n</code></pre> <p>Load TNG50 graph data from processed .pt file.</p> <p>Parameters:</p> Name Type Description Default <code>particle_type</code> <code>str</code> <p>Particle type (\"gas\", \"stars\", \"black_holes\")</p> <code>'gas'</code> <code>snapshot</code> <code>str</code> <p>Snapshot identifier</p> <code>'snap_099.0'</code> <code>radius</code> <code>float</code> <p>Graph radius used during processing</p> <code>1.0</code> <code>max_particles</code> <code>int</code> <p>Max particles used during processing</p> <code>1000</code> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dictionary with graph data</p> Source code in <code>src\\astro_lab\\widgets\\tng50.py</code> <pre><code>def load_tng50_graph(\n    self,\n    particle_type: str = \"gas\",\n    snapshot: str = \"snap_099.0\",\n    radius: float = 1.0,\n    max_particles: int = 1000,\n) -&gt; Dict[str, Any]:\n    \"\"\"\n    Load TNG50 graph data from processed .pt file.\n\n    Args:\n        particle_type: Particle type (\"gas\", \"stars\", \"black_holes\")\n        snapshot: Snapshot identifier\n        radius: Graph radius used during processing\n        max_particles: Max particles used during processing\n\n    Returns:\n        Dictionary with graph data\n    \"\"\"\n    # Construct filename\n    filename = f\"tng50_graph_{snapshot}_parttype0_r{radius:.1f}_n{max_particles}.pt\"\n    pt_file = self.data_dir / particle_type / \"processed\" / filename\n\n    if not pt_file.exists():\n        # Try to find any matching file\n        available = self.list_available_graphs()\n        if particle_type in available:\n            logger.warning(\n                f\"Exact file not found, using: {available[particle_type][0]}\"\n            )\n            pt_file = (\n                self.data_dir\n                / particle_type\n                / \"processed\"\n                / available[particle_type][0]\n            )\n        else:\n            raise FileNotFoundError(\n                f\"No TNG50 graph data found for {particle_type}\"\n            )\n\n    logger.info(f\"\ud83d\udcc2 Loading TNG50 graph: {pt_file.name}\")\n\n    # Load PyG InMemoryDataset format\n    loaded_data = torch.load(pt_file, map_location=\"cpu\", weights_only=False)\n\n    if isinstance(loaded_data, tuple) and len(loaded_data) &gt;= 1:\n        data_dict = loaded_data[0]\n\n        # Extract key data\n        result = {\n            \"positions\": data_dict[\"pos\"].numpy(),  # [N, 3]\n            \"features\": data_dict[\"x\"].numpy(),  # [N, feature_dim]\n            \"edge_index\": data_dict[\"edge_index\"].numpy(),  # [2, num_edges]\n            \"edge_weights\": data_dict[\"edge_attr\"].numpy(),  # [num_edges, 1]\n            \"num_particles\": data_dict[\"num_particles\"],\n            \"particle_type\": data_dict[\"particle_type\"],\n            \"feature_names\": data_dict[\"feature_names\"],\n            \"snapshot_file\": data_dict[\"snapshot_file\"],\n            \"file_path\": str(pt_file),\n        }\n\n        logger.info(f\"\u2705 Loaded {result['num_particles']:,} particles\")\n        logger.info(f\"   Features: {result['feature_names']}\")\n        logger.info(f\"   Edges: {result['edge_index'].shape[1]:,}\")\n\n        return result\n\n    else:\n        raise ValueError(f\"Unexpected data format in {pt_file}\")\n</code></pre>"},{"location":"api/astro_lab.widgets/#astro_lab.widgets.TNG50Visualizer.quick_visualization","title":"quick_visualization","text":"<pre><code>quick_visualization(\n    particle_type: str = \"gas\", method: str = \"pyvista\", **kwargs\n) -&gt; Any\n</code></pre> <p>Quick visualization of TNG50 data.</p> <p>Parameters:</p> Name Type Description Default <code>particle_type</code> <code>str</code> <p>Particle type to visualize</p> <code>'gas'</code> <code>method</code> <code>str</code> <p>Visualization method (\"pyvista\" or \"blender\")</p> <code>'pyvista'</code> <code>**kwargs</code> <p>Additional arguments</p> <code>{}</code> <p>Returns:</p> Type Description <code>Any</code> <p>Visualization object (PyVista mesh or Blender objects)</p> Source code in <code>src\\astro_lab\\widgets\\tng50.py</code> <pre><code>def quick_visualization(\n    self, particle_type: str = \"gas\", method: str = \"pyvista\", **kwargs\n) -&gt; Any:\n    \"\"\"\n    Quick visualization of TNG50 data.\n\n    Args:\n        particle_type: Particle type to visualize\n        method: Visualization method (\"pyvista\" or \"blender\")\n        **kwargs: Additional arguments\n\n    Returns:\n        Visualization object (PyVista mesh or Blender objects)\n    \"\"\"\n    logger.info(f\"\ud83d\ude80 Quick TNG50 visualization: {particle_type} via {method}\")\n\n    # Load data\n    graph_data = self.load_tng50_graph(particle_type=particle_type)\n\n    # Convert to visualization format\n    if method == \"pyvista\":\n        result = self.to_pyvista_mesh(graph_data, **kwargs)\n\n        # Auto-show if possible\n        try:\n            result.plot(\n                scalars=graph_data[\"feature_names\"][0],\n                point_size=kwargs.get(\"point_size\", 5.0),\n                title=f\"TNG50 {particle_type.title()} Particles\",\n            )\n        except Exception as e:\n            logger.warning(f\"Could not auto-plot: {e}\")\n\n        return result\n\n    elif method == \"blender\":\n        return self.to_blender_objects(graph_data, **kwargs)\n\n    else:\n        raise ValueError(f\"Unknown method: {method}\")\n</code></pre>"},{"location":"api/astro_lab.widgets/#astro_lab.widgets.TNG50Visualizer.to_blender_objects","title":"to_blender_objects","text":"<pre><code>to_blender_objects(\n    graph_data: Dict[str, Any],\n    object_name: str = \"TNG50_particles\",\n    use_instancing: bool = True,\n) -&gt; List[Any]\n</code></pre> <p>Convert TNG50 graph to Blender objects.</p> <p>Parameters:</p> Name Type Description Default <code>graph_data</code> <code>Dict[str, Any]</code> <p>Graph data from load_tng50_graph()</p> required <code>object_name</code> <code>str</code> <p>Base name for Blender objects</p> <code>'TNG50_particles'</code> <code>use_instancing</code> <code>bool</code> <p>Use instancing for better performance</p> <code>True</code> <p>Returns:</p> Type Description <code>List[Any]</code> <p>List of created Blender objects</p> Source code in <code>src\\astro_lab\\widgets\\tng50.py</code> <pre><code>def to_blender_objects(\n    self,\n    graph_data: Dict[str, Any],\n    object_name: str = \"TNG50_particles\",\n    use_instancing: bool = True,\n) -&gt; List[Any]:\n    \"\"\"\n    Convert TNG50 graph to Blender objects.\n\n    Args:\n        graph_data: Graph data from load_tng50_graph()\n        object_name: Base name for Blender objects\n        use_instancing: Use instancing for better performance\n\n    Returns:\n        List of created Blender objects\n    \"\"\"\n    if bpy is None:\n        raise ImportError(\"Blender not available\")\n\n    positions = graph_data[\"positions\"]\n    features = graph_data[\"features\"]\n\n    logger.info(f\"\ud83c\udfa8 Converting to Blender: {len(positions):,} particles\")\n\n    # Create or get base mesh (sphere for particles)\n    if use_instancing:\n        # Create base sphere\n        import bpy  # Import locally after availability check\n\n        bpy.ops.mesh.primitive_uv_sphere_add(radius=1.0, location=(0, 0, 0))\n        base_sphere = bpy.context.active_object\n        base_sphere.name = f\"{object_name}_base\"\n\n        # Create collection for instances\n        collection_name = f\"{object_name}_collection\"\n        collection = bpy.data.collections.new(collection_name)\n        bpy.context.scene.collection.children.link(collection)\n\n        objects = [base_sphere]\n\n        # Use geometry nodes for instancing (more efficient)\n        # For now, create simple instances\n        for i, pos in enumerate(positions[:100]):  # Limit for performance\n            bpy.ops.object.duplicate()\n            instance = bpy.context.active_object\n            instance.location = pos\n            instance.name = f\"{object_name}_particle_{i}\"\n\n            # Scale by mass if available\n            if len(features) &gt; 0:\n                mass = features[i, 0]  # Assume first feature is mass\n                scale = np.log10(mass * 1000) if mass &gt; 0 else 1.0\n                instance.scale = (scale, scale, scale)\n\n            collection.objects.link(instance)\n            objects.append(instance)\n\n        logger.info(f\"   Created {len(objects) - 1} particle instances\")\n\n    else:\n        # Create single mesh with all particles\n        import bmesh\n\n        bm = bmesh.new()\n\n        for i, pos in enumerate(positions):\n            # Add vertex at particle position\n            vert = bm.verts.new(pos)\n\n            # Could add more sophisticated geometry here\n\n        # Create mesh\n        mesh = bpy.data.meshes.new(object_name)\n        bm.to_mesh(mesh)\n        bm.free()\n\n        # Create object\n        obj = bpy.data.objects.new(object_name, mesh)\n        bpy.context.scene.collection.objects.link(obj)\n\n        objects = [obj]\n        logger.info(\"   Created single mesh object\")\n\n    return objects\n</code></pre>"},{"location":"api/astro_lab.widgets/#astro_lab.widgets.TNG50Visualizer.to_pyvista_mesh","title":"to_pyvista_mesh","text":"<pre><code>to_pyvista_mesh(\n    graph_data: Dict[str, Any],\n    point_size: float = 5.0,\n    color_by: str = \"mass\",\n    include_edges: bool = False,\n) -&gt; PolyData\n</code></pre> <p>Convert TNG50 graph to PyVista mesh for 3D visualization.</p> <p>Parameters:</p> Name Type Description Default <code>graph_data</code> <code>Dict[str, Any]</code> <p>Graph data from load_tng50_graph()</p> required <code>point_size</code> <code>float</code> <p>Point size for rendering</p> <code>5.0</code> <code>color_by</code> <code>str</code> <p>Feature to use for coloring</p> <code>'mass'</code> <code>include_edges</code> <code>bool</code> <p>Whether to include graph edges</p> <code>False</code> <p>Returns:</p> Type Description <code>PolyData</code> <p>PyVista PolyData mesh</p> Source code in <code>src\\astro_lab\\widgets\\tng50.py</code> <pre><code>def to_pyvista_mesh(\n    self,\n    graph_data: Dict[str, Any],\n    point_size: float = 5.0,\n    color_by: str = \"mass\",\n    include_edges: bool = False,\n) -&gt; \"pv.PolyData\":\n    \"\"\"\n    Convert TNG50 graph to PyVista mesh for 3D visualization.\n\n    Args:\n        graph_data: Graph data from load_tng50_graph()\n        point_size: Point size for rendering\n        color_by: Feature to use for coloring\n        include_edges: Whether to include graph edges\n\n    Returns:\n        PyVista PolyData mesh\n    \"\"\"\n    if pv is None:\n        raise ImportError(\"PyVista not available\")\n\n    positions = graph_data[\"positions\"]\n    features = graph_data[\"features\"]\n    feature_names = graph_data[\"feature_names\"]\n\n    logger.info(f\"\ud83d\udd27 Converting to PyVista mesh: {len(positions):,} points\")\n\n    # Create point cloud\n    mesh = pv.PolyData(positions)\n\n    # Add features as point data\n    for i, name in enumerate(feature_names):\n        mesh.point_data[name] = features[:, i]\n\n    # Add particle indices\n    mesh.point_data[\"particle_id\"] = np.arange(len(positions))\n\n    # Set default coloring\n    if color_by in feature_names:\n        mesh.set_active_scalars(color_by)\n        logger.info(f\"   Coloring by: {color_by}\")\n\n    # Add edges if requested\n    if include_edges:\n        edge_index = graph_data[\"edge_index\"]\n        edge_weights = graph_data[\"edge_weights\"]\n\n        # Create lines between connected particles\n        lines = []\n        for i in range(edge_index.shape[1]):\n            start_idx = edge_index[0, i]\n            end_idx = edge_index[1, i]\n            lines.extend([2, start_idx, end_idx])  # PyVista line format\n\n        mesh.lines = np.array(lines)\n        mesh.line_data[\"edge_weight\"] = edge_weights.flatten()\n\n        logger.info(f\"   Added {edge_index.shape[1]:,} edges\")\n\n    return mesh\n</code></pre>"},{"location":"api/astro_lab.widgets/#astro_lab.widgets.cluster_and_analyze","title":"cluster_and_analyze","text":"<pre><code>cluster_and_analyze(\n    coords: Tensor,\n    algorithm: str = \"dbscan\",\n    eps: float = 0.5,\n    min_samples: int = 5,\n    n_clusters: int = None,\n    use_gpu: bool = True,\n    **kwargs\n) -&gt; Dict[str, Any]\n</code></pre> <p>Perform clustering and return comprehensive analysis.</p> <p>Parameters:</p> Name Type Description Default <code>coords</code> <code>Tensor</code> <p>Coordinate tensor [N, D]</p> required <code>algorithm</code> <code>str</code> <p>Clustering algorithm</p> <code>'dbscan'</code> <code>eps</code> <code>float</code> <p>DBSCAN epsilon parameter</p> <code>0.5</code> <code>min_samples</code> <code>int</code> <p>DBSCAN min_samples parameter</p> <code>5</code> <code>n_clusters</code> <code>int</code> <p>Number of clusters for K-means/Agglomerative</p> <code>None</code> <code>use_gpu</code> <code>bool</code> <p>Whether to use GPU acceleration</p> <code>True</code> <code>**kwargs</code> <p>Additional clustering parameters</p> <code>{}</code> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dictionary with clustering results and analysis</p> Source code in <code>src\\astro_lab\\widgets\\graph.py</code> <pre><code>def cluster_and_analyze(\n    coords: torch.Tensor,\n    algorithm: str = \"dbscan\",\n    eps: float = 0.5,\n    min_samples: int = 5,\n    n_clusters: int = None,\n    use_gpu: bool = True,\n    **kwargs,\n) -&gt; Dict[str, Any]:\n    \"\"\"\n    Perform clustering and return comprehensive analysis.\n\n    Args:\n        coords: Coordinate tensor [N, D]\n        algorithm: Clustering algorithm\n        eps: DBSCAN epsilon parameter\n        min_samples: DBSCAN min_samples parameter\n        n_clusters: Number of clusters for K-means/Agglomerative\n        use_gpu: Whether to use GPU acceleration\n        **kwargs: Additional clustering parameters\n\n    Returns:\n        Dictionary with clustering results and analysis\n    \"\"\"\n    # Perform clustering\n    labels = cluster_graph_nodes(\n        coords,\n        torch.empty(\n            (2, 0), dtype=torch.long\n        ),  # Empty edge_index for coordinate-only clustering\n        algorithm=algorithm,\n        eps=eps,\n        min_samples=min_samples,\n        n_clusters=n_clusters,\n        **kwargs,\n    )\n\n    # Calculate cluster statistics\n    unique_labels = torch.unique(labels)\n    n_clusters_found = len(unique_labels)\n\n    cluster_sizes = []\n    cluster_centers = []\n\n    for label in unique_labels:\n        mask = labels == label\n        cluster_sizes.append(mask.sum().item())\n        cluster_centers.append(coords[mask].mean(dim=0))\n\n    cluster_centers = torch.stack(cluster_centers)\n\n    # Calculate silhouette score (simplified)\n    if n_clusters_found &gt; 1:\n        # Calculate average distance to cluster center\n        avg_intra_cluster = 0\n        for i, label in enumerate(unique_labels):\n            mask = labels == label\n            cluster_coords = coords[mask]\n            center = cluster_centers[i]\n            avg_intra_cluster += torch.norm(cluster_coords - center, dim=1).mean()\n        avg_intra_cluster /= n_clusters_found\n\n        # Calculate average distance to nearest cluster center\n        avg_inter_cluster = 0\n        for i, center in enumerate(cluster_centers):\n            other_centers = torch.cat([cluster_centers[:i], cluster_centers[i + 1 :]])\n            if len(other_centers) &gt; 0:\n                min_dist = torch.norm(other_centers - center, dim=1).min()\n                avg_inter_cluster += min_dist\n        avg_inter_cluster /= n_clusters_found\n\n        silhouette_score = (avg_inter_cluster - avg_intra_cluster) / max(\n            avg_inter_cluster, avg_intra_cluster\n        )\n    else:\n        silhouette_score = 0.0\n\n    return {\n        \"algorithm\": algorithm,\n        \"n_clusters\": n_clusters_found,\n        \"labels\": labels,\n        \"cluster_sizes\": cluster_sizes,\n        \"cluster_centers\": cluster_centers,\n        \"silhouette_score\": silhouette_score,\n        \"coordinate_range\": {\n            \"min\": coords.min().item(),\n            \"max\": coords.max().item(),\n        },\n    }\n</code></pre>"},{"location":"api/astro_lab.widgets/#astro_lab.widgets.create_plotly_visualization","title":"create_plotly_visualization","text":"<pre><code>create_plotly_visualization(survey_tensor: Any, **config) -&gt; Any\n</code></pre> <p>Create Plotly visualization for SurveyTensor.</p> <p>Parameters:</p> Name Type Description Default <code>survey_tensor</code> <code>Any</code> <p>SurveyTensor object</p> required <code>**config</code> <p>Configuration options</p> <code>{}</code> <p>Returns:</p> Type Description <code>Any</code> <p>Plotly figure object</p> Source code in <code>src\\astro_lab\\widgets\\plotly_bridge.py</code> <pre><code>def create_plotly_visualization(survey_tensor: Any, **config) -&gt; Any:\n    \"\"\"\n    Create Plotly visualization for SurveyTensor.\n\n    Args:\n        survey_tensor: SurveyTensor object\n        **config: Configuration options\n\n    Returns:\n        Plotly figure object\n    \"\"\"\n    # Extract spatial coordinates\n    if hasattr(survey_tensor, \"get_spatial_tensor\"):\n        spatial_tensor = survey_tensor.get_spatial_tensor()\n        if hasattr(spatial_tensor, \"cartesian\"):\n            coords = spatial_tensor.cartesian\n            logger.info(f\"\u2705 Extracted 3D coordinates for Plotly: {coords.shape}\")\n        else:\n            raise ValueError(\"SurveyTensor has no cartesian coordinates\")\n    else:\n        raise ValueError(\"SurveyTensor has no spatial tensor\")\n\n    # Limit points for web visualization\n    max_points = config.get(\"max_points\", 10000)\n    if coords.shape[0] &gt; max_points:\n        logger.info(f\"Sampling {max_points} points for Plotly visualization\")\n        indices = torch.randperm(coords.shape[0])[:max_points]\n        coords = coords[indices]\n\n    # Convert to numpy\n    coords_np = coords.cpu().numpy()\n\n    # Create 3D scatter plot\n    fig = go.Figure(\n        data=[\n            go.Scatter3d(\n                x=coords_np[:, 0],\n                y=coords_np[:, 1],\n                z=coords_np[:, 2],\n                mode=\"markers\",\n                marker=dict(\n                    size=config.get(\"point_size\", 2),\n                    opacity=config.get(\"opacity\", 0.8),\n                    color=coords_np[:, 2],  # Color by z-value\n                    colorscale=\"Viridis\",\n                    showscale=True,\n                ),\n                name=config.get(\"name\", \"AstroLab Data\"),\n            )\n        ]\n    )\n\n    # Update layout\n    fig.update_layout(\n        title=config.get(\"title\", \"AstroLab 3D Visualization\"),\n        scene=dict(\n            xaxis_title=\"X\", yaxis_title=\"Y\", zaxis_title=\"Z\", aspectmode=\"data\"\n        ),\n        width=config.get(\"width\", 800),\n        height=config.get(\"height\", 600),\n    )\n\n    # Show if requested\n    if config.get(\"show\", True):\n        fig.show()\n\n    return fig\n</code></pre>"}]}