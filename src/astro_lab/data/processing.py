"""
Advanced data processing using AstroLab tensor system.

This module provides enhanced data processing capabilities using the tensor system,
including feature engineering, clustering, and statistical analysis.
"""

from pathlib import Path
from typing import Any, Dict, List, Optional, Union

import numpy as np
import torch
from pydantic import BaseModel, Field, field_validator

from ..tensors import ClusteringTensor, FeatureTensor, StatisticsTensor, SurveyTensor

# Import SimulationTensor as well
try:
    from ..tensors import SimulationTensor

    SIMULATION_TENSOR_AVAILABLE = True
except ImportError:
    SIMULATION_TENSOR_AVAILABLE = False
    SimulationTensor = None


class SimpleProcessingConfig(BaseModel):
    """Simplified configuration for data processing."""

    # Basic options
    device: str = Field(default="cpu", description="Device to use for processing")
    batch_size: int = Field(default=1000, description="Batch size for processing")

    # Feature engineering
    enable_feature_engineering: bool = Field(
        default=True, description="Enable feature engineering"
    )

    # Clustering
    enable_clustering: bool = Field(default=True, description="Enable clustering")
    dbscan_eps: float = Field(default=0.1, description="DBSCAN epsilon")
    dbscan_min_samples: int = Field(default=5, description="DBSCAN min samples")

    # Statistics
    enable_statistics: bool = Field(default=True, description="Enable statistics")

    @field_validator("device")
    @classmethod
    def validate_device(cls, v: str) -> str:
        """Validate device specification."""
        if v == "auto":
            return "cuda" if torch.cuda.is_available() else "cpu"
        return v


class SimpleAstroProcessor:
    """Simplified astronomical data processor using tensor operations."""

    def __init__(self, config: Optional[SimpleProcessingConfig] = None):
        """Initialize the processor."""
        self.config = config or SimpleProcessingConfig()

    def process(
        self, survey_tensor: Union[SurveyTensor, "SimulationTensor"]
    ) -> Dict[str, Any]:
        """Process survey tensor or simulation tensor with simplified operations."""
        results = {"input_tensor": survey_tensor}

        # Handle different tensor types
        if hasattr(survey_tensor, "simulation_name"):
            # SimulationTensor
            tensor_name = survey_tensor.simulation_name
            tensor_type = "simulation"
        elif hasattr(survey_tensor, "survey_name"):
            # SurveyTensor
            tensor_name = survey_tensor.survey_name
            tensor_type = "survey"
        else:
            # Fallback
            tensor_name = "unknown"
            tensor_type = "unknown"

        print(f"ğŸ”¬ Processing {tensor_name} ({tensor_type})...")
        print(f"   ğŸ“Š Input shape: {survey_tensor._data.shape}")

        try:
            # Feature Engineering
            if self.config.enable_feature_engineering:
                print("  ğŸ”§ Feature engineering...")
                feature_results = self._apply_features(survey_tensor)
                results.update(feature_results)

            # Clustering
            if self.config.enable_clustering:
                print("  ğŸ¯ Clustering...")
                cluster_results = self._apply_clustering(survey_tensor)
                results.update(cluster_results)

            # Statistics
            if self.config.enable_statistics:
                print("  ğŸ“ˆ Statistics...")
                stats_results = self._apply_statistics(survey_tensor)
                results.update(stats_results)

        except Exception as e:
            print(f"  âš ï¸ Processing failed: {e}")
            results["error"] = str(e)

        print("  âœ… Processing complete!")
        return results

    def _apply_features(self, survey_tensor: SurveyTensor) -> Dict[str, Any]:
        """Apply feature engineering."""
        try:
            # Create FeatureTensor
            feature_tensor = FeatureTensor(
                survey_tensor._data,
                feature_names=[
                    f"feature_{i}" for i in range(survey_tensor._data.shape[1])
                ],
                feature_types="mixed",
            )

            # Apply basic scaling
            scaled_data = feature_tensor.scale_features(method="standard")

            return {
                "feature_tensor": feature_tensor,
                "scaled_data": scaled_data,
                "n_features": survey_tensor._data.shape[1],
            }

        except Exception as e:
            print(f"    âš ï¸ Feature engineering failed: {e}")
            return {"feature_error": str(e)}

    def _apply_clustering(self, survey_tensor: SurveyTensor) -> Dict[str, Any]:
        """Apply clustering analysis."""
        try:
            # Use first 3 columns for clustering (or all if less than 3)
            n_cols = min(3, survey_tensor._data.shape[1])
            cluster_data = survey_tensor._data[:, :n_cols]

            # Create ClusteringTensor
            cluster_tensor = ClusteringTensor(
                cluster_data,
                coordinate_system="cartesian",
                coordinate_columns=list(range(n_cols)),
            )

            # Apply DBSCAN
            cluster_labels = cluster_tensor.dbscan_clustering(
                eps=self.config.dbscan_eps, min_samples=self.config.dbscan_min_samples
            )

            n_clusters = len(set(cluster_labels)) - (1 if -1 in cluster_labels else 0)

            return {
                "cluster_tensor": cluster_tensor,
                "cluster_labels": cluster_labels,
                "n_clusters": n_clusters,
                "n_noise": sum(1 for label in cluster_labels if label == -1),
            }

        except Exception as e:
            print(f"    âš ï¸ Clustering failed: {e}")
            return {"clustering_error": str(e)}

    def _apply_statistics(self, survey_tensor: SurveyTensor) -> Dict[str, Any]:
        """Apply statistical analysis."""
        try:
            # Create StatisticsTensor
            stats_tensor = StatisticsTensor(
                survey_tensor._data,
                coordinate_columns=[0, 1] if survey_tensor._data.shape[1] >= 2 else [0],
            )

            # Compute basic luminosity function
            if survey_tensor._data.shape[1] > 2:
                bin_centers, phi = stats_tensor.luminosity_function(
                    magnitude_column=2,  # Use 3rd column as magnitude
                    bins=10,
                    function_name="basic_lf",
                )

                return {
                    "stats_tensor": stats_tensor,
                    "luminosity_function": {"bin_centers": bin_centers, "phi": phi},
                    "n_functions": len(stats_tensor.list_functions()),
                }
            else:
                return {
                    "stats_tensor": stats_tensor,
                    "message": "Not enough columns for luminosity function",
                }

        except Exception as e:
            print(f"    âš ï¸ Statistics failed: {e}")
            return {"statistics_error": str(e)}


# Keep the original class for backward compatibility but disable problematic features
class AdvancedAstroProcessor:
    """Advanced processor with experimental features (mostly disabled)."""

    def __init__(self, config: Optional[SimpleProcessingConfig] = None):
        """Initialize with simplified config."""
        self.config = config or SimpleProcessingConfig()
        self.simple_processor = SimpleAstroProcessor(config)

    def process(self, survey_tensor: SurveyTensor) -> Dict[str, Any]:
        """Process using simplified processor."""
        print("âš ï¸ Using simplified processing (advanced features disabled)")
        return self.simple_processor.process(survey_tensor)


# For backward compatibility
ProcessingConfig = SimpleProcessingConfig

if __name__ == "__main__":
    import argparse
    import sys
    from pathlib import Path

    import polars as pl
    import torch

    parser = argparse.ArgumentParser(description="AstroLab Data Processing CLI")
    parser.add_argument(
        "--survey",
        type=str,
        required=True,
        help="Survey name: gaia, sdss, tng50, tng50_temporal",
    )
    parser.add_argument(
        "--output", type=str, required=True, help="Output file path (.parquet or .pt)"
    )
    parser.add_argument(
        "--max-samples",
        type=int,
        default=None,
        help="Maximum number of samples/particles",
    )
    parser.add_argument(
        "--snapshot-id",
        type=int,
        default=None,
        help="Snapshot ID (for temporal datasets)",
    )
    args = parser.parse_args()

    print(f"ğŸš€ Processing survey: {args.survey}")
    print(f"   Output: {args.output}")
    print(f"   Max samples: {args.max_samples}")
    if args.snapshot_id is not None:
        print(f"   Snapshot ID: {args.snapshot_id}")

    # Loader mapping
    from .core import (
        load_gaia_data,
        load_sdss_data,
        load_tng50_data,
        load_tng50_temporal_data,
    )

    survey_loaders = {
        "gaia": load_gaia_data,
        "sdss": load_sdss_data,
        "tng50": load_tng50_data,
        "tng50_temporal": load_tng50_temporal_data,
    }
    if args.survey not in survey_loaders:
        print(f"âŒ Unknown survey: {args.survey}")
        sys.exit(1)

    # Load data
    loader = survey_loaders[args.survey]
    loader_kwargs = {"max_samples": args.max_samples}
    if args.survey == "tng50_temporal" and args.snapshot_id is not None:
        loader_kwargs["snapshot_id"] = args.snapshot_id
    dataset = loader(**loader_kwargs)

    # Save output
    output_path = Path(args.output)
    if output_path.suffix == ".parquet":
        print(f"ğŸ’¾ Saving as Parquet: {output_path}")
        dataset.data.write_parquet(str(output_path))
        print(f"âœ… Done: {output_path}")
    elif output_path.suffix == ".pt":
        print(f"ğŸ’¾ Saving as PyTorch tensor: {output_path}")
        torch.save(dataset.data.to_pandas(), str(output_path))
        print(f"âœ… Done: {output_path}")
    else:
        print(f"âŒ Unknown output file type: {output_path.suffix}")
        sys.exit(2)
