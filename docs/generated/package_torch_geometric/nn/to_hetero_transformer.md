# to_hetero_transformer

Part of `torch_geometric.nn`
Module: `torch_geometric.nn.to_hetero_transformer`

## Functions (7)

### `check_add_self_loops(module: torch.nn.modules.module.Module, edge_types: List[Tuple[str, str, str]]) -> None`

### `get_dict(mapping: Optional[Dict[str, Any]]) -> Dict[str, Any]`

### `get_submodule(module: torch.nn.modules.module.Module, target: str) -> torch.nn.modules.module.Module`

### `get_unused_node_types(node_types: List[str], edge_types: List[Tuple[str, str, str]]) -> Set[str]`

### `is_uninitialized_parameter(x: Any) -> bool`

### `key2str(key: Union[str, Tuple[str, str, str]]) -> str`

### `to_hetero(module: torch.nn.modules.module.Module, metadata: Tuple[List[str], List[Tuple[str, str, str]]], aggr: str = 'sum', input_map: Optional[Dict[str, str]] = None, debug: bool = False) -> torch.fx.graph_module.GraphModule`

Converts a homogeneous GNN model into its heterogeneous equivalent in
which node representations are learned for each node type in
:obj:`metadata[0]`, and messages are exchanged between each edge type in
:obj:`metadata[1]`, as denoted in the `"Modeling Relational Data with Graph
Convolutional Networks" <https://arxiv.org/abs/1703.06103>`_ paper.

.. code-block:: python

    import torch
    from torch_geometric.nn import SAGEConv, to_hetero

    class GNN(torch.nn.Module):
        def __init__(self):
            super().__init__()
            self.conv1 = SAGEConv((-1, -1), 32)
            self.conv2 = SAGEConv((32, 32), 32)

        def forward(self, x, edge_index):
            x = self.conv1(x, edge_index).relu()
            x = self.conv2(x, edge_index).relu()
            return x

    model = GNN()

    node_types = ['paper', 'author']
    edge_types = [
        ('paper', 'cites', 'paper'),
        ('paper', 'written_by', 'author'),
        ('author', 'writes', 'paper'),
    ]
    metadata = (node_types, edge_types)

    model = to_hetero(model, metadata)
    model(x_dict, edge_index_dict)

where :obj:`x_dict` and :obj:`edge_index_dict` denote dictionaries that
hold node features and edge connectivity information for each node type and
edge type, respectively.

The below illustration shows the original computation graph of the
homogeneous model on the left, and the newly obtained computation graph of
the heterogeneous model on the right:

.. figure:: ../_figures/to_hetero.svg
  :align: center
  :width: 90%

  Transforming a model via :func:`to_hetero`.

Here, each :class:`~torch_geometric.nn.conv.MessagePassing` instance
:math:`f_{\theta}^{(\ell)}` is duplicated and stored in a set
:math:`\{ f_{\theta}^{(\ell, r)} : r \in \mathcal{R} \}` (one instance for
each relation in :math:`\mathcal{R}`), and message passing in layer
:math:`\ell` is performed via

.. math::

    \mathbf{h}^{(\ell)}_v = \bigoplus_{r \in \mathcal{R}}
    f_{\theta}^{(\ell, r)} ( \mathbf{h}^{(\ell - 1)}_v, \{
    \mathbf{h}^{(\ell - 1)}_w : w \in \mathcal{N}^{(r)}(v) \}),

where :math:`\mathcal{N}^{(r)}(v)` denotes the neighborhood of :math:`v \in
\mathcal{V}` under relation :math:`r \in \mathcal{R}`, and
:math:`\bigoplus` denotes the aggregation scheme :attr:`aggr` to use for
grouping node embeddings generated by different relations
(:obj:`"sum"`, :obj:`"mean"`, :obj:`"min"`, :obj:`"max"` or :obj:`"mul"`).

Args:
    module (torch.nn.Module): The homogeneous model to transform.
    metadata (Tuple[List[str], List[Tuple[str, str, str]]]): The metadata
        of the heterogeneous graph, *i.e.* its node and edge types given
        by a list of strings and a list of string triplets, respectively.
        See :meth:`torch_geometric.data.HeteroData.metadata` for more
        information.
    aggr (str, optional): The aggregation scheme to use for grouping node
        embeddings generated by different relations
        (:obj:`"sum"`, :obj:`"mean"`, :obj:`"min"`, :obj:`"max"`,
        :obj:`"mul"`). (default: :obj:`"sum"`)
    input_map (Dict[str, str], optional): A dictionary holding information
        about the type of input arguments of :obj:`module.forward`.
        For example, in case :obj:`arg` is a node-level argument, then
        :obj:`input_map['arg'] = 'node'`, and
        :obj:`input_map['arg'] = 'edge'` otherwise.
        In case :obj:`input_map` is not further specified, will try to
        automatically determine the correct type of input arguments.
        (default: :obj:`None`)
    debug (bool, optional): If set to :obj:`True`, will perform
        transformation in debug mode. (default: :obj:`False`)

## Classes (10)

### `Any`

Special type indicating an unconstrained type.

- Any is compatible with every type.
- Any assumed to have all methods.
- All values assumed to be instances of Any.

Note that all the above statements are true from the point of view of
static type checkers. At runtime, Any should not be used with instance
checks.

### `Graph`

``Graph`` is the main data structure used in the FX Intermediate Representation.
It consists of a series of ``Node`` s, each representing callsites (or other
syntactic constructs). The list of ``Node`` s, taken together, constitute a
valid Python function.

For example, the following code

.. code-block:: python

    import torch
    import torch.fx


    class MyModule(torch.nn.Module):
        def __init__(self):
            super().__init__()
            self.param = torch.nn.Parameter(torch.rand(3, 4))
            self.linear = torch.nn.Linear(4, 5)

        def forward(self, x):
            return torch.topk(
                torch.sum(self.linear(x + self.linear.weight).relu(), dim=-1), 3
            )


    m = MyModule()
    gm = torch.fx.symbolic_trace(m)

Will produce the following Graph::

    print(gm.graph)

.. code-block:: text

    graph(x):
        %linear_weight : [num_users=1] = self.linear.weight
        %add_1 : [num_users=1] = call_function[target=operator.add](args = (%x, %linear_weight), kwargs = {})
        %linear_1 : [num_users=1] = call_module[target=linear](args = (%add_1,), kwargs = {})
        %relu_1 : [num_users=1] = call_method[target=relu](args = (%linear_1,), kwargs = {})
        %sum_1 : [num_users=1] = call_function[target=torch.sum](args = (%relu_1,), kwargs = {dim: -1})
        %topk_1 : [num_users=1] = call_function[target=torch.topk](args = (%sum_1, 3), kwargs = {})
        return topk_1

For the semantics of operations represented in the ``Graph``, please see :class:`Node`.

.. note::
    Backwards-compatibility for this API is guaranteed.

#### Methods

- **`output_node(self) -> torch.fx.node.Node`**
  .. warning::

- **`find_nodes(self, *, op: str, target: Optional[ForwardRef('Target')] = None, sort: bool = True)`**
  Allows for fast query of nodes

- **`graph_copy(self, g: 'Graph', val_map: dict[torch.fx.node.Node, torch.fx.node.Node], return_output_node=False) -> 'Optional[Argument]'`**
  Copy all nodes from a given graph into ``self``.

### `GraphModule`

GraphModule is an nn.Module generated from an fx.Graph. Graphmodule has a
``graph`` attribute, as well as ``code`` and ``forward`` attributes generated
from that ``graph``.

.. warning::

    When ``graph`` is reassigned, ``code`` and ``forward`` will be automatically
    regenerated. However, if you edit the contents of the ``graph`` without reassigning
    the ``graph`` attribute itself, you must call ``recompile()`` to update the generated
    code.

.. note::
    Backwards-compatibility for this API is guaranteed.

#### Methods

- **`to_folder(self, folder: Union[str, os.PathLike], module_name: str = 'FxModule')`**
  Dumps out module to ``folder`` with ``module_name`` so that it can be

- **`add_submodule(self, target: str, m: torch.nn.modules.module.Module) -> bool`**
  Adds the given submodule to ``self``.

- **`delete_submodule(self, target: str) -> bool`**
  Deletes the given submodule from ``self``.

### `Module`

Base class for all neural network modules.

Your models should also subclass this class.

Modules can also contain other Modules, allowing them to be nested in
a tree structure. You can assign the submodules as regular attributes::

    import torch.nn as nn
    import torch.nn.functional as F

    class Model(nn.Module):
        def __init__(self) -> None:
            super().__init__()
            self.conv1 = nn.Conv2d(1, 20, 5)
            self.conv2 = nn.Conv2d(20, 20, 5)

        def forward(self, x):
            x = F.relu(self.conv1(x))
            return F.relu(self.conv2(x))

Submodules assigned in this way will be registered, and will also have their
parameters converted when you call :meth:`to`, etc.

.. note::
    As per the example above, an ``__init__()`` call to the parent class
    must be made before assignment on the child.

:ivar training: Boolean represents whether this module is in training or
                evaluation mode.
:vartype training: bool

#### Methods

- **`forward(self, *input: Any) -> None`**
  Define the computation performed at every call.

- **`register_buffer(self, name: str, tensor: Optional[torch.Tensor], persistent: bool = True) -> None`**
  Add a buffer to the module.

- **`register_parameter(self, name: str, param: Optional[torch.nn.parameter.Parameter]) -> None`**
  Add a parameter to the module.

### `Node`

``Node`` is the data structure that represents individual operations within
a ``Graph``. For the most part, Nodes represent callsites to various entities,
such as operators, methods, and Modules (some exceptions include nodes that
specify function inputs and outputs). Each ``Node`` has a function specified
by its ``op`` property. The ``Node`` semantics for each value of ``op`` are as follows:

- ``placeholder`` represents a function input. The ``name`` attribute specifies the name this value will take on.
  ``target`` is similarly the name of the argument. ``args`` holds either: 1) nothing, or 2) a single argument
  denoting the default parameter of the function input. ``kwargs`` is don't-care. Placeholders correspond to
  the function parameters (e.g. ``x``) in the graph printout.
- ``get_attr`` retrieves a parameter from the module hierarchy. ``name`` is similarly the name the result of the
  fetch is assigned to. ``target`` is the fully-qualified name of the parameter's position in the module hierarchy.
  ``args`` and ``kwargs`` are don't-care
- ``call_function`` applies a free function to some values. ``name`` is similarly the name of the value to assign
  to. ``target`` is the function to be applied. ``args`` and ``kwargs`` represent the arguments to the function,
  following the Python calling convention
- ``call_module`` applies a module in the module hierarchy's ``forward()`` method to given arguments. ``name`` is
  as previous. ``target`` is the fully-qualified name of the module in the module hierarchy to call.
  ``args`` and ``kwargs`` represent the arguments to invoke the module on, *excluding the self argument*.
- ``call_method`` calls a method on a value. ``name`` is as similar. ``target`` is the string name of the method
  to apply to the ``self`` argument. ``args`` and ``kwargs`` represent the arguments to invoke the module on,
  *including the self argument*
- ``output`` contains the output of the traced function in its ``args[0]`` attribute. This corresponds to the "return" statement
  in the Graph printout.

.. note::
    Backwards-compatibility for this API is guaranteed.

#### Methods

- **`prepend(self, x: 'Node') -> None`**
  Insert x before this node in the list of nodes in the graph. Example::

- **`append(self, x: 'Node') -> None`**
  Insert ``x`` after this node in the list of nodes in the graph.

- **`update_arg(self, idx: int, arg: Union[tuple['Argument', ...], collections.abc.Sequence['Argument'], collections.abc.Mapping[str, 'Argument'], slice, range, ForwardRef('Node'), str, int, float, bool, complex, torch.dtype, torch.Tensor, torch.device, torch.memory_format, torch.layout, torch._ops.OpOverload, torch.SymInt, torch.SymBool, torch.SymFloat, NoneType]) -> None`**
  Update an existing positional argument to contain the new value

### `NodeType`

str(object='') -> str
str(bytes_or_buffer[, encoding[, errors]]) -> str

Create a new string object from the given object. If encoding or
errors is specified, then the object must expose a data buffer
that will be decoded using the given encoding and error handler.
Otherwise, returns the result of object.__str__() (if defined)
or repr(object).
encoding defaults to sys.getdefaultencoding().
errors defaults to 'strict'.

#### Methods

- **`encode(self, /, encoding='utf-8', errors='strict')`**
  Encode the string using the codec registered for encoding.

- **`replace(self, old, new, count=-1, /)`**
  Return a copy with all occurrences of substring old replaced by new.

- **`split(self, /, sep=None, maxsplit=-1)`**
  Return a list of the substrings in the string, using sep as the separator string.

### `ToHeteroTransformer`

A :class:`Transformer` executes an FX graph node-by-node, applies
transformations to each node, and produces a new :class:`torch.nn.Module`.
It exposes a :func:`transform` method that returns the transformed
:class:`~torch.nn.Module`.
:class:`Transformer` works entirely symbolically.

Methods in the :class:`Transformer` class can be overridden to customize
the behavior of transformation.

.. code-block:: none

    transform()
        +-- Iterate over each node in the graph
            +-- placeholder()
            +-- get_attr()
            +-- call_function()
            +-- call_method()
            +-- call_module()
            +-- call_message_passing_module()
            +-- call_global_pooling_module()
            +-- output()
        +-- Erase unused nodes in the graph
        +-- Iterate over each children module
            +-- init_submodule()

In contrast to the :class:`torch.fx.Transformer` class, the
:class:`Transformer` exposes additional functionality:

#. It subdivides :func:`call_module` into nodes that call a regular
   :class:`torch.nn.Module` (:func:`call_module`), a
   :class:`MessagePassing` module (:func:`call_message_passing_module`),
   or a :class:`GlobalPooling` module (:func:`call_global_pooling_module`).

#. It allows to customize or initialize new children modules via
   :func:`init_submodule`

#. It allows to infer whether a node returns node-level or edge-level
   information via :meth:`is_edge_level`.

Args:
    module (torch.nn.Module): The module to be transformed.
    input_map (Dict[str, str], optional): A dictionary holding information
        about the type of input arguments of :obj:`module.forward`.
        For example, in case :obj:`arg` is a node-level argument, then
        :obj:`input_map['arg'] = 'node'`, and
        :obj:`input_map['arg'] = 'edge'` otherwise.
        In case :obj:`input_map` is not further specified, will try to
        automatically determine the correct type of input arguments.
        (default: :obj:`None`)
    debug (bool, optional): If set to :obj:`True`, will perform
        transformation in debug mode. (default: :obj:`False`)

#### Methods

- **`validate(self)`**

- **`placeholder(self, node: torch.fx.node.Node, target: Any, name: str)`**

- **`get_attr(self, node: torch.fx.node.Node, target: Any, name: str)`**

### `Transformer`

A :class:`Transformer` executes an FX graph node-by-node, applies
transformations to each node, and produces a new :class:`torch.nn.Module`.
It exposes a :func:`transform` method that returns the transformed
:class:`~torch.nn.Module`.
:class:`Transformer` works entirely symbolically.

Methods in the :class:`Transformer` class can be overridden to customize
the behavior of transformation.

.. code-block:: none

    transform()
        +-- Iterate over each node in the graph
            +-- placeholder()
            +-- get_attr()
            +-- call_function()
            +-- call_method()
            +-- call_module()
            +-- call_message_passing_module()
            +-- call_global_pooling_module()
            +-- output()
        +-- Erase unused nodes in the graph
        +-- Iterate over each children module
            +-- init_submodule()

In contrast to the :class:`torch.fx.Transformer` class, the
:class:`Transformer` exposes additional functionality:

#. It subdivides :func:`call_module` into nodes that call a regular
   :class:`torch.nn.Module` (:func:`call_module`), a
   :class:`MessagePassing` module (:func:`call_message_passing_module`),
   or a :class:`GlobalPooling` module (:func:`call_global_pooling_module`).

#. It allows to customize or initialize new children modules via
   :func:`init_submodule`

#. It allows to infer whether a node returns node-level or edge-level
   information via :meth:`is_edge_level`.

Args:
    module (torch.nn.Module): The module to be transformed.
    input_map (Dict[str, str], optional): A dictionary holding information
        about the type of input arguments of :obj:`module.forward`.
        For example, in case :obj:`arg` is a node-level argument, then
        :obj:`input_map['arg'] = 'node'`, and
        :obj:`input_map['arg'] = 'edge'` otherwise.
        In case :obj:`input_map` is not further specified, will try to
        automatically determine the correct type of input arguments.
        (default: :obj:`None`)
    debug (bool, optional): If set to :obj:`True`, will perform
        transformation in debug mode. (default: :obj:`False`)

#### Methods

- **`placeholder(self, node: torch.fx.node.Node, target: Any, name: str)`**

- **`get_attr(self, node: torch.fx.node.Node, target: Any, name: str)`**

- **`call_message_passing_module(self, node: torch.fx.node.Node, target: Any, name: str)`**

### `defaultdict`

defaultdict(default_factory=None, /, [...]) --> dict with default factory

The default factory is called without arguments to produce
a new value when a key is not present, in __getitem__ only.
A defaultdict compares equal to a dict with the same items.
All remaining arguments are treated the same as if they were
passed to the dict constructor, including keyword arguments.

#### Methods

- **`copy(...)`**
  D.copy() -> a shallow copy of D.

### `deque`

deque([iterable[, maxlen]]) --> deque object

A list-like sequence optimized for data accesses near its endpoints.

#### Methods

- **`append(...)`**
  Add an element to the right side of the deque.

- **`appendleft(...)`**
  Add an element to the left side of the deque.

- **`clear(...)`**
  Remove all elements from the deque.
