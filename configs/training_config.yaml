# AstroLab Training Configuration
# Updated for 2025 best practices
#
# Parameter Mapping:
# Config YAML              -> CLI train.py           -> LightningModule
# training.learning_rate   -> learning_rate          -> learning_rate
# training.weight_decay    -> weight_decay           -> weight_decay
# training.gradient_accumulation_steps -> gradient_accumulation_steps -> gradient_accumulation_steps
# training.gradient_clip_val -> gradient_clip_val    -> gradient_clip_val
# training.gradient_clip_algorithm -> gradient_clip_algorithm -> gradient_clip_algorithm
# training.scheduler_type  -> scheduler_type         -> scheduler_type
# training.warmup_steps    -> warmup_steps           -> warmup_steps
# training.use_compile     -> use_compile            -> use_compile
# training.use_ema         -> use_ema                -> use_ema
# training.ema_decay       -> ema_decay              -> ema_decay
# training.label_smoothing -> label_smoothing        -> label_smoothing

# Data Configuration
data:
  dataset: gaia
  batch_size: 32
  max_samples: 1000
  k_neighbors: 8
  # Data loading optimizations
  num_workers: null # Auto-detect optimal workers
  pin_memory: true
  persistent_workers: true
  prefetch_factor: 2
  return_tensor: true

# Model Configuration
model:
  name: gaia_classifier
  type: gaia_classifier # Will use ModelFactory
  params:
    hidden_dim: 128
    num_layers: 3
    dropout: 0.1
    conv_type: gcn
    # Auto-detect output_dim from data

# Training Configuration
training:
  # Basic settings
  max_epochs: 20
  learning_rate: 0.001
  weight_decay: 0.0001

  # Advanced optimization (2025)
  gradient_accumulation_steps: 1
  gradient_clip_val: 1.0
  gradient_clip_algorithm: norm

  # Scheduler configuration
  scheduler_type: cosine # Options: cosine, onecycle, plateau, cosine_warm_restarts
  warmup_steps: 0

  # Precision and compilation
  precision: 16-mixed # Options: 32, 16-mixed, bf16-mixed
  use_compile: false # PyTorch 2.0 compile

  # Model EMA (Exponential Moving Average)
  use_ema: false
  ema_decay: 0.999

  # Label smoothing for better generalization
  label_smoothing: 0.0

  # Callbacks
  patience: 10
  use_swa: false # Stochastic Weight Averaging
  swa_lr: 0.001
  swa_epoch_start: 0.8

  # Hardware settings
  devices: auto # Number of GPUs or 'auto'
  strategy: auto # Options: auto, ddp, fsdp

  # FSDP settings (for large models)
  fsdp_cpu_offload: false
  mixed_precision: bf16 # For FSDP

  # Performance settings
  deterministic: true
  benchmark: false
  profiler: null # Options: simple, advanced, pytorch

  # Logging settings
  log_every_n_steps: 50
  val_check_interval: 1.0

# MLflow Configuration
mlflow:
  experiment_name: astro_training_2025
  tracking_uri: file:./data/experiments
# Gradient accumulation schedule (optional)
# gradient_accumulation_schedule:
#   0: 1   # epoch 0: accumulate 1 batch
#   5: 2   # epoch 5: accumulate 2 batches
#   10: 4  # epoch 10: accumulate 4 batches
