training:
  max_epochs: 100
  learning_rate: 0.001
  weight_decay: 0.0001
  optimizer: adamw
  scheduler: cosine
  warmup_epochs: 5
  gradient_clip_val: 1.0
  accumulate_grad_batches: 1
  early_stopping: true
  early_stopping_patience: 10
  early_stopping_monitor: val_loss
  early_stopping_mode: min
  checkpoint_monitor: val_loss
  checkpoint_mode: min
  checkpoint_save_top_k: 3
  precision: 16-mixed
  devices: auto
  accelerator: auto
  log_every_n_steps: 10
  val_check_interval: 1.0
  limit_train_batches: 1.0
  limit_val_batches: 1.0
  enable_progress_bar: true
  enable_model_summary: true
